<?xml version="1.0" encoding="UTF-8" ?>
<!-- This is a WordPress eXtended RSS file generated by WordPress as an export of your site. -->
<!-- It contains information about your site's posts, pages, comments, categories, and other content. -->
<!-- You may use this file to transfer that content from one site to another. -->
<!-- This file is not intended to serve as a complete backup of your site. -->

<!-- To import this information into a WordPress site follow these steps: -->
<!-- 1. Log in to that site as an administrator. -->
<!-- 2. Go to Tools: Import in the WordPress admin panel. -->
<!-- 3. Install the "WordPress" importer from the list. -->
<!-- 4. Activate & Run Importer. -->
<!-- 5. Upload this file using the form provided on that page. -->
<!-- 6. You will first be asked to map the authors in this export file to users -->
<!--    on the site. For each author, you may choose to map to an -->
<!--    existing user on the site or to create a new user. -->
<!-- 7. WordPress will then import each of the posts, pages, comments, categories, etc. -->
<!--    contained in this file into your site. -->

	<!-- generator="WordPress/5.2.5" created="2020-04-03 21:43" -->
<rss version="2.0"
	xmlns:excerpt="http://wordpress.org/export/1.2/excerpt/"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:wp="http://wordpress.org/export/1.2/"
>

<channel>
	<title>Onoffswitch.net</title>
	<link>https://onoffswitch.net</link>
	<description>make stuff break stuff, an engineering blog</description>
	<pubDate>Fri, 03 Apr 2020 21:43:07 +0000</pubDate>
	<language>en-US</language>
	<wp:wxr_version>1.2</wp:wxr_version>
	<wp:base_site_url>https://onoffswitch.net</wp:base_site_url>
	<wp:base_blog_url>https://onoffswitch.net</wp:base_blog_url>

		<wp:author><wp:author_id>1</wp:author_id><wp:author_login><![CDATA[akropp]]></wp:author_login><wp:author_email><![CDATA[akropp@gmail.com]]></wp:author_email><wp:author_display_name><![CDATA[akropp]]></wp:author_display_name><wp:author_first_name><![CDATA[]]></wp:author_first_name><wp:author_last_name><![CDATA[]]></wp:author_last_name></wp:author>

		<wp:category>
		<wp:term_id>2</wp:term_id>
		<wp:category_nicename><![CDATA[code]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Code]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>3</wp:term_id>
		<wp:category_nicename><![CDATA[cross-post]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Cross Post]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>4</wp:term_id>
		<wp:category_nicename><![CDATA[discussion]]></wp:category_nicename>
		<wp:category_parent><![CDATA[code]]></wp:category_parent>
		<wp:cat_name><![CDATA[Discussion]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>5</wp:term_id>
		<wp:category_nicename><![CDATA[imported]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Imported]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>6</wp:term_id>
		<wp:category_nicename><![CDATA[rants]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Rants]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>7</wp:term_id>
		<wp:category_nicename><![CDATA[snippets]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Snippets]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>8</wp:term_id>
		<wp:category_nicename><![CDATA[tech-talks]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Tech talks]]></wp:cat_name>
	</wp:category>
		<wp:category>
		<wp:term_id>1</wp:term_id>
		<wp:category_nicename><![CDATA[uncategorized]]></wp:category_nicename>
		<wp:category_parent><![CDATA[]]></wp:category_parent>
		<wp:cat_name><![CDATA[Uncategorized]]></wp:cat_name>
	</wp:category>
			<wp:tag>
		<wp:term_id>9</wp:term_id>
		<wp:tag_slug><![CDATA[dotnet]]></wp:tag_slug>
		<wp:tag_name><![CDATA[.NET]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>10</wp:term_id>
		<wp:tag_slug><![CDATA[64-bit]]></wp:tag_slug>
		<wp:tag_name><![CDATA[64 bit]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>11</wp:term_id>
		<wp:tag_slug><![CDATA[64bit]]></wp:tag_slug>
		<wp:tag_name><![CDATA[64bit]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>12</wp:term_id>
		<wp:tag_slug><![CDATA[abstract-syntax-trees]]></wp:tag_slug>
		<wp:tag_name><![CDATA[abstract syntax trees]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>13</wp:term_id>
		<wp:tag_slug><![CDATA[active-patterns]]></wp:tag_slug>
		<wp:tag_name><![CDATA[active patterns]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>14</wp:term_id>
		<wp:tag_slug><![CDATA[actors]]></wp:tag_slug>
		<wp:tag_name><![CDATA[actors]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>15</wp:term_id>
		<wp:tag_slug><![CDATA[aeson]]></wp:tag_slug>
		<wp:tag_name><![CDATA[aeson]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>16</wp:term_id>
		<wp:tag_slug><![CDATA[aetr]]></wp:tag_slug>
		<wp:tag_name><![CDATA[aetr]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>17</wp:term_id>
		<wp:tag_slug><![CDATA[akka]]></wp:tag_slug>
		<wp:tag_name><![CDATA[akka]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>18</wp:term_id>
		<wp:tag_slug><![CDATA[algorithms]]></wp:tag_slug>
		<wp:tag_name><![CDATA[algorithms]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>19</wp:term_id>
		<wp:tag_slug><![CDATA[angular]]></wp:tag_slug>
		<wp:tag_name><![CDATA[angular]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>20</wp:term_id>
		<wp:tag_slug><![CDATA[angularjs]]></wp:tag_slug>
		<wp:tag_name><![CDATA[angularjs]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>21</wp:term_id>
		<wp:tag_slug><![CDATA[architecture]]></wp:tag_slug>
		<wp:tag_name><![CDATA[architecture]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>22</wp:term_id>
		<wp:tag_slug><![CDATA[arithmetic]]></wp:tag_slug>
		<wp:tag_name><![CDATA[arithmetic]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>23</wp:term_id>
		<wp:tag_slug><![CDATA[arrows]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Arrows]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>24</wp:term_id>
		<wp:tag_slug><![CDATA[as3]]></wp:tag_slug>
		<wp:tag_name><![CDATA[AS3]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>25</wp:term_id>
		<wp:tag_slug><![CDATA[asgard]]></wp:tag_slug>
		<wp:tag_name><![CDATA[asgard]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>26</wp:term_id>
		<wp:tag_slug><![CDATA[asm]]></wp:tag_slug>
		<wp:tag_name><![CDATA[asm]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>27</wp:term_id>
		<wp:tag_slug><![CDATA[asp-net]]></wp:tag_slug>
		<wp:tag_name><![CDATA[asp.net]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>28</wp:term_id>
		<wp:tag_slug><![CDATA[aspects]]></wp:tag_slug>
		<wp:tag_name><![CDATA[aspects]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>29</wp:term_id>
		<wp:tag_slug><![CDATA[ast]]></wp:tag_slug>
		<wp:tag_name><![CDATA[ast]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>30</wp:term_id>
		<wp:tag_slug><![CDATA[async]]></wp:tag_slug>
		<wp:tag_name><![CDATA[async]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>31</wp:term_id>
		<wp:tag_slug><![CDATA[athena]]></wp:tag_slug>
		<wp:tag_name><![CDATA[athena]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>32</wp:term_id>
		<wp:tag_slug><![CDATA[autocomplete]]></wp:tag_slug>
		<wp:tag_name><![CDATA[autocomplete]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>33</wp:term_id>
		<wp:tag_slug><![CDATA[automation]]></wp:tag_slug>
		<wp:tag_name><![CDATA[automation]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>34</wp:term_id>
		<wp:tag_slug><![CDATA[aws]]></wp:tag_slug>
		<wp:tag_name><![CDATA[aws]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>35</wp:term_id>
		<wp:tag_slug><![CDATA[b-tree]]></wp:tag_slug>
		<wp:tag_name><![CDATA[b-tree]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>36</wp:term_id>
		<wp:tag_slug><![CDATA[best-practices]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Best Practices]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>37</wp:term_id>
		<wp:tag_slug><![CDATA[bfs]]></wp:tag_slug>
		<wp:tag_name><![CDATA[bfs]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>38</wp:term_id>
		<wp:tag_slug><![CDATA[bits]]></wp:tag_slug>
		<wp:tag_name><![CDATA[bits]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>39</wp:term_id>
		<wp:tag_slug><![CDATA[bloom-filter]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Bloom filter]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>40</wp:term_id>
		<wp:tag_slug><![CDATA[brainfuck]]></wp:tag_slug>
		<wp:tag_name><![CDATA[brainfuck]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>41</wp:term_id>
		<wp:tag_slug><![CDATA[branches]]></wp:tag_slug>
		<wp:tag_name><![CDATA[branches]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>42</wp:term_id>
		<wp:tag_slug><![CDATA[bug]]></wp:tag_slug>
		<wp:tag_name><![CDATA[bug]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>43</wp:term_id>
		<wp:tag_slug><![CDATA[byte]]></wp:tag_slug>
		<wp:tag_name><![CDATA[byte]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>44</wp:term_id>
		<wp:tag_slug><![CDATA[bytecode]]></wp:tag_slug>
		<wp:tag_name><![CDATA[bytecode]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>45</wp:term_id>
		<wp:tag_slug><![CDATA[c]]></wp:tag_slug>
		<wp:tag_name><![CDATA[c#]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>46</wp:term_id>
		<wp:tag_slug><![CDATA[csharp]]></wp:tag_slug>
		<wp:tag_name><![CDATA[C#]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>47</wp:term_id>
		<wp:tag_slug><![CDATA[cpp]]></wp:tag_slug>
		<wp:tag_name><![CDATA[C++]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>48</wp:term_id>
		<wp:tag_slug><![CDATA[careers]]></wp:tag_slug>
		<wp:tag_name><![CDATA[careers]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>49</wp:term_id>
		<wp:tag_slug><![CDATA[cassandra]]></wp:tag_slug>
		<wp:tag_name><![CDATA[cassandra]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>50</wp:term_id>
		<wp:tag_slug><![CDATA[cassieq]]></wp:tag_slug>
		<wp:tag_name><![CDATA[cassieq]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>51</wp:term_id>
		<wp:tag_slug><![CDATA[cicd]]></wp:tag_slug>
		<wp:tag_name><![CDATA[cicd]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>52</wp:term_id>
		<wp:tag_slug><![CDATA[classes]]></wp:tag_slug>
		<wp:tag_name><![CDATA[classes]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>53</wp:term_id>
		<wp:tag_slug><![CDATA[classification]]></wp:tag_slug>
		<wp:tag_name><![CDATA[classification]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>54</wp:term_id>
		<wp:tag_slug><![CDATA[classloader]]></wp:tag_slug>
		<wp:tag_name><![CDATA[classloader]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>55</wp:term_id>
		<wp:tag_slug><![CDATA[clojure]]></wp:tag_slug>
		<wp:tag_name><![CDATA[clojure]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>56</wp:term_id>
		<wp:tag_slug><![CDATA[closures]]></wp:tag_slug>
		<wp:tag_name><![CDATA[closures]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>57</wp:term_id>
		<wp:tag_slug><![CDATA[clr]]></wp:tag_slug>
		<wp:tag_name><![CDATA[clr]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>58</wp:term_id>
		<wp:tag_slug><![CDATA[combinator]]></wp:tag_slug>
		<wp:tag_name><![CDATA[combinator]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>59</wp:term_id>
		<wp:tag_slug><![CDATA[combinators]]></wp:tag_slug>
		<wp:tag_name><![CDATA[combinators]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>60</wp:term_id>
		<wp:tag_slug><![CDATA[computation-expression]]></wp:tag_slug>
		<wp:tag_name><![CDATA[computation expression]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>61</wp:term_id>
		<wp:tag_slug><![CDATA[conference]]></wp:tag_slug>
		<wp:tag_name><![CDATA[conference]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>62</wp:term_id>
		<wp:tag_slug><![CDATA[configuration]]></wp:tag_slug>
		<wp:tag_name><![CDATA[configuration]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>63</wp:term_id>
		<wp:tag_slug><![CDATA[containers]]></wp:tag_slug>
		<wp:tag_name><![CDATA[containers]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>64</wp:term_id>
		<wp:tag_slug><![CDATA[continuation-passing]]></wp:tag_slug>
		<wp:tag_name><![CDATA[continuation passing]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>65</wp:term_id>
		<wp:tag_slug><![CDATA[coproduct]]></wp:tag_slug>
		<wp:tag_name><![CDATA[coproduct]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>66</wp:term_id>
		<wp:tag_slug><![CDATA[css]]></wp:tag_slug>
		<wp:tag_name><![CDATA[css]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>67</wp:term_id>
		<wp:tag_slug><![CDATA[csv]]></wp:tag_slug>
		<wp:tag_name><![CDATA[csv]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>68</wp:term_id>
		<wp:tag_slug><![CDATA[dailyprogrammer]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dailyprogrammer]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>69</wp:term_id>
		<wp:tag_slug><![CDATA[dalloc]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dalloc]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>70</wp:term_id>
		<wp:tag_slug><![CDATA[dapper]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dapper]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>71</wp:term_id>
		<wp:tag_slug><![CDATA[data-structure]]></wp:tag_slug>
		<wp:tag_name><![CDATA[data structure]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>72</wp:term_id>
		<wp:tag_slug><![CDATA[databases]]></wp:tag_slug>
		<wp:tag_name><![CDATA[databases]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>73</wp:term_id>
		<wp:tag_slug><![CDATA[date]]></wp:tag_slug>
		<wp:tag_name><![CDATA[date]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>74</wp:term_id>
		<wp:tag_slug><![CDATA[dc]]></wp:tag_slug>
		<wp:tag_name><![CDATA[DC]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>75</wp:term_id>
		<wp:tag_slug><![CDATA[debugging]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Debugging]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>76</wp:term_id>
		<wp:tag_slug><![CDATA[dependencies]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dependencies]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>77</wp:term_id>
		<wp:tag_slug><![CDATA[dependency-injection]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dependency injection]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>78</wp:term_id>
		<wp:tag_slug><![CDATA[deploy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[deploy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>79</wp:term_id>
		<wp:tag_slug><![CDATA[design]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Design]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>80</wp:term_id>
		<wp:tag_slug><![CDATA[design-patterns]]></wp:tag_slug>
		<wp:tag_name><![CDATA[design patterns]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>81</wp:term_id>
		<wp:tag_slug><![CDATA[distributed]]></wp:tag_slug>
		<wp:tag_name><![CDATA[distributed]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>82</wp:term_id>
		<wp:tag_slug><![CDATA[dns]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dns]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>83</wp:term_id>
		<wp:tag_slug><![CDATA[docker]]></wp:tag_slug>
		<wp:tag_name><![CDATA[docker]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>84</wp:term_id>
		<wp:tag_slug><![CDATA[droid]]></wp:tag_slug>
		<wp:tag_name><![CDATA[droid]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>85</wp:term_id>
		<wp:tag_slug><![CDATA[dsl]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dsl]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>86</wp:term_id>
		<wp:tag_slug><![CDATA[dynamic-proxy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[dynamic proxy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>87</wp:term_id>
		<wp:tag_slug><![CDATA[ecs]]></wp:tag_slug>
		<wp:tag_name><![CDATA[ecs]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>88</wp:term_id>
		<wp:tag_slug><![CDATA[endianess]]></wp:tag_slug>
		<wp:tag_name><![CDATA[endianess]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>89</wp:term_id>
		<wp:tag_slug><![CDATA[engineering]]></wp:tag_slug>
		<wp:tag_name><![CDATA[engineering]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>90</wp:term_id>
		<wp:tag_slug><![CDATA[enumerable]]></wp:tag_slug>
		<wp:tag_name><![CDATA[enumerable]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>91</wp:term_id>
		<wp:tag_slug><![CDATA[envz]]></wp:tag_slug>
		<wp:tag_name><![CDATA[envz]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>92</wp:term_id>
		<wp:tag_slug><![CDATA[equality]]></wp:tag_slug>
		<wp:tag_name><![CDATA[equality]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>93</wp:term_id>
		<wp:tag_slug><![CDATA[events]]></wp:tag_slug>
		<wp:tag_name><![CDATA[events]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>94</wp:term_id>
		<wp:tag_slug><![CDATA[expression-tree]]></wp:tag_slug>
		<wp:tag_name><![CDATA[expression tree]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>95</wp:term_id>
		<wp:tag_slug><![CDATA[f]]></wp:tag_slug>
		<wp:tag_name><![CDATA[F#]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>96</wp:term_id>
		<wp:tag_slug><![CDATA[failure]]></wp:tag_slug>
		<wp:tag_name><![CDATA[failure]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>97</wp:term_id>
		<wp:tag_slug><![CDATA[fault-tolerance]]></wp:tag_slug>
		<wp:tag_name><![CDATA[fault tolerance]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>98</wp:term_id>
		<wp:tag_slug><![CDATA[finatra]]></wp:tag_slug>
		<wp:tag_name><![CDATA[finatra]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>99</wp:term_id>
		<wp:tag_slug><![CDATA[flood-fill]]></wp:tag_slug>
		<wp:tag_name><![CDATA[flood fill]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>100</wp:term_id>
		<wp:tag_slug><![CDATA[fogbugz]]></wp:tag_slug>
		<wp:tag_name><![CDATA[fogbugz]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>101</wp:term_id>
		<wp:tag_slug><![CDATA[folds]]></wp:tag_slug>
		<wp:tag_name><![CDATA[folds]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>102</wp:term_id>
		<wp:tag_slug><![CDATA[fparsec]]></wp:tag_slug>
		<wp:tag_name><![CDATA[fparsec]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>263</wp:term_id>
		<wp:tag_slug><![CDATA[frontend]]></wp:tag_slug>
		<wp:tag_name><![CDATA[frontend]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>103</wp:term_id>
		<wp:tag_slug><![CDATA[fsharp]]></wp:tag_slug>
		<wp:tag_name><![CDATA[fsharp]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>104</wp:term_id>
		<wp:tag_slug><![CDATA[functional]]></wp:tag_slug>
		<wp:tag_name><![CDATA[functional]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>105</wp:term_id>
		<wp:tag_slug><![CDATA[functional-reactive]]></wp:tag_slug>
		<wp:tag_name><![CDATA[functional reactive]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>106</wp:term_id>
		<wp:tag_slug><![CDATA[functors]]></wp:tag_slug>
		<wp:tag_name><![CDATA[functors]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>107</wp:term_id>
		<wp:tag_slug><![CDATA[futures]]></wp:tag_slug>
		<wp:tag_name><![CDATA[futures]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>108</wp:term_id>
		<wp:tag_slug><![CDATA[garbage-collection]]></wp:tag_slug>
		<wp:tag_name><![CDATA[garbage collection]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>109</wp:term_id>
		<wp:tag_slug><![CDATA[gc]]></wp:tag_slug>
		<wp:tag_name><![CDATA[gc]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>110</wp:term_id>
		<wp:tag_slug><![CDATA[geometry-dash-icons]]></wp:tag_slug>
		<wp:tag_name><![CDATA[geometry dash icons]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>111</wp:term_id>
		<wp:tag_slug><![CDATA[geometry-dash-meltdown]]></wp:tag_slug>
		<wp:tag_name><![CDATA[geometry dash meltdown]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>112</wp:term_id>
		<wp:tag_slug><![CDATA[git]]></wp:tag_slug>
		<wp:tag_name><![CDATA[git]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>113</wp:term_id>
		<wp:tag_slug><![CDATA[go]]></wp:tag_slug>
		<wp:tag_name><![CDATA[go]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>114</wp:term_id>
		<wp:tag_slug><![CDATA[godaddy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[godaddy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>115</wp:term_id>
		<wp:tag_slug><![CDATA[golang]]></wp:tag_slug>
		<wp:tag_name><![CDATA[golang]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>116</wp:term_id>
		<wp:tag_slug><![CDATA[graphs]]></wp:tag_slug>
		<wp:tag_name><![CDATA[graphs]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>117</wp:term_id>
		<wp:tag_slug><![CDATA[guice]]></wp:tag_slug>
		<wp:tag_name><![CDATA[guice]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>118</wp:term_id>
		<wp:tag_slug><![CDATA[haproxy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[haproxy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>119</wp:term_id>
		<wp:tag_slug><![CDATA[hash]]></wp:tag_slug>
		<wp:tag_name><![CDATA[hash]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>120</wp:term_id>
		<wp:tag_slug><![CDATA[haskell]]></wp:tag_slug>
		<wp:tag_name><![CDATA[haskell]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>121</wp:term_id>
		<wp:tag_slug><![CDATA[hazelcast]]></wp:tag_slug>
		<wp:tag_name><![CDATA[hazelcast]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>122</wp:term_id>
		<wp:tag_slug><![CDATA[heap]]></wp:tag_slug>
		<wp:tag_name><![CDATA[heap]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>123</wp:term_id>
		<wp:tag_slug><![CDATA[heaps]]></wp:tag_slug>
		<wp:tag_name><![CDATA[heaps]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>124</wp:term_id>
		<wp:tag_slug><![CDATA[http]]></wp:tag_slug>
		<wp:tag_name><![CDATA[http]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>125</wp:term_id>
		<wp:tag_slug><![CDATA[ide]]></wp:tag_slug>
		<wp:tag_name><![CDATA[ide]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>126</wp:term_id>
		<wp:tag_slug><![CDATA[iis]]></wp:tag_slug>
		<wp:tag_name><![CDATA[iis]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>127</wp:term_id>
		<wp:tag_slug><![CDATA[image]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Image]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>128</wp:term_id>
		<wp:tag_slug><![CDATA[initialization]]></wp:tag_slug>
		<wp:tag_name><![CDATA[initialization]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>129</wp:term_id>
		<wp:tag_slug><![CDATA[instagram]]></wp:tag_slug>
		<wp:tag_name><![CDATA[instagram]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>130</wp:term_id>
		<wp:tag_slug><![CDATA[interpreter]]></wp:tag_slug>
		<wp:tag_name><![CDATA[interpreter]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>131</wp:term_id>
		<wp:tag_slug><![CDATA[io]]></wp:tag_slug>
		<wp:tag_name><![CDATA[IO]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>132</wp:term_id>
		<wp:tag_slug><![CDATA[ios]]></wp:tag_slug>
		<wp:tag_name><![CDATA[ios]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>133</wp:term_id>
		<wp:tag_slug><![CDATA[iphone]]></wp:tag_slug>
		<wp:tag_name><![CDATA[iphone]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>134</wp:term_id>
		<wp:tag_slug><![CDATA[iterator]]></wp:tag_slug>
		<wp:tag_name><![CDATA[iterator]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>135</wp:term_id>
		<wp:tag_slug><![CDATA[jackson]]></wp:tag_slug>
		<wp:tag_name><![CDATA[jackson]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>136</wp:term_id>
		<wp:tag_slug><![CDATA[java]]></wp:tag_slug>
		<wp:tag_name><![CDATA[java]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>137</wp:term_id>
		<wp:tag_slug><![CDATA[js]]></wp:tag_slug>
		<wp:tag_name><![CDATA[JavaScript]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>138</wp:term_id>
		<wp:tag_slug><![CDATA[jenkins]]></wp:tag_slug>
		<wp:tag_name><![CDATA[jenkins]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>264</wp:term_id>
		<wp:tag_slug><![CDATA[jira]]></wp:tag_slug>
		<wp:tag_name><![CDATA[jira]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>139</wp:term_id>
		<wp:tag_slug><![CDATA[json]]></wp:tag_slug>
		<wp:tag_name><![CDATA[json]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>140</wp:term_id>
		<wp:tag_slug><![CDATA[lambda]]></wp:tag_slug>
		<wp:tag_name><![CDATA[lambda]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>141</wp:term_id>
		<wp:tag_slug><![CDATA[lambdas]]></wp:tag_slug>
		<wp:tag_name><![CDATA[lambdas]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>142</wp:term_id>
		<wp:tag_slug><![CDATA[language-implementation]]></wp:tag_slug>
		<wp:tag_name><![CDATA[language implementation]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>143</wp:term_id>
		<wp:tag_slug><![CDATA[lazy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[lazy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>144</wp:term_id>
		<wp:tag_slug><![CDATA[leadership]]></wp:tag_slug>
		<wp:tag_name><![CDATA[leadership]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>145</wp:term_id>
		<wp:tag_slug><![CDATA[lexer]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Lexer]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>146</wp:term_id>
		<wp:tag_slug><![CDATA[library]]></wp:tag_slug>
		<wp:tag_name><![CDATA[library]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>147</wp:term_id>
		<wp:tag_slug><![CDATA[linq]]></wp:tag_slug>
		<wp:tag_name><![CDATA[linq]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>148</wp:term_id>
		<wp:tag_slug><![CDATA[logging]]></wp:tag_slug>
		<wp:tag_name><![CDATA[logging]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>149</wp:term_id>
		<wp:tag_slug><![CDATA[lombok]]></wp:tag_slug>
		<wp:tag_name><![CDATA[lombok]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>150</wp:term_id>
		<wp:tag_slug><![CDATA[low-level]]></wp:tag_slug>
		<wp:tag_name><![CDATA[low-level]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>151</wp:term_id>
		<wp:tag_slug><![CDATA[lucene]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Lucene]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>152</wp:term_id>
		<wp:tag_slug><![CDATA[machine-learing]]></wp:tag_slug>
		<wp:tag_name><![CDATA[machine learing]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>153</wp:term_id>
		<wp:tag_slug><![CDATA[machine-learning]]></wp:tag_slug>
		<wp:tag_name><![CDATA[machine learning]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>154</wp:term_id>
		<wp:tag_slug><![CDATA[macro]]></wp:tag_slug>
		<wp:tag_name><![CDATA[macro]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>155</wp:term_id>
		<wp:tag_slug><![CDATA[markdown]]></wp:tag_slug>
		<wp:tag_name><![CDATA[markdown]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>156</wp:term_id>
		<wp:tag_slug><![CDATA[maven]]></wp:tag_slug>
		<wp:tag_name><![CDATA[maven]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>157</wp:term_id>
		<wp:tag_slug><![CDATA[maybe-monad]]></wp:tag_slug>
		<wp:tag_name><![CDATA[maybe monad]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>158</wp:term_id>
		<wp:tag_slug><![CDATA[mdc]]></wp:tag_slug>
		<wp:tag_name><![CDATA[MDC]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>159</wp:term_id>
		<wp:tag_slug><![CDATA[meetup]]></wp:tag_slug>
		<wp:tag_name><![CDATA[meetup]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>160</wp:term_id>
		<wp:tag_slug><![CDATA[memory-leak]]></wp:tag_slug>
		<wp:tag_name><![CDATA[memory leak]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>161</wp:term_id>
		<wp:tag_slug><![CDATA[migration]]></wp:tag_slug>
		<wp:tag_name><![CDATA[migration]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>162</wp:term_id>
		<wp:tag_slug><![CDATA[mockito]]></wp:tag_slug>
		<wp:tag_name><![CDATA[mockito]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>163</wp:term_id>
		<wp:tag_slug><![CDATA[monad]]></wp:tag_slug>
		<wp:tag_name><![CDATA[monad]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>164</wp:term_id>
		<wp:tag_slug><![CDATA[monads]]></wp:tag_slug>
		<wp:tag_name><![CDATA[monads]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>165</wp:term_id>
		<wp:tag_slug><![CDATA[mongodb]]></wp:tag_slug>
		<wp:tag_name><![CDATA[mongodb]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>166</wp:term_id>
		<wp:tag_slug><![CDATA[mongoose]]></wp:tag_slug>
		<wp:tag_name><![CDATA[mongoose]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>261</wp:term_id>
		<wp:tag_slug><![CDATA[monorepo]]></wp:tag_slug>
		<wp:tag_name><![CDATA[monorepo]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>167</wp:term_id>
		<wp:tag_slug><![CDATA[mysql]]></wp:tag_slug>
		<wp:tag_name><![CDATA[MySql]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>168</wp:term_id>
		<wp:tag_slug><![CDATA[naive-bayes]]></wp:tag_slug>
		<wp:tag_name><![CDATA[naive bayes]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>169</wp:term_id>
		<wp:tag_slug><![CDATA[neo4j]]></wp:tag_slug>
		<wp:tag_name><![CDATA[neo4j]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>170</wp:term_id>
		<wp:tag_slug><![CDATA[netduino]]></wp:tag_slug>
		<wp:tag_name><![CDATA[netduino]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>171</wp:term_id>
		<wp:tag_slug><![CDATA[neural-networks]]></wp:tag_slug>
		<wp:tag_name><![CDATA[neural networks]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>172</wp:term_id>
		<wp:tag_slug><![CDATA[node]]></wp:tag_slug>
		<wp:tag_name><![CDATA[node]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>173</wp:term_id>
		<wp:tag_slug><![CDATA[node-js]]></wp:tag_slug>
		<wp:tag_name><![CDATA[node.js]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>174</wp:term_id>
		<wp:tag_slug><![CDATA[nosql]]></wp:tag_slug>
		<wp:tag_name><![CDATA[NoSql]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>175</wp:term_id>
		<wp:tag_slug><![CDATA[nuget]]></wp:tag_slug>
		<wp:tag_name><![CDATA[nuget]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>176</wp:term_id>
		<wp:tag_slug><![CDATA[null]]></wp:tag_slug>
		<wp:tag_name><![CDATA[null]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>177</wp:term_id>
		<wp:tag_slug><![CDATA[null-ref]]></wp:tag_slug>
		<wp:tag_name><![CDATA[null ref]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>178</wp:term_id>
		<wp:tag_slug><![CDATA[ops]]></wp:tag_slug>
		<wp:tag_name><![CDATA[ops]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>179</wp:term_id>
		<wp:tag_slug><![CDATA[option]]></wp:tag_slug>
		<wp:tag_name><![CDATA[option]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>180</wp:term_id>
		<wp:tag_slug><![CDATA[osgi]]></wp:tag_slug>
		<wp:tag_name><![CDATA[osgi]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>181</wp:term_id>
		<wp:tag_slug><![CDATA[osx]]></wp:tag_slug>
		<wp:tag_name><![CDATA[osx]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>182</wp:term_id>
		<wp:tag_slug><![CDATA[paradoxical]]></wp:tag_slug>
		<wp:tag_name><![CDATA[paradoxical]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>183</wp:term_id>
		<wp:tag_slug><![CDATA[parser]]></wp:tag_slug>
		<wp:tag_name><![CDATA[parser]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>184</wp:term_id>
		<wp:tag_slug><![CDATA[parsing]]></wp:tag_slug>
		<wp:tag_name><![CDATA[parsing]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>185</wp:term_id>
		<wp:tag_slug><![CDATA[partition]]></wp:tag_slug>
		<wp:tag_name><![CDATA[partition]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>186</wp:term_id>
		<wp:tag_slug><![CDATA[path-finding]]></wp:tag_slug>
		<wp:tag_name><![CDATA[path finding]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>187</wp:term_id>
		<wp:tag_slug><![CDATA[pattern-matching]]></wp:tag_slug>
		<wp:tag_name><![CDATA[pattern matching]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>188</wp:term_id>
		<wp:tag_slug><![CDATA[patterns]]></wp:tag_slug>
		<wp:tag_name><![CDATA[patterns]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>189</wp:term_id>
		<wp:tag_slug><![CDATA[paxos]]></wp:tag_slug>
		<wp:tag_name><![CDATA[paxos]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>190</wp:term_id>
		<wp:tag_slug><![CDATA[plugin]]></wp:tag_slug>
		<wp:tag_name><![CDATA[plugin]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>191</wp:term_id>
		<wp:tag_slug><![CDATA[postgres]]></wp:tag_slug>
		<wp:tag_name><![CDATA[postgres]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>192</wp:term_id>
		<wp:tag_slug><![CDATA[postsharp]]></wp:tag_slug>
		<wp:tag_name><![CDATA[postsharp]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>193</wp:term_id>
		<wp:tag_slug><![CDATA[powershell]]></wp:tag_slug>
		<wp:tag_name><![CDATA[powershell]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>265</wp:term_id>
		<wp:tag_slug><![CDATA[productivity]]></wp:tag_slug>
		<wp:tag_name><![CDATA[productivity]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>194</wp:term_id>
		<wp:tag_slug><![CDATA[projects]]></wp:tag_slug>
		<wp:tag_name><![CDATA[projects]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>195</wp:term_id>
		<wp:tag_slug><![CDATA[puppet]]></wp:tag_slug>
		<wp:tag_name><![CDATA[puppet]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>196</wp:term_id>
		<wp:tag_slug><![CDATA[python]]></wp:tag_slug>
		<wp:tag_name><![CDATA[python]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>197</wp:term_id>
		<wp:tag_slug><![CDATA[queue]]></wp:tag_slug>
		<wp:tag_name><![CDATA[queue]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>198</wp:term_id>
		<wp:tag_slug><![CDATA[quorum]]></wp:tag_slug>
		<wp:tag_name><![CDATA[quorum]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>262</wp:term_id>
		<wp:tag_slug><![CDATA[react]]></wp:tag_slug>
		<wp:tag_name><![CDATA[react]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>199</wp:term_id>
		<wp:tag_slug><![CDATA[realtime]]></wp:tag_slug>
		<wp:tag_name><![CDATA[realtime]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>200</wp:term_id>
		<wp:tag_slug><![CDATA[reddit]]></wp:tag_slug>
		<wp:tag_name><![CDATA[reddit]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>201</wp:term_id>
		<wp:tag_slug><![CDATA[reflection]]></wp:tag_slug>
		<wp:tag_name><![CDATA[reflection]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>202</wp:term_id>
		<wp:tag_slug><![CDATA[regex]]></wp:tag_slug>
		<wp:tag_name><![CDATA[regex]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>203</wp:term_id>
		<wp:tag_slug><![CDATA[regular-expressions]]></wp:tag_slug>
		<wp:tag_name><![CDATA[regular expressions]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>204</wp:term_id>
		<wp:tag_slug><![CDATA[resharper]]></wp:tag_slug>
		<wp:tag_name><![CDATA[resharper]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>205</wp:term_id>
		<wp:tag_slug><![CDATA[review]]></wp:tag_slug>
		<wp:tag_name><![CDATA[review]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>206</wp:term_id>
		<wp:tag_slug><![CDATA[rmq]]></wp:tag_slug>
		<wp:tag_name><![CDATA[rmq]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>207</wp:term_id>
		<wp:tag_slug><![CDATA[ruby]]></wp:tag_slug>
		<wp:tag_name><![CDATA[ruby]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>208</wp:term_id>
		<wp:tag_slug><![CDATA[runtime]]></wp:tag_slug>
		<wp:tag_name><![CDATA[runtime]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>209</wp:term_id>
		<wp:tag_slug><![CDATA[rx]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Rx]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>210</wp:term_id>
		<wp:tag_slug><![CDATA[salt]]></wp:tag_slug>
		<wp:tag_name><![CDATA[salt]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>211</wp:term_id>
		<wp:tag_slug><![CDATA[sbt]]></wp:tag_slug>
		<wp:tag_name><![CDATA[sbt]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>212</wp:term_id>
		<wp:tag_slug><![CDATA[scala]]></wp:tag_slug>
		<wp:tag_name><![CDATA[scala]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>213</wp:term_id>
		<wp:tag_slug><![CDATA[scheduled-tasks]]></wp:tag_slug>
		<wp:tag_name><![CDATA[scheduled tasks]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>214</wp:term_id>
		<wp:tag_slug><![CDATA[sensu]]></wp:tag_slug>
		<wp:tag_name><![CDATA[sensu]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>215</wp:term_id>
		<wp:tag_slug><![CDATA[serialization]]></wp:tag_slug>
		<wp:tag_name><![CDATA[serialization]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>216</wp:term_id>
		<wp:tag_slug><![CDATA[service-oriented-architecture]]></wp:tag_slug>
		<wp:tag_name><![CDATA[service oriented architecture]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>217</wp:term_id>
		<wp:tag_slug><![CDATA[services]]></wp:tag_slug>
		<wp:tag_name><![CDATA[services]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>218</wp:term_id>
		<wp:tag_slug><![CDATA[servicestack]]></wp:tag_slug>
		<wp:tag_name><![CDATA[servicestack]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>219</wp:term_id>
		<wp:tag_slug><![CDATA[setup]]></wp:tag_slug>
		<wp:tag_name><![CDATA[setup]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>220</wp:term_id>
		<wp:tag_slug><![CDATA[shapeless]]></wp:tag_slug>
		<wp:tag_name><![CDATA[shapeless]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>221</wp:term_id>
		<wp:tag_slug><![CDATA[shell]]></wp:tag_slug>
		<wp:tag_name><![CDATA[shell]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>222</wp:term_id>
		<wp:tag_slug><![CDATA[signalr]]></wp:tag_slug>
		<wp:tag_name><![CDATA[SignalR]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>223</wp:term_id>
		<wp:tag_slug><![CDATA[slf4j]]></wp:tag_slug>
		<wp:tag_name><![CDATA[slf4j]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>224</wp:term_id>
		<wp:tag_slug><![CDATA[sockets]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Sockets]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>225</wp:term_id>
		<wp:tag_slug><![CDATA[sqlite]]></wp:tag_slug>
		<wp:tag_name><![CDATA[SQLite]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>226</wp:term_id>
		<wp:tag_slug><![CDATA[stack]]></wp:tag_slug>
		<wp:tag_name><![CDATA[stack]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>227</wp:term_id>
		<wp:tag_slug><![CDATA[sudoku]]></wp:tag_slug>
		<wp:tag_name><![CDATA[sudoku]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>228</wp:term_id>
		<wp:tag_slug><![CDATA[svn]]></wp:tag_slug>
		<wp:tag_name><![CDATA[svn]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>229</wp:term_id>
		<wp:tag_slug><![CDATA[synchronization]]></wp:tag_slug>
		<wp:tag_name><![CDATA[synchronization]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>230</wp:term_id>
		<wp:tag_slug><![CDATA[tail-recursion]]></wp:tag_slug>
		<wp:tag_name><![CDATA[tail recursion]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>231</wp:term_id>
		<wp:tag_slug><![CDATA[tasks]]></wp:tag_slug>
		<wp:tag_name><![CDATA[tasks]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>232</wp:term_id>
		<wp:tag_slug><![CDATA[tcp]]></wp:tag_slug>
		<wp:tag_name><![CDATA[TCP]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>233</wp:term_id>
		<wp:tag_slug><![CDATA[tech-talk]]></wp:tag_slug>
		<wp:tag_name><![CDATA[tech talk]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>234</wp:term_id>
		<wp:tag_slug><![CDATA[tempating]]></wp:tag_slug>
		<wp:tag_name><![CDATA[tempating]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>235</wp:term_id>
		<wp:tag_slug><![CDATA[terraform]]></wp:tag_slug>
		<wp:tag_name><![CDATA[terraform]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>236</wp:term_id>
		<wp:tag_slug><![CDATA[test-tools]]></wp:tag_slug>
		<wp:tag_name><![CDATA[test tools]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>237</wp:term_id>
		<wp:tag_slug><![CDATA[testing]]></wp:tag_slug>
		<wp:tag_name><![CDATA[testing]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>238</wp:term_id>
		<wp:tag_slug><![CDATA[text-editors]]></wp:tag_slug>
		<wp:tag_name><![CDATA[text editors]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>239</wp:term_id>
		<wp:tag_slug><![CDATA[threading]]></wp:tag_slug>
		<wp:tag_name><![CDATA[threading]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>240</wp:term_id>
		<wp:tag_slug><![CDATA[threads]]></wp:tag_slug>
		<wp:tag_name><![CDATA[threads]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>241</wp:term_id>
		<wp:tag_slug><![CDATA[thrift]]></wp:tag_slug>
		<wp:tag_name><![CDATA[thrift]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>242</wp:term_id>
		<wp:tag_slug><![CDATA[time]]></wp:tag_slug>
		<wp:tag_name><![CDATA[time]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>266</wp:term_id>
		<wp:tag_slug><![CDATA[tooling]]></wp:tag_slug>
		<wp:tag_name><![CDATA[tooling]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>243</wp:term_id>
		<wp:tag_slug><![CDATA[topic]]></wp:tag_slug>
		<wp:tag_name><![CDATA[topic]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>244</wp:term_id>
		<wp:tag_slug><![CDATA[toy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[toy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>245</wp:term_id>
		<wp:tag_slug><![CDATA[tracing]]></wp:tag_slug>
		<wp:tag_name><![CDATA[tracing]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>246</wp:term_id>
		<wp:tag_slug><![CDATA[travis]]></wp:tag_slug>
		<wp:tag_name><![CDATA[travis]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>247</wp:term_id>
		<wp:tag_slug><![CDATA[trie]]></wp:tag_slug>
		<wp:tag_name><![CDATA[trie]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>248</wp:term_id>
		<wp:tag_slug><![CDATA[types]]></wp:tag_slug>
		<wp:tag_name><![CDATA[types]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>249</wp:term_id>
		<wp:tag_slug><![CDATA[typescript]]></wp:tag_slug>
		<wp:tag_name><![CDATA[typescript]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>250</wp:term_id>
		<wp:tag_slug><![CDATA[utilities]]></wp:tag_slug>
		<wp:tag_name><![CDATA[Utilities]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>251</wp:term_id>
		<wp:tag_slug><![CDATA[utilitiy]]></wp:tag_slug>
		<wp:tag_name><![CDATA[utilitiy]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>252</wp:term_id>
		<wp:tag_slug><![CDATA[value]]></wp:tag_slug>
		<wp:tag_name><![CDATA[value]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>253</wp:term_id>
		<wp:tag_slug><![CDATA[velocity]]></wp:tag_slug>
		<wp:tag_name><![CDATA[velocity]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>254</wp:term_id>
		<wp:tag_slug><![CDATA[version-control]]></wp:tag_slug>
		<wp:tag_name><![CDATA[version control]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>255</wp:term_id>
		<wp:tag_slug><![CDATA[video]]></wp:tag_slug>
		<wp:tag_name><![CDATA[video]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>256</wp:term_id>
		<wp:tag_slug><![CDATA[wcf]]></wp:tag_slug>
		<wp:tag_name><![CDATA[wcf]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>257</wp:term_id>
		<wp:tag_slug><![CDATA[webapi]]></wp:tag_slug>
		<wp:tag_name><![CDATA[webapi]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>258</wp:term_id>
		<wp:tag_slug><![CDATA[x86]]></wp:tag_slug>
		<wp:tag_name><![CDATA[x86]]></wp:tag_name>
	</wp:tag>
		<wp:tag>
		<wp:term_id>259</wp:term_id>
		<wp:tag_slug><![CDATA[zsh]]></wp:tag_slug>
		<wp:tag_name><![CDATA[zsh]]></wp:tag_name>
	</wp:tag>
			<wp:term>
		<wp:term_id><![CDATA[9]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dotnet]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[.NET]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[10]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[64-bit]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[64 bit]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[11]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[64bit]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[64bit]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[12]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[abstract-syntax-trees]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[abstract syntax trees]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[13]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[active-patterns]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[active patterns]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[14]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[actors]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[actors]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[15]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[aeson]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[aeson]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[16]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[aetr]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[aetr]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[17]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[akka]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[akka]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[18]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[algorithms]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[algorithms]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[19]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[angular]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[angular]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[20]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[angularjs]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[angularjs]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[21]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[architecture]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[architecture]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[22]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[arithmetic]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[arithmetic]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[23]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[arrows]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Arrows]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[24]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[as3]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[AS3]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[25]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[asgard]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[asgard]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[26]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[asm]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[asm]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[27]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[asp-net]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[asp.net]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[28]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[aspects]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[aspects]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[29]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[ast]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[ast]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[30]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[async]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[async]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[31]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[athena]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[athena]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[32]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[autocomplete]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[autocomplete]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[33]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[automation]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[automation]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[34]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[aws]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[aws]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[35]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[b-tree]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[b-tree]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[36]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[best-practices]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Best Practices]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[37]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[bfs]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[bfs]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[38]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[bits]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[bits]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[39]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[bloom-filter]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Bloom filter]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[40]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[brainfuck]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[brainfuck]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[41]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[branches]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[branches]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[42]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[bug]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[bug]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[43]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[byte]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[byte]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[44]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[bytecode]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[bytecode]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[45]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[c]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[c#]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[46]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[csharp]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[C#]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[47]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[cpp]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[C++]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[48]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[careers]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[careers]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[49]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[cassandra]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[cassandra]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[50]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[cassieq]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[cassieq]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[51]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[cicd]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[cicd]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[52]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[classes]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[classes]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[53]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[classification]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[classification]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[54]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[classloader]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[classloader]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[55]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[clojure]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[clojure]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[56]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[closures]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[closures]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[57]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[clr]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[clr]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[2]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[code]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Code]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[58]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[combinator]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[combinator]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[59]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[combinators]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[combinators]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[60]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[computation-expression]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[computation expression]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[61]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[conference]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[conference]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[62]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[configuration]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[configuration]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[63]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[containers]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[containers]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[64]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[continuation-passing]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[continuation passing]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[65]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[coproduct]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[coproduct]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[3]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[cross-post]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Cross Post]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[66]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[css]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[css]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[67]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[csv]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[csv]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[68]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dailyprogrammer]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dailyprogrammer]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[69]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dalloc]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dalloc]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[70]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dapper]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dapper]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[71]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[data-structure]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[data structure]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[72]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[databases]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[databases]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[73]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[date]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[date]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[74]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dc]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[DC]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[75]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[debugging]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Debugging]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[76]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dependencies]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dependencies]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[77]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dependency-injection]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dependency injection]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[78]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[deploy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[deploy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[79]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[design]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Design]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[80]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[design-patterns]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[design patterns]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[4]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[discussion]]></wp:term_slug>
		<wp:term_parent><![CDATA[code]]></wp:term_parent>
		<wp:term_name><![CDATA[Discussion]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[81]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[distributed]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[distributed]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[82]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dns]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dns]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[83]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[docker]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[docker]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[84]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[droid]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[droid]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[85]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dsl]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dsl]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[86]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[dynamic-proxy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[dynamic proxy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[87]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[ecs]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[ecs]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[88]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[endianess]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[endianess]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[89]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[engineering]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[engineering]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[90]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[enumerable]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[enumerable]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[91]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[envz]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[envz]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[92]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[equality]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[equality]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[93]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[events]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[events]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[94]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[expression-tree]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[expression tree]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[95]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[f]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[F#]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[96]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[failure]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[failure]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[97]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[fault-tolerance]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[fault tolerance]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[98]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[finatra]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[finatra]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[99]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[flood-fill]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[flood fill]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[100]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[fogbugz]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[fogbugz]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[101]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[folds]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[folds]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[102]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[fparsec]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[fparsec]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[263]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[frontend]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[frontend]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[103]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[fsharp]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[fsharp]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[104]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[functional]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[functional]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[105]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[functional-reactive]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[functional reactive]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[106]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[functors]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[functors]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[107]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[futures]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[futures]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[108]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[garbage-collection]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[garbage collection]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[109]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[gc]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[gc]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[110]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[geometry-dash-icons]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[geometry dash icons]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[111]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[geometry-dash-meltdown]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[geometry dash meltdown]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[112]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[git]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[git]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[113]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[go]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[go]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[114]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[godaddy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[godaddy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[115]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[golang]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[golang]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[116]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[graphs]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[graphs]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[117]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[guice]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[guice]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[118]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[haproxy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[haproxy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[119]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[hash]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[hash]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[120]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[haskell]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[haskell]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[121]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[hazelcast]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[hazelcast]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[122]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[heap]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[heap]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[123]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[heaps]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[heaps]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[124]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[http]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[http]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[125]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[ide]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[ide]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[126]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[iis]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[iis]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[127]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[image]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Image]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[5]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[imported]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Imported]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[128]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[initialization]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[initialization]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[129]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[instagram]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[instagram]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[130]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[interpreter]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[interpreter]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[131]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[io]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[IO]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[132]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[ios]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[ios]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[133]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[iphone]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[iphone]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[134]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[iterator]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[iterator]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[135]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[jackson]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[jackson]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[136]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[java]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[java]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[137]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[js]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[JavaScript]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[138]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[jenkins]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[jenkins]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[264]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[jira]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[jira]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[139]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[json]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[json]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[140]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[lambda]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[lambda]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[141]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[lambdas]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[lambdas]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[142]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[language-implementation]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[language implementation]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[143]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[lazy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[lazy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[144]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[leadership]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[leadership]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[145]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[lexer]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Lexer]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[146]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[library]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[library]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[147]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[linq]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[linq]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[148]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[logging]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[logging]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[149]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[lombok]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[lombok]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[150]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[low-level]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[low-level]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[151]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[lucene]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Lucene]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[152]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[machine-learing]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[machine learing]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[153]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[machine-learning]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[machine learning]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[154]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[macro]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[macro]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[155]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[markdown]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[markdown]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[156]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[maven]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[maven]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[157]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[maybe-monad]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[maybe monad]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[158]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[mdc]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[MDC]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[159]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[meetup]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[meetup]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[160]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[memory-leak]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[memory leak]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[161]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[migration]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[migration]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[162]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[mockito]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[mockito]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[163]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[monad]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[monad]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[164]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[monads]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[monads]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[165]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[mongodb]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[mongodb]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[166]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[mongoose]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[mongoose]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[261]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[monorepo]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[monorepo]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[167]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[mysql]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[MySql]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[168]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[naive-bayes]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[naive bayes]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[169]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[neo4j]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[neo4j]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[170]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[netduino]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[netduino]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[171]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[neural-networks]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[neural networks]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[172]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[node]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[node]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[173]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[node-js]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[node.js]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[174]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[nosql]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[NoSql]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[175]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[nuget]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[nuget]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[176]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[null]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[null]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[177]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[null-ref]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[null ref]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[178]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[ops]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[ops]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[179]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[option]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[option]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[180]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[osgi]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[osgi]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[181]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[osx]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[osx]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[182]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[paradoxical]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[paradoxical]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[183]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[parser]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[parser]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[184]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[parsing]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[parsing]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[185]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[partition]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[partition]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[186]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[path-finding]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[path finding]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[187]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[pattern-matching]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[pattern matching]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[188]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[patterns]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[patterns]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[189]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[paxos]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[paxos]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[190]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[plugin]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[plugin]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[191]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[postgres]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[postgres]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[192]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[postsharp]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[postsharp]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[193]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[powershell]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[powershell]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[265]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[productivity]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[productivity]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[194]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[projects]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[projects]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[195]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[puppet]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[puppet]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[196]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[python]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[python]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[197]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[queue]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[queue]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[198]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[quorum]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[quorum]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[6]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[rants]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Rants]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[262]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[react]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[react]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[199]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[realtime]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[realtime]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[200]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[reddit]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[reddit]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[201]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[reflection]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[reflection]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[202]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[regex]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[regex]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[203]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[regular-expressions]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[regular expressions]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[204]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[resharper]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[resharper]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[205]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[review]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[review]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[206]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[rmq]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[rmq]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[207]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[ruby]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[ruby]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[208]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[runtime]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[runtime]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[209]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[rx]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Rx]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[210]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[salt]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[salt]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[211]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[sbt]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[sbt]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[212]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[scala]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[scala]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[213]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[scheduled-tasks]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[scheduled tasks]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[214]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[sensu]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[sensu]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[215]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[serialization]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[serialization]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[216]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[service-oriented-architecture]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[service oriented architecture]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[217]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[services]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[services]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[218]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[servicestack]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[servicestack]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[219]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[setup]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[setup]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[220]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[shapeless]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[shapeless]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[221]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[shell]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[shell]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[222]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[signalr]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[SignalR]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[223]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[slf4j]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[slf4j]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[7]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[snippets]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Snippets]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[224]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[sockets]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Sockets]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[225]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[sqlite]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[SQLite]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[226]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[stack]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[stack]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[227]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[sudoku]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[sudoku]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[228]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[svn]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[svn]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[229]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[synchronization]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[synchronization]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[230]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tail-recursion]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[tail recursion]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[231]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tasks]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[tasks]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[232]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tcp]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[TCP]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[233]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tech-talk]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[tech talk]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[8]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tech-talks]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Tech talks]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[234]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tempating]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[tempating]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[235]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[terraform]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[terraform]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[236]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[test-tools]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[test tools]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[237]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[testing]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[testing]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[238]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[text-editors]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[text editors]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[239]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[threading]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[threading]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[240]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[threads]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[threads]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[241]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[thrift]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[thrift]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[242]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[time]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[time]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[266]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tooling]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[tooling]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[260]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[nav_menu]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[top-navigation]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Top Navigation]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[243]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[topic]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[topic]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[244]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[toy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[toy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[245]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[tracing]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[tracing]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[246]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[travis]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[travis]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[247]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[trie]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[trie]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[248]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[types]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[types]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[249]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[typescript]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[typescript]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[1]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[category]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[uncategorized]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Uncategorized]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[250]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[utilities]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[Utilities]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[251]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[utilitiy]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[utilitiy]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[252]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[value]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[value]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[253]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[velocity]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[velocity]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[254]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[version-control]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[version control]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[255]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[video]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[video]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[256]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[wcf]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[wcf]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[257]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[webapi]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[webapi]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[258]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[x86]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[x86]]></wp:term_name>
	</wp:term>
		<wp:term>
		<wp:term_id><![CDATA[259]]></wp:term_id>
		<wp:term_taxonomy><![CDATA[post_tag]]></wp:term_taxonomy>
		<wp:term_slug><![CDATA[zsh]]></wp:term_slug>
		<wp:term_parent><![CDATA[]]></wp:term_parent>
		<wp:term_name><![CDATA[zsh]]></wp:term_name>
	</wp:term>
			<wp:term><wp:term_id>260</wp:term_id><wp:term_taxonomy>nav_menu</wp:term_taxonomy><wp:term_slug><![CDATA[top-navigation]]></wp:term_slug><wp:term_name><![CDATA[Top Navigation]]></wp:term_name>
</wp:term>

	<generator>https://wordpress.org/?v=5.2.5</generator>

		<item>
		<title>About</title>
		<link>https://onoffswitch.net/about/</link>
		<pubDate>Mon, 25 Feb 2013 16:27:39 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?page_id=11</guid>
		<description></description>
		<content:encoded><![CDATA[I'm just a guy who likes to code. Find me in the  Seattle metro area. Check me out on the following networks:

<a href="https://twitter.com/devshorts"><img src="http://onoffswitch.net/wp-content/uploads/2013/02/twitter-bird-light-bgs24x24.png" alt="@devshorts" width="24" height="24" class="alignnone size-full wp-image-2656" /></a> <a href="http://stackoverflow.com/users/310196/devshorts"><img src="http://onoffswitch.net/wp-content/uploads/2013/02/stackoverflow.png" alt="stackoverflow" width="24" height="24" class="alignnone size-full wp-image-2655" /></a> <a href="https://github.com/devshorts"><img src="http://onoffswitch.net/wp-content/uploads/2013/02/github24x24.png" alt="devshorts github" width="24" height="24" class="alignnone size-full wp-image-2653" /></a> <a href="http://www.linkedin.com/pub/anton-kropp/16/368/80b"><img src="http://onoffswitch.net/wp-content/uploads/2013/02/linkedin24x24.png" alt="anton kropp linkedin" width="24" height="24" class="alignnone size-full wp-image-2654" /></a>

When I'm not playing with computers, I'm also playing music as <a href="http://fortheagenda.bandcamp.com/" target="_blank" rel="noopener noreferrer">for the agenda</a> and learning jui jitsu

All of the original code published in this blog is available under the <a href="http://onoffswitch.net/onoff-license/">following license</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>11</wp:post_id>
		<wp:post_date><![CDATA[2013-02-25 16:27:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-25 16:27:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[about]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[page]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
														<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>5295</wp:comment_id>
			<wp:comment_author><![CDATA[Sandra]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandra@komunity.io]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[209.58.128.97]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2020-03-05 21:25:25]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2020-03-05 21:25:25]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,
 
My name is Sandra and I’m a community director at komunity.io. We are a small team of like-minded people building a community for professionals, we like to say "the slack for people". Our core focus is to connect people seeking help to people who need help, the emphasis is in connecting with a purpose and building meaningful relationships.
 
I’m personally inviting you to join komunity.io and make a difference by helping people and in-turn connect with amazing people who can help you.  We want to create a useful and productive platform for all.
 
Cheers!
Sandra]]></wp:comment_content>
			<wp:comment_approved><![CDATA[0]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1583443525.306562900543212890625;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title></title>
		<link>https://onoffswitch.net/2019/07/01/12/</link>
		<pubDate>Mon, 01 Jul 2019 01:22:32 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/2019/07/01/</guid>
		<description></description>
		<content:encoded><![CDATA[ ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>12</wp:post_id>
		<wp:post_date><![CDATA[2019-07-01 01:22:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-07-01 01:22:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[12]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>2</wp:menu_order>
		<wp:post_type><![CDATA[nav_menu_item]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="nav_menu" nicename="top-navigation"><![CDATA[Top Navigation]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[post_type]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_menu_item_parent]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_object_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[11]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_object]]></wp:meta_key>
		<wp:meta_value><![CDATA[page]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_target]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_classes]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{i:0;s:0:"";}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_xfn]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Lucene Utilities</title>
		<link>https://onoffswitch.net/?p=21</link>
		<pubDate>Tue, 21 Aug 2012 14:39:10 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=21</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://lucene.apache.org/core/">Lucene</a> is a high-performance scaleable search engine library that is a wonderful solution when you are faced with the problem of storing and searching large amounts of text based data.   Storing data in Lucene is easy: all you need to do is say where you want to store your database and start creating "documents" which represent a set of data. On that document you can then set fields (aka records) which have a name and a value and then add the document to the engine. We are using<a href="http://incubator.apache.org/lucene.net/"> Lucene.net</a> in the following post, so if you aren't familiar with Lucene it is worth perusing some Lucene.net <a href="http://codeclimber.net.nz/archive/2009/09/02/lucene.net-your-first-application.aspx">tutorials</a> before continuing.

We are using Lucene to quickly search through tons of log files that we have.  There's no way anyone wants to search through over 100 GB of log files, so we decided it was necessary to parse each log and dump the entries into Lucene.

A document in our case is a line in a log corresponding to an atomic entry. Each of the fields is a piece of metadata on the log: log time, log text, log type, the location on disk of where this log file is, etc.  In general, inserting and querying out of Lucene is easy but you have to build out a document object that represents your data to give to Lucene to add. We wanted to generalize the logic of working with Lucene so that we could easily add and edit fields for our documents and to be able to reuse the Lucene query/insert logic with different document types. This way if we wanted to have different Lucene indexes for different data types we could reuse all our code without having to ever manually construct a document.

In order to do this we built a few utilities to help interact with Lucene. In a later post we'll share our generic search class that you can use with these utilities to abstract creating, inserting, and querying into Lucene databases.
<h1>Putting Data In</h1>
First we need to get data into the engine. We like typed data since it allows compile time checking so we decided to use reflection as a way to get data from an object and convert it to a Lucene document with its related metadata fields. Whenever we think about mapping one object to another I immediately think about using reflection since its dynamic and relatively simple. But in order to know enough about the data we're reflecting on we need a way to tag the fields. Attributes are a great way to tag data and to be able to get metadata about a field later. We created an attribute called <code>LuceneSearchFieldAttribute</code> which looks like this:

[csharp]
public class LuceneSearchFieldAttribute : Attribute
{
    private bool _store = true;
    public Boolean Store
    {
        get { return _store; }
        set { _store = value; }
    }

    public Boolean Analyze { get; set; }

    private int _trimAmount = NullValue.Integer;
    public int TrimAmount
    {
        get { return _trimAmount; }
        set { _trimAmount = value; }
    }

    public Boolean ForceToLower { get; set; }

    public Boolean NumericField { get; set; }
}
[/csharp]
<ul>
	<li><strong>Store</strong> - This represents whether we store the data in lucene or not. If you don't store a field then it is searchable, but is not returned in the resulting set. This is used when you are using Lucene as a search index only and not also a database. An easy to digest explanation of Lucene storage attributes is available from<a href="http://stackoverflow.com/questions/650643/lucene-indexing-store-and-indexing-modes-explained"> this stack overflow post</a>:</li>
	<li><strong>Analyze</strong> - This represents whether we want to tokenize this field. This is used for if we're searching for substring matches on this fields. Non-tokenized values can only be searched by using the exact match.</li>
	<li><strong>TrimAmount</strong> - For our purposes we didn't want to store enormous fields for each document. This field, if its set, let us indicate to store only X characters in a field</li>
	<li><strong>NumericField</strong> - Lucene supports string fields and numeric fields. If our field was numeric we wanted to make sure we tagged it so we knew what to do with it later</li>
</ul>
Now that we have a way to tag data we need a way to use this metadata. We created a base class that objects can inherit from called <code>LuceneDocument</code> which was responsible for using this metadata and constructing a Lucene Document that we could insert into the engine.

Before we show the class I want to mention that you will see a call to a function named <code>LuceneUtils.ConvertDate</code>. That's because Lucene doesn't have a concept of dates: it only stores strings or numerals. I personally found the Lucene searching on numerals to be extremely difficult. In order to store dates and to easily search them (without having to tokenize them) we convert them into a known string representation. We use the same utility to insert dates into the engine and returns dates from our query. This gives us the ability to search by date ranges since Lucene can search for ranges using a <code>TermRangeFilter</code> as well as allowing Lucene to return to us sorted entries by date string.

[csharp]
public static class LuceneUtils
{
    public static String ConvertDate(DateTime date)
    {
        return date.ToString(&amp;quot;yyyyMMddHHmmss&amp;quot;);
    }

    public static DateTime ConvertDate(String date)
    {
        var year = Convert.ToInt32(date.Substring(0, 4));
        var month = Convert.ToInt32(date.Substring(4, 2));
        var day = Convert.ToInt32(date.Substring(6, 2));
        var hour = Convert.ToInt32(date.Substring(8, 2));
        var minute = Convert.ToInt32(date.Substring(10, 2));
        var second = Convert.ToInt32(date.Substring(12, 2));

        return new DateTime(year, month, day, hour, minute, second);
    }
}
[/csharp]

Back to the document base class: it looks nasty, but that's just because we're iterating over the object and finding the attributes associated with each piece of data. I always think reflection code looks nasty, but its really just a bunch of linq. The idea is that once we're done we'll get a document with fields that map to the same members of our class! Lets break this class down a little. Here is the high level overview:

[csharp]
public abstract class LuceneDocument
{
    public virtual List Fields
    {
        get
        {
            // returns a list of fields of this object that are tagged with LuceneSearchFieldAttribute
        }
    }

    public virtual Document Document
    {
        get
        {
            //return a new document with fields populated from this object's tagged methods
        }
    }

    private void AddStringField(Document doc, string name, object value, LuceneSearchFieldAttribute attribute, Field.Store store, Field.Index analyzed)
    {
        // add a string field to the document
    }

    private void AddNumericField(Document doc, string name, object value, Field.Store store, Field.Index analyzed)
    {
       // add a numeric field to the document
    }
}
[/csharp]

Lets fill in some of the blanks here. The fields method is relatively straight forward:

[csharp]
private List _fields;
public virtual List Fields
{
    get
    {
        if(_fields == null)
        {
            // get all the public properties of the object who are tagged with field names
        var publicProperties = GetType().GetProperties(BindingFlags.Public | BindingFlags.Instance |
                                                        BindingFlags.OptionalParamBinding |
                                                        BindingFlags.CreateInstance).ToList();

        List taggedMethods = publicProperties.Where(prop =&amp;gt; prop.GetCustomAttributes(typeof (LuceneSearchFieldAttribute), false).FirstOrDefault() != null).ToList();

            _fields = taggedMethods.Select(p =&amp;gt; p.Name).ToList();
        }
        return _fields;
    }
}
[/csharp]

We're using reflection here (we do this a lot!) to find the fields that are tagged with our attribute. Nothing too crazy.

The <code>Document</code> function is the bulk of the work: find the field, determine the type, get the data out of the object and into the field, add the field to the document.

[csharp]
public virtual Document Document
{
    get
    {
        var doc = new Document();

        // get all the public properties of the object who are tagged with field names
        var publicProperties = GetType().GetProperties(BindingFlags.Public | BindingFlags.Instance |
                                                        BindingFlags.OptionalParamBinding |
                                                        BindingFlags.CreateInstance).ToList();

        List taggedMethods = publicProperties.Where(prop =&amp;gt; prop.GetCustomAttributes(typeof (LuceneSearchFieldAttribute), false).FirstOrDefault() != null).ToList();

        // lunene likes it when things are added in the same order, so lets do that
        // we'll just create an anonymous object here to track our property name, value, and the attribute values
        var methods = from i in taggedMethods.Select(p =&amp;gt; new
                      {
                           name = p.Name,
                           value = p.GetValue(this, null),
                           attribute = p.GetCustomAttributes(typeof (LuceneSearchFieldAttribute), false)
                                .FirstOrDefault() as LuceneSearchFieldAttribute
                      })
                      orderby i.name
                      select i;

        // for each method lets extract the attribute information and add them to the dumcnet
        foreach (var method in methods)
        {
            Field.Store store = method.attribute.Store ? Field.Store.YES : Field.Store.NO;
            Field.Index analyzed = method.attribute.Analyze ? Field.Index.ANALYZED : Field.Index.NOT_ANALYZED;

            bool useNumericField = method.attribute.NumericField;

            if(useNumericField)
            {
                AddNumericField(doc, method.name, method.value, store, analyzed);
                continue;
            }

            AddStringField(doc, method.name, method.value, method.attribute, store, analyzed);
        }

        return doc;
    }
}
[/csharp]

The last two methods are basically just pulling a string or integer out of the object and creating a field. The string method checks if the field is a date and if so uses the known date to string storage pattern that we'll use later.

[csharp]
private void AddStringField(Document doc, string name, object value, LuceneSearchFieldAttribute attribute, Field.Store store, Field.Index analyzed)
{
    string stringVal;
    if (value is DateTime)
    {
        stringVal = LuceneUtils.ConvertDate((DateTime)value);
    }
    else
    {
        stringVal = value != null ? value.ToString() : &amp;quot;&amp;quot;;
    }

    if (!NullValue.IsNull(attribute.TrimAmount))
    {
        var trimAmount = attribute.TrimAmount;
        var trimNeeded = stringVal.Length &amp;gt; trimAmount;

        if (trimNeeded)
        {
            stringVal = stringVal.Substring(0, trimAmount) + &amp;quot; ...trimmed&amp;quot;;
        }
    }

    if (attribute.ForceToLower)
    {
        stringVal = stringVal.ToLowerInvariant();
    }

    var field = new Field(name, stringVal, store, analyzed);
    doc.Add(field);
}

private void AddNumericField(Document doc, string name, object value, Field.Store store, Field.Index analyzed)
{
    var numericField = new NumericField(name, store, analyzed == Field.Index.ANALYZED ? true : false);
    numericField.SetIntValue((int)value);
    doc.Add(numericField);
}
[/csharp]

Now we've created a small base class that we can inherit from that maps our current object into a Lucene Document.  We can add more Lucene specific configuration via the attributes if we want to later.
<h1>Getting Data Out</h1>
Since we have a way to create a document and insert it, we need a way to transfer a document back into our object. What we created here is a static class with a single public function called <code>Populate</code>. This takes a Lucene Document class and a generic type T and uses reflection to populate the generic types fields whose names match the fields in the Document. This makes it easy for us to use typed objects with Lucene. As long as the Lucene database has documents with fields that match our objects, then we can just call <code>LuceneUtils&lt;T&gt;.Populate(LuceneDocument)</code>. This does the reverse logic of <code>LuceneDocument</code> base class. It knows what to do with certain data types such as Guids, enumerations, and Dates in order to re-fill our objects.

The class has three functions: <code>Populate</code>, <code>GetFields</code>, and <code>SetFields</code>. <code>GetFields</code> creates a Dictionary of the available fields from the lucene document and maps them to their value. <code>SetFields</code> goes through the target item and sets its properties based on name. <code>Populate</code> just puts it all together.

[csharp]
public static class LuceneUtils&amp;lt;T&amp;gt; where T : new()
{
    public static T Populate(Document doc)
    {
        var val = new T();
        SetFields(GetFields(doc), val);
        return val;
    }

    private static Dictionary GetFields(Document doc)
    {
        var fields = doc.GetFields();

        var returnDict = new Dictionary();

        foreach (Field field in fields)
        {
            returnDict[field.Name()] = doc.Get(field.Name());
        }

        return returnDict;
    }

    private static void SetFields(Dictionary fields, Object obj)
    {
        // get all the public properties of the object who are tagged with field names
        var publicProperties = obj.GetType().GetProperties(BindingFlags.Public | BindingFlags.Instance |
                                                        BindingFlags.OptionalParamBinding |
                                                        BindingFlags.CreateInstance)
            .Where(prop =&amp;gt; prop.GetCustomAttributes(typeof(LuceneSearchFieldAttribute), false) != null);

        var methods = publicProperties.Select(p =&amp;gt; p.Name);

        foreach (var method in methods)
        {
            if (fields.ContainsKey(method))
            {
                object val = fields[method];
                PropertyInfo property = publicProperties.Where(p =&amp;gt; p.Name == method).FirstOrDefault();

                object newValue;

                if (property.PropertyType.IsEnum)
                {
                    newValue = Enum.Parse(property.PropertyType, val.ToString(), true);
                }
                else
                {
                    if (property.PropertyType == typeof(DateTime))
                    {
                        newValue = LuceneUtils.ConvertDate(val as String);
                    }
                    else if (property.PropertyType == typeof(Guid))
                    {
                        newValue = new Guid(val.ToString());
                    }
                    else
                    {
                        newValue = Convert.ChangeType(val, property.PropertyType, null);
                    }
                }
                property.SetValue(obj, newValue, null);
            }
        }
    }
}
[/csharp]

<strong>Putting it all together</strong>

Now that there is a way to create a lucene document and a way to extract a lucene document lets see how we can use this. Lets make an object that represents our data:

[csharp]
public class User : LuceneDocument
{
   [LuceneSearchField]
   public string Name { get; set; }

   [LuceneSearchField(NumericField = true)]
   public int Age { get; set; }
}
[/csharp]

In order to use this object now all we have to do is add it to an open index

[csharp]
var user = new User
{
    Name = &amp;quot;Anton&amp;quot;,
    Age = 28
};

IndexWriter.AddDocument(user.Document);
[/csharp]

Where IndexWriter is an open index of type <code>Lucene.Net.IndexWriter</code>.

We've basically made it so we don't have to manually deal with mapping a lucene document result back into our target data type. For each item in our return query we can use the previous utilities to extract out our user object!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>21</wp:post_id>
		<wp:post_date><![CDATA[2012-08-21 14:39:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-08-21 14:39:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dotnet"><![CDATA[.NET]]></category>
		<category domain="post_tag" nicename="csharp"><![CDATA[C#]]></category>
		<category domain="post_tag" nicename="lucene"><![CDATA[Lucene]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
		<category domain="post_tag" nicename="utilitiy"><![CDATA[utilitiy]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[878115048]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>YUV to RGB conversion</title>
		<link>https://onoffswitch.net/?p=176</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=176</guid>
		<description></description>
		<content:encoded><![CDATA[There isn't clear much documentation on how to convert between different <a href="http://en.wikipedia.org/wiki/Color_space">colorspaces</a> on the internet, at least not that I've found. It's a little bit like voodoo. You can pick up some information here, or some information there, but in the end its up to you to piece it all together. At some point we had an issue where we had a raw YUV frame and needed to convert a section of it to RGB. This kind of problem arises because sometimes the input format isn't the format you want to work with (either a library wants a different colorspace, or an algorithm is easier to work with in a different colorspace) so its worth knowing how to convert them. Unfortunately, the conversion isn't lossless so keep that in mind when choosing a primary colorspace for your work.
<h2>YUV vs RGB</h2>
YUV and RGB are two different ways of encoding a visual pixel. They are known as colorspaces. In fact there are many different kinds of colorspaces: <a href="http://www.fourcc.org/yuv.php#Packed YUV Formats">packed</a> and <a href="http://www.fourcc.org/yuv.php#Planar YUV Formats">planar</a> for both <a href="http://www.fourcc.org/yuv.php">yuv</a> and <a href="http://www.fourcc.org/rgb.php">rgb</a> subsets. All you really need to know about this is that they are different ways of putting the bytes together.

In terms of what the acronyms stand for I think most everyone knows what RGB is. RGB is represented by one byte for the red color, one byte for the blue color, and one byte for the green color. Together this forms the representation of a single pixel. YUV, while not as often talked about, is just as common. In fact this colorspace is frequently encountered when grabbing raw frames from webcams or hardware capture cards. YUV is represented by
<ul>
	<li><strong>Y</strong> - One byte for the <a href="http://en.wikipedia.org/wiki/Luma_(video)">luminance</a>. This is how bright the color is.</li>
	<li><strong>UV</strong> - Two bytes for the U and V, which are known as chroma. <a href="http://en.wikipedia.org/wiki/Chrominance">Wikipedia</a> has the best explanation for this: "Chrominance is usually represented as two color-difference components: U = B' − Y' (blue − luma) and V = R' − Y' (red − luma)"</li>
</ul>
<h2>Packed vs Planar</h2>
Packed formats put the y, u, and v (or r,g,b) bytes together linearly in an array in some known order (depending on colorspace type). You can usually access them like

[c language="++"]
int index = 0;
byte y = rawFrameData[index];
byte u = rawFrameData[index + 1];
byte v = rawFrameData[index + 2];
[/c]

And as you increment your index you get the pixel for the row/column as you iterate through your array. The array, while linear, usually represents rows and columns flattened. 

Here is what a packed format looks like for UYVY (a type of YUV format):

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/08/uyvy.gif"><img class="alignnone size-full wp-image-213" src="http://tech.blinemedical.com/wp-content/uploads/2012/08/uyvy.gif" alt="" width="600" height="100" /></a>

In terms of packing formats, planar is more complicated, since it represents all the y's first, then all the u's, then all the v's. This means to access the array you need to flatten what the matrix is into a 1 dimensional array and calculate the offsets. If you look at the charts in fourcc's site, it shouldn't be too hard to figure out how to calculate the offsets for a certain pixel at row X column Y.

Here is what planar looks like represented as a matrix. However, in the actual data array all of the Y bytes are first, then all the U bytes, then all the V bytes so you have to do a little bit of pointer arithmetic to line up what is a Y and a U and a V.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/08/yplane.gif"><img class="alignnone size-full wp-image-214" src="http://tech.blinemedical.com/wp-content/uploads/2012/08/yplane.gif" alt="" width="200" height="127" /></a> <a href="http://tech.blinemedical.com/wp-content/uploads/2012/08/u2plane.gif"><img class="alignnone size-full wp-image-212" src="http://tech.blinemedical.com/wp-content/uploads/2012/08/u2plane.gif" alt="" width="200" height="127" /></a> <a href="http://tech.blinemedical.com/wp-content/uploads/2012/08/v2plane.gif"><img class="alignnone size-full wp-image-215" src="http://tech.blinemedical.com/wp-content/uploads/2012/08/v2plane.gif" alt="" width="200" height="127" /></a>

<h2>Convert YUV to RGB</h2>
Unfortunatley, converting between the colorspaces is not trivial. Even <a href="http://www.fourcc.org/fccyvrgb.php">fourcc</a> doesn't seem to have a conclusive answer to what is the right formula. I'm not an image expert and I'm not going to pretend like I understand the matrix transformations involved in colorspace conversions but here is what worked for me:

[c language="++"]
/************************************************************************/
/* Calculate RGB values from input YUV values		                    */
/************************************************************************/
RgbPixel YuvFrame::GetRgbPixel(UINT8 currentY, UINT8 currentU, UINT8 currentV) {
	RgbPixel rgbPixel;

	int subY = 16;
	int subU = 128;
	int subV = 128;

	int rY = 298;	int rU = 0; 	int rV = 409;

	int gY = 298;	int gU = 100;	int gV = 128;

	int bY = 298;	int bU = 516;	int bV = 0;

	rgbPixel.r = Clip( (rY*(currentY-subY)+								rV*(currentV-subV)) &amp;gt;&amp;gt; 8	);
	rgbPixel.g = Clip( (gY*(currentY-subY) - gU*(currentU-subU) -		gV*(currentV-subV)) &amp;gt;&amp;gt; 8	);
	rgbPixel.b = Clip( (bY*(currentY-subY) + bU*(currentU-subU))		&amp;gt;&amp;gt; 8	);

	return rgbPixel;
}

/************************************************************************/
/* Forces values greater than 255 to be 255								*/
/* And values less than 0 to be 0										*/
/************************************************************************/
UINT8 YuvFrame::Clip(double val) {
	if(val &amp;gt; 255){
		return 255;
	}
	else if(val &amp;lt; 0){
		return 0;
	}
	return (UINT8)val;
}
[/c]

An RgbPixel class is simple a struct that looks like this:

[c language="++"]
struct RgbPixel
{
	unsigned char r;
	unsigned char g;
	unsigned char b;
}
[/c]

For more information go to fourcc.org or visit thedoom9 boards. There is a wealth of knowledge of people who have worked on vlc and other video and image processing libraries that can provide more help.

Images from <a href="http://www.fourcc.org/">fourcc.org</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>176</wp:post_id>
		<wp:post_date><![CDATA[2013-01-14 15:51:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cpp"><![CDATA[C++]]></category>
		<category domain="post_tag" nicename="image"><![CDATA[Image]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[878116333]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Queued Deferred Instantiation in AS3</title>
		<link>https://onoffswitch.net/?p=244</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=244</guid>
		<description></description>
		<content:encoded><![CDATA[In AS3/flex there is the idea of a <a href="http://livedocs.adobe.com/flex/3/html/help.html?content=layoutperformance_05.html">creation policy</a> which determines the way that components are created by their parent containers. For the most part everyone leaves things at "auto", but when you start developing a large application you run into performance issues when too many things are created when they don't have to be.  Child components inherit their creation policy from the parent.

A view stack, for example, does not create any of its children until you actually select to view it. This is known as deferred instantiation, and the viewstack does this for you automatically. However, sometimes you don't have things in a viewstack.  You could use the "queued" creation policy, which creates the first child in a container, then the second, etc, in order.  This works fine assuming everything is in the same container. But what if you want to mimic the queued creation policy but linked through different parent containers, and have some children automatically created before others? For example, imagine you have a (pseudocode) display heirarchy like this

[csharp]
&lt;App&gt;
	&lt;container1&gt;
	  &lt;component1/&gt;
	  &lt;component2/&gt;
	&lt;/container1&gt;
	&lt;container2&gt;
	  &lt;component3&gt;
	  	&lt;item1/&gt;
	  	&lt;item2/&gt;
	  &lt;/component3&gt;
	&lt;/container2&gt;  
&lt;/App&gt;
[/csharp]  
 
How do you make it so that <code>component1</code> shows up first, then <code>item1</code> in <code>component3</code>, then <code>component2</code> in <code>container1</code>, but making sure that the parent containers (<code>container1</code>, <code>container2</code>) are ALREADY created?  You can't set creation policy none and then create the items manually using createComponentsFromDescriptors because that would create everything. Setting creation policy queued would create all of <code>container1</code> and then all of <code>container2</code>.  You really want to create and add a component, then when that component is done create and add the next component, regardless of who is its parent.

For us, we have an internal dashboard panel that we want to visibly structure in a certain way (not in the same parent container) but control which <a href="http://en.wikipedia.org/wiki/Portlet">portlets</a> load first ordered by least processing to most processing. This way the user sees the page build out as fast as possible and isn't left with a loading screen thinking that the app is broken when really its building all its components.

For this situation we created a "QueuedCreation" helper class.  


Here is how it would look:

[csharp]
// create our components in as3
var component1:ComponentType = new ComponentType();
var component2:ComponentType = new ComponentType();
var item1:ItemType = new ItemType();
var item2:ItemType = new ItemType();

// create a queued creator instance
var queuedCreator:QueuedCreator = new QueuedCreator();

// add(displayObject, container) which will register to add container.addChild(displayObject)
queuedCreator.add(component1, container1);
queuedCreator.add(item1, container2);
queuedCreator.add(component2, container1);
queuedCreator.add(item2, container2);
[/csharp]

At this point we expect that container 1 is already created but has no children, same with <code>container2</code>. Then we'll schedule to add component1 to <code>container1</code>. When <code>component1</code> is created and added we'll then add item1 to container2.  After item1 is added we'll add <code>component2</code> to <code>container1</code>, followed by <code>item2</code> to <code>container2</code>.  

This will give the appearance of the components loading one after the other. You can even have canvases in the containers already that show something like "Loading" and hide the loading when all the items are loaded.

To start the process just do:

[csharp]
queuedCreator.start();
[/csharp]

And it will fire off the creation in the order you requested.  We've exposed a couple events to tell you when something was created and when the process is complete as well. Here is the full class:
  
[csharp]
[Event(name=&quot;itemCreated&quot;, type=&quot;flash.events.Event&quot;)]
[Event(name=&quot;itemsComplete&quot;, type=&quot;flash.events.Event&quot;)]
public class QueuedCreator extends EventDispatcher
{
	public function QueuedCreator()
	{
	}

	private var childCount:int = 0;

	private var creationOrder:ArrayCollection = new ArrayCollection();

	public function add(displayObject:DisplayObject, parent:Container):void{
		displayObject.addEventListener(FlexEvent.UPDATE_COMPLETE, childCompleteHandler, false, 0, true);

		creationOrder.addItem(function():void{
			parent.addChild(displayObject);
		});
	}

	private function buildChildren():void
	{
		creationOrder[childCount]();
		childCount++;
	}

	private function childCompleteHandler(event:FlexEvent):void
	{
		dispatchEvent(new Event(&quot;itemCreated&quot;))
		if (childCount &lt; creationOrder.length)
		{
			buildChildren();
		}
		else{
			dispatchEvent(new Event(&quot;itemsComplete&quot;))
			reset();
		}
	}

	private function reset():void
	{
		childCount = 0;
		creationOrder.removeAll();
	}

	public function start():void{
		if(creationOrder != null &amp;&amp; creationOrder.length &gt; 0){
			buildChildren();
		}
	}
}
[/csharp]

Something to keep in mind is that because we are using weak references to the event handlers in the <code>QueuedCreator</code> class that you should make sure to hold a reference to the queued creator from your class otherwise the object could get garbage collected and not finish.  The alternative is to modify the class and make the event handler a strong link, but then you'll have extra linked handlers that require more cleanup.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>244</wp:post_id>
		<wp:post_date><![CDATA[2012-10-09 09:39:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="as3"><![CDATA[AS3]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Maintain div open status across asp.net postbacks</title>
		<link>https://onoffswitch.net/?p=317</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=317</guid>
		<description></description>
		<content:encoded><![CDATA[I ran into a situation where I was using jquery to open and close a collection of divs on an aspx page that changes its state based on postback data.  Unfortunately when you do a postback the div open/close status is lost and that makes the page feel inconsistent and jerky.  I came up with a simple solution to track the div status in a hidden input field that the backing asp.net page knows about, and on post-back re-applying a styling to any previously open divs. 

First we need a place to store the ID's of the open divs. I've added a hidden input field to the bottom of my page that accomplishes that. In the aspx page I have:

[csharp]
&lt;asp:HiddenField ID=&quot;openDivs&quot; runat=&quot;server&quot;/&gt;
[/csharp]

This way I can reference the values in <code>openDivs</code> later from the backing code.

Each div which we need to collapse and expand has "hideAbleClass" assigned to it and is by default closed.  I also wrote a javascript a function named <code>toggle</code> that is responsible for opening and closing the divs, as well as updating the backing <code>openDivs</code> field with which divs are open. The logic is simple: first we clear out the <code>openDivs</code> hidden input value, then iterate over all the tagged divs on the page and update their Id's (corresponding to their backing asp.net id's) into a semicolon delimited list in the hidden field.

For the initial css, it's important that all the div's be initialized style with the class level css so that they default to <code>display:none</code> since we'll end up overriding their class stylings with inline styles later.  Defining a class styling also lets us fall back to a class style when we remove an inline style.

[css]
&lt;style type=&quot;text/css&quot;&gt; 
    .hideAbleClass
    {
        display:none;    
    }
&lt;/style&gt;
[/css]

Here is a sample panel that would use the <code>toggle</code> function

[html]
&lt;a class=&quot;clickToggleDivClass&quot; onclick=&quot;toggle('_panelID')&quot;&gt;&lt;h1&gt;Click here to open this div!&lt;/h1&gt;&lt;/a&gt;	
&lt;asp:Panel ID=&quot;_panelID&quot; runat=&quot;server&quot; Visible=&quot;true&quot; CssClass=&quot;hideAbleClass&quot;&gt;
    &lt;!-- content --&gt;
&lt;/asp:Panel&gt;
[/html]

When we click the link we pass the <code>_panelID</code> to the toggle function and it handles maintaining the status of open div's for us.

[js]
function toggle(itemId) {
    $('#' + itemId).slideToggle('fast',
        function () {
            var openDivValue = &quot;&quot;;

            $(&quot;#openDivs&quot;).val(openDivValue);

            $(&quot;.hideAbleClass&quot;).each(function (item) {
                if ($(this).is(&quot;:visible&quot;)) {
                    openDivValue +=$(this).attr(&quot;id&quot;) +&quot;;&quot;;
                }
            });

            $(&quot;#openDivs&quot;).val(openDivValue);
        });
}
[/js]

Now we have a page level storage of what is going on with all our open div's.  Anything that is shown gets added to the hidden input and all we have to do is on a postback re-apply the display css to any divs. 

[csharp]
protected void Page_Load(object sender, EventArgs e)
{
    if(IsPostBack)
    {
        MaintainOpenPositions(FindControl(&quot;parentContainer&quot;));
    }
}

/// &lt;summary&gt;
/// Keeps divs open if they were previously open on a postbakc
/// otherwise resets their display field to revert back to their css class default
/// &lt;/summary&gt;
/// &lt;param name=&quot;findControl&quot;&gt;&lt;/param&gt;
private void MaintainOpenPositions(Control findControl)
{
    var openDivList = openDivs.Value.Split(new[]{';'}, StringSplitOptions.RemoveEmptyEntries);

    foreach (Control c in findControl.Controls)
    {
        if (c is Panel)
        {
            MaintainOpenPositions(c);

            var panel = c as Panel;
            if(!CollectionUtil.IsNullOrEmpty(openDivList) &amp;&amp; 
                        openDivList.Any(openId=&gt; openId.Contains(panel.ID.ToLowerInvariant(), 
                                                 StringComparison.InvariantCultureIgnoreCase)))
            {
                panel.Attributes.Add(&quot;style&quot;, &quot;display:block&quot;);
            }
            else
            {
                panel.Attributes.Add(&quot;style&quot;, &quot;display:&quot;);
            }
                    
        }
    }
}
[/csharp]

You can see that on page load we look for all controls under a div called <code>parentContainer</code>. I did this so I can encapsulate the open/close state maintaining to only work for a subset of the page. In <code>MaintainOpenPositions</code> we recursively search the page for all panel's and find if the panel ID is in the list of openDiv's that we tracked in the hidden field. If it is, we want to set the inline css style to <code>display:block</code> which will show the panel, otherwise we reset the styling to be an empty string (<a href="http://stackoverflow.com/questions/3522643/how-to-revert-back-to-normal-after-displaynone-for-table-row"><code>display:</code></a>) which lets the div use class level css.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>317</wp:post_id>
		<wp:post_date><![CDATA[2012-09-04 14:14:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="asp-net"><![CDATA[asp.net]]></category>
		<category domain="post_tag" nicename="csharp"><![CDATA[C#]]></category>
		<category domain="post_tag" nicename="js"><![CDATA[JavaScript]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Dealing with SQLite&#039;s limit on maximum number of variables</title>
		<link>https://onoffswitch.net/?p=338</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=338</guid>
		<description></description>
		<content:encoded><![CDATA[SQLite and other database engines allow you to construct a sql clause searching for all items in a range, looking something like:

[sql]
SELECT * From Table 
Where TableItems In (1, 2, 3, 4);
[/sql]

This is great since now you can pull a bunch of elements back in a single SQL call, minimizing the time spent doing work on disk.  The fewer SQL calls you can make the better, so its best to pull a lot at once. But, careless coding can lead to building SQL like this:

[csharp]
List&lt;Users&gt; GetUsers(List&lt;int&gt; userIds)
{
    var sql = @&quot;SELECT * From USERS where UserID IN (&quot; + userIds.ToCommaList() + &quot;)&quot;;
    return GetData.ExecuteSql(sql);
}
[/csharp]

Looks fine, but what happens as your software grows and now that passed in list of <code>userIds</code> gets pretty big?  Unforutnately SQLite has a default limitation on the <a href="http://www.sqlite.org/limits.html">maximum number</a> of variables that a SQL statement can have, and if you encounter this you'll get this fun message

<blockquote>
too many SQL variables
</blockquote>

So what do we do? You may think to loop over your sql calls and do it one by one, but that'll just slow things down considerably. You could increase the default parameter limit in sqlite (<code>SQLITE_MAX_VARIABLE_NUMBER</code>), but that also comes at the risk of using more memory since that limit is there for a reason:

<blockquote>To prevent excessive memory allocations, the maximum value of a host parameter number is SQLITE_MAX_VARIABLE_NUMBER, defaults to 999</blockquote>

A simple solution if all you are doing is pulling back data using "IN" statements and if you don't need any dependent data (such as where clauses relating to data you are pulling back) is to break the call up into separate calls. So you can do 

[sql]SELECT * From Tables Where Items IN (1-100)[/sql]

and then add the result of that to

[sql]SELECT * From Tables Where Items IN (101-200)[/sql]

etc.

I wanted to generalize this and wrap all my basic sql getters in something that could do this for me. The idea was I wanted something like this

[csharp]var totalList = BuildAndAppend(SourceList, (subset) =&gt; ReturnsListUsingDataFromSubset(subset));[/csharp]

<code>SourceList</code> could be anything, for example it could be a List of TableItems (sticking with the generic example).  The ReturnListFunction would take a <code>List</code> and return lets say a list of Table objects.  The build and append would be responsible for stitching it all together.  This way we've seperated out the problem from our internal storage calls so we don't have to worry about this when writing the SQL.

Here is the BuildAndAppend function

[csharp]
/// &lt;summary&gt;
/// Passes max items from the source subset to the supplied builder function
/// and aggregates the results
/// &lt;/summary&gt;
/// &lt;typeparam name=&quot;T&quot;&gt;&lt;/typeparam&gt;
/// &lt;typeparam name=&quot;Y&quot;&gt;&lt;/typeparam&gt;
/// &lt;param name=&quot;source&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;builder&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;max&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static List&lt;T&gt; BuildAndAppend&lt;T, Y&gt;(List&lt;Y&gt; source, Func&lt;List&lt;Y&gt;, List&lt;T&gt;&gt; builder, int max = 900)
{
    if (source.Count() &lt; max)
    {
        return builder(source);
    }

    // create a local copy of the list so to not modify the original
    var list = new List&lt;Y&gt;(source);

    // create a list to store the resulting set
    var items = new List&lt;T&gt;(source.Count());

    // take up to the first max items
    var subItems = list.TakeAndRemove(max).ToList();

    // while we have pending items, keep building new ones and
    // append to the resulting list
    while (subItems.Count() &gt; 0)
    {
        items.AddRange(builder(subItems));

        subItems = list.TakeAndRemove(max).ToList();
    }

    return items;
}
[/csharp]

It uses an extension method called <code>TakeAndRemove</code> that is this:

[csharp]
public static IEnumerable&lt;T&gt; TakeAndRemove&lt;T&gt;(this List&lt;T&gt; source, int val)
{
    if(source.Count() &lt; val)
    {
        var copy = source.ToList();
        source.Clear();
        return copy;
    }

    var found = source.Take(val).ToList();
    source.RemoveRange(0, found.Count() - 1);
    return found;
}
[/csharp]

As a final example, assume we have a function like

[csharp]
public List&lt;Tables&gt; GetTables(List&lt;TableItems&gt; sourceItems)
{
    var sql = &quot;SELECT * From Tables where TableItems IN  ( &quot; + GetTableItemIdsAsParameters(sourceItems) + &quot;)&quot;;
    return BuildListFromSql(sql);
}
[/csharp]

Which by itself returns to you a list of tables based on the id's of the source table items.  Then we can use our new <code>BuildAndAppend</code> function like this without having to worry about how large the table items list is

[csharp]
List&lt;Tables&gt; tables = BuildAndAppend(sourceItems, GetTables);
[/csharp]

While this does work, the best solution for this problem would be to understand your dataset and to make sure you aren't pulling back more than you need to. But if you do need to pull back in clause sql with over 999 items then you can use this function to do it in large chunks. This certainly beats looping 999+ times and allows your application to not crash if it does encounter large sets.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>338</wp:post_id>
		<wp:post_date><![CDATA[2013-02-27 15:40:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="post_tag" nicename="sqlite"><![CDATA[SQLite]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Shared netduino lcd writer</title>
		<link>https://onoffswitch.net/?p=376</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=376</guid>
		<description></description>
		<content:encoded><![CDATA[I was playing with my <a href="http://netduino.com/">netduino</a> where I had multiple threads running doing different work and I wanted to have some of those threads write to an attached <a href="https://www.sparkfun.com/products/9395">lcd panel</a>.  What I needed was a shared <a href="http://en.wikipedia.org/wiki/Singleton_pattern" target="_blank" rel="noopener noreferrer">singleton</a> that managed the competing resources for the lcd.  Just because it's running on a microprocessor doesn't mean you can't use a high level architectural design pattern.  I wrote a simple LcdWriter singleton that synchronizes requests to the serial port and now I can re-use this in other app's that I write.

So now I need to do if I want to write to the hooked up LCD is 

[csharp]
LcdWriter.Instance.Write(&quot;foo&quot;);
[/csharp]

There's really nothing too fancy about this. I spawn a new thread when the singleton fires up and it waits on a mutex to pulse it and tell it that it has data.  When it runs it locks on a shared lock object, clears the screen, resets the cursor position, and outputs whatever is stored in the <code>_lcdDisplay</code> field.  The only downside here is that I didn't expose a way to control which port your lcd has to be connected to. If we wanted to we could take an lcd port enum with the write function and have this singleton be a facade on top of another class that does the threadsafe work per serial port.

[csharp]
public class LcdWriter
{
    #region Data

    private SerialLcd _serialInterface;
    private static object _lockObject = new object();

    private static string _lcdDisplay = string.Empty;
    private static AutoResetEvent mutex = new AutoResetEvent(false);

    private static LcdWriter _instance;

    #endregion

    #region Singleton and Constructor 

    public static LcdWriter Instance
    {
        get
        {
            lock (_lockObject)
            {
                return _instance ?? (_instance = new LcdWriter());
            }
        }
    }

    private LcdWriter()
    {
        _serialInterface = new SerialLcd(SerialPorts.COM2);

        ThreadUtil.Start(() =&gt;
        {
            while (true)
            {
                _serialInterface.ClearDisplay();
                _serialInterface.SetCursorPosition(1, 1);

                lock (_lockObject)
                {
                    _serialInterface.Write(_lcdDisplay);
                }

                mutex.WaitOne();
            }
        });
    }

    #endregion


    #region Writer

    public void Write(string text)
    {
        lock(_lockObject)
        {
            _lcdDisplay = text;
            mutex.Set();
        }
    }

    #endregion
}
[/csharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>376</wp:post_id>
		<wp:post_date><![CDATA[2012-11-19 09:56:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="csharp"><![CDATA[C#]]></category>
		<category domain="post_tag" nicename="netduino"><![CDATA[netduino]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Iterating zip&#039;s within zip&#039;s in memory</title>
		<link>https://onoffswitch.net/?p=491</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=491</guid>
		<description></description>
		<content:encoded><![CDATA[Sometimes you don't want to unzip a file to disk just to access it's internals. This is pretty easy usually with most c# zip libraries, but what gets more complicated is when you start having zip's within <a href="http://qkme.me/3qw724">zip's within zip's</a> and you want to recursively iterate through everything in memory! Or maybe you want to search for something and only extract out the one file that you care about.

We ran into this situation a few times and decided to abstract out iterating over hierarchical data structures. This way all we need to do is know if an item is a file and we can get its data stream, as well as if anything has any children (such as files in a zip or files in a directory) without having to care about what is in the node.

For this library we've defined the concept of a <code>FileSystemNode</code>. All nodes derive from <code>FileSystemNode</code> and the base class is responsible for resolving what kind of sub node type it can be. For example, if the file is a zip file, it will resolve the "node" to be a <code>ZipContainer</code>. If the file is a directory it'll resolve to <code>RealDirectory</code>, etc. Now we have a unified API for working with heirarchal data. We can easily add more hierarchical types for custom data containers if we wanted to later.

What we have in common are the following functions:

<ul>
	<li><code>Children</code> - Returns a list of any defined children - (this would be files in a directory, or files inside of a zip)</li>
	<li><code>IsDirectory</code> - <em>Tells us if the node is a directory</em> (this is true for zip files and directories, but not for files inside of a zip that aren't also zips)</li>
	<li><code>IsFile</code> - Tells us if the node is a file</li>
	<li><code>InputStream</code> - Gives us the input stream of whatever node we're on. If this is a directory it gives us nothing, but if its a zip file or regular file (on disk) it'll extract the data and return that as a stream</li>
	<li><code>Name</code> - The name of the item</li>
	<li><code>GetFile(string path)</code> - Returns to use a file of the same path (whether its a file in a directory or a file name in a zip archive)</li>
	<li><code>GetFiles</code> - Retuns us a list of file system nodes searched on the zip or directory</li>
</ul>

Using these basic properties we can get, find, and act on data inside of a folder or a zip. What really makes this nice is that we no longer have to care what we're acting on. If someone passed in a file thats great, if someone passed in a zip we can work with that too. We've decoupled our logic from what we're actually working with and that lets us write simple unified code regardless of what kind of file system item is passed in.

Lets jump into an example:

To create a file system node we can use a static Create method on the base class to resolve the appropriate container and give us a base class reference:

[csharp]
FileSystemNode zipContainer = FileSystemNode.Create(@&quot;../../../Resources/Outer.7z&quot;);
[/csharp]

In this example (which is copied from the supplied unit tests project in our github) resolves a 7z file as a zip container. This particular zip has a structure like this:

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/09/outerzip-e1347562933955.png"><img src="http://tech.blinemedical.com/wp-content/uploads/2012/09/outerzip-e1347562933955.png" alt="" width="601" height="217" class="alignnone size-full wp-image-498" /></a>

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/09/innerzip-e1347562953561.png"><img src="http://tech.blinemedical.com/wp-content/uploads/2012/09/innerzip-e1347562953561.png" alt="" width="601" height="195" class="alignnone size-full wp-image-497" /></a>

And since we've resolved <code>zipContainer</code> we can list the contents of it using <code>zipContainer.Children</code>. That will give us:

<em>File1.txt
InnerZip.7z</em>

But remember, we haven't unzipped this, this is all in memory.  Internally we're iterating over the <code>ArchiveFileInfo</code> collection of a zipped file and continuously resolving the child components. If it had been a directory we would've listed the files in the directory under the hood, but the abstraction is the same.

[csharp]
public override IEnumerable&lt;FileSystemNode&gt; Children
{
    get
    {
        // When reading the .zip's contents, attempt to resolve any zipped file
        // in case it is also a container (ex. nested zip)
        return (Zip.ArchiveFileData.Select(
            entry =&gt; ResolvePossibleContainer(GetContent(entry))));
    }
}
[/csharp]

In the example zip I posted above, if we want to go into the file <code>InnerZip.7z</code> we could just recurse through the outer zip's children's property without having to worry about extracting out any subzips. It's all done in memory so its fast and cheap (assuming your zip isn't enormous). Even if you have a large zip file, if you are only looking to extract certain items (that aren't within zips themselves) you won't have to extract all the other miscellaneous files.  

Now, lets say we want to find out what the data is in one of our file system node items is.  When we have an item we can just access its InputStream object and internally would do this:

[csharp]
using (var stream = new StreamReader(child.InputStream))
{
    Console.WriteLine(&quot;{0} contents: {1}&quot;, child.Name, stream.ReadToEnd());
}
[/csharp]

Where <code>child</code> is a <code>FileSystemNode</code>. At this point it doesn't matter if the child is a zip entry, or a file, the result is the same.  

In practice I've only used this when I have zip's and it's made my life so much easier not having to care about zips within zips within zips. Realistically though I haven't found it that useful or realistic to be mixing directories, files, zips all in one, but there's nothing to say that couldn't happen. If you had some other mixed sort of file system data all you'd need to do is create a new subclass of <code>FileSystemNode</code> and the abstraction layer would work just fine.

Check the full source and tests at our <a title="File System Traversal source" href="https://github.com/blinemedical/FileSystemTraversal" target="_blank" rel="noopener noreferrer">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>491</wp:post_id>
		<wp:post_date><![CDATA[2012-09-17 17:52:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[iterating-zips-within-zips-in-memory]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[841752010]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Throttled Object</title>
		<link>https://onoffswitch.net/?p=639</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=639</guid>
		<description></description>
		<content:encoded><![CDATA[I wanted to share a little utility class I have been finding use for recently.  All it does is take care of caching an item for a certain period of time and when the time has expired and the item is requested it can either build out a new item for you (using a supplied builder function) or lazily build one out for you (return the last cache and queue into the threadpool the builder to re-create the cache).

I like using this when I'm requesting data that can block and I don't particularly care if I'm a second or two out of date. Here is an example unit test. 

[csharp]
public void TestThrottle()
{
    var throttle = new ThrottledObject&lt;TestItem&gt;(TimeSpan.FromMilliseconds(2000), Generator);

    Observable.Interval(TimeSpan.FromMilliseconds(500))
        .Subscribe(_ =&gt;
                        Console.WriteLine(&quot;[now: {0}] - Last time generated: {1}&quot;,
                                                    DateTime.Now,
                                                    throttle.LazyGetItem.TimeGenerated));

    Thread.Sleep(TimeSpan.FromSeconds(10));
}
[/csharp]

I create a throttled object and say that we won't re-generate the object (<code>TestItem</code>) any faster than every 2000ms.  If we request it after the last period then we'll re-execute the generator lazily (in a thread pool).  Then we'll use an Rx observable to execute my subscribed function every 500ms and see when was the last time our throttled item was generated.

The generator function is this:

[csharp]
public TestItem Generator()
{
    Thread.Sleep(TimeSpan.FromSeconds(1));
    return new TestItem
                {
                    TimeGenerated = DateTime.Now
                };
}
[/csharp]

And our <code>TestItem</code> class is just a simple stub for the test

[csharp]
public class TestItem
{
    public DateTime TimeGenerated { get; set; }
}
[/csharp]

This whole thing spits out:
<code>
[now: 8:09:06 PM] - Last time generated: 8:09:06 PM
[now: 8:09:07 PM] - Last time generated: 8:09:06 PM
[now: 8:09:07 PM] - Last time generated: 8:09:06 PM
[now: 8:09:08 PM] - Last time generated: 8:09:06 PM
[now: 8:09:08 PM] - Last time generated: 8:09:09 PM
[now: 8:09:09 PM] - Last time generated: 8:09:09 PM
[now: 8:09:09 PM] - Last time generated: 8:09:09 PM
[now: 8:09:10 PM] - Last time generated: 8:09:09 PM
[now: 8:09:10 PM] - Last time generated: 8:09:09 PM
[now: 8:09:11 PM] - Last time generated: 8:09:09 PM
[now: 8:09:11 PM] - Last time generated: 8:09:09 PM
[now: 8:09:12 PM] - Last time generated: 8:09:12 PM
[now: 8:09:12 PM] - Last time generated: 8:09:12 PM
[now: 8:09:13 PM] - Last time generated: 8:09:12 PM
[now: 8:09:13 PM] - Last time generated: 8:09:12 PM
[now: 8:09:14 PM] - Last time generated: 8:09:12 PM
[now: 8:09:14 PM] - Last time generated: 8:09:12 PM
[now: 8:09:15 PM] - Last time generated: 8:09:12 PM
[now: 8:09:15 PM] - Last time generated: 8:09:16 PM
[now: 8:09:16 PM] - Last time generated: 8:09:16 PM
</code>

An example of where I used this was in an internal application we have at B-Line Medical where we have to make some time intensive initialization calls when a user logs in.  These calls generate an initialization data object that is sent over the wire.  In order to speed this up I used a ThrottledObject to wrap the initialization data generation so when the user first hits our site we lazily initialize the data and keep it in a static cache for about 10-20 seconds. If the user logs in with a cached authentication token or by manually logging in then we can re-use the cached data object (bypassing the database calls).  If after 20 seconds they don't log in we'll just request a non-lazy created initialization object, but the 90% use case is the log in is fast and we can leverage the time loading the page to do backend work.

It may seem like a micro-optimization but I think its important to get the user up and running in the app as soon as possible given that for our application having some slightly stale initial data is OK.  Making use of "down time" is a cool way to get easy speed up and the throttled object makes it easy for us to do without blocking any other calls.


Full source available on our <a href="https://github.com/blinemedical/Throttled-Object">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>639</wp:post_id>
		<wp:post_date><![CDATA[2012-10-03 13:46:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[throttled-object-2]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[852610159]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Flex, SWF&#039;s, IIS and GZip</title>
		<link>https://onoffswitch.net/?p=910</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=910</guid>
		<description></description>
		<content:encoded><![CDATA[I was recently investigating an issue with a swf preloader not completing properly.  The symptom was that the preloader never thought it loaded all of the data and so sat infinitely trying to complete. 

Oddly only some people were noticing the issue.  I had never seen it, but <a href="http://tech.blinemedical.com/author/dj-larkin/" target="_blank" rel="noopener noreferrer">DJ</a> (our QA manager) kept seeing the problem.  The first thing I do in these situations is to as closely mimic the environment of the user if possible. I was using chrome and DJ was using firefox. I switched over to firefox, cleared my local cache and loaded up the swf. It worked just fine. I reloaded the page and then suddenly I saw the problem.  I kept refreshing and was able to consistently reproduce this issue even though others using firefox never saw the problem and I was never able to see it in chrome or internet explorer.

Since I never saw this when loading the swf off my local machine, I built a debug version of the swf by adding a <code>-debug=true</code> to the swf <a href="http://livedocs.adobe.com/flex/3/html/help.html?content=compilers_14.html">compiler options</a> and tossed this onto the target machine. This way I was able to remotely attach to swf and step through the code.  

I saw that the preloader had downloaded 2,465,651 bytes but was expecting 2,468,887. Strangely enough the swf on disk was the size that the preloader had downloaded so where did these extra bytes come from?  Thankfully when I am debugging these issues I religiously run <a href="http://www.charlesproxy.com/" target="_blank" rel="noopener noreferrer">Charles</a> in the background.  I compared the http request/response of the first working request vs the subsequent failed requests. 

<h1>Successful response</h1>

[csharp]
HTTP/1.1 200 OK
Transfer-Encoding: chunked
Content-Type: application/x-shockwave-flash
Content-Encoding: gzip
Last-Modified: Wed, 26 Sep 2012 20:02:39 GMT
Accept-Ranges: bytes
ETag: &quot;84e548e1219ccd1:0&quot;
Vary: Accept-Encoding
Server: Microsoft-IIS/7.5
X-Powered-By: ASP.NET
Date: Wed, 26 Sep 2012 20:11:05 GMT
[/csharp]

<h1>Failed response</h1>

[csharp]
HTTP/1.1 200 OK
Content-Type: application/x-shockwave-flash
Content-Encoding: gzip
Last-Modified: Wed, 26 Sep 2012 20:02:39 GMT
Accept-Ranges: bytes
ETag: &quot;80d937e1219ccd1:0&quot;
Vary: Accept-Encoding
Server: Microsoft-IIS/7.5
X-Powered-By: ASP.NET
Date: Wed, 26 Sep 2012 20:11:12 GMT
Content-Length: 2468887
[/csharp]

After looking at the two responses I noticed that one had <code>Transfer-Encoding: chunked</code> field and the other had the <code>Content-Length</code> set.

<ul>
<li><a href="http://en.wikipedia.org/wiki/Chunked_transfer_encoding" target="_blank" rel="noopener noreferrer"><code>Transfer-Encoding: chunked</code></a> is a mechanism of downloading content from a server in chunks without knowing how much data is being sent.  The response continues until a final zero length chunk is sent.</li>
<li><a href="http://en.wikipedia.org/wiki/List_of_HTTP_header_fields#content-length-response-header" target="_blank" rel="noopener noreferrer"><code>Content-Length</code></a> tells the client how large the response will be.</li>
</ul>  

So when we didn't have the content length everything worked. Interestingly enough, the content length was also the same size that the flex preloader was expecting.  It wasn't until another co-worker noticed that the content length (2,468,887) was larger than the actual file (2,465,651) that we noticed that the response's <a href="http://www.gzip.org/#intro">gzip</a> encoding had actually inflated the file size.  For this swf the inflation amount was around 0.1%. 

<a href="http://onoffswitch.net/wp-content/uploads/2013/02/swfIncreaseByGzip.png"><img src="http://onoffswitch.net/wp-content/uploads/2013/02/swfIncreaseByGzip-300x248.png" alt="" class="alignnone size-medium wp-image-938" /></a>

Apparently if the http response headers have a content length field then the flex preloader will expect the content length even though though the decompressed file is LESS than the content length and the request had finished.  This particular swf was encoded using the <a href="http://stackoverflow.com/questions/7887660/how-to-enable-lzma-compression-in-flash-player-11">new</a> swf <a href="http://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm">lzma</a> compression and so for fun we tried a regular swf and found that gzip inflated the swf by almost 50%!  A 3MB swf would be 4.5MB if sent over the wire with gzip compression enabled.

The preloader worked the first time because the preloader didn't know how much it wanted, and assumed that once the request was finished that that was how much it should get.  The subsequent times it was relying on the content headers thinking that was how much it was going to get, even though the decompressed response content was less.

Once we realized that it was <a href="http://www.microsoft.com/technet/prodtechnol/WindowsServer2003/Library/IIS/25d2170b-09c0-45fd-8da4-898cf9a7d568.mspx?mfr=true">IIS's gzip</a> messing with flex the fix was easy.  We removed the <code>application/x-shockwave-flash</code> <a href="http://en.wikipedia.org/wiki/Internet_media_type">MIME type</a> in the machine's <code>application.config</code> under the compression settings to prevent swf's from getting compressed over the wire

[xml]
&lt;httpCompression directory=&quot;%SystemDrive%inetpubtempIIS Temporary Compressed Files&quot;&gt;
        &lt;scheme name=&quot;gzip&quot; dll=&quot;%Windir%system32inetsrvgzip.dll&quot; /&gt;
        &lt;dynamicTypes&gt;
                &lt;clear /&gt;
                &lt;add mimeType=&quot;text/*&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;message/*&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/x-javascript&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/x-amf&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/json&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/json; charset=utf-8&quot; enabled=&quot;true&quot; /&gt;

                &lt;!-- notice the swf mime type --&gt;
                &lt;add mimeType=&quot;application/x-shockwave-flash&quot; enabled=&quot;false&quot; /&gt; 
                
                &lt;add mimeType=&quot;*/*&quot; enabled=&quot;false&quot; /&gt;
        &lt;/dynamicTypes&gt;
        &lt;staticTypes&gt;
                &lt;clear /&gt;
                &lt;add mimeType=&quot;text/*&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;message/*&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/x-javascript&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/atom+xml&quot; enabled=&quot;true&quot; /&gt;
                &lt;add mimeType=&quot;application/xaml+xml&quot; enabled=&quot;true&quot; /&gt;
                
                &lt;!-- notice the swf mime type --&gt;
                &lt;add mimeType=&quot;application/x-shockwave-flash&quot; enabled=&quot;false&quot; /&gt; 
                
                &lt;add mimeType=&quot;*/*&quot; enabled=&quot;false&quot; /&gt;
        &lt;/staticTypes&gt;
    &lt;/httpCompression&gt;
[/xml]

To do this in code is also pretty easy:

For IIS6 or machines with the IIS6 Managment Compatability enabled run the command only with the MIME extensions you want. Any extensions that were there previously would be removed

[csharp]%windir%system32cscript.exe adsutil.vbs set W3SVC/Filters/Compression/gzip/HcFileExtensions 'js' 'css' 'doc' 'docx'[/csharp]

To query it you can do 

[csharp]%windir%system32cscript.exe adsutil.vbs get W3SVC/Filters/Compression/gzip/HcFileExtensions[/csharp]

For IIS7:

[csharp]
using (var server = new ServerManager())
{
    var applicationHostConfiguration = server.GetApplicationHostConfiguration();

    var urlCompressionSection = applicationHostConfiguration.GetSection(&quot;system.webServer/urlCompression&quot;);
    urlCompressionSection.SetAttributeValue(&quot;doDynamicCompression&quot;, &quot;true&quot;);
    urlCompressionSection.SetAttributeValue(&quot;doStaticCompression&quot;, &quot;true&quot;);

    var httpCompressionSection = applicationHostConfiguration.GetSection(&quot;system.webServer/httpCompression&quot;);
    httpCompressionSection.OverrideMode = OverrideMode.Allow;

    var dynamicTypesCollection = httpCompressionSection.GetCollection(&quot;dynamicTypes&quot;);
    var staticTypesCollection = httpCompressionSection.GetCollection(&quot;staticTypes&quot;);

    // Remove all previous so we dont have duplicates.
    dynamicTypesCollection.Clear();
    staticTypesCollection.Clear();

    AddMimeType(dynamicTypesCollection, @&quot;text/*&quot;, true);
    AddMimeType(dynamicTypesCollection, @&quot;message/*&quot;, true);
    AddMimeType(dynamicTypesCollection, @&quot;application/x-javascript&quot;, true);
    AddMimeType(dynamicTypesCollection, @&quot;application/x-amf&quot;, true);
    AddMimeType(dynamicTypesCollection, @&quot;application/json&quot;, true);
    AddMimeType(dynamicTypesCollection, @&quot;application/json; charset=utf-8&quot;, true);
    AddMimeType(dynamicTypesCollection, @&quot;application/x-shockwave-flash&quot;, false);
    AddMimeType(dynamicTypesCollection, @&quot;*/*&quot;, false);

    AddMimeType(staticTypesCollection, @&quot;text/*&quot;, true);
    AddMimeType(staticTypesCollection, @&quot;message/*&quot;, true);
    AddMimeType(staticTypesCollection, @&quot;application/x-javascript&quot;, true);
    AddMimeType(staticTypesCollection, @&quot;application/atom+xml&quot;, true);
    AddMimeType(staticTypesCollection, @&quot;application/xaml+xml&quot;, true);
    AddMimeType(staticTypesCollection, @&quot;application/x-shockwave-flash&quot;, false);
    AddMimeType(staticTypesCollection, @&quot;*/*&quot;, false);

    server.CommitChanges();
}

private static void AddMimeType(ConfigurationElementCollection collection, String mimeType , bool enabled )
{
    if (collection == null)
    {
        return;
    }

    var mimeTypeElement = collection.CreateElement();
    mimeTypeElement.Attributes[&quot;mimeType&quot;].Value = mimeType;
    mimeTypeElement.Attributes[&quot;enabled&quot;].Value = enabled ;
    collection.Add(mimeTypeElement);
}
[/csharp]

Now, without gzip, when the content length header was sent everything was worked.

But why was the server sometimes sending transfer chunked vs content length? It may be that the compressed swf was cached server side and at that point it knew the compressed content length.  Here are two articles that talk about when IIS 7.5 sends  <a href="http://stackoverflow.com/questions/8582637/why-is-asp-net-replacing-a-content-length-header-with-a-transfer-encoding-header" target="_blank" rel="noopener noreferrer">transfer chunked</a> vs <a href="http://geekswithblogs.net/GruffCode/archive/2012/01/02/lsquocontent-lengthrsquo-header-replaced-with-lsquotransfer-encoding-chunkedrsquo-in-asp-.net.aspx" target="_blank" rel="noopener noreferrer">content length</a>.

While I never figured out why I only ever saw this in firefox, I did notice that we got different response codes from the server when loading in chrome and IE.  In chrome and IE we properly got <a href="http://stackoverflow.com/questions/1665082/http-status-code-200-cache-vs-status-code-304" target="_blank" rel="noopener noreferrer">304</a> (content not modified) code when loading the swf.  This means that it had cached the swf client side in the browser, made a request to see if it had changed, and the server replied back saying it hadn't changed. This way the browser just used the cached swf and bypassed all the content header issues which explains why we never saw this there.

On top of that, Chrome sometimes never even made a request to the server and just loaded up the cached swf but IE consistently made a request and received a not modified 304, which is what I would have expected

[csharp]
Response	HTTP/1.1 304 Not Modified
Content-Type	application/x-shockwave-flash
ETag	&quot;4767cb0f79dcd1:0&quot;
X-Powered-By	ASP.NET
Content-Length	3241105
Last-Modified	Sat, 29 Sep 2012 04:00:45 GMT
[/csharp]

In general the point of the exercise was that you should be cognizant of what is getting compressed over the wire.  Not everything should be compressed, and sometimes compression can actually make your response sizes larger!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>910</wp:post_id>
		<wp:post_date><![CDATA[2013-02-27 15:40:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[flex-swfs-iis-and-gzip]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[864641255]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>The Cost of Threads</title>
		<link>https://onoffswitch.net/?p=1142</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1142</guid>
		<description></description>
		<content:encoded><![CDATA[We all know that creating too many threads is a bad idea, but not everyone knows the details of why this is true. I wanted to quickly summarize the reasons for limiting thread creation:
<h1>Scheduling</h1>
Threads are the fundamental schedulable item in a kernel scheduler. The more active threads you have, the slower things will be. <a href="http://en.wikipedia.org/wiki/Fair-share_scheduling" target="_blank" rel="noopener noreferrer">Fairness</a> ensures that every thread, in every process, will eventually get a <a href="http://en.wikipedia.org/wiki/Preemption_(computing)#Time_slice" target="_blank" rel="noopener noreferrer">timeslice</a>, so having too many threads means you have to wait that much longer for everything to eventually run. Depending on the ordering of thread scheduling, a costly <a href="http://en.wikipedia.org/wiki/Context_switch" target="_blank" rel="noopener noreferrer">process context switch</a> could occur. Too many context switches could result in virtual memory thrashing, causing you to load paged memory from disk every time the process is context switched back in. In general, scheduling is done with <a href="http://en.wikipedia.org/wiki/Round-robin_scheduling" target="_blank" rel="noopener noreferrer">round robin</a> and priorities in Windows (except for fixed priority real-time threads). It should be mentioned that inactive threads, threads that are blocked or in a thread sleep sit in a wait queue and don't affect the scheduler.
<h1>Memory Usage</h1>
By default, Windows threads get a <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms686774(v=vs.85).aspx" target="_blank" rel="noopener noreferrer">1MB stack</a> to work with. This means if you spin up 1000 threads, you are using 1GB of memory JUST in thread stack space. The more threads allocated, the higher your chances of getting page faults and requiring your stack to be paged off of disk. This can really slow things down, since you are now constantly moving things between memory and disk (going back to the virtual memory trashing remark). You can <a href="http://www.atalasoft.com/cs/blogs/rickm/archive/2008/04/22/increasing-the-size-of-your-stack-net-memory-management-part-3.aspx" target="_blank" rel="noopener noreferrer">adjust the stack size</a> in C#, or using the windows API. However, I don't think it's a smart idea to go around doing this just to increase your thread count. Various other kernels, including Linux, have differing default stack allocations but the memory allocation issue in these kernels is the same.
<h1>Design</h1>
If you are wondering if you have made too many threads, you probably have. Ask yourself: are all the threads actually doing work, or are they sitting idle? If they are working, then this is a good case for an event driven design, where you can leverage the same thread for more work. Alternatively, you can use the <a href="http://msdn.microsoft.com/en-us/library/ms973903.aspx" target="_blank" rel="noopener noreferrer">threadpool</a> or <a href="http://msdn.microsoft.com/en-us/library/dd460717.aspx" target="_blank" rel="noopener noreferrer">task parallel library</a> to schedule work and distribute load. If you are doing IO work (and building a win32 native app) you can use <a href="http://xania.org/200807/iocp" target="_blank" rel="noopener noreferrer">io completion ports</a>.

Having too many threads attacking the same problem also means that each thread does the work slowly, since it can only do things bit by bit. Trying to decrease computation time by adding threads could unintentionally increase the time it took to solve the problem! Try adding a semaphore to limit the number of concurrent threads on the same work item. Make this value <a href="http://tech.blinemedical.com/configure-all-the-things/" target="_blank" rel="noopener noreferrer">configurable</a> so you can fine tune it on different systems.
<h1>The Dilemma</h1>
Depending on your application, using more advanced thread management techniques can make maintenance difficult. If you really need that many threads, you should weigh the cost of implementation/maintenance with adding more machines for your problem. But, chances are that if you make some smart choices early, you will end up with a clean and easy-to-scale infrastructure.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1142</wp:post_id>
		<wp:post_date><![CDATA[2012-12-12 17:30:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>3</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-12-12 17:30:33]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-12-12 17:30:33]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Cool! I like explanations about why best practices exist. Thanks, Anton.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>Home</title>
		<link>https://onoffswitch.net/2019/07/01/home/</link>
		<pubDate>Mon, 01 Jul 2019 01:22:33 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/2019/07/01/home/</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1143</wp:post_id>
		<wp:post_date><![CDATA[2019-07-01 01:22:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-07-01 01:22:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[home]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>1</wp:menu_order>
		<wp:post_type><![CDATA[nav_menu_item]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="nav_menu" nicename="top-navigation"><![CDATA[Top Navigation]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[custom]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_menu_item_parent]]></wp:meta_key>
		<wp:meta_value><![CDATA[0]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_object_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1143]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_object]]></wp:meta_key>
		<wp:meta_value><![CDATA[custom]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_target]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_classes]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{i:0;s:0:"";}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_xfn]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_menu_item_url]]></wp:meta_key>
		<wp:meta_value><![CDATA[http://onoffswitch.net/]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Sorts</title>
		<link>https://onoffswitch.net/?p=3363</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3363</guid>
		<description></description>
		<content:encoded><![CDATA[If you're going to add a sort ID to a database, make sure to leave skips

SimulatorCompanyID,SimulatorCompanyGuid,SimulatorCompanyName,SimulatorCompanySort
1,,Gaumard®,1
2,,Laerdal®,2
3,,METI®,3
4,,Other,4
5,,CAE,5 <-- COMMON!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3363</wp:post_id>
		<wp:post_date><![CDATA[2013-03-18 19:58:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>License</title>
		<link>https://onoffswitch.net/onoff-license/</link>
		<pubDate>Sat, 27 Apr 2013 21:33:37 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?page_id=3668</guid>
		<description></description>
		<content:encoded><![CDATA[All software posted in this blog and in my github (<a href="http://github.com/devshorts" target="_blank" rel="noopener noreferrer">http://github.com/devshorts</a>) is available through the MIT license below.


<blockquote>The MIT License (MIT)

Copyright (c) 2013 Anton Kropp

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.</blockquote>

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3668</wp:post_id>
		<wp:post_date><![CDATA[2013-04-27 21:33:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-27 21:33:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[onoff-license]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[page]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
														<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tech talk: Kanban and Scrum</title>
		<link>https://onoffswitch.net/?p=3831</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3831</guid>
		<description></description>
		<content:encoded><![CDATA[At today's tech talk we talked about Kanban and Scrum.  Kanban ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3831</wp:post_id>
		<wp:post_date><![CDATA[2013-05-16 16:49:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-kanban-scrum]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Angular http interceptor with typescript</title>
		<link>https://onoffswitch.net/?p=4029</link>
		<pubDate>Thu, 20 Jun 2013 00:35:10 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4029</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4029</wp:post_id>
		<wp:post_date><![CDATA[2013-06-20 00:35:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-20 00:35:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						</item>
					<item>
		<title>jira_new</title>
		<link>http://onoffswitch.net/wp-content/uploads/2019/08/jira_new.gif</link>
		<pubDate>Mon, 12 Aug 2019 19:22:47 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/wp-content/uploads/2019/08/jira_new.gif</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8028</wp:post_id>
		<wp:post_date><![CDATA[2019-08-12 19:22:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-08-12 19:22:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[jira_new]]></wp:post_name>
		<wp:status><![CDATA[inherit]]></wp:status>
		<wp:post_parent>8027</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[attachment]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
						<wp:attachment_url><![CDATA[https://onoffswitch.net/wp-content/uploads/2019/08/jira_new.gif]]></wp:attachment_url>
											<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_attachment_metadata]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:5:{s:5:"width";i:754;s:6:"height";i:479;s:4:"file";s:20:"2019/08/jira_new.gif";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:20:"jira_new-150x150.gif";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/gif";}s:6:"medium";a:4:{s:4:"file";s:20:"jira_new-300x191.gif";s:5:"width";i:300;s:6:"height";i:191;s:9:"mime-type";s:9:"image/gif";}}s:10:"image_meta";a:12:{s:8:"aperture";s:1:"0";s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";s:1:"0";s:9:"copyright";s:0:"";s:12:"focal_length";s:1:"0";s:3:"iso";s:1:"0";s:13:"shutter_speed";s:1:"0";s:5:"title";s:0:"";s:11:"orientation";s:1:"0";s:8:"keywords";a:0:{}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_attached_file]]></wp:meta_key>
		<wp:meta_value><![CDATA[2019/08/jira_new.gif]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>new_branch</title>
		<link>http://onoffswitch.net/wp-content/uploads/2019/08/new_branch.gif</link>
		<pubDate>Mon, 12 Aug 2019 19:24:08 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/wp-content/uploads/2019/08/new_branch.gif</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8029</wp:post_id>
		<wp:post_date><![CDATA[2019-08-12 19:24:08]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-08-12 19:24:08]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[new_branch]]></wp:post_name>
		<wp:status><![CDATA[inherit]]></wp:status>
		<wp:post_parent>8027</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[attachment]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
						<wp:attachment_url><![CDATA[https://onoffswitch.net/wp-content/uploads/2019/08/new_branch.gif]]></wp:attachment_url>
											<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_attached_file]]></wp:meta_key>
		<wp:meta_value><![CDATA[2019/08/new_branch.gif]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_attachment_metadata]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:5:{s:5:"width";i:754;s:6:"height";i:479;s:4:"file";s:22:"2019/08/new_branch.gif";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:22:"new_branch-150x150.gif";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/gif";}s:6:"medium";a:4:{s:4:"file";s:22:"new_branch-300x191.gif";s:5:"width";i:300;s:6:"height";i:191;s:9:"mime-type";s:9:"image/gif";}}s:10:"image_meta";a:12:{s:8:"aperture";s:1:"0";s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";s:1:"0";s:9:"copyright";s:0:"";s:12:"focal_length";s:1:"0";s:3:"iso";s:1:"0";s:13:"shutter_speed";s:1:"0";s:5:"title";s:0:"";s:11:"orientation";s:1:"0";s:8:"keywords";a:0:{}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>resume</title>
		<link>http://onoffswitch.net/wp-content/uploads/2019/08/resume.gif</link>
		<pubDate>Mon, 12 Aug 2019 19:27:40 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/wp-content/uploads/2019/08/resume.gif</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8030</wp:post_id>
		<wp:post_date><![CDATA[2019-08-12 19:27:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-08-12 19:27:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[closed]]></wp:ping_status>
		<wp:post_name><![CDATA[resume]]></wp:post_name>
		<wp:status><![CDATA[inherit]]></wp:status>
		<wp:post_parent>8027</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[attachment]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
						<wp:attachment_url><![CDATA[https://onoffswitch.net/wp-content/uploads/2019/08/resume.gif]]></wp:attachment_url>
											<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_attached_file]]></wp:meta_key>
		<wp:meta_value><![CDATA[2019/08/resume.gif]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_attachment_metadata]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:5:{s:5:"width";i:754;s:6:"height";i:479;s:4:"file";s:18:"2019/08/resume.gif";s:5:"sizes";a:2:{s:9:"thumbnail";a:4:{s:4:"file";s:18:"resume-150x150.gif";s:5:"width";i:150;s:6:"height";i:150;s:9:"mime-type";s:9:"image/gif";}s:6:"medium";a:4:{s:4:"file";s:18:"resume-300x191.gif";s:5:"width";i:300;s:6:"height";i:191;s:9:"mime-type";s:9:"image/gif";}}s:10:"image_meta";a:12:{s:8:"aperture";s:1:"0";s:6:"credit";s:0:"";s:6:"camera";s:0:"";s:7:"caption";s:0:"";s:17:"created_timestamp";s:1:"0";s:9:"copyright";s:0:"";s:12:"focal_length";s:1:"0";s:3:"iso";s:1:"0";s:13:"shutter_speed";s:1:"0";s:5:"title";s:0:"";s:11:"orientation";s:1:"0";s:8:"keywords";a:0:{}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Etiquette of ownership</title>
		<link>https://onoffswitch.net/2012/09/18/etiquette-of-ownership/</link>
		<pubDate>Tue, 18 Sep 2012 20:43:44 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=596</guid>
		<description></description>
		<content:encoded><![CDATA[- talk about professionalism when publishing open source or public code
- read between the lines when someone asks a question. try to determine what they are asking, if its a bug, if its not a bug. 
- as the creator you know all the details about what you made, give people the benefit of the doubt and be patient when explaining it
- remember that your answers reflect on you and the things you are related to, a poorly judged comment can undermine lots of work
- this applies not only to public source but internal interactions as well
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>596</wp:post_id>
		<wp:post_date><![CDATA[2012-09-18 20:43:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-18 20:43:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[etiquette-of-ownership]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Time to think</title>
		<link>https://onoffswitch.net/2012/09/18/time-to-think/</link>
		<pubDate>Tue, 18 Sep 2012 20:46:32 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=598</guid>
		<description></description>
		<content:encoded><![CDATA[- talk about how its beneficial to turn off and do other things
- engage in hobbies, interests other than computers
- find article that talked about "work is for the workstation"
- sitting at your desk doesn't count, there are too many distractions, coworkers, pending bugs, broken builds, IM's, etc.
- get outside, relax, don't think about work
- the best ideas come when you aren't pushing yourself on them
- avoid burnout, if you aren't thinking clearly take a break. eat a snack, take a walk.
- proper nutrition and excessive can help a lot, trying to work through these things is ineffective
- know how you work, what works for you might not work for someone else and vice versa]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>598</wp:post_id>
		<wp:post_date><![CDATA[2012-09-18 20:46:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-18 20:46:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[time-to-think]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Configure all the things</title>
		<link>https://onoffswitch.net/2013/02/11/configure-all-the-things/</link>
		<pubDate>Mon, 11 Feb 2013 17:19:41 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=652</guid>
		<description></description>
		<content:encoded><![CDATA[Imagine a world where there was only ever one form factor for a tool. One size Philips head. One size hammer. One size car. One EQ setting. It would be a pretty rigid world. Instead, thankfully, we live in a world where there are lots of options, choices, and variations. This lets us pick the specific tools we need to help us do our job, as well as tailor them to the workflow we like. In software, we're always writing tools to help people solve problems. Configuration is one of the ways that makes software flexible.

Configuration doesn't just imply things that are public to users, like the background color or autosave frequency, but it can expose non-compile time settings that help you tune your application to unknown environments. Most software can't possibly be tested in every environment that it will be used. However, if you make your code flexible and configurable, you can minimize damage and buy yourself some time when things go bad. You can possibly even find a configuration that works around the problem. Configuration can improve someone's experience by modifying assumptions you made while developing with a toggle or tweak of a configuration value.

I personally think that just about everything should be configurable, unless it's absolutely never going to change. Even then, make it configurable, because it may change in the future. Think about your favorite command line tools, and the extensibility they have. They're powerful because they are dynamic. They can be configured for a myriad of options and scenarios.

Some examples of things that should be configurable:
<ul>
	<li><strong>Timeouts</strong>. Any time you are setting timeouts for something, you should make this configurable. Maybe that 5-minute timeout you thought was impossible is actually happening, and you need to make it 6 minutes.</li>
	<li><strong>Retries</strong>. If you are going to retry something a certain number of times, you should make this value configurable. You should also make the interval of the retry configurable.</li>
	<li><strong>Thresholds</strong>. If something has a threshold, it should be configurable. You should be able to tighten the threshold or loosen it</li>
	<li><strong>Max and mins</strong>. Anything that has an upper or lower limit should be configurable.</li>
	<li><strong>Optional UI items</strong>. Some parts of an application aren't used by everyone. Even if you never plan on doing it, make it toggleable. You can toggle a widget or piece of the application that isn't useful to a certain user set on or off. Some clients may not like item XYZ always showing up, even if you think its a core part of the application. If you can just disable it with ease, then that makes them happy not to see it. It makes you happy because you don't have to make any custom changes.</li>
	<li><strong>Things that start at runtime</strong>. Code internals should be able to be toggled. If you have a thread that spins up on startup, and is suddenly <a href="http://tech.blinemedical.com/wp-content/uploads/2012/09/swedishchef.jpg" target="_blank" rel="noopener noreferrer">borking</a> everything, then you can have a way to temporarily turn it off. This can save you, your support team, and the client a big headache while you find out why its not working. More than once I've run across something unexpected that happened, resulting in a runaway thread or thrashing disk. Wrapping everything in a configuration toggle lets you turn it off, and safely bypass an entire swath of the application. Defensive programming can go a long way, but sometimes you need to immediately turn off an entire section and not let it hit your code.</li>
	<li><strong>Execution of 3rd party libraries/programs</strong>. Imagine you do a netstat on startup, to see who is connected to what. However, netstat is choking on the network, and delaying your startup by 30 seconds. You should make sure to wrap this kind of execution in a configuration, so that you can turn it on or off, depending on the scenario.</li>
	<li><strong>Optional features</strong>. Sometimes we have pet projects, and we love working on them, but nobody asked for it. If it works, the client may think this is the best thing ever. If it doesn't work, they really don't want it screwing up core requirements. These kinds of things should all be configurable, so you can turn them off if desired.</li>
	<li><strong>Paths</strong>. If you can configure where things go you should. A client has an F: drive? No problem. Don't hard-code values anywhere.</li>
</ul>
While taking configuration via the command line is a simple and flexible thing to do, it doesn't scale well and can be difficult to maintain. I find that a known xml file is a great way to store a config. You don't need to expose default values, or any public options if you don't want to, but the fact that they are there is what matters. In C#, use the <a href="http://www.distribucon.com/blog/MarkingDefaultValuesToControlXMLSerialization.aspx" target="_blank" rel="noopener noreferrer"><code>DefaultValue</code></a> attribute on a serializable class to only write values if the property is anything other than the default. This way, you can load and save your configs without exposing the internals of your configuration class. The upside of this is that your configs stay clean, and you can easily see what has changed from the default. The downside is that if you plan on exposing these options, then they won't show up by default. If you go this route, it would be worth keeping a master list of configuration options, where they are located (which config file, xml block, etc), what the default value is, what the option does, and why it exists.

You should also handle the situation where configs can't be found. I like to make sure that my config loading code is reasonably robust. It will look in a set of known folders, and move up the local hierarchy until it can find something. Depending on your application architecture, this can be a reasonable solution. If you have a multi-process application, you can design it so that there is only one config for everyone. This makes maintaining configuration options easier, since it's all in one file. If you still can't find the configs, log an exception or a warning and then resort to the default values. Don't create a hard dependency on configuration, if you can avoid it. This makes re-using code that relies on configurations easier, since you don't need to have a configuration file to use it.

It's also pretty easy to live-load your configurations, instead of just once on startup. If you create a file-watcher on your config class, and re-load it when it's changed, then you can have live up-to-date configurations. If your config file is live-editable, you can even throttle reloading the config at some regular intervals using Rx, so you don't spam your system with configuration reloading. Just make sure to reference all your configuration options directly from the config class, and don't make local copies of config values (or <a href="http://en.wikipedia.org/wiki/Closure_(computer_science)" target="_blank" rel="noopener noreferrer">close</a> on them). Below is a simple example that will call a reload config function in 2 second intervals, while a file is being edited.

[csharp]
private void InitFileWatcher()
{
    _watcher = new FileSystemWatcher(Path.GetDirectoryName(Path.GetFullPath(Config.Path)), Path.GetFileName(Config.Path));
    Observable.FromEventPattern&lt;FileSystemEventHandler, FileSystemEventArgs&gt;(
        ev =&gt; _watcher.Changed += ev,
        ev =&gt; _watcher.Changed -= ev)
        .Sample(TimeSpan.FromSeconds(Config.Instance.ConfigReloadTime)).Subscribe(ReloadConfig);

    _watcher.NotifyFilter = NotifyFilters.LastWrite;
    _watcher.EnableRaisingEvents = true;
}

private void ReloadLogConfig(EventPattern&lt;FileSystemEventArgs&gt; obj)
{
    Log.Debug(this, &quot;{0} path was changed ({1}), reloading config&quot;, obj.EventArgs.FullPath, obj.EventArgs.ChangeType);
}
[/csharp]

I think configuration is just one part of defensive and flexible programming. Not all configurable items are about turning something on or off or avoiding bugs. They should also be about performance-tuning and optimization. In the end, if you aren't sure, make it configurable. You'll find that the worst that happens when you do is that you don't ever use that configuration value.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/09/configureAllThethings..png"><img class="alignnone size-full wp-image-1009" alt="" src="http://tech.blinemedical.com/wp-content/uploads/2012/09/configureAllThethings..png" width="319" height="241" /></a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>652</wp:post_id>
		<wp:post_date><![CDATA[2013-02-11 17:19:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-11 17:19:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[configure-all-the-things]]></wp:post_name>
		<wp:status><![CDATA[epending-review]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="best-practices"><![CDATA[Best Practices]]></category>
		<category domain="post_tag" nicename="configuration"><![CDATA[configuration]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[878706677]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>4</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-02-11 16:10:12]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-02-11 16:10:12]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[" Possibly even find a configuration that works around the problem. " - Not quite a complete sentence, maybe "You could even find a configuration" or "Your code could even find a configuration?"

"Sometimes we have pet projects, and we love working on them, but nobody asked for it. " - THIS IS TRUE OF MY LIFE

"I like to make sure that my code looking for configs is reasonably robust." - This is a little confusing. Did you mean "I like to make sure that when I write code that looks for configs, it's reasonably robust?" I think the part that's confusing is "my code looking for configs is." I'm not sure what part "is" or what part is "looking," or if it's all one thing that "is?"

Great post! I especially like Ally Brosh at the end.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>First day at work stories</title>
		<link>https://onoffswitch.net/2012/09/21/first-day-at-work-stories/</link>
		<pubDate>Fri, 21 Sep 2012 13:29:13 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=698</guid>
		<description></description>
		<content:encoded><![CDATA[- talk about not getting computer booted
- didn't know visual studio
- afraid to ask questions
- intimidated
- now i'm awesome]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>698</wp:post_id>
		<wp:post_date><![CDATA[2012-09-21 13:29:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-21 13:29:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[first-day-at-work-stories]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[869942454]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Memory Barriers and Instruction Reordering</title>
		<link>https://onoffswitch.net/2012/09/23/memory-barriers/</link>
		<pubDate>Sun, 23 Sep 2012 16:31:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=826</guid>
		<description></description>
		<content:encoded><![CDATA[<h1>Memory barriers</h1>

As if locking can't be complicated enough, you have to consider what happens with processor instruction <a href="http://blogs.msdn.com/b/itgoestoeleven/archive/2008/03/07/joys-of-compiler-and-processor-reordering.aspx">reodering</a>. Modern processors reorder certain instructions to make as much use of the instruction pipeline as possible. Some instructions take more time than others. For example, if you're going to do an instruction that requires you to load data from a register, now that instruction is blocked while the load occurs. Modern architectures (processors and compilers) are smart enough to let a second instruction go through while the initial instruction is blocked on the load. All sorts of crazy magic happens here but what you need to know is that just because you wrote code in a certain order does not mean it will actually get executed in that order. 

Processors maintain program ordering but not physical ordering of instructions. Obviously this is problematic when you want to deal with the most up to date information. This is where the concept of a <a href="http://en.wikipedia.org/wiki/Memory_barrier">memory barrier</a> comes into play. This is a cpu instruction that tells the processor to execute the instructions sequentially: don't reorder anything.  In most languages when you create a lock you also create a memory barrier which forces things to execute in order and be as up to date as possible.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>826</wp:post_id>
		<wp:post_date><![CDATA[2012-09-23 16:31:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-23 16:31:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[memory-barriers]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[869941643]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>What makes a good team</title>
		<link>https://onoffswitch.net/2012/09/23/what-makes-a-good-team/</link>
		<pubDate>Sun, 23 Sep 2012 19:06:15 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=867</guid>
		<description></description>
		<content:encoded><![CDATA[- things to look for when hiring
- how to interpret answers during hiring
- how to divy up work
- knowledge sharing
- promote exploration, passion, etc
- bad apple study
- social interaction
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>867</wp:post_id>
		<wp:post_date><![CDATA[2012-09-23 19:06:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-23 19:06:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[what-makes-a-good-team]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[869941390]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>SignalR to Flex</title>
		<link>https://onoffswitch.net/2012/09/27/signalr-to-flex/</link>
		<pubDate>Thu, 27 Sep 2012 09:19:59 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=907</guid>
		<description></description>
		<content:encoded><![CDATA[- talk about javascript bridge
- invoking the same function in as3
- using fluorine to serliaize binary as base 64 json
- deserialization on the front-end
- mention donwnside of using base64 
- upside is you don't have to deal with flex sockets, crossdomain files, reconnections, etc.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>907</wp:post_id>
		<wp:post_date><![CDATA[2012-09-27 09:19:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-27 09:19:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[signalr-to-flex]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[869941053]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Building IEnumerable in Java</title>
		<link>https://onoffswitch.net/?p=4313</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4313</guid>
		<description></description>
		<content:encoded><![CDATA[In an effort to be more familiar with Java I decided today to build a clone of the IEnumerable higher order functions that C# gives you. In particular I wanted to implement Select, Where, and SelectMany, except I'll call them by their real names: map, filter, and flatMap.

The first thing a good engineer does is <del datetime="2013-11-11T18:26:37+00:00">cheat</del> do their research, so I checked out how the .NET enumerable iterators and implementations are put together.  The implementations are extremely succinct and neat, mostly because they can leverage the <code>yield</code> operator.  However, you can mimic the same thing in Java leveraging the Java iterators (which C# is also doing under the hood).

The goal is to be able to chain operators together onto a List item (that exposes an iterator) and minimize the number of traversals we need to do over the list.  A naive approach would be for each chain to process the list, then yield the entire list to the next element (again processing it), etc etc.  A better way is to try and have each element only process a single item at a time. This way it's more of a pipeline than a sequence of list manipulations. This is exactly how .NET does it and which is why LINQ operations are so effective.  

First let me show the final goal (and end product)

[java 1="static" 2="void" 3="main(String[" language="public"] arsg) throws InterruptedException, ExecutionException { 
    List&lt;String&gt; strings = asList(&quot;o&quot;, &quot;ba&quot;, &quot;baz&quot;, &quot;booo&quot;);                             
                                                                                         
    List&lt;String&gt; items = Enumerable.init(strings)                                        
                            .filter(i -&gt; i.length() &gt; 2)                                 
                            .flatMap(i -&gt; strings)                                       
                            .filter(i -&gt; i.length() &gt; 3)                                 
                            .map(i -&gt; &quot;four!&quot;)                                           
                            .toList();                                                   
                                                                                         
    for(String x : items){                                                               
        System.out.println(x);                                                           
    }                                                                                    
}                                                                                        
[/java]

I have a list of random words, which I filter out words that are less than 2 characters long, then I just discard that and for each result flatMap the original list (so if two words have a length greater than 2 then I will yield the source list twice), then from the newly yielded list select only words that more than 3 characters ("booo", which was yielded twice by the flatMap), then convert the final strings to a new string. The final result should print out "four!" twice, which it does.  On top of that, I've expressed the enumerable lazily so I decided to cache it as a demonstration of future enumerations.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4313</wp:post_id>
		<wp:post_date><![CDATA[2013-11-11 18:37:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-ienumerable-java]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Onboarding a new software engineer</title>
		<link>https://onoffswitch.net/?p=4374</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4374</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4374</wp:post_id>
		<wp:post_date><![CDATA[2014-01-26 03:26:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[onboarding-engineer]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>haskell test viewer</title>
		<link>https://onoffswitch.net/?p=4409</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4409</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4409</wp:post_id>
		<wp:post_date><![CDATA[2014-02-13 02:33:38]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[haskell-test-viewer]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Managing service oriented local deployments</title>
		<link>https://onoffswitch.net/?p=4486</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4486</guid>
		<description></description>
		<content:encoded><![CDATA[Service oriented architectures are becoming more and more prevalent today. For good reason too, they scale well both technically and personally.  That means that you can toss more machines to handle the service, as well as separate teams of engineers behind service responsibilities (letting you scale your teams).

If you are working with WCF services, like I am, then you can mimic a local deployment of your application by leveraging visual studio file system publishing to a local hosted IIS directory.  This is great since you can have some service builds be local to your machine, some pointing to test services (deployed on an integration cloud), and have all the services pointing to the same database (like a shared SQL Server instance).

When you work with one service at a time configuration is easy.  Update your web.config (or a local transform) to point to either local services, local databases, or remote services and remote databases.  But, things get a lot more annoying when you start having 3, or 4, or 5+ local services all pointing to different locations.  The web.config managment becomes tedious and error prone, and can really slow down development and debugging.

<h2>Description of the problem</h2>

Thankfully, the solution to handling this is easy. What if we had a tool that could read in a config that said which service contract to point to where, and which database name to point to which SQL server host and update all deployed web.config's with this info. Then all we had to do is run this tool against the deployed root and we can immediately switch over all references of services and connection strings.

Let me demonstrate. Pretend we have a web.config with the following info:

[xml]
&lt;connectionStrings&gt;
    &lt;add name=&quot;SomeName&quot; connectionString=&quot;Data Source=targetMachine;Initial Catalog=Target_Database;Integrated Security=True;&quot; providerName=&quot;System.Data.SqlClient&quot;/&gt;
    &lt;add name=&quot;OtherDatabase&quot; connectionString=&quot;Data Source=otherMachine;Initial Catalog=Other_Database;Integrated Security=True;&quot; providerName=&quot;System.Data.SqlClient&quot;/&gt;
&lt;/connectionStrings&gt;
[/xml]

And also maybe wcf client information

[xml]
&lt;client&gt;
    &lt;endpoint address=&quot;http://remoteMachine/services/Service.svc&quot;
        binding=&quot;basicHttpBinding&quot; bindingConfiguration=&quot;ServiceBindings&quot;
        contract=&quot;ExternalSvc.ExternalService&quot; name=&quot;BasicHttp_ExternalEmailService&quot; /&gt;
&lt;/client&gt;
[/xml]

We want to switch all data source tags to be something else given the connection name, as well as switch over all endpoint addresses to another location given the contract they implement.

<h2>Leveraging type providers</h2>

If we leverage the XML type provider, we can really quickly get the web.config up and parsed

[fsharp]
[&lt;Literal&gt;]
let sampleFile = &quot;sample.xml&quot;

type WebConfig = XmlProvider&lt;sampleFile&gt;
[/fsharp]

And we can create a JSON configuration object to represent where databases and services should point to:

[fsharp]
type AppConfigs = {
    Services: Dictionary&lt;string, string&gt;
    DBs: Dictionary&lt;string, string&gt;
}

let jsonToConfig str = JsonConvert.DeserializeObject&lt;AppConfigs&gt;(str)

let lookup (config : Dictionary&lt;string, string&gt;) item =     
    let (found, newValue) = config.TryGetValue(item)     

    if found then Some newValue 
    else None
[/fsharp]

We'll also need a way to find the elements in the web.config we care about:

[fsharp]
let findEndPoints (config: AppConfigs) (w: WebConfig.Configuration)  = 
    try
        [        
            for endpoint in w.SystemServiceModel.Client.Endpoints do                               
                yield endpoint        
        ]
    with
        | ex -&gt; 
            printfn &quot;no endpoints found&quot;
            []

let findDatabases (config: AppConfigs) (w: WebConfig.Configuration)  = 
    try
        [
            for connectionString in w.ConnectionStrings.Adds do
                yield connectionString
        ]
    with
        | ex -&gt; 
            printfn &quot;No connection strings found&quot;
            []

[/fsharp]

Now that we've found the particular XML elements via the strongly typed type provider, we'll need to be able to update the underlying XElement with the new information we want. 

Updating the web.config's WCF client endpoint address is easy, it's just an attribute. But updating only the <code>"Data Source="</code> portion of the connection string is a little trickier. Here is a function that will take a valid connection string and replace "Data Source=" with the target database, but maintain the rest of the string

[fsharp]
let replaceDataSourceWith newDbName sourceString  = 
    let splits = split ';' sourceString 
    let ``dataSource=`` = &quot;Data Source=&quot;
    [
        yield ``dataSource=`` + newDbName

        for split in splits do
            if not (startsWith ``dataSource=`` split) then 
                yield split
    ] 
        |&gt; Seq.intersperse &quot;;&quot;
        |&gt; Seq.reduce (+)
[/fsharp]

And we can leverage this now by looking for endpoints and connection string xelements and pair their new values into a tuple list

[fsharp]
let newEndpointValues config (endpoints: WebConfig.Endpoint list) = 
    [
        for endpoint in endpoints do        
            match endpoint.Contract |&gt; lookup config.Services with
                | Some(newAddress) -&gt;  yield (endpoint.XElement, newAddress)
                | _ -&gt; ()
    ]

let newDatabaseValues config (connectionStrings: WebConfig.Add3 list) = 
    [
        for connectionString in connectionStrings do 
            match connectionString.Name |&gt; lookup config.DBs with
                | Some(sourceDB) when not (isNullOrEmpty sourceDB) -&gt; 
                    let mergedConnectionString = connectionString.ConnectionString |&gt; replaceDataSourceWith sourceDB
                    yield (connectionString.XElement, mergedConnectionString)
                | _ -&gt; ()   
    ]
[/fsharp]

Finally we need a way to actually update an attribute

[fsharp]
let modifyXmlAttribute attributeName (xmlElement : XElement, newValue) =     
    xmlElement.Attribute(XName.Get(attributeName)).SetValue(newValue)
[/fsharp]

Now finally the main runner

[fsharp]
let updateEndpoints (xml: WebConfig.Configuration) (config: AppConfigs) = 
    xml |&gt; findEndPoints config
        |&gt; newEndpointValues config
        |&gt; List.iter (modifyXmlAttribute &quot;address&quot;)

let updateConnections (xml: WebConfig.Configuration) (config: AppConfigs) =
    xml |&gt; findDatabases config
        |&gt; newDatabaseValues config
        |&gt; List.iter (modifyXmlAttribute &quot;connectionString&quot;)
[/fsharp]

And finally...main

[fsharp]
let sampleJson = @&quot;
{ 
    &quot;&quot;Services&quot;&quot;: { 
        &quot;&quot;Test.Mgmt.Users&quot;&quot; : &quot;&quot;http://localhost/Users.svc&quot;&quot; 
    }, 
    &quot;&quot;DBs&quot;&quot;: { 
        &quot;&quot;DBName1&quot;&quot;:&quot;&quot;localhost&quot;&quot;,
        &quot;&quot;DBName2&quot;&quot;: &quot;&quot;localhost1&quot;&quot;,
        &quot;&quot;SomeOtherDb&quot;&quot; : &quot;&quot;localhost2&quot;&quot; 
    } 
}&quot;

[&lt;EntryPoint&gt;]
let main argv = 
    
    if Array.length argv &lt;&gt; 2 then 
        printfn &quot;Usage: &lt;deployed root&gt; &lt;json config path&gt;&quot;
        printfn &quot;&quot;
        printfn &quot;Json format:&quot;
        printfn &quot;&quot;
        printfn &quot;%s&quot; sampleJson
        1
    else
        let root = argv.[0]
        let config = argv.[1] |&gt; File.ReadAllText |&gt; jsonToConfig

        for file in Directory.EnumerateFiles(root, &quot;Web.config&quot;, SearchOption.AllDirectories) do
            try
                let webXml = WebConfig.Load file
                updateEndpoints webXml config
                updateConnections webXml config
                webXml.XElement |&gt; save file
                printfn &quot;Updated %s&quot; file
            with
                | ex -&gt; printfn &quot;Failed to modify file %s: %A&quot; file ex
        0 // return an integer exit code
[/fsharp]

<h2>Conclusion</h2>

Now it's much easier to manage a multi deployed local service oriented WCF system.  If you want to point all your services to a certain database or endpoint address you can in one fell swoop.

For full source check my <a href="https://github.com/devshorts/Playground/blob/master/WCFConfigManager/WebTransform/Program.fs">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4486</wp:post_id>
		<wp:post_date><![CDATA[2014-03-23 00:56:09]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[managing-service-oriented-local-deployments]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="service-oriented-architecture"><![CDATA[service oriented architecture]]></category>
		<category domain="post_tag" nicename="wcf"><![CDATA[wcf]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Patching objects with mongoDB</title>
		<link>https://onoffswitch.net/?p=4511</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4511</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4511</wp:post_id>
		<wp:post_date><![CDATA[2014-04-27 19:43:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[patching-objects-mongodb]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Managing shared settings with nuget packages</title>
		<link>https://onoffswitch.net/?p=4513</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4513</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4513</wp:post_id>
		<wp:post_date><![CDATA[2014-04-27 19:44:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[managing-shared-settings-nuget-packages]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Ruby pry debugging</title>
		<link>https://onoffswitch.net/?p=4542</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4542</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4542</wp:post_id>
		<wp:post_date><![CDATA[2014-08-24 02:45:14]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ruby-pry-debugging]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>QConn SF</title>
		<link>https://onoffswitch.net/?p=4554</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4554</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4554</wp:post_id>
		<wp:post_date><![CDATA[2014-10-14 20:57:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[qconn-sf]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Hooking into zshs completion with a custom function</title>
		<link>https://onoffswitch.net/?p=4595</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4595</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4595</wp:post_id>
		<wp:post_date><![CDATA[2015-03-11 23:26:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[hooking-zshs-completion-custom-function]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>On Hiring Engineers</title>
		<link>https://onoffswitch.net/?p=4842</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4842</guid>
		<description></description>
		<content:encoded><![CDATA[Yes, another hiring engineers blog post. 

I've been on both sides of the interviewing circuit many times. I've moved around in my career and never been afraid to look for new opportunities, and I've been on hiring teams looking to hire new people. It's been written about endlessly before, but there's nothing more infuriating than asking, or being asked, trick questions that don't show the candidate or the team much of anything.

Interviewing, like security and testing, is all about layers.  There's no one size fits all question, or even questions.  To find truly great candidates is a layered approach consisting of quality screening, code collaboration, discussion, and questions.

<h2>The Resume Filter</h2>

<h2>The Phone Screen</h2>

A good phone screen can weed out 90% of the candidates who make it through the resume filter.   From an interviewers side, its important to be prompt. And when I mean prompt, I mean if you said 3pm you better call at 3pm on the dot. A candidates first real impression of you is whether you are on the ball or not.  Showing up 5 minutes late is 5 minutes of the candidate thinking "are they even going to call? whats going on?". At least thats what goes through my mind when I'm interviewing.  

From there its good to introduce yourself, the company, and lay out the plan for the next hour. Is it going to be code questions? Screen share? Etc.  I personally like starting with some basic "are you full of shit" questions. Things like "whats a linked list?", "whats a thread do?" are easy for pretty much anyone.  At this point if someone is any good they start to ease up since you've softballed the easy stuff.  If you do a screen share, doing a simple problem like calculating how much someone owes in taxes if you give them a set of brackets is great. It doesn't require fancy magic, it can be solved in any language using primitive constructs: for loop, classes, the plus sign.  No crazy gotcha's.  That said, a good candindate will talk through a lot of things that aren't even required for the problem. Things to listen for are are they asking questions, are they explaining their decisions, are they having fun with it? 

It's just as important to sell yourself to the candidate as it is for them to you. Not paying attention, or not asking your own followup questions is a sign to the candidate that you don't care and their company doesn't care. It also might signal that you suck to them, and nobody wants to go work for sucky people.

<h2>The in person</h2>

<h2>The followup</h2>

<h2>The negotation</h2>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4842</wp:post_id>
		<wp:post_date><![CDATA[2016-07-22 00:26:50]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[hiring-engineers]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Jenkins 2.0, Terraform, and ECS</title>
		<link>https://onoffswitch.net/?p=4913</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4913</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4913</wp:post_id>
		<wp:post_date><![CDATA[2017-04-08 04:02:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Dynamic call interceptors</title>
		<link>https://onoffswitch.net/?p=4921</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4921</guid>
		<description></description>
		<content:encoded><![CDATA[[code]

/**
 * Marker trait to know if we converted a scala future to a java one
 *
 * @tparam T
 */
trait ScalaConvertedFuture[T] {
  def originalFuture: Future[T]
}

class ScalaToJavaFuture[T](future: Future[T]) {
  def toJavaFuture: JFuture[T] = {
    new JFuture[T] with ScalaConvertedFuture[T] {
      override def isCancelled: Boolean = throw new UnsupportedOperationException

      override def get(): T = Await.result(future, Duration.Inf)

      override def get(timeout: Long, unit: TimeUnit): T = Await.result(future, Duration.create(timeout, unit))

      override def cancel(mayInterruptIfRunning: Boolean): Boolean = throw new UnsupportedOperationException

      override def isDone: Boolean = future.isCompleted

      override def originalFuture: Future[T] = future
    }
  }
}

class JavaToScalaFuture[T](f: JFuture[T]) {
  def toScalaFuture(): Future[T] = {
    f match {
      case lf: ListenableFuture[T] =&gt; {
        val p = Promise[T]
        Futures.addCallback(lf, new FutureCallback[T]() {
          def onSuccess(t: T): Unit = {
            p.success(t)
          }

          def onFailure(t: Throwable): Unit = p.failure(t)
        })
        p.future
      }

      // if the java future is actually a scala converted future, return the raw future
      // bypassing the extra thread creation
      case p: ScalaConvertedFuture[T] =&gt; {
        p.originalFuture
      }

      case p: JFuture[T] =&gt; {
        val wrappedPromise = Promise[T]()
        new Thread(new Runnable {
          override def run(): Unit = {
            wrappedPromise.complete(Try {
              p.get
            })
          }
        }).start()

        wrappedPromise.future
      }

      case _ =&gt; throw new IllegalArgumentException(&quot;Only instances of ListenableFuture may be transformed to scala futures.&quot;)
    }
  }
}
[/code]

[code]

object CallInterceptor {
  /**
   * An interceptor that does doesn't proxy
   */
  def passThrough[T: Manifest]: CallInterceptor[T] = new CallInterceptor[T] {
    override def createProxy(source: T): T = source

    override def execute[Response](methodName: String, f: =&gt; Response) = f

    override def execute[Response](methodName: String, f: =&gt; Future[Response]) = f
  }
}

abstract class CallInterceptor[T: Manifest] {
  def createProxy(source: T): T = {
    val clazz = manifest[T].runtimeClass
    JProxy.newProxyInstance(
      clazz.getClassLoader,
      Array(clazz),
      new InvocationHandler {
        override def invoke(proxy: scala.Any, method: Method, args: Array[AnyRef]): AnyRef = {
          try {
            method.getReturnType match {
              case m if m == classOf[JFuture[_]] =&gt;
                val javaFuture = method.invoke(source, args: _*).asInstanceOf[JFuture[_]]

                execute(method.getName, javaFuture.toScalaFuture()).toJavaFuture
              case m if m == classOf[Future[_]] =&gt;
                execute(method.getName, method.invoke(source, args: _*).asInstanceOf[Future[_]])
              case _ =&gt;
                execute(method.getName, method.invoke(source, args: _*))
            }
          } catch {
            // Rethrow the original exception
            case NonFatal(e) if e.getCause != null =&gt; throw e.getCause
            case NonFatal(e) =&gt; throw e
          }
        }
      }).asInstanceOf[T]
  }

  def execute[Response](methodName: String, f: =&gt; Response): Response

  def execute[Response](methodName: String, f: =&gt; Future[Response]): Future[Response]
}
[/code]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4921</wp:post_id>
		<wp:post_date><![CDATA[2017-07-14 23:14:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dynamic-call-interceptors]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Markov chains from your slack convos</title>
		<link>https://onoffswitch.net/?p=4966</link>
		<pubDate>Thu, 01 Jan 1970 00:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4966</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4966</wp:post_id>
		<wp:post_date><![CDATA[2017-11-08 01:59:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[0000-00-00 00:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[]]></wp:post_name>
		<wp:status><![CDATA[draft]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Handle reconnections to signalR host</title>
		<link>https://onoffswitch.net/2012/08/21/handle-reconnections-to-signalr-host/</link>
		<pubDate>Tue, 21 Aug 2012 14:38:32 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=155</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/handle-reconnections-to-signalr-host/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

<a href="https://github.com/SignalR/SignalR/">SignalR</a> does a great job of dealing with reconnecting to a host when either the client disconnects or the server disconnects. This is pretty handy since it handles all the intricacies of a persistent http connection for you. But what it doesn't deal with is the initial negotiation to a server. If that fails you are stuck retrying yourself. I wrote a simple reconnection function that leverages the scheduling functionality of <a href="http://msdn.microsoft.com/en-us/data/gg577609.aspx">Rx</a> to continuously try to reconnect to the server.

For our SignalR usage (version 0.5.2), I'm using the exposed <a href="https://github.com/SignalR/SignalR/wiki/QuickStart-Hubs">Hub</a> functionality, not the<a href="https://github.com/SignalR/SignalR/wiki/QuickStart-Persistent-Connections"> persistent connections</a> since I liked the encapsulation that Hub's gave us. In the following example, we have a local member variable called <code>Connection</code> which is a <code>HubConnection</code> type created with this code.

[csharp]
HubConnection Connection = new HubConnection(Url);
[/csharp]

HubConnection has a Start method that you use to initialize connections to the Url.  <code>Connection.Start()</code> internally creates an asynchronous task that looks like this, after unwrapping the nicely packaged methods:

[csharp]
private Task Negotiate(IClientTransport transport)
{
    var negotiateTcs = new TaskCompletionSource&lt;object&gt;();

    transport.Negotiate(this).Then(negotiationResponse =&gt;
    {
        VerifyProtocolVersion(negotiationResponse.ProtocolVersion);

        ConnectionId = negotiationResponse.ConnectionId;

        var data = OnSending();
        StartTransport(data).ContinueWith(negotiateTcs);
    })
    .ContinueWithNotComplete(negotiateTcs);

    var tcs = new TaskCompletionSource&lt;object&gt;();
    negotiateTcs.Task.ContinueWith(task =&gt;
    {
        try
        {
            // If there's any errors starting then Stop the connection
            if (task.IsFaulted)
            {
                Stop();
                tcs.SetException(task.Exception);
            }
            else if (task.IsCanceled)
            {
                Stop();
                tcs.SetCanceled();
            }
            else
            {
                tcs.SetResult(null);
            }
        }
        catch (Exception ex)
        {
            tcs.SetException(ex);
        }
    },
    TaskContinuationOptions.ExecuteSynchronously);

    return tcs.Task;
}
[/csharp]

The comment above the <code>IsFaulted</code> check says that if the server fails to connect, an exception is set and the transport is closed<code></code>. Since SignalR utilizes the task parallel library we can just call Start() again and get a new task.

Here is the snippet we use to continuously reconnect:

[csharp]
/// &lt;summary&gt;
/// Handles if the connection start task fails and retries every 5 seconds until
/// it succeeds
/// &lt;/summary&gt;
/// &lt;param name=&quot;startTask&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;connectionSucessAction&quot;&gt;&lt;/param&gt;
private void HandleConnectionStart(Task startTask)
{
    startTask.ContinueWith(task =&gt;
    {
        try
        {
            if (task.IsFaulted)
            {
                // make sure to observe the exception or we can get an aggregate exception
                foreach (var e in task.Exception.Flatten().InnerExceptions)
                {
                    Log.WarnOnce(this, &quot;Observed exception trying to handle connection start: &quot; + e.Message);
                }

                Log.WarnOnce(this, &quot;Unable to connect to url {0}, retrying every 5 seconds&quot;, Url);
                RetryConnectionStart();
            }
            else
            {
                // do success actions
            }
        }
        catch(Exception ex)
        {
            Log.ErrorOnce(this, &quot;Error handling connection start, retrying&quot;, ex);
            RetryConnectionStartRescheduler();
        }
    });
}

private void RetryConnectionStartRescheduler()
{
    ThreadUtil.ScheduleToThreadPool(TimeSpan.FromSeconds(5),
        () =&gt;
        {
            try
            {
                HandleConnectionStart(Connection.Start());
            }
            catch(Exception ex)
            {
                Log.ErrorOnce(this, &quot;Error retrying connection start, retrying&quot;, ex);
                RetryConnectionStartRescheduler();
            }
        });

}
[/csharp]

<code>ThreadUtil.ScheduleToThreadPool</code> is a wrapper we have on top of the Rx framework's threadpool scheduler. Internally it looks like this

[csharp]
public static void ScheduleToThreadPool(TimeSpan executeTime, Action action)
{
    Scheduler.ThreadPool.Schedule(DateTime.Now.Add(executeTime), action);
}
[/csharp]

It's important to note that you have to touch the exception object of a faulted task or use the exception Handle method in order to avoid an <a href="http://msdn.microsoft.com/en-us/library/dd997415.aspx">UnobservedTaskExceptions</a>. Those happen to unobserved exceptions which are then rethrown on the finalizer thread.

In conclusion, by leveraging tasks and a couple simple scheduling utilities, we can cleanly and asynchronously schedule a new task to connect periodically. When we finally connect we can continue with our initialization logic. At this point the remaining signalR reconnection logic is handled by the hub.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>155</wp:post_id>
		<wp:post_date><![CDATA[2012-08-21 14:38:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-08-21 14:38:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[handle-reconnections-to-signalr-host]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dotnet"><![CDATA[.NET]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
		<category domain="post_tag" nicename="rx"><![CDATA[Rx]]></category>
		<category domain="post_tag" nicename="signalr"><![CDATA[SignalR]]></category>
		<category domain="post_tag" nicename="tasks"><![CDATA[tasks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[830832847]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561163778;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:289;}i:1;a:1:{s:2:"id";i:2365;}i:2;a:1:{s:2:"id";i:4091;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A collection of simple AS3 string helpers</title>
		<link>https://onoffswitch.net/2012/09/04/a-collection-of-simple-as3-string-helpers/</link>
		<pubDate>Tue, 04 Sep 2012 18:21:36 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=265</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/a-collection-of-simple-as3-string-helpers/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

We all know that it's smart to create helper and utility classes when faced with a problem that can be encapsulated, but sometimes we forget that even the smallest of things can be put into a helper. Whenever you find yourself writing something more than once you should think about encapsulating that logic. Even small logical elements should be moved to a separate function. It helps with readability, maintainability, and a separation of logic. It also makes things easier to test. Here are a couple of ActionScript string utilities we use. We have tons of them and I'll be posting snippets here and there of ones we find useful.

<h2>Check if a list is empty.</h2>

I use this one everywhere! It seems silly, but this may be the most used helper function in our entire application.

[csharp]
public static function isEmpty(list:IList):Boolean {
    return list == null || list.length == 0;
}
[/csharp]

<h2>Flatten a list into a delimited string</h2>

It's handy to be able to say given a list of objects, print out a comma (or delimiter) seperated string representing that list. This function takes a list, a function that formats each item, and an optional delimiter. An example usage is:

[csharp]
var foldedString:String = foldToDelimitedList(listOfUsers,
	function(item:Object):String{
		return (item as UserData).userName;
	});
[/csharp]

Which would give you something like "user1, user2, user3".

[csharp]
public static function foldToDelimitedList(vals:ArrayCollection, formatter:Function, delim:String = &quot;, &quot;):String{
	var retString:String = &quot;&quot;;
	var count:int = 0;
	for each(var item:Object in vals){
		retString += formatter(item);

		count++;

		if(count &lt; vals.length){
			retString += delim;
		}
	}
	return retString;
}
[/csharp]

<h2>Find an item in a list</h2>

This one is handy when you want to know if something is in a list based on a certain property. If it finds the item it will return to you the index it found. You use it like this:

[csharp]
var index:int = findItem(list, &quot;someProperty&quot;, &quot;expectedPropertyValue&quot;);
[/csharp]

For an element whose property <code>someProperty</code> matches the value <code>expectedPropertyValue</code>, it will return the first found index.

[csharp]
public static function findItem(dataProvider:Object, propName:String, value:Object, useLowerCase:Boolean = false):int {

	if (value == null) {
		return -1;
	}

	var max:int = dataProvider.length;
	if (useLowerCase) {
		value = value.toString().toLocaleLowerCase();
	}
	for(var i:int=0; i&lt;max; i++) {
		var item:Object = dataProvider[i];

		if (item == null) {
			continue;
		}
		var loopValue:Object = item[propName];
		if (loopValue == null) {
			continue;
		}

		if (loopValue == value || (useLowerCase &amp;&amp; loopValue.toString().toLocaleLowerCase() == value)) {
			return i;
		}
	}
	return -1;
}
[/csharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>265</wp:post_id>
		<wp:post_date><![CDATA[2012-09-04 14:21:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-09-04 18:21:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[a-collection-of-simple-as3-string-helpers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="as3"><![CDATA[AS3]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
		<category domain="post_tag" nicename="utilities"><![CDATA[Utilities]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[830834780]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559667323;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:4862;}i:2;a:1:{s:2:"id";i:2365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Async producer/consumer the easy way</title>
		<link>https://onoffswitch.net/2012/11/23/async-producerconsumer-the-easy-way/</link>
		<pubDate>Fri, 23 Nov 2012 15:56:35 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=532</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/async-producerconsumer-the-easy-way/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

In .net 4, a new class called <a href="http://msdn.microsoft.com/en-us/library/dd267312.aspx"><code>BlockingCollection</code></a> was introduced, which let you have a <a href="http://en.wikipedia.org/wiki/Thread_safety">threadsafe</a> <a href="http://en.wikipedia.org/wiki/Producer-consumer_problem">producer/consumer</a> queue. Anyone consuming a <code>BlockingCollection</code> blocks automatically until new items are added. This lets you easily add items to the collection in one thread and use another synchronized thread to consume items. This class is great since before this existed, you had to do all this work with mutexes and it was a lot of extra work (and more error prone). In general, a good time to use a decoupled producer consumer pattern is when you have a slow consuming function and a producer thread that is time sensitive.

Even though <code>BlockingCollection</code> effectively synchronizes your producer/consumer, you still have to create <a href="http://en.wikipedia.org/wiki/Boilerplate_code">boilerplate</a> to manage the producer thread and the consumer thread. Also if you wanted to add extra exception handling or a <a href="http://msdn.microsoft.com/en-us/library/dd997364.aspx">cancellation token,</a> you'd have to add all that yourself too. I wrapped this all up in a <code><a href="https://github.com/blinemedical/BlockingCollectionWrapper/blob/master/BlockingCollectionWrapper/BlockingCollectionWrapper.cs" target="_blank" rel="noopener noreferrer">BlockingCollectionWrapper</a></code> class that handles all this for you.


<h1>An example</h1>
Here is an example where the consumer takes one second each time it consumes an item.

[csharp]
private readonly ManualResetEvent _testMutex = new ManualResetEvent(false);

[Test]
public void TestCollection()
{
    // create the wrapper
    var asyncCollection = new BlockingCollectionWrapper&lt;string&gt;();

    asyncCollection.FinishedEvent += FinishedEventHandler;

    // make sure we dispose of it. this will stop the internal thread
    using (asyncCollection)
    {
        // register a consuming action
        asyncCollection.QueueConsumingAction = (producedItem) =&gt;
        {
            Thread.Sleep(TimeSpan.FromSeconds(1));
            Console.WriteLine(DateTime.Now + &quot;: Consuming item: &quot; + producedItem);
        };

        // start consuming
        asyncCollection.Start();

        // start producing
        for (int i = 0; i &lt; 10; i++)
        {
            Console.WriteLine(DateTime.Now + &quot;: Produced item &quot; + i);
            asyncCollection.AddItem(i.ToString());
        }
    }

    // wait for the finished handler to pulse this
    _testMutex.WaitOne();

    Assert.True(asyncCollection.Finished);
}

private void FinishedEventHandler(object sender, BlockingCollectionEventArgs e)
{
    _testMutex.Set();
}
[/csharp]

This prints out

[csharp]
9/17/2012 6:22:43 PM: Produced item 0
9/17/2012 6:22:43 PM: Produced item 1
9/17/2012 6:22:43 PM: Produced item 2
9/17/2012 6:22:43 PM: Produced item 3
9/17/2012 6:22:43 PM: Produced item 4
9/17/2012 6:22:43 PM: Produced item 5
9/17/2012 6:22:43 PM: Produced item 6
9/17/2012 6:22:43 PM: Produced item 7
9/17/2012 6:22:43 PM: Produced item 8
9/17/2012 6:22:43 PM: Produced item 9
9/17/2012 6:22:44 PM: Consuming item: 0
9/17/2012 6:22:45 PM: Consuming item: 1
9/17/2012 6:22:46 PM: Consuming item: 2
9/17/2012 6:22:47 PM: Consuming item: 3
9/17/2012 6:22:48 PM: Consuming item: 4
9/17/2012 6:22:49 PM: Consuming item: 5
9/17/2012 6:22:50 PM: Consuming item: 6
9/17/2012 6:22:51 PM: Consuming item: 7
9/17/2012 6:22:52 PM: Consuming item: 8
9/17/2012 6:22:53 PM: Consuming item: 9
[/csharp]

First, I created the blocking collection wrapper and made sure to put it in a <code>using</code> block since it's disposable (the thread waiting on the blocking collection will need to be cleaned up). Then I registered a function to be executed each time an item is consumed. Calling <code>Start()</code> begins consuming. Once I'm done - even after the using block disposes of the wrapper - the separate consumer thread could still be running (processing whatever is left), but it is no longer blocking on additions and will complete consuming any pending items.
<h1>The wrapper</h1>
When you call <code>.Start()</code> we start our independent consumer thread.

[csharp]
/// &lt;summary&gt;
/// Start the consumer
/// &lt;/summary&gt;
public void Start()
{
    _cancellationTokenSource = new CancellationTokenSource();
    _thread = new Thread(QueueConsumer) {Name = &quot;BlockingConsumer&quot;};
    _thread.Start();
}
[/csharp]

This is the queue consumer that runs in the separate thread that executes the registered consumer action. The consuming action is locked to make changing the consuming action threadsafe.

[csharp]
/// &lt;summary&gt;
/// The actual consumer queue that runs in a seperate thread
/// &lt;/summary&gt;
private void QueueConsumer()
{
    try
    {
        // Block on _queue.GetConsumerEnumerable 
        // When an item is added to the _queue it will unblock and let us consume
        foreach (var item in _queue.GetConsumingEnumerable(_cancellationTokenSource.Token))
        {
            // get a synchronized snapshot of the action
            Action&lt;T&gt; consumerAction = QueueConsumingAction;
                
            // execute our registered consuming action
            if (consumerAction != null)
            {
                consumerAction(item);
            }
        }

        // dispose of the token source
        if (_cancellationTokenSource != null)
        {
            _cancellationTokenSource.Dispose();
        }

        //Log.Debug(this, &quot;Done with queue consumer&quot;);

        Finished = true;

        if (FinishedEvent != null)
        {
            FinishedEvent(this, new BlockingCollectionEventArgs());
        }
    }
    catch(OperationCanceledException)
    {
        //Log.Debug(this, &quot;Blocking collection&lt;{0}&gt; cancelled&quot;, typeof(T));
    }
    catch (Exception ex)
    {
        //Log.Error(this, ex, &quot;Error consuming from queue of type {0}&quot;, typeof(T));
    }
}
[/csharp]

And when the wrapper is disposed, we set <code><a href="http://msdn.microsoft.com/en-us/library/dd287086.aspx" target="_blank" rel="noopener noreferrer">CompleteAdding</a></code> on the blocking collection which tells the collection to stop waiting for new additions and finish out whatever is left in the queue.

[csharp]
protected void Dispose(bool disposing)
{
    if(disposing)
    {
        if (_queue !=null &amp;&amp; !_queue.IsAddingCompleted)
        {
            // mark the queue as complete
            // the BlockingConsumer thread will now
            // just process the remaining items
            _queue.CompleteAdding();
        }
    }
}

public void Dispose()
{
    Dispose(true);
}
[/csharp]

The remaining properties and functions on the wrapper let you
<ul>
	<li>Force abort the consumer thread</li>
	<li>Register a Finished event handler; disposing of the wrapper doesn't mean that no more work is being done. It means that you are no longer adding items and the queue is effectively "closed". Depending on your consumer function though, this could take some time to complete. This is why it's good to hook into the finished event so you can be sure that all your processing is complete.</li>
	<li>Manually mark the queue as AddedComplete (so the thread stops blocking)</li>
	<li>Manually cancel the queue</li>
	<li>Check if the queue is ended by looking at the <code>Finished</code> property</li>
</ul>
So to reiterate, the basic idea here is
<ul>
	<li>Create a separate thread that has appropriate exception handling to be blocked while consuming the queued items</li>
	<li>Handle cancellation gracefully</li>
	<li>Be able to properly end our spawned thread so we don't have anything leftover</li>
</ul>

It should be noted that even though this wrapper is built for a single consumer/single producer design, since we are leveraging <code>GetConsumingEnumerable</code> we could modify the wrapper to allow for <a href="http://stackoverflow.com/questions/7528173/multiple-consumers-and-querying-a-c-sharp-blockingcollection" target="_blank" rel="noopener noreferrer">multiple threads acting as consumers</a> on the same enumerable.  This could give us a single producer/multiple synchronized consumer pattern where only one consumer thread gets the particular item but multiple consumer threads exist and can do work.

Full source and tests provided at our <a href="https://github.com/blinemedical/BlockingCollectionWrapper">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>532</wp:post_id>
		<wp:post_date><![CDATA[2012-11-23 10:56:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-23 15:56:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[async-producerconsumer-the-easy-way]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="design-patterns"><![CDATA[design patterns]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[asynch-producerconsumer-the-easy-way]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[940776371]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561850503;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4435;}i:1;a:1:{s:2:"id";i:4394;}i:2;a:1:{s:2:"id";i:2447;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>5</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-11-19 18:22:42]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-11-19 18:22:42]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[" The basic pattern is you add items from one thread into it and you have another thread that would be blocked on the collection until items are added." - This sentence is a little awkward. Mostly the beginning.

"First I created the blocking collection wrapper and wrapped it in a using block. It's disposable because" - What's disposable?

I also made a few grammatical edits, let me know if they aren't actually correct!

Thanks for producing all this helpful stuff, Anton!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>6</wp:comment_id>
			<wp:comment_author><![CDATA[Single producer many consumer | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/single-producer-consumer/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-26 22:31:37]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-26 22:31:37]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] BlockingCollection in .NET supports thread safe multiple consumers, but only 1 item will ever get dequeued from your collection. That means that if you have multiple threads waiting on a consuming enumerable, only one of them will get a result (not all of them). That&#8217;s not that good if you want to have copies of your item dispatched to multiple subscribers. But, if that is what you want, check out this other post of mine. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>7</wp:comment_id>
			<wp:comment_author><![CDATA[Guest]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[Guest@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[142.134.59.127]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-06-25 16:57:27]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-06-25 16:57:27]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Please provide a license to your code on your github page]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>8</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[64.202.160.73]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-08 00:17:25]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-08 00:17:25]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[All the licenses are MIT]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>7</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>9</wp:comment_id>
			<wp:comment_author><![CDATA[Steve Hayles]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[stevehayles@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[86.164.249.73]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2017-02-24 20:06:50]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2017-02-24 20:06:50]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi,

Nice idea and it works fairly well but I think the code in QueueConsumer needs some attention

Calling cancel() on the _cancellationTokenSource will throw an OperationCancelledException which you catch but it will effectively bypass the code setting Finished to true and firing any attached Finished eventhandler.

This should probably go in a finally section after the exception handlers.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Inter process locking</title>
		<link>https://onoffswitch.net/2012/10/12/inter-process-locking/</link>
		<pubDate>Fri, 12 Oct 2012 14:54:01 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=738</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/inter-process-locking/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

Locking in a single-process multi-threaded application is important enough to understand, but locking in a multi-process application takes on a new level of complexity. <a href="http://www.albahari.com/threading/part2.aspx#_Locking">Locking</a> makes sure that only one execution unit ever accesses a <a href="http://en.wikipedia.org/wiki/Critical_section">critical section</a>. This is just fancy way of saying everyone can't access the same resource at the same time; the critical section is the code path that is synchronized.
<h1>Inter process locking</h1>
There are resources that can be accessed outside of the <a href="http://en.wikipedia.org/wiki/Virtual_address_space">logical address space</a> of a process, such as files, and these are available to all processes. If you are writing a multiple process application, and are sharing these resources, you should synchronize them. For these situations, you should use a named mutex. A named mutex registers a global handle in the operating system that any process can request and use.

By giving a mutex a name, anyone can access it via its name. This is cool, but it's trickier than standard intra process locking (like using the <code>lock</code> statement on a reference object). If the mutex isn't properly handled, you can easily corrupt other programs that are expecting this mutex to function. So now instead of just crashing (or <a href="http://en.wikipedia.org/wiki/Deadlock">deadlocking</a>) your program you can crash a bunch of others! You have to really take care and understand the locking mechanism to get this right.

Though most of the complexity in locking is abstracted away you can still run into issues if you don't handle your locks properly in .NET. We always look to encapsulate reusable, and especially complex, functionality into wrapper classes or utility functions so I created the <a href="https://github.com/devshorts/Inter-process-mutex/blob/master/InterProcessMutex/InterProcessMutexLock.cs" target="_blank" rel="noopener noreferrer"><code>InterProcessMutex</code></a> class that handles the major pitfalls of named mutexes:
<ul>
	<li><strong>Permissions</strong>. One process can create a mutex that another process doesn't have access to.</li>
	<li><strong>Abandoned mutexes</strong>. If a process or thread holds a mutex but doesn't release it and then exits, it will count as an <a href="http://msdn.microsoft.com/en-us/library/system.threading.abandonedmutexexception.aspx">abandoned mutex</a></li>
	<li><strong>Initial ownership</strong>. It can be somewhat confusing as to who owns the mutex initially. The wrapper makes sure that nobody initially owns the mutex, so its open for the taking by anyone</li>
</ul>
All you have to do now is use the <code>InterProcessMutex</code> in a <code>using</code> block and it clearly indicates the critical section. Any process can instantiate the same lock and the wrapper takes care of the rest. Take a look at our <a href="https://github.com/devshorts/Inter-process-mutex" target="_blank" rel="noopener noreferrer">github</a> for full source and unit tests.

[csharp]
using (new InterProcessMutexLock(mutexName))
{
     // critical section
}
[/csharp]

Beyond using locking mechanisms built into the framework and helpful wrapper classes, it's also important to understand exactly how locking works (both intra- and inter-process locks). Since it's good to know what the magic behind the scenes is doing, we'll first go over a general definition of locking, and then delve into a couple different types of locks and how they are implemented.  For anyone interested, <a href="http://www.amazon.com/Operating-Concepts-Seventh-Abraham-Silberschatz/dp/0471694665/ref=sr_1_1?s=books&amp;ie=UTF8&amp;qid=1348423387&amp;sr=1-1&amp;keywords=operating+systems+and+concepts+7th+edition" target="_blank" rel="noopener noreferrer"><em>Operating System Concepts</em></a> is a great book and I recommend you read it if you are curious about operating system algorithms. It's a fun read and has great easy to digest explanations with examples.
<h1>Locking overview</h1>
Locking is a general term describing the solution to the critical section problem. The solution has to satisfy three conditions. I'm going to use the term <code>execution unit</code> to describe both process and threads.
<ul>
	<li><strong>Mutual exclusion</strong>. If an execution unit is in a critical section, then no other execution unit can be in its critical section</li>
	<li><strong>Progress</strong>. Execution units outside of their critical section can't block other execution units. If more than one execution unit wants to enter the critical section simultaneously, then there is some deterministic outcome of who can enter (i.e. nobody waits forever, someone gets to choose)</li>
	<li><strong>Bounded waiting</strong>. Anyone waiting on a critical section will eventually be able to get in</li>
</ul>
<h1>Locking by disabling preemption</h1>
In old style kernels, on single processor machines, the original way of doing critical sections was to disable <a href="http://en.wikipedia.org/wiki/Preemption_(computing)">preemptive</a> <a href="http://en.wikipedia.org/wiki/Interrupt">interrupts</a>. Processors can dispatch interrupts that the kernel can catch, block any currently executing processes, and execute some unit of work before returning processes back to what they were doing. Basically, it's a "<em>hey, stop what you are doing, do this other thing, then go back to what you are doing</em>" kind of thing. When a critical section was going to be reached, the kernel paused all the interrupts. When the critical section was done it resumed them. This kind of sucks, though, because it would stop everything (like your clock) from getting updated. On multi-processor systems, which is most modern day computers, this isn't even a feasible solution since you really don't want to stop all interrupts from happening on all cores.
<h1>Spin locks</h1>
Spin locking is a type of <a href="http://en.wikipedia.org/wiki/Busy_waiting">busy wait</a> lock and is used by the kernel internally when there isn't going to be much contention. While it wastes CPU cycles, it saves on overhead in context switching and process rescheduling. It can also be implemented in a single space, user or kernel, so you save on space switching overhead. The downside is that if the spinlock is held for a long duration, it will be pretty wasteful. Just try putting in an empty <code>while(true);</code> in your code to see!

In a spinlock, the execution unit continually tests a condition to see if its true. If false, it continues with the critical section. If true, it then just keeps testing. In most modern architectures there are instructions that let us test and toggle a variable <a href="http://en.wikipedia.org/wiki/Linearizability">atomically</a> (in one CPU instruction) which we can leverage to write a spinlock. There are two ways of doing this:
<ul>
	<li><a href="http://en.wikipedia.org/wiki/Test-and-set">Test and Set</a>. This sets the value of the passed in address to a new value, but returns the original value of the address. i.e. If you pass in <code>&amp;lock</code> that is set to 0, it will set <code>lock</code> to 1 and return 0.</li>
	<li><a href="http://en.wikipedia.org/wiki/Compare-and-swap">Compare and Swap</a>. This is basically like test-and-set but only toggles the value if the testing address is the same as the input. Compare and swap is a more general version of test and set and is used in modern day architectures. We'll trace through this later in the post</li>
</ul>
<h1>Test and Set</h1>
Test and set is an atomic function that generally looks like this (the function is atomic when it's executed at the processor level, not in c pseudocode)

[csharp]
bool testAndSet(int * lock){
   int previousLockValue = *lock;
   *lock = 1;
   return previousLockValue == 1;
}
[/csharp]

And can be used to spinlock a critical section like below

[csharp]
int lock = 0;

void synchroFunction(){
    // check if we can aquire the lock
    // spin wait here.

    while (testAndSet(&amp;lock)){
       // do nothing
    }

    // critical section

    // make sure to release the lock
    // the next testAndSet will return false, reset the lock
    // and exit the spinlock
    lock = 0;
}
[/csharp]

Following the example, if it's not locked yet (initial lock is false), then the first execution unit acquires the lock and sets the lock to true. It also bails out of the while loop to execute its critical section, since it returned false from the <code>testAndSet</code> function (nobody held the lock). At this point it has the lock, and continues to have the lock, until it later sets the lock to false (which is usually an atomic function as well).
<h1>Compare and Swap</h1>
In the <a href="http://www.garlic.com/~lynn/2001e.html#73">1970's</a>, compare-and-swap replaced test-and-set for most architectures and is still used today for lock-free algorithms as well as lock handling. It looks something like this (again remember this example is not atomic code, this is only atomic when this instruction is implemented in the cpu):

[csharp]
compare_and_swap(int *addr, int currentValue, int newVal){
 	int addressValue = *addr;
 	if(addressValue == currentValue){
 		*addressValue = newVal;
 	}
 	return addressValue;
}
[/csharp]

Compare and swap takes the address of an item storing the lock as well as a captured snapshot of whatever lock value an execution unit has and the expected new value. It only updates the lock reference if the captured value is equal to the value in the address.

[csharp]
int lock = 0;

const int LOCKED = 1;

void synchroFunction(){

    // check if we can aquire the lock
    // spin wait here.

    while (compare_and_swap(&amp;lock, lock, LOCKED)){
       // do nothing
    }

    // critical section

    // make sure to release the lock
    toggleLock(&amp;lock);
}
[/csharp]

Lets trace it, remembering that the lock address only gets set if the passed in lock argument is the same as the address. If the initial value of <code>lock = 0</code>, the trace looks like this. Lets pretend the address of <code>lock</code> is <code>0xABC</code>
[table]
PROCESS,ARGUMENTS, *(0xABC),RETURNS,END RESULT
ProcessA,(0xABC 0 1), 1, 0, aquired lock
ProcessB ,(0xABC 0 1), 1, 1, spins
[/table]

<em>(<code>*(0xABC)</code> is the value at address 0xABC) </em>

Process A does the compare and swap, passing in what it thinks the value of the current lock is (0). At the same time, Process B executes compare and swap, also passing in the value of 0 for the lock. But only one of them gets to execute the instruction, since the instruction is atomic. Assuming Process A executed first, it sets the value at the address of lock (0xABC) to 1 and returns 0 (the original lock value). This means it acquired the lock and exits its spinlock, since 0 was returned. Then Process B executes its compare-and-swap and finds that the value at address lock (0xABC) is already 1, but it passed it the original value of 0, so it does NOT get to acquire the lock and returns the current value of the lock (1). It keeps spinwaiting.

In C# a compare-and-set equivalent is the <a href="http://msdn.microsoft.com/en-us/library/bb297966.aspx"><code>Interlocked.CompareExchanged</code></a> function.
<h1>Spin locks without atomic instructions</h1>
On processors that didn't have atomic swap functions, spin locks were implemented using <a href="http://en.wikipedia.org/wiki/Peterson's_algorithm">petersons algorithm</a>. The idea here is you have an array keeping track of which process is ready to enter its critical section and a variable that is tracking who is actually in the critical section. Each execution unit only writes to its index in the array, so no contention here, and they all share the tracking variable. Eventually someone "grabs" the lock by both being ready and setting the tracker variable (by being the last to write to it). Here is a rough approximation of what that looks like. ProcessId is the current process.

In a two process example it looks like this.

[csharp]
    // ready to be in the critical section
    readyArray[currentProcessId] = true;

    // let anyone else get into the critical section
    turndId = otherProcessId;

    while (readyArray[currentProcessId] == true &amp;&amp; turndId == otherProcessId)
    {
        // busy wait
    }

    // critical section

    ...

    // end of critical section. we're no longer ready to be in the section anymore
    readyArray[currentProcessId] = false;
[/csharp]

When a process who is ready to get into the critical section marks that its ready. The next variable <code>turnId</code> is the source of the contention. Someone is going to set it, but both won't be able to set it. Whichever write actually succeeds blocks the other process forcing it to go into a spinlock. When the acquired process is done, it'll toggle its <code>readyArray</code> value and the waiting process breaks out of its busy wait and executes.
<h1>Mutexes</h1>
Mutexes accomplish the same goals as spinlocks, but differ in that they are an operating system provided abstraction that tells the OS to put a thread to sleep, instead of busy wait. With a mutex, threads/processes wait on a certain memory address using a <a href="http://www.helenos.org/doc/design/html.chunked/sync.html#id2531479">wait queue</a>. When the value at that address is changed, the OS wakes up all the waiting execution units and they can attempt to re-acquire a lock. They're more complicated to write, and I won't go into them. For more info read up on <a href="http://lwn.net/Articles/360699/">futexes</a> in linux which are a good explanation of how to build mutexes.
<h1>Locks in C#</h1>
Finally, we can briefly touch on the <a href="http://stackoverflow.com/questions/5111779/lock-monitor-internal-implementation-in-net"><code>lock</code></a> keyword. C# uses a <a href="http://stackoverflow.com/questions/301160/what-are-the-differences-between-various-threading-synchronization-options-in-c">monitor,</a> which is basically a combination of kernel space mutexes and user space spinlocking to implement the <code>lock</code> keyword. <a href="http://stackoverflow.com/questions/5111779/lock-monitor-internal-implementation-in-net">Internally</a>, it uses the compare-and-swap atomic instruction to first try and aquire the lock, using a spinwait lock. If a thread sits in a spinwait for too long, then it can be switched over to use a mutex. This way it tries to gracefully level the playing field: fast locking if the lock isn't contended, but less cpu cycles if its going to wait too long in a spinlock.
<h1>More information</h1>
For more reading check
<ul>
	<li><a href="http://bartoszmilewski.com/2008/09/01/thin-lock-vs-futex/" target="_blank" rel="noopener noreferrer">Thin lock vs futex</a> - By Bartosz Milewski</li>
	<li><a href="http://stackoverflow.com/questions/1485924/how-are-mutexes-implemented" target="_blank" rel="noopener noreferrer">How are mutexes implemented</a> - (stackoverflow question)</li>
	<li><a href="http://www.bluebytesoftware.com/blog/2009/02/24/TheMagicalDuelingDeadlockingSpinLocks.aspx" target="_blank" rel="noopener noreferrer">Spinlocking deadlock avoidance</a> - by Joe Duffy</li>
	<li><a href="http://pages.cs.wisc.edu/~remzi/Classes/537/Spring2011/Book/threads-locks-hw.pdf" target="_blank" rel="noopener noreferrer">Locks and hardware support</a> - chapter 27 from Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau's free online operating systems book</li>
	<li><a href="http://attractivechaos.wordpress.com/2011/10/06/multi-threaded-programming-efficiency-of-locking/" target="_blank" rel="noopener noreferrer">Efficiency of locking</a> - by Attractive Chaos (anonymous)</li>
	<li><a href="http://software.intel.com/en-us/articles/effective-implementation-of-locks-using-spin-locks/">Effective implementations of spinlocks</a> - via intel</li>
</ul>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>738</wp:post_id>
		<wp:post_date><![CDATA[2012-10-12 10:54:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-10-12 14:54:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[inter-process-locking]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
		<category domain="post_tag" nicename="synchronization"><![CDATA[synchronization]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[853995183]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561472792;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:383;}i:1;a:1:{s:2:"id";i:390;}i:2;a:1:{s:2:"id";i:2447;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>10</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-10-11 15:29:34]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-10-11 15:29:34]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA["We always look to encapsulate reusable, and especially complex, functionality into wrapper classes or utility functions, so I created a nice InterProcessMutex class that handles the major pitfalls of named mutexes:" - I can't tell if this is a run-on sentence.

"Beyond using locking mechanisms built into the framework and helpful wrapper classes, it's also important to understand exactly how locking works (both intra- and inter-process locks). Since it's good to know what the magic behind the scenes is doing, we'll first go over a general definition of locking, and then delve into a couple different types of locks and how they are implemented.

With that in mind, I brushed off the old operating systems textbook and fired up my google-fu, since I had forgotten how locks are actually implemented and needed a refresher. For anyone interested, Operating System Concepts is a great book and I recommend you read it if you are curious about operating system algorithms. It's a fun read and has great easy to digest explanations with examples." - The first paragraph doesn't flow well into the second.

Other than that, I added a few commas and other tiny grammatical things. Looks good, even though I only understand the tiniest bit what it said! :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>Run with real data</title>
		<link>https://onoffswitch.net/2012/10/29/run-with-real-data/</link>
		<pubDate>Mon, 29 Oct 2012 21:29:24 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1043</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/run-with-real-data/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

More often than not the test data in development environments is full of garbage. Most applications have a baseline set of test data, just enough to get the database to function. As engineers develop they tend to make heavy use of "<em>asdf</em>" and "<em>dfkkfklkasdflsaf</em>," combined with a liberal sprinkling of <a href="http://en.wikipedia.org/wiki/Lorem_ipsum">lorem ipsum</a> and other nonsense, to populate new data. This is fine to get started, since frequently as you develop you need to wipe the dataset clean and start over but this false data gives an incorrect view of the applications interface and performance. No client is going to have 5 "<em>asdf</em>" fields as their data. Instead, visible fields get stressed and assumptions about how your application handles data are challenged. You may not have expected a particular combobox to display a 200 character item, but that's what the client needs. Maybe you didn't expect a certain list page to grow to 20,000 items, so you never paginated or set default filters. But, that's the data the client created, and you need to account for it.

An application that is sleek and zippy at first, can become a monstrous behemoth when faced with data sets it doesn't expect to handle. It can freeze or crash, but even worse is the "slow death." The app loses speed slowly, over time, and becomes frustrating or annoying to use.

There are two sides to the story here; testing with empty data sets, and testing with real world large data sets (if you can get a hold of them).
<h1>Clean Data Sets</h1>
Clean data sets expose a specific set of problems relating to the client's first application experience. It should have the bare minimum of what a client will see when they first install, or start using your application. As you're developing, it's easy to forget what things are like for a client. You work in a world of test data, but the clean data set can expose all sorts of small things: something didn't align properly, some box isn't populated with a "None" entry, all that empty space looks goofy and should be dynamically sized, or countless other minor details. These kinds of small bugs make for a bad user experience. Clean data sets expose the way your app works in the absence of data, and that's important.

The absence of user data also can expose the kind of default data you should be shipping with your app. Is your usage that every time you load up the app with a clean data set, you create item x,  y, and z? Are these common items? Will a client appreciate these things being pre-populated? If so, you should include them. Casual users appreciate default values, since it can get them up and running quickly without the need for boilerplate. Maybe you should offer batch input functionality, so the client can quickly go from zero to usable. Working with a constant set of test data will never reveal these things, you only notice them when you start fresh.

Clean data can also mean a clean install. If you are working in your development environment, or even some testing environment, make sure to fully wipe the target test machines. Go so far as reinstalling the operating system, and start from scratch. Is there some install step that you have to set a registry key by hand? What about setting permissions on a folder for some service account? You would've long since forgotten what you did, since incremental deployments don't have to do those steps. Starting fresh every so often is always a good idea.
<h1>Large Data Sets</h1>
While the clean data set matters for new users, what matters for keeping users in your app is testing the large data set. Maybe there is a power user who is using your app more than an average user, and generating tons of data. You need to account for this. Do you have automation functionality that can that be leveraged to create lots of data? If so, you should certainly be developing and testing your application with that same data. This is where you'll really notice big performance problems. Service calls can grind to a halt, display pages take a long time to render, race conditions are exposed, bottlenecks uncovered, etc. There are numerous tools available on the internet to help generate realistic data. <a href="http://www.fakenamegenerator.com/order.php" target="_blank" rel="noopener noreferrer">This one</a>, for example, creates an identity complete with credit card, social security number, birthdate, height, etc. and lets you order in bulk for free (up to 50,000 users).

It's one thing to use a large data set of test data you generate, but it's another to get a hold of real client data. A client can slowly generate data over years of use, and it's extremely difficult to mimic that much real data in a controlled QA environment. When you use real client data, you can almost immediately find aggravation points and quickly address them. Does the app take longer than normal to load? Maybe the user thinks that's normal, but you know it's not. Do visual elements still work properly with the client data set? Do things need to be tweaked so you can see the data better, and the client can more easily do their work? The client may never have reported an issue but just because they never reported it doesn't mean it's not there.

Client data also can have missing data in areas you wouldn't expect. Test data often fills in all the blanks, but clients could be focusing heavily in an area that wasn't originally designed to be used that heavily, or vice versa, they aren't using an area designed for heavy load. These are details you only find when running "<a href="http://en.wikipedia.org/wiki/Eating_your_own_dog_food" target="_blank" rel="noopener noreferrer">as the client does</a>".
<h1>Client data and privacy</h1>
When using client data, you can come across sensitive personal information like credit card numbers, addresses, and <a href="http://www.hhs.gov/ocr/privacy/index.html" target="_blank" rel="noopener noreferrer">health records</a>. You should take great care to <a href="http://en.wikipedia.org/wiki/Data_masking" target="_blank" rel="noopener noreferrer">obfuscate</a> this data before ever using it. At the same time, you should strive to preserve data integrity, since completely obfuscated data can be meaningless or invalid. There's a balancing act here, but in the end it's more important to respect the privacy of clients. There are a few ways to do this, and all of these things can be automated.
<ul>
	<li><strong>Full masking</strong>. Here, you would replace all words and numbers with other random words and numbers. You want to preserve capitalization, punctuation, and word length, but you can replace words with garbage. This gives you an indication of length, format, and usage. Don't just replace them randomly, though. If you find a word, create a replacement for it and keep track of it. If you see the same word again, use the same replacement. This can tell you word frequency, and if something is being continually re-iterated by the client. These patterns can also help identify areas to automate client actions.</li>
	<li><strong>Partial masking</strong>. With partial masking, you can just do sensitive areas such as usernames, addresses, phone numbers, statistics (such as test scores or health records) and any other kind of personal identifying information. Doing partial masking maintains data context, from which you can infer client intentions.</li>
	<li><strong>Adjust dates</strong>. Instead of using the actual date, offset all date groupings by a random time. By doing this you can maintain date relationships (i.e. if A and B are related and happened at 10 minutes apart, you will maintain that relationship) but you don't need to know what was the original A and B. Offsetting all related groups gives you meaningful, but at the same time obfuscated, dates.</li>
	<li><strong>Network addresses</strong>. This is an easy one to overlook. If you store network addresses anywhere, you should change them to point to known local machines, or make them invalid. If your client is open via publicly accessible routes, or even through an internal provided VPN, you don't want your development or test machines to accidentally contact their computers and apply edits.</li>
	<li><strong>Encryption</strong>. If possible, encrypt your client data when you store it. If your machines get compromised you don't want to have accidentally also compromised your clients data.</li>
</ul>
<h1>Application under load</h1>
If you use a large data set you should simulate client load scenarios. The application may function wonderfully when only one or two people use it, assuming a shared distributed app. What happens when 200 people hammer on it at once? What about 2000? Is it possible for a client to have a virus scanner running? Is it thrashing the disk? Can we make optimizations that help these scenarios? Can we make these optimizations configurable? I frequently find myself writing test-apps that spawn multiple threads, and do actions at some insane interval (like every 10ms). This way, you can test areas of the application thoroughly for performance and reliability of your system.

I think it's also worthwhile to use your application, like a user would, when you are stress testing it. Develop against it! You'll find what annoys you, what doesn't work, what works well, etc. Bullet proofing your app as best as possible against these scenarios is what is going to make a client love using your program.
<h1>Improvements</h1>
I've found that there are a few quick places you can always look to find improvements
<ul>
	<li><strong>Data over the wire</strong>. Always check data transport costs. Whether this is inter process communication, or client to server, it doesn't matter. Sending data isn't cheap and you should minimize what you send. Check the cost of serialization, remove extraneous data, sever object cycles, ideally map things to a DTO.</li>
	<li><strong>Add a facade</strong>. Sometimes things just aren't designed to handle the data from the get go. An easy way to separate logical concerns is to put a facade in front of problem areas. This is mostly useful for storage classes and service calls. Having a facade that can translate your storage calls into view specific DTOs keeps your storage logic separate from your view logic. It also acts as a programmatic firewall. You can work behind the facade to fix underlying issues. As long as the front of the facade still works, then you've segmented the areas.
<ul>
	<li>It should be noted that there is a limit to things you want to put a facade over. Don't use a facade to sweep problems under the rug. It should be used to give you better decoupling and breathing room to work.</li>
</ul>
</li>
	<li><strong>Side effects</strong>. Is there something that is hanging around after some action? Are file's not cleaned up? Is the disk fragmented? Are sockets sitting in CLOSED_WAIT and not properly getting <a href="http://stackoverflow.com/questions/898828/c-sharp-finalize-dispose-pattern" target="_blank" rel="noopener noreferrer">disposed</a> of? Sometimes you won't notice these things on the small scale, but in a larger scale they can become major issues.</li>
	<li><strong>Run a profiler</strong>. For managed (C#) applications, a profiler is a no-brainer. Find that random linq statement that is getting re-evaluated in a loop. You'd be surprised at how many easy wins you can find when you profile your code.</li>
	<li><strong>Think about caching</strong>. I'm not suggesting everything should be cached, or that it's always an appropriate solution. Frequently, though, slow-downs can be related to pulling more data than you need, or more often than is necessary. I once found a bug where we were marshaling 4MB of data from unmanaged to managed code every 50ms. This was a huge bottleneck, but we only found it when we simulated heavy user load. A simple cache on the data, that was invalidated when the source data was changed, let us scale to 10 times the load with minimal effort and no major code changes.</li>
	<li><strong>Think about bottlenecks</strong>. Is something CPU bound or IO bound? If it's IO bound, does it have to be? Can you do it in chunks? Can it be asynchronous? If it's CPU bound, can it be batched? Does it have to happen now? Can it be distributed? Run tools like perfmon and the sysinternals suite to see what things are actually doing. Maybe you don't realize how often you are hitting the disk. If you avoided opening the file 50 times, and just read it all at once, things would go faster. Maybe use a memory map and dump it to disk at intervals. Is there a database call in a loop? Change the SQL to pull back more at once, instead of little by little. Do you have a for loop that you're hitting frequently? Maybe translate it to a dictionary, and use it as a lookup instead. Small changes that are run frequently can add up.</li>
	<li><strong>Ordering</strong>. Sometimes, all you need to do is change the order in which something happens. Is something that is taking a long time, blocking something that takes a short time? Invert the sequence. Have the fast thing happen first, then the later ones. This gives the user an impression that something is happening, and not just broken.</li>
	<li><strong>Optimize the 90% case</strong>. If something happens a lot, try and optimize it. This can cut down logarithmically on perceived performance.</li>
	<li><strong>Progress</strong>. If you can't speed something up, give an indication of progress. People are less likely to get frustrated if they know things are actually happening, and not just sitting there</li>
	<li><strong>Cancellation</strong>. If something takes a long time, give the user the option to cancel it. Maybe they didn't mean to hit that button that kicks off 20 minutes of work. They should be able to end the sequence, if they want.</li>
</ul>
There are obviously lots of other things you can do. In the end, you should remember the following: when you get pissed off at your tools for being slow, non-responsive, or difficult to use, think about how your application is the client's tool. Save them the grief.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1043</wp:post_id>
		<wp:post_date><![CDATA[2012-10-29 17:29:24]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-10-29 21:29:24]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[run-with-real-data]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="best-practices"><![CDATA[Best Practices]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[880047619]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[development-datasets]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559148092;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1268;}i:1;a:1:{s:2:"id";i:4800;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Auto reset vs manual reset</title>
		<link>https://onoffswitch.net/2012/10/11/auto-reset-vs-manual-reset/</link>
		<pubDate>Thu, 11 Oct 2012 19:33:56 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1119</guid>
		<description></description>
		<content:encoded><![CDATA[Pulse vs set]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1119</wp:post_id>
		<wp:post_date><![CDATA[2012-10-11 19:33:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-10-11 19:33:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[auto-reset-vs-manual-reset]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Dropped packets with promiscuous raw sockets and winsock</title>
		<link>https://onoffswitch.net/2012/11/09/dropped-packets-with-promiscuous-raw-sockets/</link>
		<pubDate>Fri, 09 Nov 2012 22:34:56 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1268</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/dropped-packets-with-promiscuous-raw-sockets/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

Lately in my spare time, I've been working on a tool that will decode serialized AMF over a tcp connection. AMF stands for <a href="http://en.wikipedia.org/wiki/Action_Message_Format" target="_blank" rel="noopener noreferrer">action message format</a> and is used to serialize binary data to actionscript applications. The idea is to have the tool work the way <a href="http://www.charlesproxy.com/" target="_blank" rel="noopener noreferrer">Charles</a> does for JSON/AMF over http/https, except over TCP sockets. I really like the way Charles works, and it'd be nice to not have to go to <a href="http://www.wireshark.org/" target="_blank" rel="noopener noreferrer">Wireshark</a> and try and piece through binary data when I'm debugging.

So how would I do this? TCP sockets are connection oriented, you connect to some host and port and you only recieve and send data to that port. That's great and all, but you can't always inject yourself as a proxy in a connection; it'd be nice to be able to just sit in the middle of a conversation and observe without interfering. Thankfully you actually can do this by creating a <a href="http://en.wikipedia.org/wiki/Raw_socket" target="_blank" rel="noopener noreferrer">raw</a> <a href="http://en.wikipedia.org/wiki/Promiscuous_mode" target="_blank" rel="noopener noreferrer">promiscuous</a> socket which captures all information regardless of port. This lets you inspect data like ip headers and tcp/udp/icmp/etc headers of all packets going through your network card (regardless if they are even for you!).

Raw sockets work the way they sound; they gives you the raw information including IP headers and other protocol headers (depending on which mode you set the socket in). Promiscuous mode tells your network card to not filter packets based on port or IP, just to give you everything. This way you can inspect all the packets going through your machine. For my project, a <a href="http://tech.blinemedical.com/author/faisal-mansoor/" target="_blank" rel="noopener noreferrer">coworker</a> suggested I use <a href="http://www.WinPcap.org/" target="_blank" rel="noopener noreferrer">WinPcap</a> but I didn't want to create a hard dependency (you need to install a driver) to it for what I thought would be some basic c++ so I started off with just raw sockets.

Initially, this worked great. I was able to re-assemble fragmented tcp packets, inspect IP/TCP headers, correlate data by source/destination port, etc. pretty easily. But I noticed that sometimes when I was reassembling a large packet that was fragmented (greater than the <a href="http://en.wikipedia.org/wiki/Maximum_transmission_unit" target="_blank" rel="noopener noreferrer">MTU</a>), I wouldn't get all of the packets. I fired up Wireshark and compared my results. It was pretty clear that I was missing almost half of the remaining packets. When the data sent was less than the MTU, I got it just fine. Clearly something was being dropped, and I wasn't <a href="http://developerweb.net/viewtopic.php?id=5885" target="_blank" rel="noopener noreferrer">the only one</a> who had this problem.

Maybe I just wasn't reading fast enough? I commented out all code other than directly reading off the socket and just printed the size of the packets I got. Even then, I still wasn't matching up to what Wireshark had.

Here was a stripped down version of my basic capture code for reference, nothing fancy. Just reading off a socket created with <code>SOCK_RAW</code>

[c language="++"]
#define High4Bits(x)  ((x&gt;&gt;4) &amp; 0x0F)

void Run()
{
	sockaddr_in socketDefinition;

	socketPtr = socket( AF_INET, SOCK_RAW, IPPROTO_IP   );

	BindSocketToIp(socketPtr, socketDefinition);

	CreatePromisciousSocket(socketPtr);

	while(true){
		int bytesRead = recv( socketPtr, packet, LS_MAX_PACKET_SIZE, 0 );

		if ( bytesRead-&gt;ver_ihl) != 4 ){
			delete packet;
			return;
		}

		ipHeaderSize = Low4Bits(ipHeader-&gt;ver_ihl);
		ipHeaderSize *= sizeof(DWORD);

		switch( ipHeader-&gt;protocol )
		{
		case 6: // TCP
			{
				char * tcpHeaderStart = &amp;packet[ipHeaderSize];

				if(TargetPortFound(tcpHeaderStart)){

					printf(&quot;Got tcp/ip packet size %dn&quot;, bytesRead);
				}

				break;
			}
		}
	}
}

void CreatePromisciousSocket(SOCKET socketPtr){
	int optval = 1;
	DWORD dwLen = 0;

	if ( WSAIoctl( socketPtr,
		SIO_RCVALL,
		&amp;optval,
		sizeof(optval),
		NULL,
		0,
		&amp;dwLen,
		NULL,
		NULL ) == SOCKET_ERROR )

	{
		printf( &quot;Error setting promiscious mode: WSAIoctl  = %ldn&quot;, WSAGetLastError() );
		throw &quot;Error setting promsocous mode&quot;;
	}
}

void BindSocketToIp(SOCKET socketPtr, sockaddr_in socketDefinition){
	char localIp[20] = &quot;192.168.1.2&quot;;

	socketDefinition.sin_family = AF_INET;

	socketDefinition.sin_addr.s_addr = inet_addr(localIp);

	if ( bind( socketPtr, (struct sockaddr *)&amp;socketDefinition, sizeof(socketDefinition) ) == SOCKET_ERROR )
	{
		printf( &quot;Error: bind = %ldn&quot;, WSAGetLastError() );
		throw &quot;Error binding&quot;;
	}
}

bool TargetPortFound(char *packet, int targetPort)
{
	TCPHEADER *tcp_header = (TCPHEADER *)packet;

	if(htons(tcp_header-&gt;source_port) == targetPort){
		return true;
	}

	return false;
}

[/c]

The side by side comparison of filtering on port 21935 was:

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/11/wireSharkPacketsBroken..jpg"><img class="aligncenter size-medium wp-image-1414" src="http://tech.blinemedical.com/wp-content/uploads/2012/11/wireSharkPacketsBroken.-300x151.jpg" alt="" width="300" height="151" /></a>

Bear with me while I explain what we're looking at here. This is a capture of a 50k AMF response simultaneously with Wireshark (on the right) and my program using raw sockets (on the left). Both sides should have had about 40 packets come through, but you can see that my program on the left is missing a bunch of the larger packets compared to Wireshark on the right. After the 40 packets came through a small 38 byte AMF message was sent and you can see that both my program and Wireshark got the packet. Somehow a bunch of packets went missing for me! Don't be confused by the different numbers. On the left hand side (my program using raw sockets), the size includes IP header, TCP header, and payload length. On the right, in Wireshark, I've highlighted JUST the data payload. So the highlighted area on the right is 40 bytes LESS than the highlighted area on the left (IP header is 20 bytes, and TCP header is 20 bytes). So if you see 78 on the left, that's really a 38 byte payload plus 40 bytes of header. This can make things a little confusing, but it all matches up.

<a href="http://tangentsoft.net/wskfaq/advanced.html" target="_blank" rel="noopener noreferrer">TangentSoft's</a> advanced winsock FAQ tipped me off to the actual problem:
<blockquote>Most other common desktop operating systems have some way to ask the kernel to do some of the filtering for you. Not so with SIO_RCVALL. You want this, because your program is probably interested in only some packets, so you have to filter out the ones you aren’t interested in. At gigabit speeds, it can take a surprising amount of CPU power to do this. <strong>You might not be able to do it fast enough to prevent the kernel from running out of buffer space, forcing it to drop packets</strong>. Doing at least some of the filtering in the kernel can make this practical, since it saves a kernel to user space context switch for each filtered packet.</blockquote>
It turns out the kernel couldn't buffer enough information for me. The raw socket was giving me all the packets going through my NIC, not just the ports that I wanted and each packet that I got required a kernel to user space context switch. Just watch Wireshark with just a tcp filter and see how much traffic is going through, it's more than you think.

The socket's <a href="http://stackoverflow.com/a/1507551/310196" target="_blank" rel="noopener noreferrer">default buffer</a> size is only 8k so if I am making a single call that is sending 50k of AMF then that can easily bump out other packets and also get bumped out itself! While I could've <a href="http://msdn.microsoft.com/en-us/library/windows/hardware/ff570832(v=vs.85).aspx" target="_blank" rel="noopener noreferrer">increased the buffer size</a>, the documentation says it's only available on Windows Vista and later, and only some protocols (like TCP) support it. If I ever wanted to expand my tool to use other protocols then I could face this issue again. On top of that, in researching about the socket buffer size I found this quote from an old <a href="http://www.sockets.com/ch16.htm" target="_blank" rel="noopener noreferrer">windows socket's programming</a> book:
<blockquote><strong>You can (and should) avoid dependence on some optional features by redesigning your application</strong>. For example, you shouldn't require a specific amount of receive buffer space for your application to function. This <strong>doesn't require WinSocks to support the SO_RCVBUF socket option</strong>, so you may not be able to specify the system buffer space you get.</blockquote>
Through the course of my research I'd uncovered a whole slew of negative reasons to use raw sockets other than the dropped packets. Raw sockets aren't supported on all Windows versions (<a href="http://seclists.org/nmap-hackers/2005/4" target="_blank" rel="noopener noreferrer">like Windows XP</a>) and you had to be an admin to run an application using raw sockets. This means distribution of this app or using it on a client to debug could be problematic.

At this point I was frustrated enough to scrap my raw sockets idea and switch to WinPcap. WinPcap works lower in the networking stack than a raw socket does. Raw sockets work at level 3 (network layer) but WinPcap and its associated driver sit at level 2, the data link layer. Just as a reminder, here is the ubiqutous "TCP/IP Stack" (image taken from <a href="http://www.tcpipguide.com/free/t_TCPIPArchitectureandtheTCPIPModel-2.htm" target="_blank" rel="noopener noreferrer">tcpipguide.com</a>)

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/10/tcpiplayers.png"><img class="aligncenter size-medium wp-image-1284" src="http://tech.blinemedical.com/wp-content/uploads/2012/10/tcpiplayers-300x257.png" alt="" width="300" height="257" /></a>

The real power of WinPcap is the kernel-level filtering it can do based on filter text you pass it, alleviating you costly context switches. Remember that Wireshark filter you always put in? This is what it does.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/10/wiresharkfilter..png"><img class="aligncenter size-full wp-image-1333" src="http://tech.blinemedical.com/wp-content/uploads/2012/10/wiresharkfilter..png" alt="" width="623" height="28" /></a>

Once I switched over to WinPcap instead of just pure raw sockets I started getting all my data without having to increase any buffer sizes. Thankfully the WinPcap <a href="http://www.winpcap.org/docs/docs_412/html/group__wpcap__tut.html" target="_blank" rel="noopener noreferrer">examples</a> are well documented, and it's a pretty close drop-in for raw sockets anyways, so the amount of work to switch over was pretty minimal.

Here are the new side by side screenshots of my capture application vs Wireshark. This time, all the packets are there. The data sizes I've highlighted match up this time because with WinPcap I'm actually getting the ethernet frame header, the ip header, AND the tcp header. This matches with the "length" field in Wireshark.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/10/wireSharkPackets.jpg"><img class="alignnone size-medium wp-image-1345" src="http://tech.blinemedical.com/wp-content/uploads/2012/10/wireSharkPackets-300x140.jpg" alt="" width="300" height="140" /></a>

After I had done all this socket research, and already changed my code to use WinPCap, I went back and increased the buffer size as a test and I was able to finally get all the packets I wanted. In the end all I really needed was the following snippet after I had bound my socket.

[c]
int bufferLength;
int bufferLengthPtrSize = sizeof(int);

getsockopt(socketPtr, SOL_SOCKET, SO_RCVBUF, (char *)&amp;bufferLength, &amp;bufferLengthPtrSize);

printf(&quot;default socket buffer bytes %dn&quot;, bufferLength);

int buffsize = 50000;

setsockopt(socketPtr, SOL_SOCKET, SO_RCVBUF, (char *)&amp;buffsize, sizeof(buffsize));

getsockopt(socketPtr, SOL_SOCKET, SO_RCVBUF, (char *)&amp;bufferLength, &amp;bufferLengthPtrSize);

printf(&quot;updated socket buffer bytes %dn&quot;, bufferLength);
[/c]

This prints out:

[c]
default socket buffer bytes 8192
updated socket buffer bytes 50000
[/c]

Though while this did work, I'm glad I went with WinPCap since depending on socket throughput I might still have run into issues.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1268</wp:post_id>
		<wp:post_date><![CDATA[2012-11-09 17:34:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-09 22:34:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dropped-packets-with-promiscuous-raw-sockets]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
		<category domain="post_tag" nicename="sockets"><![CDATA[Sockets]]></category>
		<category domain="post_tag" nicename="tcp"><![CDATA[TCP]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[900402478]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561125259;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4737;}i:1;a:1:{s:2:"id";i:1587;}i:2;a:1:{s:2:"id";i:4286;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>11</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-11-05 17:41:41]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-11-05 17:41:41]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA["TCP sockets are connection oriented, you connect to some host and port and you only recieve and send data to that port. " - This sentence is (I think?) a run-on sentence. Please clarify.

At least one instance of "wireshark"  isn't capitalized. If it's a brand/product, we should capitalize all instances of it. Also true of any other software brand/product.

You switch narrative tense a lot. Generally, it's proper to stick to one. So if you start out with "I'm doing this," then don't start the next paragraph with "I went there."

Other than that, it looks good! Thanks!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>RESTful web endpoints on Netduino Plus</title>
		<link>https://onoffswitch.net/2012/12/05/rest-endpoints-with-netduino-web-server/</link>
		<pubDate>Wed, 05 Dec 2012 21:04:59 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1587</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/rest-endpoints-with-netduino-web-server/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

I have a <a href="http://www.Netduino.com/Netduinoplus/specs.htm" target="_blank" rel="noopener noreferrer">Netduino plus</a> at home and I love it. Not only can you use C# to write for it, but you get full visual studio integration including live breakpoints! I got the Netduino plus over the Netduino because the Netduino plus has a built in ethernet jack and ethernet stack support. This way I could access my microcontroller over the web if I wanted to (and who wouldn't?).

But to expose your Netduino to the web you need to write a simple web server. Basically open a socket at port 80 and read/write requests out to it. The Netduino community is great at sharing code and I quickly found a nice web server by <a href="http://www.schuurmans.cc/multi-threaded-web-server-for-Netduino-plus" target="_blank" rel="noopener noreferrer">Jasper Schuurmans</a>. His server code let you define <a href="http://en.wikipedia.org/wiki/Representational_state_transfer" target="_blank" rel="noopener noreferrer">RESTful</a> routes like this

[csharp]
http://NetduinoIPAddress/targetFunction/arg1/arg2/...
[/csharp]

Which was super cool. It even filtered out non-registered commands, allowing you to control what requests would trigger a "<em>command found</em>" event. Here is the basic main of his demo.

[csharp]
public static void Main()
{
    // Instantiate a new web server on port 80.
    WebServer server = new WebServer(80);

    // Add a handler for commands that are received by the server.
    server.CommandReceived += new WebServer.CommandReceivedHandler(server_CommandReceived);

    // Add a command that the server will parse.
    // Any command name is allowed; you will decide what the command does
    // in the CommandReceived handler. The server will only fire CommandReceived
    // for commands that are defined here and that are called with the proper
    // number of arguments.
    // In this example, I define a command 'SetLed', which needs one argument (on/off).
    // With this statement, I defined that we can call our server on (for example)
    // http://[server-ip]/SetLed/on
    // http://[server-ip]/SetLed/off
    server.AllowedCommands.Add(new WebCommand(&quot;SetLed&quot;, 1));

    // Start the server.
    server.Start();

    // Make sure Netduino keeps running.
    while (true)
    {
        Debug.Print(&quot;Netduino still running...&quot;);
        Thread.Sleep(10000);
    }
}

/// &lt;summary&gt;
/// Handles the CommandReceived event.
/// &lt;/summary&gt;
private static void server_CommandReceived(object source, WebCommandEventArgs e)
{

    Debug.Print(&quot;Command received:&quot; + e.Command.CommandString);

    switch (e.Command.CommandString)
    {
        case &quot;SetLed&quot;:
            {
                // Do you stuff with the command here. Set a led state, return a
                // sampled value of an analog input, whatever.
                // Use the ReturnString property to (optionally) return something
                // to the web user.

                // Read led state from command and set led state.
                bool state = ( e.Command.Arguments[0].Equals(&quot;on&quot;) ? true : false);
                onBoardLed.Write(state);

                // Return feedback to web user.
                e.ReturnString = &quot;&lt;html&gt;&lt;body&gt;You called SetLed with argument: &quot; + e.Command.Arguments[0].ToString() + &quot;&lt;/body&gt;&lt;/hmtl&gt;&quot;;
                break;
            }
    }
}
[/csharp]

While this certainly works, there were a few things I didn't like about this setup:
<ul>
	<li>You have to route the logic from a single switch statement. If you were building more than one restful endpoint in your Netduino, this centralized switch statement would get messy.</li>
	<li>You have to declare the target argument length when registering the command. This means that if you update the target function's argument parameters, you also have to update the registration code.</li>
	<li>The server was single-threaded. It uses events to alter program flow. But since events execute in the dispatchers thread, if your execution code took a while, you basically stalled the entire server.</li>
<li>REST endpoints were actually case sensitive</li>
</ul>
<h1>The reworked final copy</h1>
Before we dig into what I changed, lets look at my final reworked main and you can compare it to the original main I posted above:

[csharp]
public static void Main()
{
    LcdWriter.Instance.Write(&quot;Web Demo Ready!&quot; + DateTime.Now.TimeOfDay);

    WebServerWrapper.InitializeWebEndPoints(new ArrayList
                                                {
                                                    new BasicPage()
                                                });

    WebServerWrapper.StartWebServer();

    RunUtil.KeepRunning();
}
[/csharp]

Here, <code>BasicPage</code> is an object that encapsulates its route definitions as well as what to invoke when a target route is found.  Next, I'm registering the object with a web service wrapper and then starting the web server.  This way, I've removed the command handling from our main loop and encapsulated logic into individual components.


<h1>Injecting endpoints</h1>
In order to get rid of the central switch statement, I wanted to encapsulate all the logic of endpoint name, endpoint arguments, and target function to invoke in a single object. This would let me build a single class whose sole job was to be executed when the web server routed it the command. On top of that, you now can cleanly maintain endpoint state and other information all within a single object. So, if you were building an endpoint whose job is to show you the temperature of your refrigerator over the last 3 hours, you can store that information in your endpoint object and when the endpoint is invoked, print out some nice html that shows the current and historical data.

As an example, let's create a class that prints whatever arguments were received from the server onto a connected LCD. First we'll have it implement a target interface called <code>IEndPointProvider</code> which looks like this:

[csharp]
public interface IEndPointProvider
{
    void Initialize();
    ArrayList AvailableEndPoints();
}
[/csharp]
<ul>
	<li><code>Initialize</code> would be class specific initialization logic. If we don't need to use resources until we are about to fire up the server then we can put that logic into here.</li>
	<li><code>AvailableEndPoints</code> is a list of <code>EndPoints</code> that we can use to register with the server. In case you're wondering about the <code>ArrayList</code>, .NET Micro <a href="http://informatix.miloush.net/Microframework/Articles/CisFeatures.aspx" target="_blank" rel="noopener noreferrer">doesn't support generics</a>, so we're not using something like <code>List&lt;T&gt;</code></li>
</ul>
And here is my implementation of <code>IEndPointProvider</code> which echos the arguments to a connected LCD:

[csharp]
public class BasicPage : IEndPointProvider
{
    #region Endpoint initialization

    public void Initialize() { }

    public ArrayList AvailableEndPoints()
    {
        var list = new ArrayList
            {
                new EndPoint
                    {
                        Action = Echo,
                        Name = &quot;echoArgs&quot;,
                        Description = &quot;Writes the URL arguments to a serial LCD hooked up to COM1&quot;
                    }
            };
        return list;
    }

    #endregion

    #region Endpoint Execution

    private string Echo(EndPointActionArguments misc, string[] items)
    {
        String text = &quot;&quot;;
        if (items != null &amp;&amp; items.Length &gt; 0)
        {
            foreach (var item in items)
            {
                text += item + &quot; &quot;;
            }
        }
        else
        {
            text = &quot;No arguments!&quot;;
        }

        LcdWriter.Instance.Write(text);

        return &quot;OK. Wrote out: &quot; + (text.Length == 0 ? &quot;n/a&quot; : text);
    }

    #endregion
}
[/csharp]

You can see that we're exposing an array list of <code>EndPoint</code> objects that define the action to execute, what the target action's name is (i.e. the REST endpoint), and a short description about what the endpoint does (for an API listing we can create later).  

The target function <code>Echo</code> takes an <code>EndPointActionArguments</code> object that contains some state about the current connection, and a list of objects representing the variable arguments to the REST endpoint.
<h1>End point</h1>

Let's take a look at what an endpoint is. 

[csharp]
public delegate string EndPointAction(EndPointActionArguments arguments, params string[] items);

public class EndPointActionArguments
{
    public Socket Connection { get; set; }
}

public class EndPoint
{
    private string[] _arguments;

    public bool UsesManualSocket { get; set; }

    public string Description { get; set; }

    /// &lt;summary&gt;
    /// The function to be called when the endpoint is hit
    /// &lt;/summary&gt;
    public EndPointAction Action
    {
        private get; set;
    }

    /// &lt;summary&gt;
    /// The name of the endpoint, this is basically the servers route
    /// &lt;/summary&gt;
    public String Name { get; set; }

    public string[] Arguments { set { _arguments = value; } }

    /// &lt;summary&gt;
    /// Execute this endpoint. We'll call the action with the supplied arguments and
    /// return whatever string the action returns.
    /// &lt;/summary&gt;
    /// &lt;returns&gt;&lt;/returns&gt;
    public String Execute(EndPointActionArguments misc)
    {
        if (Action != null)
        {
            return Action(misc, _arguments);
        }
        return &quot;Unknown action&quot;;
    }
}
[/csharp]

An <code>EndPoint</code> has a delegate named <code>Action</code> for a function with a signature

[csharp]
string Foo(EndPointActionArguments arguments, params string[] items)
[/csharp]

The <code>Action</code> would return a string that the web server will then write back out onto the target socket. We also pass an <code>EndPointActionArguments</code> to the delegate which contains a reference to the original socket request (outgoing to the client) and serves as encapsulation if we want to add more parameters to send through to the endpoint later. The last argument is a variable list of strings that relates to the REST url argument list.

An endpoint <code>Description</code> defines what the endpoint does; we'll use this to describe the endpoint in a default API listing if the server gets a request it doesn't know about.

<code>UseManualSocket</code> is a boolean that will indicate to the server that the endpoint handled the socket request manually (i.e. it held onto the request) and that the server shouldn't close the socket; the endpoint will deal with socket cleanup.
<h1>Getting the endpoint to the server</h1>
Now that I've encapsulated action/state information into a single class, I wrapped Jasper's original web server with a new facade. The facade will hide some of the internals of the server such as starting the web server, registering endpoints (from <code>IEndPointProvider</code> instances), and provides a single entry point for found commands. When we start the server we'll pass along our registered endpoints with the actual server. If we wanted to do more endpoint manipulation later, we now have a centralized point of access before the endpoints get to the server.

Keeping with the original event dispatching mechanism, I moved the handling of the <code>EndPointReceived</code> event into the wrapper and out of the main program.

[csharp]
/// &lt;summary&gt;
/// Wrapper class on top of a multi threaded web server
/// Allows classes to register REST style endpoints
/// &lt;/summary&gt;
public static class WebServerWrapper
{
    private static WebServer _server;
    private static ArrayList _endPoints;

    /// &lt;summary&gt;
    /// Register REST endpoint for callback invocation with the web server
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;endPoints&quot;&gt;&lt;/param&gt;
    private static void RegisterEndPoints(ArrayList endPoints)
    {
        if(_endPoints == null)
        {
            _endPoints = new ArrayList();
        }

        foreach(var endPoint in endPoints)
        {
            _endPoints.Add(endPoint);
        }
    }

    public static void InitializeWebEndPoints(ArrayList items)
    {
        foreach (IEndPointProvider endpoint in items)
        {
            endpoint.Initialize();
            RegisterEndPoints(endpoint.AvailableEndPoints());
        }
    }

    /// &lt;summary&gt;
    /// Start listening on the port and enable any registered callbacks
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;port&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;enabledLedStatus&quot;&gt;&lt;/param&gt;
    public static void StartWebServer(int port = 80, bool enabledLedStatus = true)
    {
        _server = new WebServer(port, enabledLedStatus);

        _server.EndPointReceived += EndPointHandler;

        foreach (EndPoint endpoint in _endPoints)
        {
            _server.RegisterEndPoint(endpoint);
        }

        // Initialize the server.
        _server.Start();
    }

    /// &lt;summary&gt;
    /// We'll get an endpoint invocation from the web server
    /// so we can execute the endpoint action and response based on its supplied arguments
    /// in a separate thread, hence the event. we'll set the event return string
    /// so the web server can know how to respond back to the ui in a seperate thread
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;source&quot;&gt;&lt;/param&gt;
    /// &lt;param name=&quot;e&quot;&gt;&lt;/param&gt;
    private static void EndPointHandler(object source, EndPoinEventArgs e)
    {
        var misc = new EndPointActionArguments
                        {
                            Connection = e.Connection
                        };

        e.ReturnString = e.Command.Execute(misc);

        // we can override the manual use of the socket if we returned a value other than null
        if (e.ReturnString != null &amp;&amp; e.Command.UsesManualSocket)
        {
            e.ManualSent = false;
        }
        else
        {
            e.ManualSent = e.Command.UsesManualSocket;
        }
    }
}
[/csharp]
<h1>A few web server changes</h1>
Jaspers web server is simple and ingenious. I like it's simplicity and it was easy to extend. When the web server receives a request, it parses the first line of a raw http GET from the header to figure out it's "route". As an example, here is a request I generated for <em>http://localhost/function/arg1/arg2</em>. Everything after the first line is discarded since we just care about the <em>/function/arg1/arg2</em> part

[csharp]
GET /function/arg1/arg2 HTTP/1.1
Host: localhost
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8
Accept-Encoding: gzip,deflate,sdch
Accept-Language: en-US,en;q=0.8
Accept-Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3
Cookie: ASP.NET_SessionId=ue1s3blzxxwbrrohasgwpbbv
[/csharp]

Once it has the right request url from the header, the server will see if any registered endpoint <code>Name</code> property matches the request name. If it does it'll parse the remaining arguments. This all happens in <code>InterpretRequest</code>. I didn't change any of this logic. What I changed was what <code>InterpretRequest</code> returns and how the final command is dispatched. Here is the main server listening loop:

[csharp]
using (var server = new Socket(AddressFamily.InterNetwork, SocketType.Stream, ProtocolType.Tcp))
{
    server.Bind(new IPEndPoint(IPAddress.Any, Port));

    server.Listen(1);

    while (!_cancel)
    {
        var connection = server.Accept();

        if (connection.Poll(-1, SelectMode.SelectRead))
        {
            // Create buffer and receive raw bytes.
            var bytes = new byte[connection.Available];

            connection.Receive(bytes);

            // Convert to string, will include HTTP headers.
            var rawData = new string(Encoding.UTF8.GetChars(bytes));

            //====================================
            // My changes begin here
            //====================================
            EndPoint endPoint = InterpretRequest(rawData);

            if (endPoint != null)
            {
                if (_enableLedStatus)
                {
                    PingLed();
                }

                // dispatch the endpoint
                var e = new EndPoinEventArgs(endPoint, connection);

                if (EndPointReceived != null)
                {
                    ThreadUtil.SafeQueueWorkItem(() =&gt;
                        {
                            EndPointReceived(null, e);

                            if (e.ManualSent)
                            {
                                // the client should close the socket
                            }
                            else
                            {
                                var response = e.ReturnString;

                                SendResponse(response, connection);
                            }
                        });
                }
            }
            else
            {
                // if we didn't match a response return with the generic API listing
                SendResponse(GetApiList(), connection);
            }
        }

    }
}
[/csharp]

What I modified from the original server code was
<ul>
	<li>InterpretRequest now returns an <code>EndPoint</code> with a string array of arguments. Previously, it looked for only the number of arguments that were registered to it. Now it parses as much as is there giving you a clean variable argument list.</li>
	<li>Events are now dispatched in a <a href="https://github.com/blinemedical/NWebREST/blob/master/NetDuinoUtils/Utils/ThreadUtil.cs" target="_blank" rel="noopener noreferrer">custom threadpool</a>, since <a href="http://netmf.codeplex.com/workitem/78" target="_blank" rel="noopener noreferrer">.NET Micro doesn't have any</a> built in threadpooling. The threadpool is a collection of 3 threads that pull off an event queue and execute. This way the web server is asynchronous and won't ever block for other requests. You could easily just have it fire off independent threads if you wanted to, but I found a threadpool to be more effective since you don't need to spin up new threads (and allocate extra thread stack space) each time.</li>
	<li>If a request comes in that doesn't match any endpoint, we'll print out all the available endpoints with their description. This is a nice API listing for you.</li>
	<li>Event arguments contain a reference to the original socket if you need it.</li>
	<li>If an endpoint is going to to manually write to the socket and close the socket later, it can set the <code>UsesManualSocket</code> flag on registration. The wrapper then tells the server that the executed endpoint manually sent data to the socket and is expected to close it. This can be useful if you want to maintain a persistent connection in your endpoint (maybe you are streaming something per client). By default the server will write out the string response from the endpoint and close the socket.</li>
	<li>Optionally pulse the onboard LED whenever a request comes in. This is useful for debugging and viewing activity.</li>
	<li>I updated the code that searched for endpoint name and compared it to url request to be case insensitive. Even though the <a href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec3.html#sec3.2.3" target="_blank" rel="noopener noreferrer">w3c spec</a> says comparing  urls should be case sensitive (with some exceptions), <a href="http://stackoverflow.com/questions/778203/are-there-any-naming-convention-guidelines-for-rest-apis" target="_blank" rel="noopener noreferrer">by convention</a> REST endpoints are case insensitive</li>
</ul>
<h1>Review</h1>
Lets take a look again at our main program block.

[csharp]
public static void Main()
{
    LcdWriter.Instance.Write(&quot;Web Demo Ready!&quot; + DateTime.Now.TimeOfDay);

    WebServerWrapper.InitializeWebEndPoints(new ArrayList
                                                {
                                                    new BasicPage()
                                                });

    WebServerWrapper.StartWebServer();

    RunUtil.KeepRunning();
}
[/csharp]

You can see that we've now decoupled public interaction with the web server, as well as allow each class to define whatever routes it wants. If we wanted to rename the route <code>EchoArgs</code> and have it point to another function, it'd be trivial to change that within <code>BasicPage</code>. If we wanted <code>BasicPage</code> to implement two functions such as <code>EchoArgs</code> and <code>BlinkLEDABunch</code> we could do that, all without having to update our main entrypoint.

Just to recap, the basic pattern here is:

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/11/flow.png"><img class="aligncenter size-medium wp-image-1731" src="http://tech.blinemedical.com/wp-content/uploads/2012/11/flow-300x179.png" alt="Program Flow Diagram" width="300" height="179" /></a>
<ul>
	<li>First register all <code>IEndPointProvider</code>s with the web server wrapper.</li>
	<li>Then start web server.</li>
	<li>When a request comes in, the server will find a matching endpoint by name and dispatch the <code>EndPointReceived</code> event which is caught by the wrapper.</li>
	<li>The wrapper executes the target endpoint in a separate thread and returns the endpoints result.</li>
</ul>
From a users perspective, you just create your class, expose your endpoint, and everything works.
<h1>Demo</h1>
Firing up the app

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/11/netduinoOutput1-e1353965312698.jpg"><img class="aligncenter size-medium wp-image-1604" src="http://tech.blinemedical.com/wp-content/uploads/2012/11/netduinoOutput1-e1353965295230-300x225.jpg" alt="Netduion output: starting app" width="300" height="225" /></a>

Using curl to send some arguments

[csharp]
&gt;curl http://192.168.2.11/echoargs/heyguys!/whatsup!
OK. Wrote out: heyguys! whatsup!
[/csharp]

Results on the Netduino

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/11/netduinoOutput2-e1353965375217.jpg"><img class="aligncenter size-medium wp-image-1605" src="http://tech.blinemedical.com/wp-content/uploads/2012/11/netduinoOutput2-e1353965375217-300x225.jpg" alt="Netduion output: endpoint executed" width="300" height="225" /></a>

The API listing (this prints when no known route was found)

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/11/apiList..png"><img class="aligncenter size-medium wp-image-1617" src="http://tech.blinemedical.com/wp-content/uploads/2012/11/apiList.-300x255.png" alt="The API listing" width="300" height="255" /></a>
<h1>The source</h1>
Full source and demo code available at our <a href="https://github.com/blinemedical/NWebREST" target="_blank" rel="noopener noreferrer">github</a>. Note, the project is built against .net micro 4.2. I've run the code on .net micro 4.1 and 4.2 and everything worked fine. For reference, currently my Netduino is on firmware 4.2.0.0. RC3, though I'm not relying on any major framework specific choices here so it should continue to work fine for later revisions.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1587</wp:post_id>
		<wp:post_date><![CDATA[2012-12-05 16:04:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-12-05 21:04:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rest-endpoints-with-netduino-web-server]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
		<category domain="post_tag" nicename="netduino"><![CDATA[netduino]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[960058829]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_6644b5b4a8d30fa08744bc9c40b01817]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561690990;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4939;}i:1;a:1:{s:2:"id";i:3392;}i:2;a:1:{s:2:"id";i:4919;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_05817a7a6a9145d7919112c8afdc35ce]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_44de245a5cb4857f2c9dd76d6025a72f]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>12</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-12-03 15:36:44]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-12-03 15:36:44]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA["Injecting Endpoints" section needs to have a consistent verb tense. Other than that, I just added a little punctuation! Looks great! I allllllllmost understood what this one said. :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>13</wp:comment_id>
			<wp:comment_author><![CDATA[Sharktear]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[solidtear@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[188.94.144.233]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-07-11 10:32:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-07-11 10:32:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi,
great article! I've done a similar optimization of Jaspers web server but this solution (instead of the switch in singular class) bring some code space problem... every service need a class implementation... and with just 10 or more service the code space available is out of space (with Netduino Plus)....]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>14</wp:comment_id>
			<wp:comment_author><![CDATA[Rener]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[renerlemess@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[200.169.187.110]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-11-04 21:42:46]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-11-04 21:42:46]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I need the example de first code. Is possible? Tks]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Yet another functional tic tac toe</title>
		<link>https://onoffswitch.net/2012/11/26/yet-another-functional-tic-tac-toe/</link>
		<pubDate>Mon, 26 Nov 2012 17:32:23 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1649</guid>
		<description></description>
		<content:encoded><![CDATA[http://www.jasonwhaley.com/blog/2011/12/06/tictactoe-functional-programming-evolution/

http://onbeyondlambda.blogspot.com/2012/05/functional-tic-tac-toe.html


no variable sized boards
no computer/player interactions
non-generic win functions

mine:
leveraging the same functions for many uses
only one place where i had to use mutability
leveraged interfaces as well as module encapsulation
dynamic
tokens can be swapped in for other types (images/etc) with no nmajor code changes

https://github.com/devshorts/FTicTac

thoughts on F#

type inference isn't always right
have to work from bottom up
difficult to read
too easy to fall into the "tuple trap"

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1649</wp:post_id>
		<wp:post_date><![CDATA[2012-11-26 17:32:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-11-26 17:32:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[yet-another-functional-tic-tac-toe]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_c5c8a13d219a339345b728c2643de707]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_bcce76ed02a623dfc91ae63d53a4de10]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_8d10ad6774e0a0f0fc0a31a1207d6a02]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[946477904]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Reading input in F#</title>
		<link>https://onoffswitch.net/2012/12/21/reading-input-in-f/</link>
		<pubDate>Fri, 21 Dec 2012 20:35:59 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1828</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/reading-input-in-f/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

I've been playing with F# lately, much to the the chagrin of <a href="http://tech.blinemedical.com/author/samuel-neff/" target="_blank" rel="noopener noreferrer">Sam</a>, but I still think it's fun as an excersize in thinking differently. I also find its terseness lets you prototype ideas quickly, encouraging you to experiment and tweak your code. However, I'm much more used to imperative programming, so when I started writing an F# program that needed user input I hit a small roadblock: how do I get input, validate, and ask for it again if I want to stay purely functional and leverage immutable values?

In an imperative language you might write something like this:

[csharp]
String path = null;
while(true)
{
    path = Console.ReadLine();
    if (File.Exists(path))
    {
        break;
    }

    Console.WriteLine(&quot;File doesn't exist&quot;);
} 
[/csharp]

You can't declare <code>path</code> inside the while loop or its loses its scope. If you need to use <code>path</code> outside of the while loop, then it might seem like you have to let path be mutable. But, what if we did this:

[csharp]
private String GetPath()
{
    while(true)
    {
        var path = Console.ReadLine();
        if (File.Exists(path))
        {
            return path;
        }
        Console.WriteLine(&quot;File doesn't exist&quot;);
    }
}
[/csharp]

Now we don't ever update any variables. We only ever use direct assignment. This sounds pretty functional to me. But, we still can't directly translate into F#. Remembering that in F# the last statement is the return value, what does this return?

[csharp]
let falseItem =
    while true do
        false
[/csharp]

This is actually an infinite loop; the while loop won't ever return <code>false</code>. In F#, a while loop can't return from it's body, since the body expression return type <a href="http://msdn.microsoft.com/en-us/library/dd233208.aspx" target="_blank" rel="noopener noreferrer">has to be</a> of type unit. If you imagine the while loop as a function that takes a predicate and a lambda for the body then this makes sense. The <code>whileLoop</code> function will execute the body as long as the predicate returns true. So, in psuedocode, it kind of looks like this

[csharp]
whileLoop(predicate, body) = {
  while predicate() do {
     body()
  }
}
[/csharp]

Now what? Well, turning this while loop into a recursive structure with immutable types is actually pretty easy:

[csharp]
let rec documentPath =
    fun () -&gt;
        Console.Write(&quot;File path: &quot;)
        let path = Console.ReadLine()
        if not(File.Exists path) then
            Console.WriteLine(&quot;File does not exist&quot;)
            documentPath()
        else path
[/csharp]

The trick here is to define <code>documentPath</code> as a recursive function. Either the function returns a valid path, or it calls itself executing the next "step" in our while loop. Also, since we don't need to do any work after the recursive function call, F# can optimize this to use <a href="http://stackoverflow.com/questions/310974/what-is-tail-call-optimization" target="_blank" rel="noopener noreferrer">tail call optimization</a>. The <code>documentPath</code> variable is of type <code>unit -&gt; string</code> meaning it's a function that takes a unit type and returns a string. To actually get the path, we execute <code>documentPath()</code>, where <code>()</code> is the unit type.

Now we have a function that uses immutable types, but continuously reads in user input and won't return until the input is valid.

Though, if you really want to use imperative style loop breaks, <a href="http://tomasp.net/blog/imperative-i-return.aspx" target="_blank" rel="noopener noreferrer">you can</a>, but it's not trivial.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1828</wp:post_id>
		<wp:post_date><![CDATA[2012-12-21 15:35:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-12-21 20:35:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[reading-input-in-f]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[986440027]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559832280;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3565;}i:1;a:1:{s:2:"id";i:4365;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>15</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-12-12 16:50:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-12-12 16:50:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This one looks good too, thanks!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>Fluid API using C# types in F#</title>
		<link>https://onoffswitch.net/2012/12/07/fluid-api-using-c-types-in-f/</link>
		<pubDate>Fri, 07 Dec 2012 12:47:49 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1869</guid>
		<description></description>
		<content:encoded><![CDATA[unit identity operator

[csharp]
let (~~) (func:'a-&gt; unit) (arg:'a) = (func arg) |&gt; fun () -&gt; arg
[/csharp]

[csharp]
let print2' item = ~~ (printf &quot;%s&quot;) item

let print' item = ~~ (Console.WriteLine:string-&gt;unit) (item.ToString()) 
[/csharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1869</wp:post_id>
		<wp:post_date><![CDATA[2012-12-07 12:47:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-12-07 12:47:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[fluid-api-using-c-types-in-f]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[963196263]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Debugging piped operations in F#</title>
		<link>https://onoffswitch.net/2012/12/14/debugging-piped-sequences-f/</link>
		<pubDate>Fri, 14 Dec 2012 14:34:27 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=1873</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/debugging-piped-sequences-f/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

<h1>A little on the pipe operator</h1>
In F# you can create <a href="http://www.c-sharpcorner.com/uploadfile/rmcochran/fsharp-types-and-the-forward-pipe-operator/" target="_blank" rel="noopener noreferrer">piped operations</a> using the <code><a href="http://msdn.microsoft.com/en-us/library/dd233229.aspx" target="_blank" rel="noopener noreferrer">|&gt;</a></code> operator. This takes the output of the previous statement and funnels it as the input to the next statement. Using the pipe operator, a statement like this:

[fsharp]
x |&gt; f |&gt; g |&gt; h
[/fsharp]

Means having functions nested like this:

[fsharp]
h(g(f(x))
[/fsharp]

So a piece of code like this:

[fsharp]
let print item = Console.WriteLine(item.ToString)

let seqDebug =
        [0..1000]
                |&gt; List.map (fun i -&gt; i + 1)
                |&gt; List.filter (fun i -&gt; i &lt; 5)
                |&gt; List.head
                |&gt; print
[/fsharp]

Decompiles into this (formatting added):

[csharp]
[DebuggerBrowsable(DebuggerBrowsableState.Never)]
internal static Unit seqDebugu00407;

public static void mainu0040()
{
    Program.print(
        ListModule.Head(
            ListModule.Filter((FSharpFunc&lt;int, bool&gt;) new Program.seqDebugu004010(),
                ListModule.Map&lt;int, int&gt;((FSharpFunc&lt;int, int&gt;) new Program.seqDebugu00409u002D1(),
                    SeqModule.ToList(Operators.CreateSequence(
                        Operators.OperatorIntrinsics.RangeInt32(0, 1, 1000)))))));

    u0024Program.seqDebugu00407 = (Unit) null;
}
[/csharp]

Which really boils down to:

[csharp]
seqDebug = Print(Head(Filter(Map(sequence))))
[/csharp]

The F# syntax is nice because it lets us write code from the outside in, instead of inside out.
<h1>Debugging it</h1>
Now that we know what F# is doing, lets say we want to debug the print statement. You can't use your normal "<a href="http://msdn.microsoft.com/en-us/library/ek13f001.aspx" target="_blank" rel="noopener noreferrer">Step Over</a>" F10 key to go through your piped statement here because it compiles down to a one line group of nested functions. We could use the "Step Into" key (F11) to step into the entire sequence but then we have to execute the anonymous map lambda 1001 times just to get to the next statement. Then another 1001 for the filter. Then the head statement, and finally, our print. No thanks.

Thankfully, Visual Studio has thought of this and you can use the <em><a href="http://msdn.microsoft.com/en-us/library/7ad07721.aspx" target="_blank" rel="noopener noreferrer">Step Into Specific</a></em> functionality. This lets you see the list of nested functions at that line and you can jump into whatever you need to here. <i>Step Into Specific</i> isn't an F# only feature, but I never realized it existed until I ran into this scenario.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/stepIntoSpecific2.png"><img class="aligncenter size-medium wp-image-1954" alt="Step into Specific " src="http://tech.blinemedical.com/wp-content/uploads/2012/12/stepIntoSpecific2-300x105.png" width="300" height="105" /></a>

The example is a little trivial, since you would've just put a breakpoint in the print statement, right? But what if you are piping through F# operators like <code>List.map</code> and <code>List.filter</code>? In these cases it can be hard to know what is the direct input to these functions since the input argument is automatically applied. For these scenarios, a simple identity function can be really helpful:

[fsharp]
let identity item = item

let seqDebug =
        [0..1000]
                |&gt; List.map (fun i -&gt; i + 1)
                |&gt; identity
                |&gt; List.filter (fun i -&gt; i &lt; 5)
                |&gt; List.head
[/fsharp]

So you can sprinkle in your identity function and put breakpoints there. This way you can inject yourself into the middle of this sequence.
<h1>Piping with functions that return unit</h1>
Taking this one step further, sometimes I want to print out a value in the middle of the sequence, or call a function that has a return type of <code>unit</code> but continue piping. Because let's be honest here, when all else fails nothing beats a well placed <code>printf</code> in your code. But, we're left with a small dilemma: since pipes take the output of the last function and use it as the input to the next function we can't really use print statements. Both <code>printf</code> and <code>Console.WriteLine</code> effectively return a <code>void</code>. Putting them in the middle of a chain won't work since their output won't map to the next functions input (unless that next function takes <code>unit</code>).

However, F# lets you define <a href="http://msdn.microsoft.com/en-us/library/dd233204.aspx" target="_blank" rel="noopener noreferrer">your own operators</a>, so I created one that I like to call the "argument identity" that executes a function which returns void and then returns the original argument (acting as an argument identity function):

[fsharp]
let (~~) (func:'a-&gt; unit) (arg:'a) = (func arg) |&gt; fun () -&gt; arg
[/fsharp]

The <code>~~</code> symbol is a <a href="http://msdn.microsoft.com/en-us/library/dd233204.aspx" target="_blank" rel="noopener noreferrer">prefix</a> operator that takes a function of one argument that returns unit, then closes the argument into a function with type <code>unit -&gt; 'a</code>. Then I pipe the return value (unit) to the anonymous function (that takes unit) which will return the closed value of the original argument. Now I can do things like this:

[fsharp]
let (~~) (func:'a-&gt; unit) (arg:'a) = (func arg) |&gt; fun () -&gt; arg

let seqDebug =
        [0..1000]
                |&gt; List.map (fun i -&gt; i + 1)
                |&gt; ~~ Console.WriteLine
                |&gt; List.filter (fun i -&gt; i &lt; 3)
                |&gt; ~~ Console.WriteLine
                |&gt; List.head
                |&gt; ~~ Console.WriteLine
[/fsharp]

Which prints out

[csharp]
[1; 2; 3; ... ]
[1; 2]
1
[/csharp]

You obviously don't need your own operator, you can make it a named helper function if you want. Either way, some sort of argument identity function is useful in these scenarios.
<h1>Disassemble the pipe</h1>
And of course, when all else fails, you can break up the sequence into a series of <code>let</code> statements to debug it the old fashioned way.

[fsharp]
let seqDebugDecomposed =
        let source = [0..1000]
        let sourcePlusOne = List.map (fun i -&gt; i + 1) source
        let filteredSource = List.filter (fun i -&gt; i &lt; 3) sourcePlusOne
        let listHead = List.head filteredSource
        print listHead
[/fsharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>1873</wp:post_id>
		<wp:post_date><![CDATA[2012-12-14 09:34:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-12-14 14:34:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[debugging-piped-sequences-f]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="debugging"><![CDATA[Debugging]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="category" nicename="imported"><![CDATA[Imported]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[963156174]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559527959;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:3565;}i:2;a:1:{s:2:"id";i:4197;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>16</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2012-12-12 16:06:28]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2012-12-12 16:06:28]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Looks great! Thanks, Anton!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>Call Later with swiz</title>
		<link>https://onoffswitch.net/2012/12/17/call-later-with-swiz/</link>
		<pubDate>Mon, 17 Dec 2012 11:01:37 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2014</guid>
		<description></description>
		<content:encoded><![CDATA[talk about invlaidation manager as3]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2014</wp:post_id>
		<wp:post_date><![CDATA[2012-12-17 11:01:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-12-17 11:01:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[call-later-with-swiz]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tracing computation expressions</title>
		<link>https://onoffswitch.net/2013/01/18/tracing-computation-expressions/</link>
		<pubDate>Fri, 18 Jan 2013 20:43:48 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2020</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/tracing-computation-expressions/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

F# has a novel syntax feature called <a href="http://en.wikibooks.org/wiki/F_Sharp_Programming/Computation_Expressions" target="_blank" rel="noopener noreferrer"><em>computation expressions,</em></a> which lets you build complex monadic expressions with minimal syntax. Commonly shied away from, a <a href="http://stackoverflow.com/questions/2704652/monad-in-plain-english-for-the-oop-programmer-with-no-fp-background/2704795#2704795" target="_blank" rel="noopener noreferrer">monad</a> is simple: it's a function whose input is some state. A monad usually manipulates the state and returns a new state (which can be handed off to another monad).

Monad's are maybe best known from <a href="http://www.haskell.org/haskellwiki/Monad" target="_blank" rel="noopener noreferrer">Haskell</a>, but they exist in scheme, ML, clojure, scala, and even show up in <a href="http://devtalk.net/csharp/chained-null-checks-and-the-maybe-monad/" target="_blank" rel="noopener noreferrer">C#</a> and other imperative languages.

While the computation expression syntax is cool, short of the <a href="http://en.wikipedia.org/wiki/Monad_(functional_programming)#The_Maybe_monad" target="_blank" rel="noopener noreferrer">maybe monad</a>, and the baked-in <a href="http://msdn.microsoft.com/en-us/library/dd233250.aspx" target="_blank" rel="noopener noreferrer">async keyword</a>, I wasn't sure what I could do with this. Thankfully, I found an interesting post by <a href="http://langexplr.blogspot.com/2008/10/using-f-computation-expressions-to-read.html" target="_blank" rel="noopener noreferrer">Luis Diego Fallas</a> who posted an F# code sample leveraging computation expressions to read binary formatted files. However, if you are like me, and trying to better understand the power of computation expressions, tracing through these samples can be difficult. It's not because they are poorly written, but mostly because they are jumping into more complex usages.

To clarify what's going on with computation expressions, I wanted to show what is passing through the monad. Computation expressions (also known as monads or workflows) can be tricky because you are overriding the language syntax. So what is a "let!" statement in one workflow is not the same as in another workflow.  On top of that statements themselves can return a value, not just have a left hand side assignment. If you feel your brain start to hurt, that's OK. It will get better. 

<h2>An example</h2>

Let's start with a small sample to see how information passes through the workflow. This makes it easier to understand what computation expressions are doing:

[fsharp]
open System

type State =
    | Current of int * State
    | Terminated

type Builder() =
    member this.Bind(value:int, rest:int-&gt;State) =
        State.Current((value, rest(value)))

    member this.Return(returnValue:int) = State.Current(returnValue, State.Terminated)

let builder = new Builder()

let build _ = builder{
                        let! x = 1
                        let! y = 2
                        return 3
                }

let stateChain = build()

let rec formatState (chain:State) =
            match chain with
                | State.Terminated -&gt; Console.WriteLine()
                | State.Current(i, next) -&gt; Console.WriteLine(&quot;State: {0}&quot;, i)
                                            formatState next

formatState stateChain

Console.ReadKey() |&gt; ignore
[/fsharp]

Which prints out

[csharp]
State: 1
State: 2
State: 3
[/csharp]
<h2>Desugaring</h2>
Before we trace through what's happening, lets desugar the expression. If you aren't familiar with the bang syntax, a let! statement will compile into an execution on the builders Bind function and a return will compile into an execution on the builders Return function. Let's take the original expression:

[fsharp]
let build _ = builder{
                        let! x = 1
                        let! y = 2
                        return 3
                }
[/fsharp]

And show the same thing but without the syntactic sugar.

[fsharp]
let desugared =
    builder.Bind(1, fun next -&gt;
                        let x = next // next = 1 since we took the input value of the bind,
                                     // and passed it to the next lambda

                        let returnedState =
                            builder.Bind(2, fun next2 -&gt;
                                                let y = next2 // next2 is the value 2 here

                                                let terminatedState = builder.Return(3)

                                                //terminated state is now
                                                //State.Current(3, State.Terminated)

                                                terminatedState
                                            )

                        // the state here is
                        // State.Current(2, State.Current(3, State.Terminated))
                        returnedState
                    )
[/fsharp]

The important thing to understand here is how the let! statement is deconstructed. The right hand side is the <code>value</code> input to the bind. Everything below the let! statement (and including the left hand side of the assignment), is inside of the bind lambda (passed to the <code>rest</code> argument of the bind function). The first thing the lambda does is actually apply the left hand side assignment with the input from the bind. In this case, the first let! statement assigns <code>x = 1</code>. Then it executes the remaining function, being the other let! and return statements.
<h2>Tracing it through</h2>
In the original sample, I've defined a discriminated union called <code>State</code> representing the current state. This union has two types. One, called <code>Current</code>, contains an integer as well as a link to the next state in the form of a tuple. The other, <code>Terminated</code>, is a value that we can use as a terminator for our state link. The example is really just for demonstration, since by being able to capture the state it's easier to understand how computation expressions work; it gives us a sense of where the monad has been.

Let's take this one step at a time, with an even simpler example based on the above code. It's important to understand the deconstruction. The compiler will translate our computation expressions into invocations on the builder.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-1-e1356539394542.jpg"><img class="aligncenter size-medium wp-image-2201" alt="builderDeconstruction1" src="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-1-300x227.jpg" width="300" height="227" /></a>

To desguar it we take the left hand side and everything after

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-2-e1356539595127.jpg"><img class="aligncenter size-medium wp-image-2202" alt="builderDeconstruction2" src="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-2-300x186.jpg" width="300" height="186" /></a>

And move it to a lambda. This lambda is what is going to be passed as the <code>rest</code> argument to the builder's bind function

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-3-e1356539579118.jpg"><img class="aligncenter size-medium wp-image-2203" alt="builderDeconstruction3" src="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-3-300x167.jpg" width="300" height="167" /></a>

The right hand side is going to be applied to the <code>value</code> argument of the bind function

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-4-e1356539561372.jpg"><img class="aligncenter size-medium wp-image-2204" alt="builderDeconstruction4" src="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-4-300x204.jpg" width="300" height="204" /></a>

Go back and look at how we've defined the bind function:

[fsharp]
member this.Bind(value:int, rest:int-&gt;State) =
        State.Current((value, rest(value)))
[/fsharp]

Here the <code>value</code> argument is 1. The second argument, <code>rest</code>, is the lambda. The lambda is going to have to return a <code>State</code> union since <code>State.Current</code> expects an integer, State tuple.

When we execute the lambda inside the bind we pass the value (1) to the lambda, but we've also captured the current value. This means that this bind is going to return:

[fsharp]
State.Current(1, rest(1))
[/fsharp]

So here we apply the value to the function

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-5-e1356539528361.jpg"><img class="aligncenter size-medium wp-image-2205" alt="builderDeconstruction5" src="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo-5-300x195.jpg" width="300" height="195" /></a>

This is where the left hand side statement (x) now gets assigned.

<a href="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo6-e1356539509430.jpg"><img class="aligncenter size-medium wp-image-2206" alt="builderDeconstruction6" src="http://tech.blinemedical.com/wp-content/uploads/2012/12/photo6-300x184.jpg" width="300" height="184" /></a>

Now what about

[fsharp]
builder.Return(2)
[/fsharp]

Remember we defined the return function to return

[fsharp]
member this.Return(returnValue:int) = State.Current(returnValue, State.Terminated)
[/fsharp]

So with our simplified example this will return

[fsharp]
State.Current(2, State.Terminated)
[/fsharp]

The previous lambda now returns that same value, since that's the last line of the statement. So we're back now to the original bind function:

[fsharp]
member this.Bind(value:int, rest:int-&gt;State) =
        State.Current((1, rest(1)))
[/fsharp]

But <code>rest(1)</code> returns <code>State.Current(2, State.Terminated)</code>. Our final builders return value, in this example, is

[fsharp]
State.Current(1, State.Current(2, State.Terminated))
[/fsharp]

All the computation builder syntax is doing is just sugaring our statements up to give us these broken up functions.

Back to the original sample. We added a second <code>let!</code> statement in there:

[fsharp]
let build _ = builder{
                        let! x = 1
                        let! y = 2
                        return 3
                }
[/fsharp]

Now, hopefully, you should be able to see how the final return from the computation expression is a state object representing what happened in the monad:

[fsharp]
State.Current(1, State.Current(2, State.Current(3, State.Terminated)))
[/fsharp]
<h2>Combine and Yield</h2>
Computation expressions have more than just let! and return statements though. Once you get used to tracing through and thinking about the computation builder, it becomes easier to start writing workflows. Just for kicks, I wanted to see if I could write a computation expression to evaluate a basic arithmetic expression. Here I'm using <a href="http://en.wikipedia.org/wiki/Currying" target="_blank" rel="noopener noreferrer">partial functions</a> and the <code>Combine</code> property of the builder to build out the expression. If you wanted to, you can even use computation expressions within computation expressions. There's nothing keeping you from doing that.

In general, there is a bunch of reserved syntax that maps to specific builder functions. The <a href="http://msdn.microsoft.com/en-us/library/dd233182.aspx" target="_blank" rel="noopener noreferrer">msdn</a> on computation syntax has these all defined.

[fsharp]
open System

type BuildTest() =
    member this.Combine(currentStatement, value) = currentStatement(value)
    member this.Return(value) = value
    member this.Yield(item) = item
    member this.Delay(item) = item()

let builder = new BuildTest()

type math() =
    member this.add x y = x + y
    member this.mult x y = x * y

let m = new math()

let build _ = builder{
                yield m.mult 2 // 2 * (1 + (2 + (3 + 0))
                yield m.add 1  // 1 + (2 + (3 + 0)
                yield m.add 2  // 2 + (3 + 0)
                yield m.add 3  // (3 + 0)
                return 0       // 0
            }

let run = build()

let monader = printf &quot;%s&quot; (&quot;got &quot; + run.ToString())

Console.ReadKey() |&gt; ignore
[/fsharp]

This snippet evaluates to 12.

You can even add precedence by evaluating a computation expression within the computation expressions

[fsharp]
builder{
    yield m.mult 2 // 2 * (1 + (8 + 0))
    yield m.add 1  // 1 + (8 + 0)

    let parenth = builder{
                        yield m.mult 4 // 4 * 2
                        return 2  // 2
                    }

    yield m.add parenth // 8 + 0

    return 0       // 0
}
[/fsharp]

Which evaluates to 18.
<h2>Tracing Combine and Yield</h2>
Just like before, there's a bunch of magic going on here, so it's easier if you follow along with the desugared version of the original arithmetic expression below.  

[fsharp]
let desugared = builder.Delay(
                fun () -&gt; builder.Combine(builder.Yield(m.mult 2),
                    builder.Delay(
                        fun() -&gt; builder.Combine(builder.Yield(m.add 1),
                                builder.Delay(
                                    fun() -&gt; builder.Combine(builder.Yield(m.add 2),
                                            builder.Delay(
                                                fun() -&gt; builder.Combine(builder.Yield(m.add 3),
                                                    builder.Delay(
                                                        fun() -&gt; builder.Return(0))))))))))
[/fsharp]

Each monadic function is wrapped in a <code>Delay</code>, which promptly executes it. Look at the builder's delay declaration - it takes a function and executes it.

In our builder, the <code>Yield</code> just returns the same value. It doesn't do much but we needed to implement it to use the computation expression syntax.

What we pass to the delay is an anonymous function that has a combine statement. <code>Combine</code>s take two things and produce a third. Here, we are passing the current partial function as the first argument (via the yield), and the value we want to use to evaluate this partial function as the second argument. However, the second argument isn't actually evaluated till the end. The combine will then apply the second argument (an integer) to the first argument (a partial function that takes an integer).

For the basic arithmetic example, the final delay function returns 0. You can think of this as a "seed." If you think of it like a fold operation, this is very similar. When we finally return the seed, we bubble each evaluated expression back up the stack (starting with 0), so read the desugared version from the bottom up. In this way, we are executing the current curried statement with the previous statements evaluated value in the <code>Combine</code> method of the builder. Not the most practical application, but I thought it was a fun exercise.

If you are confused why this example's desguaring contains the Delay method and the original example didn't, it's because the sugaring happens differently depending which builder constructs you use.

<h2>Under the hood</h2>
When you <a href="http://www.jetbrains.com/decompiler/" target="_blank" rel="noopener noreferrer">decompile</a> a computation expression, each monadic function gets compiled into it's own class representing a monad. In our arithmetic operation example, this is a Combine, Yield, Delay trio. It's not easy to read since the function names have been mangled, but you can see the general pattern here (formatting added).

[csharp]
[Serializable]
internal class runu004089 : FSharpFunc&lt;Unit, int&gt;
{
    internal runu004089()
    {
    }

    public override int Invoke(Unit unitVar0)
    {
        return ExpresionsTest.builder.Combine&lt;int, int&gt;(
        ExpresionsTest.builder.Yield&lt;FSharpFunc&lt;int, int&gt;&gt;(
            (FSharpFunc&lt;int, int&gt;) new ExpresionsTest.runu004089u002D1(2, ExpresionsTest.m)),
                ExpresionsTest.builder.Delay&lt;int&gt;(
                    (FSharpFunc&lt;Unit, int&gt;) new ExpresionsTest.runu004091u002D2()));
    }
}
[/csharp]

This decompliation represents the following sub-block.

[fsharp]
builder.Combine(
    builder.Yield(m.add 2), builder.Delay( (*next function*) )
)
[/fsharp]

Notice in the decompiled block that the class name is <code>runu004089</code> and the executable expression is compiled into an <code>Invoke</code> that returns an int. The decompiled assembly will actually be littered with these classes with mangled names, following a naming format of the target variable name (<code>run</code>) and an identifier (<code>u004089</code>). You can always decompile the computation expression to get a sense for how it's been desugared.
<h2>Conclusion</h2>
I said in the beginning that monads are simple, but I'll admit that I lied. Monads are tricky, there's no denying that. Maybe that's why there is <a href="https://www.google.com/search?q=%22what+is+a+monad%22" target="_blank" rel="noopener noreferrer">no shortage</a> of blog posts trying to explain the monad over and over again. But, in the end, once you wrap your head around it, I think computation expression syntax is a cool way of using the concept of a monad by decoupling what something is defined to do, vs how it's actually executed.

I highly suggest running the examples and actually stepping through them bit by bit if you are having trouble following what is happening. Being able to see a desugared version of the code and using a debugger to inspect locals while stepping through examples makes it a lot clearer to see whats happening.
<h2>More reading</h2>
If you are curious here are some links to further reading <a href="http://stackoverflow.com/questions/44965/what-is-a-monad" target="_blank" rel="noopener noreferrer">explaining monads</a> and computation expressions (such as the <a href="http://en.wikibooks.org/wiki/F_Sharp_Programming" target="_blank" rel="noopener noreferrer">F# wikibook</a>). <a href="http://blogs.msdn.com/b/dsyme/archive/2007/09/22/some-details-on-f-computation-expressions-aka-monadic-or-workflow-syntax.aspx" target="_blank" rel="noopener noreferrer">Don Syme</a> also has a few posts explaining things really well that are definitely worth checking out.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2020</wp:post_id>
		<wp:post_date><![CDATA[2013-01-18 15:43:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-01-18 20:43:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tracing-computation-expressions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="monads"><![CDATA[monads]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[979357360]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"d1e23d6ffe5a1de5892bb68020be156f";a:2:{s:7:"expires";i:1555043233;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:4463;}i:2;a:1:{s:2:"id";i:4226;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Disposing of singletons</title>
		<link>https://onoffswitch.net/2012/12/26/disposing-of-singletons/</link>
		<pubDate>Wed, 26 Dec 2012 11:40:35 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2225</guid>
		<description></description>
		<content:encoded><![CDATA[Talk about application register on shutdown.  How to deal with disposing of static items]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2225</wp:post_id>
		<wp:post_date><![CDATA[2012-12-26 11:40:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2012-12-26 11:40:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[disposing-of-singletons]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>K-Means Step by Step in F#</title>
		<link>https://onoffswitch.net/2013/01/28/k-means-step-by-step-in-f/</link>
		<pubDate>Mon, 28 Jan 2013 15:47:28 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2274</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/k-means-step-by-step-in-f/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

Recently we had a discussion on clustering techniques at one of our weekly tech talks and the <a href="http://en.wikipedia.org/wiki/K-means_clustering" target="_blank" rel="noopener noreferrer">k-means clustering</a> algorithm came up. K-means is considered one of the simplest unsupervised machine learning techniques and I thought it would be cool to try my hand at an F# implementation of it.

In short, k-means is a way to put data into groups based on distance between nearest neighbors. Technically it works with any dimension but for this post I'll stick with 1-d data to make things easy.
<h2>K-means background</h2>
Imagine you have some 1-dimensional data like

[csharp]
1, 2, 5, 14, 17, 19, 20
[/csharp]

And you want to group this into two groups. Pretty easily you can see that 1, 2, and 5 go into one group, and 14, 17, 19, 20 should go in another group. But why? For this small data set we intuitively figured out that 1, 2, and 5 are close together, but at the same time far away from 14, 17, 19, and 20, which are also close together. Really what we did was we calculated the centroid of each group and assigned the values to the centroid. A centroid is a fancy way of saying center point of a group. It's calculated by taking the average of a group.

For example, if we have the group

[csharp]
1, 2, 5
[/csharp]

What is its centroid? The answer is

[csharp]
(1 + 2 + 5) / 3 = 2.666
[/csharp]

So with k-means you end up with k centroids and a bunch of data grouped to that centroid. It's grouped to that centroid because that data is closest to that centroid vs any other centroid. To calculate the centroids and grouping k-means uses an <a href="http://home.dei.polimi.it/matteucc/Clustering/tutorial_html/kmeans.html" target="_blank" rel="noopener noreferrer">iterative process</a>:
<ul>
	<li>Pick k random points. So if you are doing a k=2 clustering, just pick 2 random points from your data set. These are going to be our initial centroids. It doesn't matter if it's right, or even if the points are the same. The algorithm is going to fine tune things for us. Technically this is called the <a href="http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/K-Means#Implementation" target="_blank" rel="noopener noreferrer">Forgy Method</a> of initialization. There are a bunch of other ways to initialize your centroids, but Forgy seems to be pretty common.</li>
	<li>For every point, calculate its distance from the centroid. So lets say we picked points 1 and 2 as our centroids. The <a href="http://en.wikipedia.org/wiki/Euclidean_distance" target="_blank" rel="noopener noreferrer">distances</a> then look like this:[csharp]
     Δ1      Δ2
1    0        1
2    1        0
5    4        3
14   13       12
17   16       15
19   18       17
20   19       18
[/csharp]</li>
	<li>Now what we do is assign each data point to its nearest centroid. So, obviously, point 1 is closest to the initial arbitrary point 1 centroid. And you can see that basically everything else is closer to the arbitrary point 2 centroid. So we're left with a grouping like this now:[csharp]
data around centroid 1: 1
data around centroid 2: 2, 5, 14, 17, 19, 20
[/csharp]</li>
	<li>Now, the next step is to calculate new centroids based on these groups. For this example, we can take the average of all the points together to find the new center. So the new centroids become 1 and 12.83. Now we go back to step 2 using the new centroids. If you keep track of the current centroid and the previous centroid, then you can stop the k-means calculation when centroids don't change between iterations. At this point everything has converged and you've got your k clusters</li>
</ul>
Unfortunately k-means doesn't always converge, so you can add a convergence delta (i.e. if the centroid between the current and previous iterations hasn't really changed much, so it's within some acceptable range) or an iteration limit (only try to cluster 100 times) so you can set bounds on the clustering.

<h2>Implementation</h2>
The first thing to do for my implementation is to define a structure representing a single data point.  I created a <code>DataPoint</code> class that takes a float list which represents the data points dimensions. So, a 1-dimesional data point would have a float list of one element. A 2-d data point has one of two points (x and y), etc. To give credit where credit is due, I took the dimensional array representation from <a href="http://www.navision-blog.de/2009/01/29/n-dimensional-k-means-clustering-with-f/" target="_blank" rel="noopener noreferrer">another F# implementation</a>.

[fsharp]
type DataPoint(input:float list) =
    member this.Data = input
    member this.Dimensions = List.length input
    override x.Equals(yobj) =
        match yobj with
        | :? DataPoint as y -&gt; (x.Data = y.Data)
        | _ -&gt; false
    static member Distance (d1:DataPoint) (d2:DataPoint) =
                                    if List.length d1.Data &lt;&gt; List.length d2.Data then
                                        0.0
                                    else
                                        let sums = List.fold2(fun acc d1Item d2Item -&gt;
                                                                     acc + (d2Item - d1Item)**2.0
                                                              ) 0.0 d1.Data d2.Data
                                        sqrt(sums)
[/fsharp]

It has its data, the dimensions, an equality overload (so I can compare data points by their data and not by reference), as well as a static helper method to calculate the euclidean distance between two n-dimensional points. Here I'm using the <code>fold2</code> method which can fold two lists simultaneously.
<h2>Alias data types</h2>
For fun, I wanted to use F#'s data aliasing feature, so I created some helper types to make dealing with tuples and other data pairing easier. I've defined the following

[fsharp]
type Centroid = DataPoint

type Cluster = Centroid * DataPoint List

type Distance = float

type DistanceAroundCentroid = DataPoint * Centroid * Distance

type Clusters = Cluster List

type ClustersAndOldCentroids = Clusters * Centroid list
[/fsharp]

The only unfortunate thing is you need to explicitly mention aliased types for the type inference to work properly. The reason is because F# will prefer native types to typed aliases (why choose <code>Distance</code> over float unless you are told to), but it would've been nice if it had some way to prefer local type aliases in a module.
<h2>Centroid assignment</h2>
This next major step is to take the current raw data list and what the current centroids are and cluster them. A cluster is a centroid and its associated data points. To make things clearer, I created a helper function <code>distFromCentroid</code> which takes a point, a centroid, and returns a tuple of point * centroid * distance.

[fsharp]
let private distFromCentroid pt centroid : DistanceAroundCentroid = (pt, centroid, (DataPoint.Distance centroid pt))
[/fsharp]

The bulk of the k-means work is in <code>assignCentroids</code>. First, for every data point, we calculate its distance from the current centroid. Then we select the centroid which is closest to our data points and create a list of point * centroid tuples. Once we have a list of point * centroid tuples, we want to group this list by the second value in the tuple: the centroid. Here, I'm leveraging the pipe operator to do the grouping and mapping. First, we group the lists by the centroid. This gives us a list of <code>centroid * (datapoint * centroid)</code> items. But this isn't quite right yet. We want to end up with a list of <code>centroid * dataPoint list</code> so I passed this temporary list through some other minor filtering and formatting.

[fsharp]
(*
    Takes the input list and the current group of centroids.
    Calculates the distance of each point from each centroid
    then assigns the data point to the centroid that is closest.
*)
let private assignCentroids (rawDataList:DataPoint list) (currentCentroids:Clusters) =
        let dataPointsWithCentroid =
            rawDataList
                |&gt; List.map(fun pt -&gt;
                                    let currentPointByEachCentroid = List.map(fun (centroid, _) -&gt; distFromCentroid pt centroid) currentCentroids
                                    let (_, nearestCentroid, _) = List.minBy(fun (_, _, distance) -&gt; distance) currentPointByEachCentroid
                                    (pt, nearestCentroid)
                            )

        (*
            |&gt; Group all the data points by their centroid
            |&gt; Select centroid * dataPointList
            |&gt; For each previous centroid, calculate a new centroid based on the aggregated list
        *)

        List.toSeq dataPointsWithCentroid
            |&gt; Seq.groupBy (fun (_, centr) -&gt; centr)
            |&gt; Seq.map(fun (centr, dataPointsWithCentroid) -&gt;
                                let dataPointList = Seq.toList (Seq.map(fun (pt, _) -&gt; pt) dataPointsWithCentroid)
                                (centr, dataPointList)
                      )
            |&gt; Seq.map(fun (cent, dataPointList) -&gt;
                                let newCent = calculateCentroidForPts dataPointList
                                (newCent, dataPointList)
                      )
            |&gt; Seq.toList

[/fsharp]
<h2>Calculating new centroids</h2>
Since we've grouped data points by centroid, we now need to calculate what the new centroid for that list is. This is where I employ a disclaimer. For my implementation of n-dimensional centroids I averaged each dimension together to get a new value. I'm not totally sure this is actually valid for arbitrary geometrical shapes (given by n-dimensions), but if it's not at least this is the only method that needs to be updated.

First I create an empty n-dimension array of all zeros, initialized by the dimensions of the first point in the list (all the dimensions should be the same so it doesn't matter which point we choose). This is the seed for list for the next fold. I'm not actually going to fold the value into a single element; I'm going to fold all indexes of the data point list together into a new array by adding each dimension together.

For example, if I have two points of two dimensions [1, 2] and [3, 4] I add 1 and 3 together to make the first dimension, then 2 and 4 together to make the second dimension. This gives me an intermediate vector of [4,6]. Then I can average each dimension by the number of data points used, so in our example we used two data points, so the new centroid would be [2, 3]. Since a centroid is the same as a data point (even though one that doesn't exist in our original raw set), we can return the new centroid as a data point. Having everything treated as data points makes the calculations easier.

[fsharp]
(*
    take each float list representing an n-dimesional space for every data point
    and average each dimension together.  I.e. element 1 for each point gets averaged together
    and element 2 gets averaged together.  This new float list is the new nDimesional centroid
*)

let private calculateCentroidForPts (dataPointList:DataPoint List) =
    let firstElem = List.head dataPointList
    let nDimesionalEmptyList = List.init firstElem.Dimensions (fun i-&gt; 0.0)
    let addedDimensions = List.fold(fun acc (dataPoint:DataPoint) -&gt; addListElements acc dataPoint.Data (+))
                                nDimesionalEmptyList
                                dataPointList

    // scale the nDimensional sums by the size
    let newCentroid = List.map(fun pt -&gt; pt/((float)(List.length dataPointList))) addedDimensions
    new DataPoint(newCentroid)
[/fsharp]

Here is the function to aggregate list elements together

[fsharp]
(*  goes through two lists and adds each element together
    i.e. index 1 with index 1, index 2 with index2
    and returns a new list of the items aggregated
*)

let private addListElements list1 list2 operator =
    let rec addListElements' list1 list2 accumulator operator =
        if List.length list1 = 0 then
            accumulator
        else
            let head1 = List.head list1
            let head2 = List.head list2
            addListElements' (List.tail list1) (List.tail list2) ((operator head1 head2)::accumulator) operator
    addListElements' list1 list2 [] operator
[/fsharp]

I'm using a hidden accumulator so that the function will be tail recursive.
<h2>Cluster runner</h2>
The above code only does one single cluster iteration though . What we need is a clustering runner that does the clustering iterations. This runner will be responsible for determining when to stop the clustering. There are four functions here:
<ul>
	<li><code>getClusters</code> and <code>getOldCentroids</code> are helper methods to pull data out of the tuples (fst and snd are built in functions)</li>
	<li><code>cluster</code> is a basic method that will cluster as many times as necessary until the centroids stop changing</li>
	<li><code>clusterWithIterationLimit</code> is the bulk method that takes in the raw data, the clustering amount (k), the max number of clustering iterations, and a convergence delta to avoid infinite loops</li>
</ul>
For each iteration create a new set of clusters, then test to see if if the centroids are the same. If they are, we've converged and we can end. If not, see if the clusters are "close enough" (the convergence delta). The final base case is if we've clustered the max iterations then return.

[fsharp]
(*
    Start with the input source and the clustering amount
    but also take in a max iteration limit as well as a converge
    delta.

    continue to cluster until the centroids stop changing or
    the iteration limit is reached OR the converge delta is achieved

    Returns a new &quot;Clusters&quot; type representing the centroid
    and all the data points associated with it
*)

let private getClusters (cluster:ClustersAndOldCentroids) = fst cluster
let private getOldCentroids (cluster:ClustersAndOldCentroids) = snd cluster

let clusterWithIterationLimit data k limit delta =
    let initialClusters:Clusters = initialCentroids (List.toArray data) k
                                    |&gt; Seq.toList
                                    |&gt; List.map (fun i -&gt; (i,[]))

    let rec cluster' (clustersWithOldCentroids:ClustersAndOldCentroids) count =
        if count = 0 then
            getClusters clustersWithOldCentroids
        else
            let newClusters = assignCentroids data (getClusters clustersWithOldCentroids)
            let previousCentroids = getOldCentroids clustersWithOldCentroids
            let newCentroids = extractCentroidsFromClusters newClusters

            // clusters didnt change
            if listsEqual newCentroids previousCentroids then
                newClusters
            else if centroidsDeltaMinimzed previousCentroids newCentroids delta then
                newClusters
            else
                cluster' (newClusters, newCentroids) (count - 1)

    cluster' (initialClusters, extractCentroidsFromClusters initialClusters) limit

(*
    Start with the input source and the clustering amount.
    continue to cluster until the centroids stop changing
    Returns a new &quot;Clusters&quot; type representing the centroid
    and all the data points associated with it
*)

let cluster data k = clusterWithIterationLimit data k Int32.MaxValue 0.0
[/fsharp]
<h2>Demo</h2>
Using the sample data from the beginning of this post, let's run it through my k-means clusterer:

[fsharp]
module KDataTest

open System
open KMeans

let kClusterValue = 2

let sampleData : KMeans.DataPoint list = [new DataPoint([1.0]);
                                          new DataPoint([2.0]);
                                          new DataPoint([5.0]);
                                          new DataPoint([14.0]);
                                          new DataPoint([17.0]);
                                          new DataPoint([19.0]);
                                          new DataPoint([20.0])]

KMeans.cluster sampleData kClusterValue
    |&gt; Seq.iter(KMeans.displayClusterInfo)

Console.ReadKey() |&gt; ignore

[/fsharp]

And the output is

[csharp]
Centroid [2.66666666666667], with data points:
[1], [2], [5]

Centroid [17.5], with data points:
[14], [17], [19], [20]
[/csharp]

Full source available at our <a href="https://github.com/blinemedical/KMeans" target="_blank" rel="noopener noreferrer">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2274</wp:post_id>
		<wp:post_date><![CDATA[2013-01-28 10:47:28]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-01-28 15:47:28]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[k-means-step-by-step-in-f]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="algorithms"><![CDATA[algorithms]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1051411444]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561012586;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4170;}i:1;a:1:{s:2:"id";i:3847;}i:2;a:1:{s:2:"id";i:1873;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>17</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-01-22 18:42:50]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-01-22 18:42:50]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA["The next step is now to calculate new centroids" - Maybe try, "Now, the next step is to calculate new centroids?"

"It makes sense since why would F# choose Distance over float," - I might rephrase this. It works well when you're talking, but is confusing when you're reading.

"Once we have this, we want to group this list by the second value in the tuple, the centroid." - I would specify what the "this" is in "once we have this."

"The above code though only does one single cluster iteration. " - I would put "though" at the end.

"If they are we've converged and we can end." - Did you mean "If they are, we've converged, and we can end?"

Also, don't forget the difference between "it's" and "its." It's = two words: it is. Its = one word, possessive. Not sure if this is your spell-check/autocorrect?

Awesome post! It was neat that I could understand it because of all the math!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>Thread Synchronization With Aspects</title>
		<link>https://onoffswitch.net/2013/02/11/thread-synchronization-with-aspects/</link>
		<pubDate>Mon, 11 Feb 2013 19:52:05 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2447</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/thread-synchronization-with-aspects/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

<a href="http://onjava.com/pub/a/onjava/2004/01/14/aop.html" target="_blank" rel="noopener noreferrer">Aspect-oriented programming</a> is an interesting way to decouple common method level logic into localized methods that can be applied on build. For C#, <a href="http://www.sharpcrafters.com/" target="_blank" rel="noopener noreferrer">PostSharp</a> is a great tool that does the heavy lifting of the MSIL rewrites to inject itself in and around your methods based on method tagging with attributes. PostSharp's offerings are split up into <a href="http://www.sharpcrafters.com/postsharp/features" target="_blank" rel="noopener noreferrer">free aspects and pro aspects</a> so it makes diving into aspect-oriented programming easy since you can get a lot done with their free offerings.

One of their free aspects, the <a href="http://doc.sharpcrafters.com/postsharp-2.1/##PostSharp-2.1.chm/html/T_PostSharp_Aspects_MethodInterceptionAspect.htm" target="_blank" rel="noopener noreferrer">method interception aspect</a>, lets you control how a method gets invoked. Using this capability, my general idea was to expose some sort of lock and wrap the method invocation automatically in lock statement using a shared object. This way, we can manage thread synchronization using aspects. 

Managing thread synchronization with aspects isn't a new idea: the PostSharp site already has <a href="http://www.sharpcrafters.com/solutions/locking" target="_blank" rel="noopener noreferrer">an example</a> of thread synchronization. However, they are using a pro feature aspect that allows them to auto-implement a new interface for tagged classes. For the purposes of my example, we can do the same thing without using the pro feature and simultaneously add a little extra functionality.

There are two things I wanted to accomplish. One was to simplify local method locking (basically what the PostSharp example solves), and the second was to facilitate locking of objects across multiple files and namespace boundaries. You can imagine a situation where you have two or more singletons who work on a shared resource. These objects need some sort of shared lock reference to synchronize on, which means you need to expose the synchronized object between all the classes. Not only does this tie classes together, but it can also get messy and error-prone as your application grows.

First, I've defined an interface that exposes a basic lock. Implementing the interface is optional as you'll see later.

[csharp]
public interface IAspectLock
{
    object Lock { get; }
}
[/csharp]

Next we have the actual aspect we'll be tagging methods with.

[csharp]
[Serializable]
public class Synchronize : MethodInterceptionAspect
{
    private static readonly object FlyweightLock = new object();

    private static readonly Dictionary&lt;string, object&gt; LocksByName = new Dictionary&lt;string, object&gt;();

    private String LockName { get; set; }

    /// &lt;summary&gt;
    /// Constructor when using a shared lock by name
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;lockName&quot;&gt;&lt;/param&gt;
    public Synchronize(String lockName)
    {
        LockName = lockName;
    }

    /// &lt;summary&gt;
    /// Constructor for when an object implements IAspectLock
    /// &lt;/summary&gt;
    public Synchronize()
    {

    }

    public override void OnInvoke(MethodInterceptionArgs args)
    {
        object locker;

        if (String.IsNullOrEmpty(LockName))
        {
            var aspectLockObject = args.Instance as IAspectLock;

            if (aspectLockObject != null)
            {
                locker = aspectLockObject.Lock;
            }
            else
            {
                throw new Exception(String.Format(&quot;Method {0} didn't define a lock name nor implement IAspectLock&quot;, args.Method.Name));
            }
        }
        else
        {
            lock (FlyweightLock)
            {
                if (!LocksByName.TryGetValue(LockName, out locker))
                {
                    locker = new object();
                    LocksByName[LockName] = locker;
                }
            }
        }

        lock (locker)
        {
            args.Proceed();
        }
    }
}
[/csharp]

The attribute can either take a string representing the name of the global lock we want to use, or, if none is provided, we can test to see if the instance implements our special interface and use its lock. When an object implements <code>IAspectLock</code> the code path is simple: get the lock from the object and use it on the method.

The second code path, when you use global lock name, lets you lock across the entire application without having to tie classes together, keeping things clean and decoupled.

For the scenario where a global lock name was defined, I used a static dictionary to keep track of the locks and corresponding reference objects to lock on based on name. This way I can maximize throughput by using a flyweight container: lock first on the dictionary just to get the lock I want, then lock on the value retrieved. The locking of the dictionary will always be fast and shouldn't be contended for that often. Uncontested locks are tested for using <a href="http://tech.blinemedical.com/inter-process-locking/" target="_blank" rel="noopener noreferrer">spinlock semantics</a> so they are usually extremely quick. Once you have the actual lock you want to use for this function, you can call <code>args.Proceed()</code> which will actually invoke the tagged method.

Just to be sure this all works, I wrote a unit test to make sure the attribute worked as expected. The test spawns 10,000 threads which will each loop 100,000 times and increment the <code>_syncTest</code> integer. The idea is to introduce a race condition. Given enough threads and enough work, some of those threads won't get the updated value of the integer and won't actually increment it. For example, at some point both threads may think <code>_syncTest</code> is 134, and both will increment to 135. If it was synchronized, the value, after two increments, should be 136. Since race conditions are timing-dependent we want to make the unit test stressful to try and maximize the probability that this would happen. Theoretically, we could run this test and never get the race condition we're expecting, since that's by definition a race condition (non-deterministic results). However, on my machine, I was able to consistently reproduce the expected failure conditions.

[csharp]
private int _syncTest = 0;
private const int ThreadCount = 10000;
private const int IterationCount = 100000;

[Test]
public void TestSynchro()
{
    var threads = new List&lt;Thread&gt;();
    for (int i = 0; i &lt; ThreadCount; i++)
    {
        threads.Add(ThreadUtil.Start(&quot;SyncTester&quot; + i, SynchroMethod));
    }

    threads.ForEach(t=&gt;t.Join());

    Assert.True(_syncTest == ThreadCount * IterationCount,
            String.Format(&quot;Expected synchronized value to be {0} but was {1}&quot;, ThreadCount * IterationCount, _syncTest));
}

[GlobalSynchronize(&quot;SynchroMethodTest&quot;)]
private void SynchroMethod()
{
    for (int i = 0; i &lt; IterationCount; i++)
    {
        _syncTest++;
    }
}
[/csharp]

When the method doesn't have the attribute we get an NUnit failure like

[csharp]
  Expected synchornized value to be 1000000000 but was 630198141
  Expected: True
  But was:  False

   at NUnit.Framework.Assert.That(Object actual, IResolveConstraint expression, String message, Object[] args)
   at NUnit.Framework.Assert.True(Boolean condition, String message)
   at AspectTests.TestSynchro() in AspectTests.cs: line 35
[/csharp]

Showing the race condition that we expected did happen (the value will change each time). When we have the method synchronized, our test passes.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2447</wp:post_id>
		<wp:post_date><![CDATA[2013-02-11 14:52:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-11 19:52:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[thread-synchronization-with-aspects]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="aspects"><![CDATA[aspects]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="post_tag" nicename="postsharp"><![CDATA[postsharp]]></category>
		<category domain="post_tag" nicename="synchronization"><![CDATA[synchronization]]></category>
		<category domain="post_tag" nicename="threading"><![CDATA[threading]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1070974672]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558136896;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4394;}i:1;a:1:{s:2:"id";i:738;}i:2;a:1:{s:2:"id";i:383;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>27</wp:comment_id>
			<wp:comment_author><![CDATA[sandy.yin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sandy.yin@blinemedical.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-02-07 16:29:27]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-02-07 16:29:27]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA["Unfortunately, this isn't a new idea:" - is it unfortunate because it's a pro feature? I wasn't super-clear on that.

"But, for my following example, we can do the same thing without using the pro feature, and at the same time add a little extra functionality." - This has quite a bit more complexity in the sentence structure than you maybe need. Try "For the purposes of my example, we can do the same thing without using the pro feature and simultaneously add a little extra functionality."

"Theoretically, we could run this test and never get a race condition, since that's by definition a race condition." - I don't really know what this sentence means. Which part is by definition a race condition?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[editorial-comment]]></wp:comment_approved>
			<wp:comment_type><![CDATA[editorial-comment]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					</item>
					<item>
		<title>First pass localization through Microsoft&#039;s Translate API</title>
		<link>https://onoffswitch.net/2013/01/18/first-pass-localization-through-microsofts-translate-api/</link>
		<pubDate>Fri, 18 Jan 2013 17:50:09 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2464</guid>
		<description></description>
		<content:encoded><![CDATA[]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2464</wp:post_id>
		<wp:post_date><![CDATA[2013-01-18 17:50:09]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-01-18 17:50:09]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[first-pass-localization-through-microsofts-translate-api]]></wp:post_name>
		<wp:status><![CDATA[pitch]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>IxD 2013 - Production ready CSS workshop</title>
		<link>https://onoffswitch.net/2013/01/28/ixd-2013-production-ready-css-workshop/</link>
		<pubDate>Mon, 28 Jan 2013 16:22:49 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2487</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/ixd-2013-production-ready-css-workshop/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

Carlo and I are in Toronto for <a href="http://interaction13.ixda.org/" target="_blank" rel="noopener noreferrer">Ixd 2013</a> for the week hoping to pick up some interesting info on interaction and design.  We were painfully reminded of the need for continual design improvement within the first hour of stepping into Canada.  We rented a Hyundai Veloster and for 30 minutes couldn't figure out how to start the car. For the uninitiated, you have to hold the brake pedal down while pushing the start button.

<a href="http://tech.blinemedical.com/wp-content/uploads/2013/01/velocsterStarter.jpg"><img src="http://tech.blinemedical.com/wp-content/uploads/2013/01/velocsterStarter-300x214.jpg" alt="velocsterStarter" width="300" height="214" class="aligncenter size-medium wp-image-2502" /></a>

Today the conference started and we attended a workshop called "Sitting in the drivers seat: Designing production level CSS".  Carlo's been doing html and css for a long time and was interested to hear what the speaker would say, whereas I am less experienced and wanted to see if I could pick up any design tips. We were pleasantly surprised when <a href="http://designaday.tumblr.com/" target="_blank" rel="noopener noreferrer">Jack Moffett</a> started and stressed that we should be focusing on blurring the lines between designer and developer.  He quoted<a href="http://www.uie.com/about/consultants/" target="_blank" rel="noopener noreferrer"> Jared Spool</a> who said that designers should stop asking "<em>do I need to code</em>" but instead should ask "<em>will learning code help me be a better designer?</em>".  Moffett stressed using developer oriented tools such as version control--svn or git--wikis to track requirements, diff tools, bug tracking software, some sort of IDE for live editing of html and css, and of course the standard Firebug, Chrome, and even Internet Explorer developer toolsets.  Using production tools that developers also use integrates the application design and development processes.  With wiki's and version control you get historical visibility about changes and decisions. This helps designers know when developers want changes, and helps developers know when designers want changes.  Conversations and issues can be tracked in bug trackers like JIRA or FogBugz and gives everyone a sense of direction.  

Moffett said that a benefit to following developer practices and learning at least some basic html and css structures is that designers can design with a practical implementation in mind. Understanding the limitations of html and css also helps the designer avoid implementation pitfalls and minimizes the amount of design, implement, tweak rounds during application development. Just like an architect should know the structure and the needs of the building crew, if the design is impracticable it doesn't matter how pretty it is.  

After the tool chain introduction Moffett led everyone through <a href="https://github.com/jackmoffett/DriverSeat" target="_blank" rel="noopener noreferrer">several examples</a> of creating reusable CSS stressing seperating content from skin. The content should be able to stay independent, while skin elements (such as font, padding, etc) can be toggled with css.  The example that I think the workshop most enjoyed was taking this: 

<a href="http://tech.blinemedical.com/wp-content/uploads/2013/01/amazonRow..png"><img src="http://tech.blinemedical.com/wp-content/uploads/2013/01/amazonRow.-300x97.png" alt="amazonRow." width="300" height="97" class="aligncenter size-medium wp-image-2492" /></a>

and refactoring it to this:

<a href="http://tech.blinemedical.com/wp-content/uploads/2013/01/amazonCol..png"><img src="http://tech.blinemedical.com/wp-content/uploads/2013/01/amazonCol.-167x300.png" alt="amazonCol." width="167" height="300" class="aligncenter size-medium wp-image-2491" /></a>

By removing inline css and a few minor content structure tweaks. Now we can toggle between row and column layout using the same html, just with different css.  I was a little surprised that Amazon's site was used for the example - I expected at first that Moffett would show it as an example of what to do, not what to NOT do. 

Moffett also worked through an example leveraging CSS inheritance to control visible state.  By doing it this way you can avoid direct css style manipulations in javascript and use css classes to define visual behaviors. For example, assume we want to control visiblity of a div that contains some photos lets set up the html and css like so.

Photos aren't visible: 

[html]
&lt;div&gt;
  &lt;div class=&quot;photos&quot;&gt;
    // some photos
  &lt;/div&gt;
&lt;/div&gt;
[/html]

Photos are visible:

[html]
&lt;div class=&quot;showPhotos&quot;&gt;
  &lt;div class=&quot;photos&quot;&gt;
    // some photos
  &lt;/div&gt;
&lt;/div&gt;
[/html]

The css for this example would look like this:

[css]
.photos {
   display:none;
}

.showPhotos .photos{
   display:block;
}
[/css]

By default the photos div isn't visible, however if we added the <code>showPhotos</code> class to the parent, the parents display property will override the <code>photos</code> display setting it to <code>block</code>.  By removing the <code>showPhotos</code> class we can hide the element since the photos display element goes back to the default. This way you can control visibility and state by toggling items at the parent level instead of the child level. Carlo also brought up in the seminar that by doing it this way you are setting it up for using css transitions and animations since you can now control state information just with css.

While it may sound trivial to UI developers or designers, it's always good to ground yourself in how to make something simple and reusable. Even the best developers need reminders.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2487</wp:post_id>
		<wp:post_date><![CDATA[2013-01-28 11:22:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-01-28 16:22:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ixd-2013-production-ready-css-workshop]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="conference"><![CDATA[conference]]></category>
		<category domain="post_tag" nicename="css"><![CDATA[css]]></category>
		<category domain="post_tag" nicename="design"><![CDATA[Design]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1051567750]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560912354;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3500;}i:1;a:1:{s:2:"id";i:4631;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>IxD 2013: Rhythm, Flow, and Style</title>
		<link>https://onoffswitch.net/2013/01/29/ixd-2013-rythm-flow-and-style/</link>
		<pubDate>Tue, 29 Jan 2013 19:43:41 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2521</guid>
		<description></description>
		<content:encoded><![CDATA[<em>This article was originally published at <a href="http://tech.blinemedical.com/ixd-2013-rythm-flow-and-style/" target="_blank" rel="noopener noreferrer">tech.blinemedical.com</a></em>

Today Carlo and I listened to a 45 minute presentation by <a href="https://twitter.com/pstahl" target="_blank" rel="noopener noreferrer">Peter Stahl</a> called "Rhythm, Flow and Style" which discussed designing flow and rhythm into applications.  Peter started with the observation that the world is full of rhythms, but not just what people think of (namely music). Rhythms exist in every part of life, chopping vegetables, walking outside, involving in conversations, filling out forms, navigating a website, etc.  All of them involve actions, pauses, and repetitions.  According to Stahl there are two types of rhythm: iterative rhythm, which is rhythm of the user (engaging with the application) and there is also motivic rhythm, which is rhythm within the application.  But to get rhythm you need flow. 

Flow, according to Stahl's presentation has several dimensions:

<ul>
<li>Known goals, with known progress</li>
<li>Perceived balance of challenge and skill</li>
<li>Sense of control</li>
<li>Focused concentration</li>
<li>Loss of self conciouslyness: becoming one with the activity</li>
<li>Time distortion</li>
<li>Self rewarding experience</li>
</ul>

and it's possible to induce it by offering clear goals, achievements, progressive challenges, progress tracking, and obvious next steps.  The ultimate intent is to keep the user engaged and challenged, where they can easily lose themselves in the application.  You can see these kinds of theories being applied to sites like <a href="http://stackoverflow.com" target="_blank" rel="noopener noreferrer">StackOverflow</a>, <a href="http://www.linkedin.com" target="_blank" rel="noopener noreferrer">LinkedIn</a>, or <a href="http://www.okcupid.com/" target="_blank" rel="noopener noreferrer">OKCupid</a>.  They have percentages indicating progress, helpful hints on how to get to the next step, progressively more advanced information, etc.  Flow engages the user and lets the application open up, giving it a sense of movement over time.

Flow, however, isn't always just about content and progress. It's also a matter of visual effects and transitions.  Transition effects can influence the perception of an application: fast and jarring can give a sense of precision and automation, whereas slow, more gentle fades, induces a sense of calmness.  

On top of rhythm and flow is style, which is used to direct the flow and rhythm. Stahl called it rhythmic style and it is affected by visual frequency, speed, size and distance, and special effects. There are lots of different kinds of rhythmic style choices:

<ul>
<li>Dazzling or engaging?</li>
<li>Zippy or comfortable?</li>
<li>Dramatic or responsive?</li>
<li>Single-use or long-term?</li>
</ul>

But the choice of style within visual components depends on content branding and audience.  

Stahl compared <a href="http://www.samsung.com" target="_blank" rel="noopener noreferrer">Samsung</a> vs <a href="http://www.vizio.com/" target="_blank" rel="noopener noreferrer">Vizio</a> (though both have changed their sites since the slides shown in presentation).  Samsung, at the time, used gentle fade transitions, whereas Vizio used a paper swipe effect. He argued that Samsung's felt more human but Vizio's was reminiscent of a copy machine: inhuman but precise.  Both have advantages and with small motivic rhythm choices you can induce different emotions in the user.

Of course though, flow rests with the user, so Stahl stressed to make sure to include the user in user interaction testing.  Stahl showed a four way split image showing two images of the user using the application, as well as the screen being viewed (the last screen I think was a description of the test):

<a href="http://tech.blinemedical.com/wp-content/uploads/2013/01/stahlPresentation.jpg"><img src="http://tech.blinemedical.com/wp-content/uploads/2013/01/stahlPresentation-300x231.jpg" alt="stahlPresentation" width="300" height="231" class="aligncenter size-medium wp-image-2523" /></a>

Finally, Stahl ended with a resonating quote that I think captures the design goals of any application:

<blockquote>It's all about the choregraphy of people's attention.  Attention is like water. It flows. It's liquid.  You create channels to divert it, and you hope that it flows the right way".  </blockquote>

I really enjoyed his presentation. Here are Stahls slides  https://www.box.com/s/ms8ssh4nc3zl6mx0n38w.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2521</wp:post_id>
		<wp:post_date><![CDATA[2013-01-29 14:43:41]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-01-29 19:43:41]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ixd-2013-rythm-flow-and-style]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="conference"><![CDATA[conference]]></category>
		<category domain="post_tag" nicename="design"><![CDATA[Design]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[dsq_thread_id]]></wp:meta_key>
		<wp:meta_value><![CDATA[1053713317]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[ixd-2013-rhythm-flow-and-style]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558715686;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:2985;}i:2;a:1:{s:2:"id";i:3500;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Building a custom lexer</title>
		<link>https://onoffswitch.net/2013/02/25/building-a-custom-lexer/</link>
		<pubDate>Mon, 25 Feb 2013 16:54:43 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=6</guid>
		<description></description>
		<content:encoded><![CDATA[As a software engineer I spend all day (hopefully) writing code.  I love code and I love that there are languages that help me solve problems and create solutions.  But as an engineer I always want to know more about the tools I work with so I recently picked up "<a href="http://www.amazon.com/Language-Implementation-Patterns-Domain-Specific-Programming/dp/193435645X" target="_blank" rel="noopener noreferrer">Language Implementation Patterns</a>" by <a href="http://www.cs.usfca.edu/~parrt/" target="_blank" rel="noopener noreferrer">Terence Parr</a> and decided I was going to learn how to build a language.  After reading through most of the book and working on examples for about 5 weeks I ended up building an interpreted <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">toy general purpose</a> language that has features like:

<ul>
<li>type inference</li>
<li>partial functions</li>
<li>static typing</li>
<li>classes</li>
<li>first class methods</li>
</ul>

The language I wrote is by no means production level. It's a toy, and it can do toy things. It can't do IO, it can't talk to the OS, it doesn't do GC, but it does evaluate expressions, call methods, basic exception handling, dynamic typing, and lets you do simple work.  

Now that I'm done (but never finished) with the project I really appreciate how much goes into making a working language. The whole project increased my respect ten fold for people like <a href="http://twitter.com/dsyme" target="_blank" rel="noopener noreferrer">Don Syme</a>, <a href="https://twitter.com/ttliu2000" target="_blank" rel="noopener noreferrer">Tao Liu</a>, <a href="http://en.wikipedia.org/wiki/Anders_Hejlsberg" target="_blank" rel="noopener noreferrer">Anders Hejlsberg</a>, <a href="http://en.wikipedia.org/wiki/Bjarne_Stroustrup" target="_blank" rel="noopener noreferrer">Bjarne Stroustrup</a>, <a href="http://en.wikipedia.org/wiki/Guido_van_Rossum" target="_blank" rel="noopener noreferrer">Guido van Rossum</a>, and countless others who have made REAL languages. But now that I'm about done with my project, I wanted to share how parts of my language were done. 

<h2>Tokenizing</h2>

The first step to implementing a domain specific language, or even a general purpose language, is to figure out what the string representing the program means.  You have to figure out that something like this:

[csharp]
int foo = 1;

void func(){
   print &quot;abc&quot;;
}
[/csharp]

Really means

[csharp]
int
foo
equals
number
semicolon
void
word
left parenthesis
right parenthesis
left bracket
word
quoted string
semicolon
right bracket
[/csharp]


To do this you need to first break up the program into a series of strings that you know are independent tokens, such as "int", and "foo". This is called tokenizing. Then you then need to go over that list and say "oh hey, I found this token 'int' but really it means <code>TokenType.Int</code>".  Converting the strings to strong types is called lexing. Translating the string into strong types makes it significantly easier to work on it later. 

While some grammars can be tokenized by breaking up on whitespace, for my language I found it easier to tokenize character by character. This means I don't have to worry about whitespace or special characters or any other kinds of weird delimiters. Granted, it's more resource intensive, but for a toy language that's OK. There are some great libraries out there that will let you define grammars for your language and will prebuild you lexers, tokenizers, parsers, tree walkers, etc (such as <a href="http://www.antlr.org/" target="_blank" rel="noopener noreferrer">ANTLR</a>) but it can be fun doing this by hand the first time.

To tokenize my grammar I created a base class called <code>TokenizableStreamBase</code> that takes a function to generate a list of items and gives you some functionality to work on that stream. The base class lets me

<ul>
<li>Consume an item. This means I've processed whatever is at the current index and we should move to the next item</li>
<li>Snapshot the stream.  I can keep track of where I am and if something happens later I can rollback to this point in the stream.</li>
<li>Commit the stream.  If I made a snapshot just discard it and continue from where I am now.</li>
</ul>

Let me give an example of why you want to use snapshots. Pretend you are trying to match the word "string", but the current tokenizer has "strint". Going character by character you would successfully match the word "strint" up until the last character. At that point you can tell it's not a "string" so you can roll back the stream to the start of the word and try something else. 

Here is the base class:

[csharp]
public class TokenizableStreamBase&lt;T&gt; where T : class
{
    public TokenizableStreamBase(Func&lt;List&lt;T&gt;&gt; extractor)
    {
        Index = 0;

        Items = extractor();

        SnapshotIndexes = new Stack&lt;int&gt;();
    }

    private List&lt;T&gt; Items { get; set; }

    protected int Index { get; set; }

    private Stack&lt;int&gt; SnapshotIndexes { get; set; }

    public virtual T Current
    {
        get
        {
            if (EOF(0))
            {
                return null;
            }

            return Items[Index];
        }
    }

    public void Consume()
    {
        Index++;
    }

    private Boolean EOF(int lookahead)
    {
        if (Index + lookahead &gt;= Items.Count)
        {
            return true;
        }

        return false;
    }

    public Boolean End()
    {
        return EOF(0);
    }

    public virtual T Peek(int lookahead)
    {
        if (EOF(lookahead))
        {
            return null;
        }

        return Items[Index + lookahead];
    }

    public void TakeSnapshot()
    {
        SnapshotIndexes.Push(Index);
    }

    public void RollbackSnapshot()
    {
        Index = SnapshotIndexes.Pop();
    }

    public void CommitSnapshot()
    {
        SnapshotIndexes.Pop();
    }
}
[/csharp]

And here is my entire tokenizer that creates a consumable stream of characters from the input source:

[csharp]
public class Tokenizer : TokenizableStreamBase&lt;String&gt;
{
    public Lexer(String source) : 
        base(() =&gt; source.ToCharArray().Select(i =&gt; i.ToString(CultureInfo.InvariantCulture))
                                       .ToList())
    {            
    }
}

[/csharp]

In a later post I'll describe how I built my parser, which is responsible for creating abstract syntax trees and logical validation of the code. The parser re-uses the tokenizer stream base and instead of using characters (like the tokenizer stream) uses a stream of tokens.  


<h2>Lexing</h2>

A tokenizer is pretty useless though without creating meaningful tokens out of that source.  To do that you need a way to match the characters to an expected literal.  In my language I've created the concept of a matcher that can emit an object of type <code>Token</code>.  Each matcher will be able to take the current tokenizer, consume characters until it finds what it wants, and if it found its match type, return the strongly typed token.  

So, lets say I want to match the word "int" to token <code>Int</code>  I'll have a matcher that knows it wants the string "int" and it will go through the lexer's current position matching while things are matching, or bailing if it encounters a character that it didn't expect. Being able to roll back a snapshot is called backtracking. Some grammars are clear enough to never have to do this. For example, if you will always have words that start with "i" and they are ALWAYS supposed to be "int" then you don't need to backtrack.  If you fail to match then the syntax is invalid.

To build the matchers, I started with an abstract base:

[csharp]
public abstract class MatcherBase : IMatcher
{
    public Token IsMatch(Tokenizer tokenizer)
    {
        if (tokenizer.End())
        {
            return new Token(TokenType.EOF);
        }

        tokenizer.TakeSnapshot();

        var match = IsMatchImpl(tokenizer);

        if (match == null)
        {
            tokenizer.RollbackSnapshot();
        }
        else
        {
            tokenizer.CommitSnapshot();
        }

        return match;
    }

    protected abstract Token IsMatchImpl(Tokenizer tokenizer);
}
[/csharp]

This takes a <code>Tokenizer</code> and hands the tokenizer to whatever subclasses the abstract base for the match implementation. The base class will make sure to handle snapshots. If the matcher returns a non-null token then it will commit the snapshot otherwise it can roll it back and let the next matcher try.

As an example, here is the matcher for whitespace tokens. For whitespace I don't really care how much whitespace there was, just that there was whitespace. In fact in the end I discard whitespace completely since it doesn't really matter. If it found whitespace it'll return a new token of token type whitespace. Otherwise it'll return null.

[csharp]
class MatchWhiteSpace : MatcherBase
{
    protected override Token IsMatchImpl(Tokenizer tokenizer)
    {
        bool foundWhiteSpace = false;

        while (!tokenizer.End() &amp;&amp; String.IsNullOrWhiteSpace(tokenizer.Current))
        {
            foundWhiteSpace = true;

            tokenizer.Consume();
        }

        if (foundWhiteSpace)
        {
            return new Token(TokenType.WhiteSpace);
        }

        return null;
    }
}
[/csharp]


Below is another matcher that finds quoted strings. You can set the quote delimiter to be either a " or a '. So "this" matches and so does 'this'.  

[csharp]
public class MatchString : MatcherBase
{
    public const string QUOTE = &quot;\&quot;&quot;;

    public const string TIC = &quot;'&quot;;

    private String StringDelim { get; set; }

    public MatchString(String delim)
    {
        StringDelim = delim;
    }

    protected override Token IsMatchImpl(Tokenizer tokenizer)
    {
        var str = new StringBuilder();

        if (tokenizer.Current == StringDelim)
        {
            tokenizer.Consume();

            while (!tokenizer.End() &amp;&amp; tokenizer.Current != StringDelim)
            {
                str.Append(tokenizer.Current);
                tokenizer.Consume();
            }

            if (tokenizer.Current == StringDelim)
            {
                tokenizer.Consume();
            }
        }

        if (str.Length &gt; 0)
        {
            return new Token(TokenType.QuotedString, str.ToString());
        }

        return null;
    }
}
[/csharp]

The next matcher (below) does the bulk of the work. This one finds built in keywords and special characters by taking an input string, and the final token it should represent. It determines if the current stream contains that token and then emits it.  The idea is if you know "int" is a built in type, and it should match to some <code>TokenType.Int</code>, you can pass that info to a <code>MatchKeyword</code> instance and it'll find the token for you if it exists. The <code>Match</code> property contains the raw string you want to match on, and the <code>TokenType</code> property represents the strongly typed type that should be paired to the raw string:

[csharp]
public class MatchKeyword : MatcherBase
{
    public string Match { get; set; }

    private TokenType TokenType { get; set; }


    /// &lt;summary&gt;
    /// If true then matching on { in a string like &quot;{test&quot; will match the first cahracter
    /// because it is not space delimited. If false it must be space or special character delimited
    /// &lt;/summary&gt;
    public Boolean AllowAsSubString { get; set; }

    public List&lt;MatchKeyword&gt; SpecialCharacters { get; set; } 

    public MatchKeyword(TokenType type, String match)
    {
        Match = match;
        TokenType = type;
        AllowAsSubString = true;
    }

    protected override Token IsMatchImpl(Tokenizer tokenizer)
    {
        foreach (var character in Match)
        {
            if (tokenizer.Current == character.ToString(CultureInfo.InvariantCulture))
            {
                tokenizer.Consume();
            }
            else
            {
                return null;
            }
        }

        bool found;

        if (!AllowAsSubString)
        {
            var next = tokenizer.Current;

            found = String.IsNullOrWhiteSpace(next) || SpecialCharacters.Any(character =&gt; character.Match == next);
        }
        else
        {
            found = true;
        }

        if (found)
        {
            return new Token(TokenType, Match);
        }

        return null;
    }
}
[/csharp]

The special characters list is an injected list of keyword matchers that let the current matcher know when things are delimited. For example, we want to support both

[csharp]
if(
[/csharp]

and 

[csharp]
if (
[/csharp]

In the first block, the keyword "if" is delimited by a special character "(" and not just whitespace.  By using special characters AND whitespace as delimiters we can have whitespace agnostic code.

For my language I supported the following special characters and keywords:

[csharp]
public enum TokenType
{
    Infer,
    Void,
    WhiteSpace,
    LBracket,
    RBracket,
    Plus,
    Minus,
    Equals,
    HashTag,
    QuotedString,
    Word,
    Comma,
    OpenParenth,
    CloseParenth,
    Asterix,
    Slash,
    Carat,
    DeRef,
    Ampersand,
    Fun,
    GreaterThan,
    LessThan,
    SemiColon,
    If,
    Return,
    While,
    Else,
    ScopeStart,
    EOF,
    For,
    Float,
    Print,
    Dot,
    True,
    False,
    Boolean,
    Or,
    Int,
    Double,
    String,
    Method,
    Class,
    New,
    Compare,
    Nil,
    NotCompare,
    Try,
    Catch
}
[/csharp]

It's a lot, but at the same time it's not nearly enough! Still, having a single matcher that can match on known tokens means extending the language at this level is quite easy. Here is the construction of the list of my matchers. Order here matters since it determines precedence.  You can see now that to add new keywords or characters to the grammar only requires defining a new enum and updating the appropriate match list.

[csharp]
private List&lt;IMatcher&gt; InitializeMatchList()
{
    // the order here matters because it defines token precedence

    var matchers = new List&lt;IMatcher&gt;(64);

    var keywordmatchers = new List&lt;IMatcher&gt; 
                            {
                                new MatchKeyword(TokenType.Void, &quot;void&quot;),
                                new MatchKeyword(TokenType.Int, &quot;int&quot;),
                                new MatchKeyword(TokenType.Fun, &quot;fun&quot;),
                                new MatchKeyword(TokenType.If, &quot;if&quot;),
                                new MatchKeyword(TokenType.Infer, &quot;var&quot;),
                                new MatchKeyword(TokenType.Else, &quot;else&quot;),
                                new MatchKeyword(TokenType.While, &quot;while&quot;),
                                new MatchKeyword(TokenType.For, &quot;for&quot;),
                                new MatchKeyword(TokenType.Return, &quot;return&quot;),
                                new MatchKeyword(TokenType.Print, &quot;print&quot;),
                                new MatchKeyword(TokenType.True, &quot;true&quot;),
                                new MatchKeyword(TokenType.False, &quot;false&quot;),
                                new MatchKeyword(TokenType.Boolean, &quot;bool&quot;),
                                new MatchKeyword(TokenType.String, &quot;string&quot;),
                                new MatchKeyword(TokenType.Method, &quot;method&quot;),
                                new MatchKeyword(TokenType.Class, &quot;class&quot;),
                                new MatchKeyword(TokenType.New, &quot;new&quot;),
                                new MatchKeyword(TokenType.Nil, &quot;nil&quot;)
                            };


    var specialCharacters = new List&lt;IMatcher&gt;
                            {
                                new MatchKeyword(TokenType.DeRef, &quot;-&gt;&quot;),
                                new MatchKeyword(TokenType.LBracket, &quot;{&quot;),
                                new MatchKeyword(TokenType.RBracket, &quot;}&quot;),
                                new MatchKeyword(TokenType.Plus, &quot;+&quot;),
                                new MatchKeyword(TokenType.Minus, &quot;-&quot;),
                                new MatchKeyword(TokenType.Equals, &quot;=&quot;),
                                new MatchKeyword(TokenType.HashTag, &quot;#&quot;),
                                new MatchKeyword(TokenType.Comma, &quot;,&quot;),
                                new MatchKeyword(TokenType.OpenParenth, &quot;(&quot;),
                                new MatchKeyword(TokenType.CloseParenth, &quot;)&quot;),
                                new MatchKeyword(TokenType.Asterix, &quot;*&quot;),
                                new MatchKeyword(TokenType.Slash, &quot;/&quot;),
                                new MatchKeyword(TokenType.Carat, &quot;^&quot;),
                                new MatchKeyword(TokenType.Ampersand, &quot;&amp;&quot;),
                                new MatchKeyword(TokenType.GreaterThan, &quot;&gt;&quot;),
                                new MatchKeyword(TokenType.LessThan, &quot;&lt;&quot;),
                                new MatchKeyword(TokenType.Or, &quot;||&quot;),
                                new MatchKeyword(TokenType.SemiColon, &quot;;&quot;),
                                new MatchKeyword(TokenType.Dot, &quot;.&quot;),
                            };

    // give each keyword the list of possible delimiters and not allow them to be 
    // substrings of other words, i.e. token fun should not be found in string &quot;function&quot;
    keywordmatchers.ForEach(keyword =&gt;
        {
            var current = (keyword as MatchKeyword);
            current.AllowAsSubString = false;
            current.SpecialCharacters = specialCharacters.Select(i =&gt; i as MatchKeyword).ToList();
        });

    matchers.Add(new MatchString(MatchString.QUOTE));
    matchers.Add(new MatchString(MatchString.TIC));

    matchers.AddRange(specialCharacters);
    matchers.AddRange(keywordmatchers);

    matchers.AddRange(new List&lt;IMatcher&gt;
                                        {
                                            new MatchWhiteSpace(),
                                            new MatchNumber(),
                                            new MatchWord(specialCharacters)
                                        });

    return matchers;
}
[/csharp]

To actually run through and get the tokens we do this

[csharp]
public IEnumerable&lt;Token&gt; Lex()
{
    Matchers = InitializeMatchList();

    var current = Next();

    while (current != null &amp;&amp; current.TokenType != TokenType.EOF)
    {
        // skip whitespace
        if (current.TokenType != TokenType.WhiteSpace)
        {
            yield return current;
        }

        current = Next();
    }
}

.... define the match list ...

private Token Next()
{
    if (Lexer.End())
    {
        return new Token(TokenType.EOF);
    }

    return 
            (from match in Matchers
            let token = match.IsMatch(Tokenizer)
            where token != null
            select token).FirstOrDefault();
}
[/csharp]

And the only thing left is in the constructor of the Tokenizer

[csharp]
public Lexer(String source)
{
    Tokenizer = new Tokenizer(source);
}
[/csharp]

<h2>Testing</h2>

Lets see it in action in a unit test. Keep in mind the tokenizer and lexer do only the most basic syntax validation, but not much. It's more about creating a typed token stream representing your code.  Later we can use the typed token stream to create meaningful data structures representing the code.

[csharp]
[Test]
public void TestTokenizer()
{
    var test = @&quot;function void int &quot;&quot;void int&quot;&quot; {} -&gt;*/test^void,5,6,7 8.0&quot;;

    var tokens = new Lexer(test).Lex().ToList();

    foreach (var token in tokens)
    {
        Console.WriteLine(token.TokenType + &quot; - &quot; + token.TokenValue);
    }
}
[/csharp]

And this prints us out

[csharp]
Word - function
Void - void
Int - int
QuotedString - void int
LBracket - {
RBracket - }
DeRef - -&gt;
Asterix - *
Slash - /
Word - test
Carat - ^
Void - void
Comma - ,
Int - 5
Comma - ,
Int - 6
Comma - ,
Int - 7
Float - 8.0
[/csharp]

Now we're in a position that we can start parsing our language. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>6</wp:post_id>
		<wp:post_date><![CDATA[2013-02-25 16:54:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-25 16:54:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-a-custom-lexer]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
		<category domain="post_tag" nicename="lexer"><![CDATA[Lexer]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561966458;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3565;}i:1;a:1:{s:2:"id";i:3016;}i:2;a:1:{s:2:"id";i:7777;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>18</wp:comment_id>
			<wp:comment_author><![CDATA[John DeHope]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[johndehope3@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[205.145.107.100]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-02-28 13:04:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-02-28 13:04:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for this article. I'm pretty comfortable with tokenizing and lexing. I'm really interested to see a walk through of your parsing solution. I always get hung up there. Thanks!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>19</wp:comment_id>
			<wp:comment_author><![CDATA[Anton]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.243.58.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-02-28 13:25:34]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-02-28 13:25:34]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks John, glad you found it useful. The next post on parsing is coming soon, I already have a working draft :). After that I plan on discussing scope and type resolution, syntactic extras like partial functions, resolving forward references, and the interpreter (which supports closures)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>18</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>20</wp:comment_id>
			<wp:comment_author><![CDATA[A handrolled language parser | onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/a-handrolled-language-parser/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-03-01 15:31:46]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-03-01 15:31:46]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] my previous post about building a custom lexer I mentioned that, for educational purposes, I created a simple toy [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>21</wp:comment_id>
			<wp:comment_author><![CDATA[How to Create a Lexer in C++ - Imprinted Studios]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.imprintedstudios.com/how-to-create-a-lexer-in-cpp/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[173.254.28.85]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-03-21 05:36:50]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-03-21 05:36:50]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] OnOffSwitch [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>22</wp:comment_id>
			<wp:comment_author><![CDATA[Jamie]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[jamie_maguire@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[80.44.118.36]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-06-14 09:28:55]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-06-14 09:28:55]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Excellent article!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>23</wp:comment_id>
			<wp:comment_author><![CDATA[Michael]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[michaelpittino@gmx.net]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[62.46.90.6]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-06-22 18:57:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-06-22 18:57:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Somehow I can't find the IMatcher and Token class in this article?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>24</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[23.23.174.237]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-06-22 20:50:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-06-22 20:50:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah, guess I didn't put it here. Anyways you can find those both at https://github.com/devshorts/LanguageCreator/blob/master/Lang/Matches/IMatcher.cs and https://github.com/devshorts/LanguageCreator/blob/master/Lang/Data/Token.cs]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>23</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>25</wp:comment_id>
			<wp:comment_author><![CDATA[Extracting scala method names from objects with macros | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/extracting-scala-method-names-objects-macros/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-08-03 00:34:04]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-08-03 00:34:04]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] have a soft spot in me for AST&#8217;s ever since I went through the exercise of building my own language. Working in Java I missed the dynamic ability to get compile time information, [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>26</wp:comment_id>
			<wp:comment_author><![CDATA[Lance]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[me_lance@msn.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[204.196.103.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2017-02-16 18:43:53]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2017-02-16 18:43:53]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi, this is an excellent article, thank you for writing it.  I had a question about a specific case and wasn't sure how to handle it or if this method will handle it.  My "language" is incredibly simple in that it only contains three possible token types: string, variable, equation.  A variable is contained in square brackets "[Variable]", an equation is the same but begins with an equal sign "[=Equation]", and everything else is a string.  I've managed to get this working using your lexer, however; I want to be able to allow nested variables as such: "[Foo[Bar]]" where the name of the outer variable is dependent on the value of the inner variable.  So if [Bar] = "tball" then the outer variable will be [Football].]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>When to abort a thread</title>
		<link>https://onoffswitch.net/2013/03/18/when-to-abort-a-thread/</link>
		<pubDate>Mon, 18 Mar 2013 08:00:06 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=365</guid>
		<description></description>
		<content:encoded><![CDATA[When is it OK to abort a thread is a question that comes up every so often. Usually everyone jumps on the bandwagon that you should never ever do a thread abort, but I don't agree. Certainly there are times when it's valid and if you understand what you are doing then it's ok to use.

The reasoning behind never using thread abort is because calling abort on a thread issues an asynchronous exception, meaning that exceptions could happen where you think there never should be exceptions such as dispose methods or finally blocks. <a href="http://www.interact-sw.co.uk/iangblog/2004/11/12/cancellation">This post</a> describes what happens with thread abort and I found it to be a good read.

But, I still don't think you should <em>never</em> use thread abort. The big issue is what if you don't have access to the code that is running in the thread? If a 3rd party library is blocking your app or is doing something uncontrollable and you need to end it you don't have a lot of options to gracefully exit. You may not be able to recompile the code and sprinkle in some cancellation tokens, or maybe you aren't comfortable with doing that since the library code is large and you're not familiar with its internals. 

If the problem is that your application won't shut down because of a runaway thread, people sometimes suggest to run the work in a background thread, but I still think that's a bad idea. Background threads <a href="http://msdn.microsoft.com/en-us/library/system.threading.thread.isbackground.aspx">are the same</a> as foreground threads, except they won't keep your application from exiting if they are still running. So background threads can still block indefinitely or do whatever they want throughout the course of your application.  

When I'm faced with this kind of a prospect, and I certainly don't come across this often, I like to make sure to sandbox that code in a thread that I can control. Below is a method I use that helps me isolate problem areas when I come into these kinds of scenarios. This is honestly a last resort. It's always better to know why something isn't working as expected. Unfortunately, somtimes in the real world you can't always devote the time, or even figure it out, so you have to work around things.

Here is <code>RunWithTimeout</code> and I hope the block comment is pretty clear.

[csharp]
/// &lt;summary&gt;
/// Helper function to wrap actions within another thread and test to see how long its run.
/// Only allows the Action() to run within the alloted time or it'll abort the wrapped thread.
///
/// BAD PRACTICE TO USE THIS IF YOU CONTROL THE CODE!!!
///
/// This function is mostly for wrapping 3rd party components that are blocking and have the potential to be
/// &quot;runaway&quot;. For example, if we're zipping a stream and the input stream continues to grow and the zip library
/// doesn't allow us to exit until the next entry. At which point we'll never be able to cancel. This is a good example
/// of when to use this function
/// &lt;/summary&gt;
/// &lt;param name=&quot;action&quot;&gt;the function to wrap in a timeout&lt;/param&gt;
/// &lt;param name=&quot;timeout&quot;&gt;how long we'll let the function run&lt;/param&gt;
/// &lt;param name=&quot;description&quot;&gt;the name of the timeout thread&lt;/param&gt;
/// &lt;param name=&quot;checkTimeMs&quot;&gt;how often the internal thread should check to see if the timeout has occured&lt;/param&gt;
/// &lt;returns&gt;Returns true if the thread executed within the allotted timeout, or false if an exception or timeout occurred&lt;/returns&gt;
public static bool RunWithTimeout(Action action, TimeSpan timeout, string description, int checkTimeMs = 250)
{
    try
    {
        var startTime = DateTime.Now;
        var thread = ThreadUtil.Start(description + &quot;-TimeoutThread&quot;, () =&gt; action());
        while (true)
        {
            var runTime = DateTime.Now - startTime;
            if (runTime &gt;= timeout &amp;&amp; thread.IsAlive)
            {
                try
                {
                    thread.Abort();
                }
                catch (Exception ex)
                {
                    Log.Error(typeof (TimeoutUtil),
                                String.Format(&quot;Unable to abort runaway thread that has excceded timeout {0}&quot;,
                                            timeout), ex);
                }

                return false;
            }

            if (!thread.IsAlive)
            {
                return true;
            }

            Thread.Sleep(TimeSpan.FromMilliseconds(checkTimeMs));
        }
    }
    catch(Exception ex)
    {
        Log.Error(typeof(TimeoutUtil), &quot;Unknown error executing action with timeout&quot;, ex);
        return false;
    }
}
[/csharp]

The basic idea here is to spin up two threads. One that does the actual work, and another to wait for a period of time, check if the first thread is done, and if not, forcibly abort it. 

And here is an NUnit test to demonstrate it. We're going to set a timeout of one second, but have our sandboxes function wait for two seconds. This means the sandbox function should be aborted since it's taking too long.  We can then test to see how long we blocked for (it should be around one second) and validate that <code>RunWithTimeout</code> returned <code>false</code> indicating that it uncleanly exited and was aborted.

[csharp]
[Test]
public void TestTimeout()
{
    var start = DateTime.Now;
    
    var didTimeOut =
        TimeoutUtil.RunWithTimeout(
                  () =&gt; WaitAction(TimeSpan.FromSeconds(2)), 
                  TimeSpan.FromSeconds(1), 
                  &quot;1secondsWithTimeout&quot;);

    var runTime = DateTime.Now - start;
    
    Console.WriteLine(&quot;Action took &quot; + runTime.TotalSeconds);
    
    Assert.False(didTimeOut);
}

private static void WaitAction(TimeSpan wait)
{
    Thread.Sleep(wait);
}
[/csharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>365</wp:post_id>
		<wp:post_date><![CDATA[2013-03-18 08:00:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-18 08:00:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[when-to-abort-a-thread]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="best-practices"><![CDATA[Best Practices]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="threads"><![CDATA[threads]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554802472;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2985;}i:1;a:1:{s:2:"id";i:532;}i:2;a:1:{s:2:"id";i:4764;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Flyweight Locking</title>
		<link>https://onoffswitch.net/2013/04/01/flyweight-locking/</link>
		<pubDate>Mon, 01 Apr 2013 08:00:57 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=383</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://en.wikipedia.org/wiki/Lock_(computer_science)">Locking</a> is a necessary aspect of multithreading code: it prevents unpredictable behavior and makes sure code that is expected to run synchronously does so.  Some situations can leverage <a href="http://yinsochen.com/thread-safe-and-or-lockless-data-structures/">lockless</a> code, but not always.  When you do need to do a lock you shouldn't do it carelessly, if you lock a section of code that does some major work (such as database access) and it blocks other pending calls you need to be cognizant that there could be a delay or bottleneck.  However, just because we have to lock doesn't mean we can't do some simple optimizations depending on what our business logic is.  If we only need to lock items per a defined group then we can leverage flyweight locking.  Lets go through an example to make this scenario clearer.

Imagine we have a WCF service that signs a student into a class where the student has a name, an id, and a classroom id that they belong to.  Something like this:

[csharp]
[ServiceContract]
public interface ISchoolService
{
    [OperationContract]
    void SignIntoClass(Student student);
}

[DataContract]
public class Student
{
    [DataMember]
    public int ClassRoomNumber { get; set; }

    [DataMember]
    public string StudentName { get; set; }

    [DataMember]
    public int StudentId { get; set; }
}
[/csharp]

And our service implementation could be 

[csharp]
public class SchoolService : ISchoolService
{
    public void SignIntoClass(Student student)
    {
        if (!StudentStorage.Instance.IsStudentInClass(student))
        {
            StudentStorage.Instance.AddStudenToClass(student);
        }
    }
}
[/csharp]

Remember that entry point for this service is multi-threaded, the same student could log in from multiple locations simultaneously and that would add them to the class twice, since both threads could evaluate 

[csharp]
StudentStorage.Instance.IsStudentInClass(student)[/csharp]

as false if the student hadn't been added yet (assuming our internal storage calls weren't atomic or threadsafe).

We'd probably be inclined to just throw a lock statement around <code>SignIntoClass</code> using a static lock object for the class, but that locks every call.  We can do better that that if we know how our data is grouped. If we only care about synchronizing students <em>per class</em> then we can use what is called a <a href="http://en.wikipedia.org/wiki/Flyweight_pattern">flyweight</a> lock and still be multi-threaded but synchronized.

A flyweight locking mechanism uses two sets of locks. One is a global lock, and one is a context lock. The global lock is used to synchronize getting context locks and the context locks are used to lock on the critical section for the action group. Lets add a flyweight lock to our student class and see what this really means

[csharp]
public class SchoolService : ISchoolService
{
    private static readonly IDictionary&lt;int, object&gt; _classroomLocks = new Dictionary&lt;int, object&gt;();

    public void SignIntoClass(Student student)
    {
        object flyweightLock;
        
        // lock everyone here on the global lock so you can get a local context lock
        lock(_classroomLocks)
        {
            if (!_classroomLocks.TryGetValue(student.ClassRoomNumber, out flyweightLock))
            {
                flyweightLock = new object();
                _classroomLocks[student.ClassRoomNumber] = flyweightLock;
            }
        }

        // now that we have a context lock we can lock our action group
        // this is where the heavy processing happens 
        lock (flyweightLock)
        {
            if (!StudentStorage.Instance.IsStudentInClass(student))
            {
                StudentStorage.Instance.AddStudenToClass(student);
            }
        }
    }
}
[/csharp]

Now what we're doing is getting a context level lock for the classrooms and using the dictionary as the global lock. All requests for a specific classroom are synchronized, but other classrooms can continue to do work even while one classroom could be busy inside of the lock.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>383</wp:post_id>
		<wp:post_date><![CDATA[2013-04-01 08:00:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-01 08:00:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[flyweight-locking]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="synchronization"><![CDATA[synchronization]]></category>
		<category domain="post_tag" nicename="threading"><![CDATA[threading]]></category>
		<category domain="post_tag" nicename="wcf"><![CDATA[wcf]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560570566;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2447;}i:1;a:1:{s:2:"id";i:738;}i:2;a:1:{s:2:"id";i:2365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Event emitters with success and fail methods for node.js</title>
		<link>https://onoffswitch.net/2013/02/27/event-emitters-with-success-and-fail-methods-for-node-js/</link>
		<pubDate>Wed, 27 Feb 2013 00:29:03 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=2635</guid>
		<description></description>
		<content:encoded><![CDATA[When it comes to node.js you hear a lot of hype, good and bad, so I've finally decided to take the plunge and investigate for myself what the fuss is about.  So far it's been interesting.  

I'm not really building anything in particular right now, I'm just playing with different tech stacks to see how things are done. One of the things that I found I liked, while experimenting with node modules, is the syntax <em>success</em> and <em>fail</em> for callback registration.  Something like this:

[javascript]
module.doSomething()
      .success(function() { })
      .fail(function() { });
[/javascript]

Using this kind of syntax I wanted to have a basic user authentication forwarder that I could wrap route calls in such that only logged in users could call the route. Non logged in users would automatically be forwarded to a twitter oauth route for auto login (done using <a href="https://github.com/bnoguchi/everyauth" target="_blank" rel="noopener noreferrer">everyauth</a>).

The first step was to create a custom event emitter object.  Coming from a .NET world this is similar to creating a class that dispatches an event delegate.  To wire up the event handlers you need to register a callback function to execute when the event happens.

For event dispatching to work, your object needs to inherit from the <code>events</code> module:

[javascript]
var EventEmitter = require(&quot;events&quot;).EventEmitter;
var sys = require(&quot;sys&quot;);

sys.inherits(yourObject, EventEmitter);
[/javascript]

And to emit events, you just emit them by name:

[javascript]
this.emit('failure');
[/javascript]

On the consumer end, to listen for events, you register a function on the object's inherited <code>on</code> member with the corresponding event name:

[javascript]
var obj = new yourObject();
obj.on('someEvent', someFunction)
[/javascript]

This is great, but I really don't like relying on strings. My biggest qualm with javascript is its lack of strong typing and runtime analysis, so anything I can do to prevent myself from making mistakes is a good thing.  

Bringing it back to my original intention, I wanted to encapsulate my authentication checker in a class that could secure API calls. It would validate that the request had a user property and if so asynchronously dispatch success, or if no user was there, a failure.  If a failure happened, it would also automatically forward the request to the appropriate authentication route.

Here is what I did

[javascript]
var EventEmitter = require(&quot;events&quot;).EventEmitter;
var sys = require(&quot;sys&quot;);

function Checker(req, res) {
    this.req = req;
    this.res = res;
    EventEmitter.call(this);
}

sys.inherits(Checker, EventEmitter);

Checker.prototype.success =
        function (fct) {
            this.on('success', fct)
            return this
        };

Checker.prototype.failure =
    function (fct) {
        this.on('failure', fct)
        return this
    };

Checker.prototype.run = function () {
    if (this.req.user === undefined || this.req.user == null) {
        this.res.redirect(&quot;/auth/twitter&quot;);
        this.emit('failure');
    }
    else {
        this.emit('success');
    }
    return this;
};

module.exports.checkAuth = function (req, res) {
    return new Checker(req, res);
};
[/javascript]

Lets see how it's used:

[javascript]
app.get(&quot;/user/home&quot;, function(req, res){
        utils.checkAuth(req, res)
            .success(function() {
                res.send(&quot;user is logged in!&quot;)
            })
            .failure(function(){
                console.log(&quot;requested not logged in&quot;)
            })
            .run();
    });
[/javascript]

After I defined the <code>Checker</code> constructor I inherited from the event emitter object.  This overrides the prototype of the object, so all other methods should be defined after you do the inheritance. Then I defined a success function that hid the <code>success</code> event listener, as well as a <code>failure</code> event listener.  All the methods return a <code>this</code> property to make the method calls <a href="http://en.wikipedia.org/wiki/Fluent_interface" target="_blank" rel="noopener noreferrer">fluent</a>.  The <code>Checker</code> constructor takes the request and result objects so we can test if the user exists.  

To use it, I called the <code>checkAuth</code> function on the exported utility class, which returns a new instance of our checker object.  Then I register success and failure functions on the returned instance. Lastly, to get things running, I call <code>run()</code> which actually tests the code.  Now I have a nice reusable authentication forwarder that I can use in any authentication required API calls.

The next step that I was thinking about was trying to see if I can get rid of the <code>.run()</code> method.   Let's change the constructor wrapper and tester function to look like this

[javascript highlight="15,16,17"]
Checker.prototype.test = function () {
    if (this.req.user === undefined || this.req.user == null) {
        this.res.redirect(&quot;/auth/twitter&quot;);
        this.emit('failure');
    }
    else {
        this.emit('success');
    }
    return this;
};

Checker.prototype.run = function(){
    var self = this;
    
    setTimeout(function(){
        self.test()
    }, 1)

    return this;
}

module.exports.checkAuth = function (req, res) {
    var checker = new Checker(req, res);
    return checker.run();
};
[/javascript]

And calling it like this:

[javascript]
app.get(&quot;/add/:track&quot;, function(req, res){
    utils.checkAuth(req, res)
        .success(function() {
            res.send(&quot;did it!&quot;);
        })
        .failure(function(){
            console.log(&quot;requested not logged in&quot;)
        });
});
[/javascript]

The main change (highlighted) was to defer the actual testing of the method with a one millisecond timeout. The run method returns the <code>this</code> object, which lets you register the success and fail callbacks. One millisecond later the test function fires and assumes the callbacks were registered.

The reason this works is explained in a snippet from John Resig's (creator of JQuery) <a href="http://ejohn.org/blog/how-javascript-timers-work/" target="_blank" rel="noopener noreferrer">blog</a>:

<blockquote>If a timer is blocked from immediately executing it will be delayed until the next possible point of execution (which will be longer than the desired delay).</blockquote>

By deferring the <code>test</code> function we're leveraging node's event loop and the single threaded nature of javascript. Events will process when they get a chance (in the order they were queued), but as long as we are blocking they won't run. So, assuming you register your callbacks during the current execution flow then the timer will be blocked from executing until you are done. 

Just to demonstrate that point, here is a modified route. I've intentionally made it a little wonky. We get an <code>checkAuth</code> object via a function return, then we add a failure registration. Then we busy wait for a period of time (essentially blocking), then register a success function. Finally we logged that the route function was complete. Then we emit the success event from the deferred timer and log our callback invocation from the success function:

[javascript]
app.get(&quot;/add/:track&quot;, function(req, res){

    function getAuth(req, res){
        return utils.checkAuth(req, res);
    }

    var auth = getAuth(req, res);

    auth.failure(function(){
        console.log(&quot;requested not logged in&quot;)
    });

    var x = 100000000;
    while(x &gt; 0){
        x = x - 1;
    }

    auth.success(function() {
        console.log(&quot;success &quot; + new Date().getTime());
        res.send('donezo');
    })

    console.log(&quot;ready &quot; + new Date().getTime());
});
[/javascript]

Notice the while loop that should sit for a few seconds. When you hit <code>http://localhost:3000/add/test</code> the console logs

[code]
registered failure function
registered success function
ready 1361995555376
success 1361995556530
[/code]

which shows that the timer was deferred until the execution flow returned. When control flow exited, the delayed test function ran, emitting the event and executing our callbacks.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2635</wp:post_id>
		<wp:post_date><![CDATA[2013-02-27 00:29:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-27 00:29:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[event-emitters-with-success-and-fail-methods-for-node-js]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="events"><![CDATA[events]]></category>
		<category domain="post_tag" nicename="js"><![CDATA[JavaScript]]></category>
		<category domain="post_tag" nicename="node-js"><![CDATA[node.js]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560295314;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3295;}i:1;a:1:{s:2:"id";i:3452;}i:2;a:1:{s:2:"id";i:3128;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A handrolled language parser</title>
		<link>https://onoffswitch.net/2013/03/01/a-handrolled-language-parser/</link>
		<pubDate>Fri, 01 Mar 2013 10:40:44 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=2735</guid>
		<description></description>
		<content:encoded><![CDATA[In my <a href="http://onoffswitch.net/building-a-custom-lexer/" target="_blank" rel="noopener noreferrer">previous post</a> about building a custom lexer I mentioned that, for educational purposes, I created a simple <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">toy programming language</a> (still unnamed).  There, I talked about building a tokenizer and lexer from scratch. In this post I'll discuss building a parser that is responsible for generating an <a href="http://stackoverflow.com/questions/1721553/how-to-construct-an-abstract-syntax-tree" target="_blank" rel="noopener noreferrer">abstract syntax tree</a> (AST) for my language. This syntax tree can then be passed to other language components such as a scope and type resolver, and finally an interpreter.  

The parser I made is a <a href="http://en.wikipedia.org/wiki/Recursive_descent_parser" target="_blank" rel="noopener noreferrer">recursive descent</a> <a href="http://stackoverflow.com/a/7141394/310196" target="_blank" rel="noopener noreferrer">packrat parser</a> that uses backtracking. Short of memoizing found AST, there aren't any other real optimizations. The goal was to create a working parser, not a production parser to distribute or use (or reuse) in any professional sense.  Like the lexer, this is an academic exercise to try and hit on some of the points covered by <a href="http://www.cs.usfca.edu/~parrt/" target="_blank" rel="noopener noreferrer">Terence Parr's</a> <a href="http://www.amazon.com/Language-Implementation-Patterns-Domain-Specific-Programming/dp/193435645X" target="_blank" rel="noopener noreferrer">Language Implementation Patterns</a> book that I recently finished reading.

I'm not going to cover much language theory because I want to jump into what I did and how it's implemented.  There are a lot of resources on context free grammars, LL parsers, left-recursion, ambiguous vs unambiguous  grammars, PEG (parsing expression grammars), shift reduce parsing, parse tables, and other subjects related to parsing and language implementation on the internet.  I'll leave explanations of those topics for someone else who is more qualified than me. But, if you are interested and are new to it (like myself), starting with Parr's book is a great first step and helps clear up some of the theoretical haze that surrounds a lot of programming language theory.

<h2>Syntax definitions</h2>

Every language is defined by a grammar, where the syntax represents the rules of the grammar.  The grammar of my language was grown organically, I didn't really go into it with any specific syntax. I knew I wanted to have lambdas, the <code>var</code> keyword, and simple stuff like if, else, while, for, function declarations and class declarations.  When I wrote the parser I just started with some basic syntax and added to it as I wanted more functionality.  I think most people give their languages a bit more thought than I did, but all's well that ends well.

In general, you can represent your grammar using <a href="http://en.wikipedia.org/wiki/Backus%E2%80%93Naur_Form" target="_blank" rel="noopener noreferrer">BNF</a>. Let's define a really simple language:

[code]
word := [A-z]+
number := \d+
operator := + | -
token := word | number
expression := token | token operator expression
variableDeclration = var word = expression
statement : = variableDeclaration | expression
ifStatement := if (expression) { statement* }
[/code]

What this translates to is:

<ul>
<li><b>word</b>. This is any group of characters at least once with no spaces. <code>foo</code> for example would be a word</li>
<li><b>number</b>. This is any number of digits (no decimals). Something like <code>1234</code></li>
<li><b>operator</b>. This is a plus sign or a minus sign</code></li>
<li><b>token</b>. Either a word or a number</li>
<li><b>expression</b>. This is either a token or a token with an operator followed by an expression. This example here is what is called right recursive, since the expression references itself on the right hand side of the operation. Recursive descent parsers <a href="http://stackoverflow.com/questions/847439/why-cant-a-recursive-descent-parser-handle-left-recursion" target="_blank" rel="noopener noreferrer">can't handle left recursion</a> since it leads to infinite loops.  </li>
<li><b>variable declaration</b>. This is the keyword <code>var</code> followed by a word followed by an <code>=</code> followed by an expression</li>
<li><b>statement</b>. Either a variable declaration or some expression</li>
<li><b>if statement</b>. The keyword <code>if</code> followed by a <code>(</code> followed by an expression, followed by a <code>)</code> followed by a <code>{</code> followed by one or more statements, followed by a <code>}</code></li>
</ul>

If you wanted to you, you could feed this general grammar (modified syntactically) to libraries that can auto generate parsers for you, but that's no fun and feels like cheating when you are learning.  

<h2>The Parsers Job</h2>

The goal of the parser is to take a strongly typed token stream from the lexer, and to create a syntax tree that we can use.  A simple way to think about it is that each bullet point in our grammar can be a class.  Imagine we have a class called <code>IfStatement</code>. It might look something like this:

[csharp]
public class IfStatement{
   public Expression Predicate { get; set; }
   public List&lt;Statement&gt; Statements { get; set; }
}
[/csharp]

We don't really care about the keyword <code>if</code> or any other of the special characters like (, ), {, and }, since the important part is that we now have a class that describes what the if statement meant: a <a href="http://en.wikipedia.org/wiki/Predicate_(mathematical_logic)" target="_blank" rel="noopener noreferrer">predicate</a>, and a list of statements to execute if the predicate is true.  This type of class is what is known as a syntax tree. It's a tree because it references other syntax nodes.  Here is an image showing what a syntax tree could look like (image taken from <a href="http://en.wikipedia.org/wiki/Abstract_syntax_tree" target="_blank" rel="noopener noreferrer">wikipedia</a>)

<img src="http://onoffswitch.net/wp-content/uploads/2013/02/531px-Abstract_syntax_tree_for_Euclidean_algorithm.svg_.png" alt="531px-Abstract_syntax_tree_for_Euclidean_algorithm.svg" width="531" height="599" class="aligncenter size-full wp-image-2751" />

When you're done parsing, you will have a root node that references the entire structure of your program. Later parts of the language chain (building out scope, types, and interpreting the code) will go through these definitions and actually evaluate what they mean. Those later passes might also add extra metadata to each node, so the AST is like our master repo of program related metadata. For now the goal is to create these classes.  

<h2>Capturing Information</h2>

Since the parsers goal is to create these classes, it needs to be able to work on an underlying token stream and create meaningful representations from that stream.  The parser knows what kinds of syntactical patterns it expects. For example, if we have the following variable declaration and assignment

[code]
int x = 5;
[/code]

We can tell it's a variable declaration and assignment because it matches the pattern of 

[code]
valid type
word
equals
valid assignment (an expression maybe or a single token?)
semicolon
[/code]

The parsers job is to take those tokens in meaningful orders and create an AST from it.  When I say "take", I mean you remove the current token from the head of the stream (or advance the token streams index pointer). Lets say we are parsing that variable declaration above, it might have a token stream that looks like this

[code]
int (keyword)
word (x)
equals (keyword)
number (5)
semicolon (keyword)
[/code]

We see that the head of the stream is a keyword that can be a valid variable type (int), so we can take it off the list and store it.  Then we expect the pattern "word", "equals", "expression", "semicolon". We can take them one at a time and while it matches keep on going.  Certain items like the semicolon you can trash. It's there to tell the parser when to stop.

<h2>Alternatives</h2>

<a href="http://en.wikipedia.org/wiki/LL_parser#LL.281.29_Conflicts" target="_blank" rel="noopener noreferrer">Sometimes</a>, however, you can't determine what an expression will be just by looking at the current token. For example, what does this mean if you only look at the first element?

[code]
1 + 1
[/code]

Is it a token of value 1? Or is it an expression of 1 + 1?  Obviously it's 1 + 1, but the parser can't always tell. Careful ordering of your parser can avoid most of these ambiguities, but when it can't, you can either peek into the stream (so you see that the next token is a + so that means expression), or simply try alternatives.  The first alternative to match wins!   

For alternatives, you try first an expression. If that fails, then you try a token. If that fails, then invalid syntax.  Remember that I mentioned that my parser is a packrat parser? All this means is that while it's trying alternatives it will cache if it found them. Later, when I actually go and try to take a certain branch I can retrieve the already parsed AST from the cache. This cuts down on a lot of extra work.

<h2>The Token Stream</h2>

In the last <a href="http://onoffswitch.net/building-a-custom-lexer/" target="_blank" rel="noopener noreferrer">post</a> about the lexer, I created a <code>TokenizableStreamBase</code> base class that handles basic snapshot/commit/rollback/consume functionality on an input stream.  Here I'm going to re-use it and pass it a stream of tokens, instead of a stream of characters.  The parser will instantiate this <code>ParseableTokenStream</code> class (which subclasses the tokenizable stream base) and use it as it's token stream.

The most basic form of the class is this:

[csharp]
public class ParseableTokenStream : TokenizableStreamBase&lt;Token&gt;
{
    public ParseableTokenStream(Lexer lexer) : base (() =&gt; lexer.Lex().ToList())
    {
    }

    ... implementation ...
}
[/csharp]

It takes the lexer, lexes the tokens, and creates an underlying token stream that we can do snapshots on.  We also have methods to test if the current item on the stream is a specific token type (defined by a known enum):

[csharp]
public Boolean IsMatch(TokenType type)
{
    return Current.TokenType == type;
}
[/csharp]

The parsing stream base also lets us "take" a specific token. If you remember from the last post, all consuming of a lexable item does is advance the internal array index.  The important part is that after we <code>Consume</code>, we've advanced to the next token in the token stream.

You'll see in my parser that sometimes I use the <code>Take</code> return value, and sometimes it's discarded. This is intentional. Even if you don't intend to use a token in the syntax tree (like a semicolon) you still have to acknowledge that it was part of the expected pattern and advance the token stream.

[csharp]
public Token Take(TokenType type)
{
    if (IsMatch(type))
    {
        var current = Current;

        Consume();

        return current;
    }

    throw new InvalidSyntax(
        String.Format(&quot;Invalid Syntax. Expecting {0} but got {1}&quot;, 
                        type, 
                        Current.TokenType));
}
[/csharp]

We can also try an alternate route. If the route function returns a non-null syntax tree we'll assume the route succeeded and cache it. Later requests for getting syntax trees at that current index will first check the cache before trying to re-build the tree (if it needs to):

[csharp]
public Boolean Alt(Func&lt;Ast&gt; action)
{
    TakeSnapshot();

    Boolean found = false;

    try
    {
        var currentIndex = Index;

        var ast = action();

        if (ast != null)
        {
            found = true;

            CachedAst[currentIndex] = new Memo
                                        {
                                            Ast = ast,
                                            NextIndex = Index
                                        };
        }
    }
    catch
    {
                
    }

    RollbackSnapshot();

    return found;
}
[/csharp]

The <code>CachedAst</code> field is defined as

[csharp]
private Dictionary&lt;int, Memo&gt; CachedAst = new Dictionary&lt;int, Memo&gt;();
[/csharp]

Where <code>Memo</code> is 

[csharp]
internal class Memo
{
    public Ast Ast { get; set; }
    public int NextIndex { get; set; }
}
[/csharp]

There are also couple of extra methods that let me try a route, and if it succeeds return it's cached results

[csharp]
public Ast Capture(Func&lt;Ast&gt; ast)
{
    if (Alt(ast))
    {
        return Get(ast);
    }

    return null;
}

/// &lt;summary&gt;
/// Retrieves a cached version if it was found during any alternate route
/// otherwise executes it
/// &lt;/summary&gt;
/// &lt;param name=&quot;getter&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public Ast Get(Func&lt;Ast&gt; getter)
{
    Memo memo;
    if (!CachedAst.TryGetValue(Index, out memo))
    {
        return getter();
    }

    Index = memo.NextIndex;

    return memo.Ast;
}
[/csharp]

The underlying stream in my parser is an array, so the inherited <code>Index</code> property keeps track of where we are in the stream. When we return a memoized syntax tree, we can seek the stream to the index directly after the last memoized token. This means we can easily jump around in our parser stream.  Hopefully this makes sense, because if we returned a cached syntax tree that spanned token items 1 through 15, we should jump immediately to token index 16 and continue parsing from there.  

For small parsing this works well, but obviously wouldn't scale with large programs.  Still, to change it so that we work on a buffered section of an infinite stream wouldn't be that much work, and in the end wouldn't modify how the actual parser behaves. This is all hidden in the shared base class (so the lexer would also improve).

<h2>Finally, Parsing</h2>

First, to tie in the section above, here is the constructor of the <a href="https://github.com/devshorts/LanguageCreator/tree/master/Lang/Parser" target="_blank" rel="noopener noreferrer">parser</a>:

[csharp]
private ParseableTokenStream TokenStream { get; set; }

public LanguageParser(Lexer lexer)
{
    TokenStream = new ParseableTokenStream(lexer);
}
[/csharp]

Next, I've defined a few syntax tree classes that the parser will use:

<img src="http://onoffswitch.net/wp-content/uploads/2013/02/ast..png" alt="ast." width="225" height="322" class="alignnone size-full wp-image-2779" />

All of the syntax tree containers inherit from the base class <code>Ast</code>. This makes working with syntax trees in the parser easy because everything can be passed around as the base, and it means we can extend the metadata that syntax trees have just by adding to the base class.  Hopefully most of the class names are self explanatory (if statement, while loop, method declaration, class dereference) just by class name.  If you're interested in class details you can go to the <a href="https://github.com/devshorts/LanguageCreator/tree/master/Lang/AST" target="_blank" rel="noopener noreferrer">github</a> and check them out. Suffice to say that they look kind of like the if statement class I pseudocoded earlier.

As an example, let me show one that I reused a lot. The <code>ScopeDeclr</code> AST gets created anytime the parser encounters a <code>{</code> followed by some statements, terminated by <code>}</code>.   

[csharp]
public class ScopeDeclr : Ast
{
    public List&lt;Ast&gt; ScopedStatements { get; private set; } 

    public ScopeDeclr(List&lt;Ast&gt; statements) : base(new Token(TokenType.ScopeStart))
    {
        ScopedStatements = statements;
    }

    public override void Visit(IAstVisitor visitor)
    {
        visitor.Visit(this);
    }

    public override AstTypes AstType
    {
        get { return AstTypes.ScopeDeclr; }
    }
}
[/csharp]

<code>ScopedStatements</code> is a list of statements that are found in the scoped block. 

I used the <code>ScopeDeclr</code> syntax tree to hold the root node of the entire application. This is because I considered the global scope (starting at the root) to be, well, a scope. The <code>ScopeDeclr</code> also turned out to be extremely useful when building out partial curried functions, a subject I'll cover in the next post about scope and type definitions. 

Here is the entrypoint to the parser:

[csharp]
public Ast Parse()
{
    var statements = new List&lt;Ast&gt;(1024);

    while (TokenStream.Current.TokenType != TokenType.EOF)
    {
        statements.Add(ScopeStart().Or(Statement));
    }

    return new ScopeDeclr(statements);
}
[/csharp]

The <code>.Or()</code> method is an extension method I added inspired by the <a href="http://en.wikibooks.org/wiki/F_Sharp_Programming/Computation_Expressions#Monad_Primer" target="_blank" rel="noopener noreferrer">maybe monad</a>. It returns the first non-null result in a chain of functions. 

[csharp]
public static class Maybe
{
    public static TInput Or&lt;TInput&gt;(this TInput input, Func&lt;TInput&gt; evaluator)
        where TInput : class
    {
        if (input != null)
        {
            return input;
        }

        return evaluator();
    }
}
[/csharp]

Lets take a look at what is a <code>Statement</code>

[csharp]
/// &lt;summary&gt;
/// Class, method declaration or inner statements
/// &lt;/summary&gt;
/// &lt;returns&gt;&lt;/returns&gt;
private Ast Statement()
{
    var ast = TokenStream.Capture(Class)
                         .Or(() =&gt; TokenStream.Capture(MethodDeclaration));

    if (ast != null)
    {
        return ast;
    }

    // must be an inner statement if the other two didn't pass
    // these are statements that can be inside of scopes such as classes
    // methods, or just global scope
    var statement = InnerStatement();

    if (TokenStream.Current.TokenType == TokenType.SemiColon)
    {
        TokenStream.Take(TokenType.SemiColon);
    }

    return statement;
}
[/csharp]

A statement can either be a class, a method declaration, or an inner statement.  I didn't want to need to put semicolons after class and method definitions, so I don't test for a semicolon there.  I also made semicolons optional, if we can unambiguously determine the grammar without needing semicolons then great, otherwise we'll use it to terminate a statement if it's there.  Though in reality you need to put in semicolons or the parser will barf.  Call it a <a href="https://www.google.com/search?q=programming+language+quirks" target="_blank" rel="noopener noreferrer">language quirk</a>.

Here is an inner statement. These are statements I considered valid within scopes such as method declarations, global scope, or inside of classes.  

[csharp]
/// &lt;summary&gt;
/// A statement inside of a valid scope 
/// &lt;/summary&gt;
/// &lt;returns&gt;&lt;/returns&gt;
private Ast InnerStatement()
{
    // ordering here matters since it resolves to precedence
    var ast = TryCatch().Or(ScopeStart)
                        .Or(LambdaStatement)
                        .Or(VariableDeclWithAssignStatement)
                        .Or(VariableDeclrStatement)
                        .Or(GetIf)
                        .Or(GetWhile)
                        .Or(GetFor)
                        .Or(GetReturn)
                        .Or(PrintStatement)
                        .Or(Expression)
                        .Or(New);

    if (ast != null)
    {
        return ast;
    }

    throw new InvalidSyntax(String.Format(&quot;Unknown expression type {0} - {1}&quot;, TokenStream.Current.TokenType, TokenStream.Current.TokenValue));
}
[/csharp]

Let's check out a few other parsers. Here is how to parse a <code>new</code> of the form

[csharp]
new thing(a, b, c)
[/csharp]

I explicity didn't put in a semicolon, since semicolons delimit statements, not just expressions.  

This gives me a class <code>NewAst</code> that has the class name (<code>thing</code>) and a list of the arguments (<code>a</code>, <code>b</code>, and <code>c</code>).

[csharp]
private Ast New()
{
    Func&lt;Ast&gt; op = () =&gt;
        {
            if (TokenStream.Current.TokenType == TokenType.New)
            {
                TokenStream.Take(TokenType.New);

                var name = new Expr(TokenStream.Take(TokenType.Word));

                List&lt;Ast&gt; args = GetArgumentList();

                return new NewAst(name, args);
            }

            return null;
        };

    return TokenStream.Capture(op);
}
[/csharp]

We test if the current token is of type <code>TokenType.New</code> and if so consumes it. Then it expects an expression (the word <code>thing</code>), and then gets a comma delimited list of arguments.  There's no semicolon because this <code>new</code> statement is part of a larger sequence of statements which will contain a reference to this <code>new</code> on the tree.  We don't really know, or care, if the statement is part of a variable declaration, or a print statement, or a function call, or whatever, as long as its valid in the grammar.

Here is a <code>while</code>

[csharp]
private Ast GetWhile()
{
    if (TokenStream.Current.TokenType == TokenType.While)
    {
        Func&lt;WhileLoop&gt; op = () =&gt;
            {
                var predicateAndStatements = GetPredicateAndStatements(TokenType.While);

                var predicate = predicateAndStatements.Item1;

                var statements = predicateAndStatements.Item2;

                return new WhileLoop(predicate, statements);
            };

        return TokenStream.Capture(op);
    }

    return null;
}
[/csharp]

Which leverages the following helper function

[csharp]
private Tuple&lt;Ast, ScopeDeclr&gt; GetPredicateAndStatements(TokenType type)
{
    TokenStream.Take(type);

    TokenStream.Take(TokenType.OpenParenth);

    var predicate = InnerStatement();

    TokenStream.Take(TokenType.CloseParenth);

    var statements = GetStatementsInScope(TokenType.LBracket, TokenType.RBracket);

    return new Tuple&lt;Ast, ScopeDeclr&gt;(predicate, statements);
} 
[/csharp]

Hopefully you can see now how this all continues on.  <code>GetStatementsInScope</code> pulls all semicolon delimited statements between a left bracket and a right bracket and returns a scope declaration block with them inside.

<h2>Expressions</h2>

I wanted to dedicate a specific section on parsing expressions because I struggled with this.  These are ones like

[code]
1 + 1
(b.x.z * 2.0)
(new class()).x == true
(f + 2) + foo() + 3 + (a - 2 - z)
[/code]

I'll be truthful here, I didn't think expressions through thoroughly before I started. For every pattern I was able to match I exposed one that I couldn't. At one point I ran into a bunch of left recursion issues.  In the end, expressions, as I've "<em>defined</em>" them look like this

[code]
operator = + | - | / | ^ | = | | | == | !=
terminal = new statement | function call | class dereference | single token 
expression' = terminal operator expression | terminal 
expression = ( expression ) | ( expression ) operator expression | expression'
[/code]

This was mostly figured out through trial and error, some pen and paper diagrams, extensive unit tests, and a lot of head scratching. Honestly, out of the whole parser this is what took the longest to get right (at least right enough).

What I did to avoid left recursion, I later realized, looks similar to what <a href="http://en.wikipedia.org/wiki/Left_recursion#Removing_left_recursion" target="_blank" rel="noopener noreferrer">wikipedia</a> suggests, which is to create a new intermediate nonterminal. This is the subset of specific terminals I called <code>terminal</code> in the BNF above.   So I'm not just matching on <em>expression operator expression</em> since that would recurse endlessly (assuming tail call recursion) or, more likely, just blow up my stack and crash.  

The expression parsing code, in the end, matches expressions of the following formats (for example). I made all the examples use a plus sign because I was lazy - any available operator works in any ordering (these cases are from my expression testing unit test)

[csharp]
1 + 2;
1 + 2 + 3;
(1 + 2) + 3;
(1 + 2 ) + (3 + 4);
1 + (2 + 3);
1 + (2 + 3) + 4;
(1 + 2 + 3 + 4);
new foo().z + 1;           
a.f().z * 2.0 + (new foo().x + 2);
(new foo().z) + 1;
(f + 2) + foo() + 3 + (a + 2 + z)
[/csharp]

Which when tested, looks something like this

[csharp]
SCOPE: 
(Int: 1 Plus: + Int: 2)
(Int: 1 Plus: + (Int: 2 Plus: + Int: 3))
((Int: 1 Plus: + Int: 2) Plus: + Int: 3)
((Int: 1 Plus: + Int: 2) Plus: + (Int: 3 Plus: + Int: 4))
(Int: 1 Plus: + (Int: 2 Plus: + Int: 3))
(Int: 1 Plus: + ((Int: 2 Plus: + Int: 3) Plus: + Int: 4))
(Int: 1 Plus: + (Int: 2 Plus: + (Int: 3 Plus: + Int: 4)))
([( new Word: foo with args n/a). (Word: z)] Plus: + Int: 1)
([( Word: a). (call Word: f with args ). (Word: z)] Asterix: * (Float: 2.0 Plus: + ([( new Word: foo with args n/a). (Word: x)] Plus: + Int: 2)))
([( new Word: foo with args n/a). (Word: z)] Plus: + Int: 1)
((Word: f Plus: + Int: 2) Plus: + (call Word: foo with args  Plus: + (Int: 3 Plus: + (Word: a Plus: + (Int: 2 Plus: + Word: z)))))
[/csharp]

Like the other parse functions, this one returns an <code>Ast</code> and does some basic alternative checking.  The <code>new</code> test, on line  3, isn't part of <code>IsValidOperand</code> because I re-use <code>IsValidOperand</code> elsewhere.  

[csharp]
private Ast Expression()
{
    if (IsValidOperand() || TokenStream.Current.TokenType == TokenType.New)
    {
        return ParseExpression();
    }

    switch (TokenStream.Current.TokenType)
    {
        case TokenType.OpenParenth:

            Func&lt;Ast&gt; basicOp = () =&gt;
                {
                    TokenStream.Take(TokenType.OpenParenth);

                    var expr = Expression();

                    TokenStream.Take(TokenType.CloseParenth);

                    return expr;
                };

            Func&lt;Ast&gt; doubleOp = () =&gt;
                {
                    var op1 = basicOp();

                    var op = Operator();

                    var expr = Expression();

                    return new Expr(op1, op, expr);
                };

            return TokenStream.Capture(doubleOp)
                              .Or(() =&gt; TokenStream.Capture(basicOp));

        default:
            return null;
    }
}
[/csharp]

What we're doing here is splitting up the operation into 3 different sections

<ul>
<li>Terminal. This is the first statement. If it's a terminal parse and return. Terminals aren't just single tokens, they are terminal expression types (like <code>new</code>, function calls, single operands, simple operations like <em>operand operator operand</em>, etc)</li>
<li>Expressions inside of parenthesis.  If we have something like <code>(1 + 1)</code>, take the parenthesis out and parse the expression.</li>
<li>Expressions inside of parenthesis, followed by an operator, followed by an expression. If we have <code>(1 + 1) + (1 - a)</code>, or <code>(1 + 1) + 2 + 3</code>, take the first section, then the operator, then try the next section</li>
</ul>

If we have a valid left operand we can parse a basic expression that is of the form

[code]
terminal | terminal operator expression
[/code]

This is right recursive! Sweet, no recursion issues. If you didn't catch why earlier, check out <a href="http://stackoverflow.com/questions/847439/why-cant-a-recursive-descent-parser-handle-left-recursion" target="_blank" rel="noopener noreferrer">this</a> stack overflow question.  The ordering of parsing here matters, I am parsing from most terms to least terms. If I switched the order (terminal first, then expression), the parser would break since we'd run into the alternative issue I mentioned in a section above.

Here is how I parsed the basic expression defined in the above BNF

[csharp]
private Ast ParseExpression()
{
    Func&lt;Func&lt;Ast&gt;, Func&lt;Ast&gt;, Ast&gt; op = (leftFunc, rightFunc) =&gt;
        {
            var left = leftFunc();

            if (left == null)
            {
                return null;
            }

            var opType = Operator();

            var right = rightFunc();

            if (right == null)
            {
                return null;
            }

            return new Expr(left, opType, right);
        };

    Func&lt;Ast&gt; leftOp = () =&gt; op(ExpressionTerminal, Expression);

    return TokenStream.Capture(leftOp)
                      .Or(() =&gt; TokenStream.Capture(ExpressionTerminal));
}
[/csharp]

Where <code>IsValidOperand</code> is

[csharp]
private bool IsValidOperand()
{
    switch (TokenStream.Current.TokenType)
    {
        case TokenType.Int:
        case TokenType.QuotedString:
        case TokenType.Word:
        case TokenType.True:
        case TokenType.Float:
        case TokenType.Nil:
        case TokenType.False:
            return true;
    }
    return false;
}
[/csharp]

And <code>ExpressionTerminal</code> is

[csharp]
private Ast ExpressionTerminal()
{
    return ClassReferenceStatement().Or(FunctionCallStatement)
                                    .Or(New)
                                    .Or(SingleToken);
}
[/csharp]

<h2>Testing the parser</h2>

At this point everything is set up! I didn't cover all of the parser functions but they are pretty similar in nature. Anyways, lets do a few tests

[csharp]
[Test]
public void TestSimpleAst()
{
    var test = @&quot;x = 1;&quot;;

    var ast = new LanguageParser(new Lexers.Lexer(test)).Parse() as ScopeDeclr;

    var expr = (ast.ScopedStatements[0] as Expr);

    Assert.IsTrue(expr.Left.Token.TokenType == TokenType.Word);
    Assert.IsTrue(expr.Right.Token.TokenType == TokenType.Int);
    Assert.IsTrue(ast.Token.TokenType == TokenType.ScopeStart);
}
[/csharp]

And something more complicated

[csharp]
[Test]
public void AstWithExpression2()
{
    var test = @&quot;int z = 1;
                {
                    int y = 5 + 4;
                }
                x = 1 + 2 ^ (5-7);&quot;;

    var ast = new LanguageParser(new Lexers.Lexer(test)).Parse() as ScopeDeclr;

    Assert.IsTrue(ast.ScopedStatements.Count == 3);
    Assert.IsTrue(ast.ScopedStatements[0] is VarDeclrAst);
    Assert.IsTrue(ast.ScopedStatements[1].Token.TokenType == TokenType.ScopeStart);
    Assert.IsTrue(ast.ScopedStatements[2] is Expr);

    Console.WriteLine(ast);
}
[/csharp]

Let me print out the above test string representation:

[code]
SCOPE: 
	Declare Word: z as Int: int with value Int: 1
	SCOPE: 
		Declare Word: y as Int: int with value (Int: 5 Plus: + Int: 4)
	(Word: x Equals: = (Int: 1 Plus: + (Int: 2 Carat: ^ (Int: 5 Minus: - Int: 7))))
[/code]

And this insane block of gibberish.  

[csharp]
[Test]
public void FunctionTest()
{
    var test = @&quot;void foo(int x, int y){ 
                    int x = 1; 
                    var z = fun() -&gt; { 
                        zinger = &quot;&quot;your mom!&quot;&quot;;
                        someThing(a + b) + 25 - (&quot;&quot;test&quot;&quot; + 5);
                    };
                }

                z = 3;

                int testFunction(){
                    var p = 23;

                    if(foo){
                        var x = 1;
                    }
                    else if(faa){
                        var y = 2;
                        var z = 3;
                    }
                    else{
                        while(1 + 1){
                            var x = fun () -&gt;{
                                test = 0;
                            };
                        }

                        if(foo){
                            var x = 1;
                        }
                        else if(faa){
                            var y = 2;
                            var z = 3;
                        }
                        else{
                            for(int i = 0; i &lt; 10; i = i + 1){
                                var x = z;
                            }
                        }
                    }
                }&quot;;

    var ast = new LanguageParser(new Lexers.Lexer(test)).Parse() as ScopeDeclr;

    Assert.IsTrue(ast.ScopedStatements.Count == 3);
    Assert.IsTrue(ast.ScopedStatements[0] is MethodDeclr);
    Assert.IsTrue(ast.ScopedStatements[1] is Expr);
    Assert.IsTrue(ast.ScopedStatements[2] is MethodDeclr);
}
[/csharp]

Well, you get the idea.

<h2>Conclusion</h2>

One thing I didn't like about how my parser turned is that the entire parsing code ended up in one class. I couldn't think of a clean way to separate all this out, since we have lots of mutual recursion going on. Each function needed to know about the other one.  I thought about passing around the lexer as a state (functional style) but when I tried that it ended up messier than I had hoped.  

Anyways, it's a lot to take in, and I showed a bunch of code, some of which is out of context. Make sure to go to the <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">github</a> if you want to poke around some more.  After doing the parser, I wouldn't do one again by hand. This is pretty tedious and it took me two frustrating weeks to get it all right (thanks for bearing with me <a href="https://twitter.com/nightCheese2" target="_blank" rel="noopener noreferrer">Tracy</a>!).  This is why you would absolutely use a pre-rolled lexing/parsing solution to get you to your abstract syntax trees.  <a href="http://en.wikipedia.org/wiki/Comparison_of_parser_generators" target="_blank" rel="noopener noreferrer">Here</a> is a list of parser generators, and I'm sure there are tons not listed. Lots of smart people have built these and will save you plenty of time in the long run.  

Like the exercise with the lexer, I'm glad I did it and I have a much better appreciation for libraries that do this all for you.

The next step (and post) will be to determine proper scoping rules, static typing (and type validation), and add some extra neat features like partial functions to our language.  This is going to be done using a scope builder that will use the visitor pattern to iterate over our syntax tree. Having the syntax tree means we can finally start doing interesting stuff and making language decisions. 

<h2>Disclaimer</h2>

If I got something wrong in the post please let me know! Like I've mentioned before I'm new at this and just sharing my findings.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2735</wp:post_id>
		<wp:post_date><![CDATA[2013-03-01 10:40:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-01 10:40:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[a-handrolled-language-parser]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
		<category domain="post_tag" nicename="lexer"><![CDATA[Lexer]]></category>
		<category domain="post_tag" nicename="monads"><![CDATA[monads]]></category>
		<category domain="post_tag" nicename="parser"><![CDATA[parser]]></category>
		<category domain="post_tag" nicename="projects"><![CDATA[projects]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561472777;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4131;}i:1;a:1:{s:2:"id";i:4068;}i:2;a:1:{s:2:"id";i:4365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>28</wp:comment_id>
			<wp:comment_author><![CDATA[paul]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[paul.kaefer@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[108.65.158.91]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-03-02 17:19:40]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-03-02 17:19:40]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm taking a class called Compiler Construction at my university. Our textbook is <a href="http://www.cs.princeton.edu/~appel/modern/java/" title="Modern Compiler Implementation in Java" rel="nofollow">Modern Compiler Implementation in Java</a>. I'd have to say it's a great textbook. So far, we've gone through writing the token scanner, parser, and part of the type checker. This class helped me understand your post. Thanks for sharing!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>29</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.243.58.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-03-02 17:53:53]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-03-02 17:53:53]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Paul, thanks for reading :). That book looks interesting, I'll have to grab a copy]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>28</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>30</wp:comment_id>
			<wp:comment_author><![CDATA[Reworking my language parser with fparsec | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/reworking-language-parser-fparsec/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-07-15 08:00:41]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-07-15 08:00:41]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] expressive. In fact I was able to do most of this in an afternoon, which is impressive considering my last C# attempt took 2 weeks to hammer [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>31</wp:comment_id>
			<wp:comment_author><![CDATA[Vince]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[vincent.dorsch@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[178.198.60.45]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-25 18:23:26]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-25 18:23:26]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey Anton, very nice and interesting post! I'm currently also reading Terence Parr's very well written textbook. Do you have maybe a (E)BNF or ANTLR grammar to your hand rolled language interpreter?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>32</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.18.209.169]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-25 18:28:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-25 18:28:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Vince, unfortunately I don't. The language sort of came about organically. But, the syntax is simple enough, wouldn't be hard to put together a bnf for it]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>31</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>33</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.18.209.169]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-25 18:30:03]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-25 18:30:03]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Actually you may be able to get a quick bnf out of my fparsec rework (I posted that later to the blog: http://onoffswitch.net/reworking-language-parser-fparsec/). Since parser combinators very closely resemble the language grammar that would probably be easier to see]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>32</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech talk: Bloom Filters</title>
		<link>https://onoffswitch.net/2013/02/28/tech-talk-bloom-filters/</link>
		<pubDate>Thu, 28 Feb 2013 16:24:44 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=2812</guid>
		<description></description>
		<content:encoded><![CDATA[Each Thursday at work my team and I do a 45 minute to an hour discussion on any technical subject that we find interesting.  We call these Thursday get togethers tech talks and I think they are awesome. We've been doing them for years and I'm hoping to start reposting our subjects and a blurb about our discussions each week after they happen.

This week's tech talk was about bloom filters.  A bloom filter is a memory efficient bit vector that contains multiple hashes of your data. It is a probabilistic data structure.  The idea is that it can definitively tell you if a piece of data does NOT exist, but it can't always tell you with certainty that data DOES  exist.  For more info check out this interactive bloom filter <a href="http://billmill.org/bloomfilter-tutorial/" target="_blank" rel="noopener noreferrer">tutorial</a>. 

The reason bloom filters are useful is because it is faster and cheaper to transmit a compressed bit vector that contains maybe/no information than to send an entire hash (or other structured container) of your data set.  Even if you aren't sending it anywhere, it's not memory efficient to store all your data for easy querying in a hash or list or whatever. Leave the data in a DB and you can construct a bloom filter representing if data is maybe in the set or definitely not in the set.  

Let's imagine a use case. Pretend you want to make a website that queries Wikipedia articles.  If you construct a bloom filter of all Wikipedia article titles, then the client (as they search) can test the bloom filter to see if an article maybe exists, or definitely does not exist. If it doesn't exist you can just say "article doesn't exist!" with certainty. If it maybe exists, then you do a query to Wikipedia and either pull back the article or return an empty result set.  The advantage here is that you have cut down on a lot of extra processing and network overhead for empty results.  Bloom filters usually have a 1% false positive rate. That means that 1% of the time you did work you really didn't need to but that also means that 99% of the time you're actually right!

You may have already encountered bloom filters without even realizing it. Think about registering for a big name site that does immediate validation of available usernames. I'd imagine that instead of doing a SQL query, the ajax call first hits a bloom filter on the backend testing if the username already exists.  It's cheaper to give a false positive here to say <em>username xyz is already taken</em> than to pull the data from the database to validate it. Let the user keep picking until you get a definite NO and then you can submit with that.  

There are a lot of use cases for bloom filters and they have a lot of interesting variations on the internal mechanisms. By tuning the bit vector size, the hash function choices, and the data you are hashing, you can have a pretty robust maybe/no container set. Cool!

For more info check out these links:

<a href="http://mikecvet.wordpress.com/2010/04/21/bloom-filters/" target="_blank" rel="noopener noreferrer">http://mikecvet.wordpress.com/2010/04/21/bloom-filters/</a>
<a href="http://stackoverflow.com/questions/6118154/when-is-a-bloom-filter-useful?rq=1" target="_blank" rel="noopener noreferrer">http://stackoverflow.com/questions/6118154/when-is-a-bloom-filter-useful?rq=1</a>
<a href="http://www.igvita.com/2010/01/06/flow-analysis-time-based-bloom-filters/" target="_blank" rel="noopener noreferrer">http://www.igvita.com/2010/01/06/flow-analysis-time-based-bloom-filters/</a>
<a href="http://zmievski.org/2009/04/bloom-filters-quickie" target="_blank" rel="noopener noreferrer">http://zmievski.org/2009/04/bloom-filters-quickie</a>
<a href="http://www.perl.com/pub/2004/04/08/bloom_filters.html" target="_blank" rel="noopener noreferrer">http://www.perl.com/pub/2004/04/08/bloom_filters.html</a>
<a href="http://stackoverflow.com/questions/4282375/what-is-the-advantage-to-using-bloom-filters" target="_blank" rel="noopener noreferrer">http://stackoverflow.com/questions/4282375/what-is-the-advantage-to-using-bloom-filters</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2812</wp:post_id>
		<wp:post_date><![CDATA[2013-02-28 16:24:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-02-28 16:24:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-bloom-filters]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="bloom-filter"><![CDATA[Bloom filter]]></category>
		<category domain="post_tag" nicename="containers"><![CDATA[containers]]></category>
		<category domain="post_tag" nicename="hash"><![CDATA[hash]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554378326;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4027;}i:1;a:1:{s:2:"id";i:3500;}i:2;a:1:{s:2:"id";i:2274;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Configure all the things</title>
		<link>https://onoffswitch.net/2013/03/02/configure-all-the-things-2/</link>
		<pubDate>Sat, 02 Mar 2013 16:46:50 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=2985</guid>
		<description></description>
		<content:encoded><![CDATA[I personally think that just about everything should be configurable, unless it's absolutely never going to change. Even then, make it configurable, because it may change in the future. Think about your favorite command line tools, and the extensibility they have. They're powerful because they are dynamic. They can be configured for a myriad of options and scenarios.

Configuration doesn't just imply things that are public to users, like the background color or autosave frequency, but it can expose non-compile time settings that help you tune your application to unknown environments. Most software can't possibly be tested in every environment that it will be used. However, if you make your code flexible and configurable, you can minimize damage and buy yourself some time when things go bad. You can possibly even find a configuration that works around the problem. Configuration can improve someone's experience by modifying assumptions you made while developing with a toggle or tweak of a configuration value.

<h2>What to configure?</h2>

Some examples of things that should be configurable (other than outward user options):
<ul>
	<li><strong>Timeouts</strong>. Any time you are setting timeouts for something, you should make this configurable. Maybe that 5-minute timeout you thought was impossible is actually happening, and you need to make it 6 minutes, at least temporarily</li>
	<li><strong>Retries</strong>. If you are going to retry something a certain number of times, you should make this value configurable. You should also make the interval of the retry configurable.</li>
	<li><strong>Thresholds</strong>. If something has a threshold, it should be configurable. You should be able to tighten the threshold or loosen it</li>
	<li><strong>Max and mins</strong>. Anything that has an upper or lower limit should be configurable.</li>
	<li><strong>Optional UI items</strong>. Some parts of an application aren't used by everyone. Even if you never plan on doing it, make it toggleable. This way you can toggle the visibility (and hopefully loading complexity) of a widget or piece of the application that isn't useful to a certain user  or user demographic. Some clients may not like item XYZ always showing up, even if you think its a core part of the application. If you can just disable it with ease, then that makes them happy not to see it. This keeps the codebase clean (less custom branches),  your support team happy, and you can keep being lazy and read reddit or whatever because you don't need to make a custom build (which of course requires unit testing, QA review, deployment, etc), which  makes you happy.</li>
	<li><strong>Things that start at runtime</strong>. Code internals should be able to be toggled. If you have a thread that spins up on startup, and is suddenly <a href="http://onoffswitch.net/wp-content/uploads/2013/03/swedishchef.jpg" target="_blank" rel="noopener noreferrer">borking</a> everything, then you can have a way to temporarily turn it off. This can save you, your support team, and the client a big headache while you find out why its not working. More than once I've run across something unexpected that happened, resulting in a runaway thread or thrashing disk. Wrapping everything in a configuration toggle lets you turn it off, and safely bypass an entire swath of the application. I've set it up before that for all items that run at startup there is a way to dynamically disable that option in a config. A config block like <code>DisabledRunTimeThreadNamePrefixes</code> means I can tack in a semicolon seperated list of known thread name prefixes (always name your threads!) and on startup those threads won't run. This is obviously not a long term solution, but does help with debugging if threads aren't playing nice or being destructive. While defensive programming can go a long way, sometimes you need to immediately turn off an entire section and not let it hit your code.</li>
	<li><strong>Execution of 3rd party libraries/programs</strong>. Imagine you do a netstat periodially, to see who is connected to what, as part of an application health monitor. However, netstat is choking on the network, and delaying other health statistics or waiting threads. You should make sure to wrap this kind of execution in a configuration, so that you can turn it on or off, depending on the scenario. </li>
	<li><strong>Optional features</strong>. Sometimes we have pet projects, and we love working on them, but nobody asked for it. If it works, the client may think this is the best thing ever. If it doesn't work, they really don't want it screwing up core requirements. These kinds of things should all be configurable, so you can turn them off if desired.</li>
	<li><strong>Paths</strong>. If you can configure where things go you should. This makes it easy to change where you log or put temporary files, etc and makes it easy to modify your application in unknown environments.  It's best to try not to hard-code values anywhere.</li>
<li><b>Late in release additions</b>. If you're releasing tomorrow, and some important feature just came in and HAS to be done (scope creep is its own issue...), wrap that feature set in a configuration.  Late in the game features are more likely to have bugs especially related to interaction of other parts in the system. This is just because they haven't had the same QA time to be vetted.  The last thing you want is to add a feature (or non-critical bug fix), think it's cool, then push it 1000+ clients only to have it break. Trust me.</li>
</ul>

<h2>Getting at a config</h2>

While taking configuration via the command line is a simple and flexible thing to do, it doesn't scale well and can be difficult to maintain. There are a lot of ways to manage configurations.  You can use an app.config, or a web.config (if its a website). You can store key value pairs like properties files do, or you can store your configuration in an xml file. Personally, I find that a known XML file is a great way to store a config.  It's easy to share, read, serialize and deserialize. 

If you decide to go the XML route for your configuration, and you're using C#, you can use the old style <code>XmlSerializer</code> or the new <code>DataContractSerializer</code>.  The XmlSerializer works well for configurations because it uses an "opt out" paradigm for serializing items. This means anything not marked with <code>XmlIgnore</code> gets serialized.  The DataContractSerializer is the other way around.  It is "opt in", so for every field you want serialized you have to mark it with <code>DataMember</code>. Technically DataContractSerializer is faster (and has other advantages), but when dealing with configs you typically aren't serializing and deserializing enormous configs thousands of times over.  Also configs tend to be relatively simple constructs so you don't need much of the fancy stuff that comes with DataContractSerializer. I like using <code>XmlSerializer</code> because it's easy. 

A neat feature of XmlSerializer is the <a href="http://www.distribucon.com/blog/MarkingDefaultValuesToControlXMLSerialization.aspx" target="_blank" rel="noopener noreferrer"><code>DefaultValue</code></a> attribute, which lets you tag members on a serializable class with what their default value should be.  When serializing a class, the serializer will only write values if the property is anything <em>other</em> than the default. This way, you can load and save your configs without exposing the internals of your configuration class. The upside of this is that your configs stay clean, and you can easily see what has changed from the default. The downside is that if you plan on exposing these options, then they won't show up by default. If you go this route, it would be worth keeping a master list of configuration options, where they are located (which config file, xml block, etc), what the default value is, what the option does, and why it exists.  

When loading the config, make sure that your config deserialization is exception safe.  If the config is corrupt, log that you couldn't' load elements and then use the default settings (unless it's mission critical to have the config settings and then it's better to fail fast).

It's also worth thinking about the situation where configs can't be found. I like to make sure that my config loading code is forgiving. It will look in a set of known folders, and move up the local hierarchy until it can find something. If it finds something it'll make sure to log <em>which</em> config it loaded. This way if there are config conflicts (for some reason if I have multiple configs floating around) I can see which config it loaded. Depending on your application architecture, this can be a reasonable solution. If you have a multi-process application, you can design it so that there is only one config for everyone. This makes maintaining configuration options easier, since it's all in one file. If you still can't find the configs, log an exception or a warning and then resort to the default values. Don't create a hard dependency on configuration, if you can avoid it. This makes re-using code that relies on configurations easier, since you don't need to have a configuration file to use it.

<h2>Live loading with filewatchers and Rx</h2>

It's also pretty easy to live-load your configurations, instead of just once on startup. If you create a file-watcher on your config class, and re-load it when it's changed, then you can have live up-to-date configurations. If your config file is live-editable, you can even throttle reloading the config at some regular intervals using <a href="http://msdn.microsoft.com/en-us/data/gg577609.aspx" target="_blank" rel="noopener noreferrer">Rx</a>, so you don't spam your system with configuration reloading. Just make sure to reference all your configuration options directly from the config class and don't make local copies of config values. Below is a simple example that will call a reload config function in 2 second intervals, while a file is being edited.

[csharp]
private void InitFileWatcher()
{
    _watcher = new FileSystemWatcher(Path.GetDirectoryName(Path.GetFullPath(Config.Path)), Path.GetFileName(Config.Path));
    Observable.FromEventPattern&lt;FileSystemEventHandler, FileSystemEventArgs&gt;(
        ev =&gt; _watcher.Changed += ev,
        ev =&gt; _watcher.Changed -= ev)
        .Sample(TimeSpan.FromSeconds(Config.Instance.ConfigReloadTime)).Subscribe(ReloadConfig);

    _watcher.NotifyFilter = NotifyFilters.LastWrite;
    _watcher.EnableRaisingEvents = true;
}

private void ReloadLogConfig(EventPattern&lt;FileSystemEventArgs&gt; obj)
{
    Log.Debug(this, &quot;{0} path was changed ({1}), reloading config&quot;, obj.EventArgs.FullPath, obj.EventArgs.ChangeType);
}
[/csharp]

<h2>Conclusion</h2>

I think configuration is just one part of defensive and flexible programming. Not all configurable items are about turning something on or off or avoiding bugs. They should also be about performance-tuning and optimization. In the end, if you aren't sure, make it configurable. You'll find that the worst that happens when you do is that you don't ever use that configuration value.

<a href="http://onoffswitch.net/wp-content/uploads/2013/03/configureAllThethings..png"><img class="alignnone size-full wp-image-1009" alt="" src="http://onoffswitch.net/wp-content/uploads/2013/03/configureAllThethings..png" width="319" height="241" /></a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2985</wp:post_id>
		<wp:post_date><![CDATA[2013-03-02 16:46:50]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-02 16:46:50]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[configure-all-the-things-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="best-practices"><![CDATA[Best Practices]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="configuration"><![CDATA[configuration]]></category>
		<category domain="post_tag" nicename="rx"><![CDATA[Rx]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1555170277;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1043;}i:1;a:1:{s:2:"id";i:4862;}i:2;a:1:{s:2:"id";i:4631;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Adding static typing and scope validation into the language, part 1</title>
		<link>https://onoffswitch.net/2013/03/04/adding-static-typing-and-scope-validation-into-the-language-part-1/</link>
		<pubDate>Mon, 04 Mar 2013 14:52:47 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3016</guid>
		<description></description>
		<content:encoded><![CDATA[Continuing on my <a href="http://onoffswitch.net/tag/language-implementation/" target="_blank" rel="noopener noreferrer">series</a> discussing the language I wrote, this next post is going to talk about the basics of static typing and scope rules.  So far my language implementation follows very closely to <a href="http://www.cs.usfca.edu/~parrt/" target="_blank" rel="noopener noreferrer">Parr's</a> examples in his book <a href="http://www.amazon.com/Language-Implementation-Patterns-Domain-Specific-Programming/dp/193435645X" target="_blank" rel="noopener noreferrer">Language Implementation Patterns</a>, which is what gave me the inspiration to do this project.  

However, starting here, I began to get confused with Parr's examples.  I didn't really see how to translate his examples into real code. On top of that  some of his diagrams weren't matching my mental model of what was going on, so that didn't help either. In general, I understood what he was doing, but I didn't want to follow his examples anymore - Parr switched over to using <a href="http://www.antlr.org/" target="_blank" rel="noopener noreferrer">ANTLR</a> for everything which didn't help me: I wanted to know how to do this <em>without</em> a generator.  So I just jumped in and took a stab at it.

For the scope builder (and later the interpreter) I did what I thought made sense. Like I've mentioned before, I haven't formally studied compiler/language design, so if a solution seems naive or egregiously wrong take it with a grain of salt (but leave a comment so I can learn how to do it better).

Anyways, this is where the project started to take off because I got to make real language decisions.  The first two things I decided on was that this language would have <a href="http://en.wikipedia.org/wiki/Type_system" target="_blank" rel="noopener noreferrer">static typing</a> and <a href="http://en.wikipedia.org/wiki/Forward_declaration" target="_blank" rel="noopener noreferrer">forward reference</a> support. I, personally, prefer static typed languages to dynamic typed languages because I like compile time checking.  I'm not going to deny that dynamic typing isn't helpful in some situations, in fact my interpreter leverages the <code>dynamic</code> data type heavily.  For fun I did build in some runtime typing into the language (which I'll cover in a later post).  The type system I implemented also supports type inference.  However, my language is pretty rudimentary and I don't support type promotion, though it wouldn't be hard to add.  

For scoping, I wanted to allow forward references for methods and class internals, but not for internal statements.  I like forward references because it lets you structure your code by ideas and locality of reference, and not strictly by bottom up execution. I like the freedom that forward references gives you.  Also I knew that solving forward references would be more difficult so I decided to tackle the problem.

For the remaining post, I'm going to discuss the groundwork required to dig into more complex parts of the scope builder. As always, if you are curious check out the <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">github project</a> for full source, examples, tests, and comments.

Let's get started.

<h2>Iterating over the syntax tree</h2>

In the last post we built a parser that generates an abstract syntax tree representing the execution of our program. Now, we need to iterate over that tree and do meaningful work. However, we're going to be going over this tree a bunch of different times and we want to do different work over it.  We could put the tree iteration into the syntax tree, but thats nasty.  Instead, I used the <a href="http://stackoverflow.com/a/255300/310196" target="_blank" rel="noopener noreferrer">visitor pattern</a> to iterate over the tree.  

I defined two interfaces. The first, is the definition for the actual visitor.

[csharp]
public interface IAstVisitor
{
    void Visit(Conditional ast);
    void Visit(Expr ast);
    void Visit(FuncInvoke ast);
    void Visit(VarDeclrAst ast);
    void Visit(MethodDeclr ast);
    void Visit(WhileLoop ast);
    void Visit(ScopeDeclr ast);
    void Visit(ForLoop ast);
    void Visit(ReturnAst ast);
    void Visit(PrintAst ast);
    void Visit(ClassAst ast);
    void Visit(ClassReference ast);
    void Visit(NewAst ast);
    void Visit(TryCatchAst ast);

    void Start(Ast ast);
}
[/csharp]

This interface will accept different syntax trees and depending on which overloaded <code>Visit</code> function is called, the visitor will know how to iterate (and do work) on that particular syntax tree.

The next interface is applied to the abstract base class <code>Ast</code>, which means that all syntax tree classes have to implement it. This interface forces the ast classes to "accept" a visitor.

[csharp]
interface IAcceptVisitor
{
    void Visit(IAstVisitor visitor);
}
[/csharp]

Below is the class used to represent an expression.  Notice its visit method. All the classes have exactly the same visit method. You have to do it this way because if the visit method was in the base class, the visitor wouldn't be able to figure out which class was trying to be visited (since they all inherit from <code>Ast</code>).

[csharp highlight="18,19,20,21"]
public class Expr : Ast
{
    public Ast Left { get; private set; }

    public Ast Right { get; private set; }

    public Expr(Token token) : base(token)
    {
    }

    public Expr(Ast left, Token token, Ast right)
        : base(token)
    {
        Left = left;
        Right = right;
    }

    public override void Visit(IAstVisitor visitor)
    {
        visitor.Visit(this);
    }

    public override AstTypes AstType
    {
        get { return AstTypes.Expression; }
    }
}
[/csharp]

Now all we need to do to iterate over the tree is create an actual visitor.  As an example, here is the function invoke overloaded <code>visit</code> method on a visitor called <code>PrintAstVisitor</code> that iterates the syntax tree and writes the tree representation to the console.  To iterate over other parts of the tree we tell the tree to visit itself using this visitor (with <code>this</code>).

[csharp]
public class PrintAstVisitor : IAstVisitor
{
    // ... other functions ....

    public void Visit(FuncInvoke ast)
    {
        Console.WriteLine(&quot;Function invoke AST&quot;);

        ast.FunctionName.Visit(this);

        ast.Arguments.ForEach(arg =&gt; arg.Visit(this));
    }

    // ... other functions ....
[/csharp]

When we are ready to actually iterate over the full tree, we need to hand off the root of the syntax tree to the visitor and tell it to start.

[csharp]
var program = @&quot;int x = 1;&quot;;

var ast = new LanguageParser(new Lexer(program)).Parse();

new PrintAstVisitor().Start(ast);
[/csharp]

<h2>Symbols, Types, and Scopes?</h2>

The goal of the <a href="https://github.com/devshorts/LanguageCreator/blob/master/Lang/Visitors/ScopeBuilderVisitor.cs" target="_blank" rel="noopener noreferrer">scope builder visitor</a> is to associate types to symbols and make sure that they are visible in the right scope.  We all intuitively know what symbols are, we work with them all the time.  

[code]
int x = 1
[/code]

<code>x</code>, here, is a symbol. It also has a type of <code>int</code>. Some types are built into the language, like <code>int</code>, <code>string</code>, <code>void</code>, etc.  Some types are user defined (like classes or structs).  The scope builder is going to figure out which expressions are which types and tag each syntax tree node with its type.

The builder also starts to do some syntax validation for us.  It's responsible for making sure our assignments, declarations, and invocations all make sense.  One of the builders job is to make sure we can't reference undefined values. For example, this is invalid:

[code]
int x = y
int y = 0;
[/code]

This is because <code>x</code> uses <code>y</code> before <code>y</code> is defined.   The scope builder is also responsible for validating static typing: we want invalid type assignment to be prevented

[code]
void x = 1;
[/code]

If we defined x to be a void then we certainly shouldn't be able to assign 1 to a void.  The scope builder should throw an exception and prevent us from doing this.  By preventing us from doing this we eliminate having to find out at runtime that our code is bogus.  Though, as the language "designer" here, if we wanted to let this happen we totally could!  It's really up to your language implementation as to how you deal with what this means.

On top of all of this, like mentioned above, the scope builder is responsible for validating that symbols are visible only where they should be.  The code below should be invalid because <code>y</code> is declared in an inner scope not visible to the print statement.  Some languages don't do it this way (javascript/actionscript), but this drives me nuts, so I made sure to do it for my own language.

[code]
int x = 1;
{
    int y = 2;
}

print y;
[/code]

The scope builder sounds like it does a lot of work (and it does), but most of these things are intertwined and the underlying code is short and sweet so there isn't too much overlapping of concerns in the same class.

<h2>Scopes</h2>

Scopes are easily represented by a stack.  Each time you encounter a new scope block (like the inner scoped <code>y</code> above) all you need to do is push a new scope onto your scope stack.  Symbols below the current scope on the stack are visible if you are in that scope, so <code>x</code> would be visible to <code>y</code>, but symbols are not allowed to be visible up the stack from the bottom.

To support classes I had one scope stack for the global space. This scoped items in what I considered "main", or really any inline code that is not within a class.  I also had a scope stack for each class I encountered. This is because you don't want classes to be able to see symbols in the global scope. Imagine this scenario:

[csharp]
class foo{
   int value = x;
}

int x = 0;
[/csharp]

This should be invalid because <code>foo</code> shouldn't be able to see <code>x</code>. <em>[Note: classes get a little weird, because you need to define the class declaration in the global scope (so you can instantiate the class), but everything inside the class is in its own scope. There are also other issues with classes that I'll talk about in a later post.]</em>

A scope, then, is nothing more than a bag of symbols and a reference to its parent scope (below it in the scope stack). 

For example, a basic <code>Scope</code> object in my language looks like this.  Notice that a scope contains a reference to its parents scope (line 5). This will get set automatically by the <code>ScopeStack</code> I'll show next.  It's important to understand that a scope will have access to all elements below it via its parent.  You can see that logic in the <code>Resolve</code> function which checks if a symbol is in the current scopes dictionary, and if not, tries the parent scope.  

[csharp]
public class Scope : IScopeable&lt;Scope&gt;
{
    public Dictionary&lt;string, Symbol&gt; Symbols { get; set; }

    public Scope EnclosingScope { get; private set; }

    public List&lt;IScopeable&lt;Scope&gt;&gt; ChildScopes { get; private set; }
         
    public Scope()
    {
        Symbols = new Dictionary&lt;string, Symbol&gt;();

        ChildScopes = new List&lt;IScopeable&lt;Scope&gt;&gt;(64);
    }

    public void SetParentScope(Scope scope)
    {
        EnclosingScope = scope;
    }

    public void Define(Symbol symbol)
    {
        Symbols[symbol.Name] = symbol;
    }

    public Symbol Resolve(String name)
    {
        Symbol o;
        if (Symbols.TryGetValue(name, out o))
        {
            return o;
        }

        if (EnclosingScope == null)
        {
            return null;
        }

        return EnclosingScope.Resolve(name);
    }
}
[/csharp]

To help with setting the parent scope each time we pushed a new scope onto the stack, I created a generic scope stack that facilitated pushing and popping scopes, as well as auto-linked scope references. This way when I access a scope reference I can go down its stack:

[csharp]
public class ScopeStack&lt;T&gt; where T : class, IScopeable&lt;T&gt;, new()
{
    private Stack&lt;T&gt; Stack { get; set; }

    public T Current { get; set; }

    public ScopeStack()
    {
        Stack = new Stack&lt;T&gt;();
    }

    public void CreateScope()
    {
        var parentScope = Current;

        if (Current != null)
        {
            Stack.Push(Current);
        }

        Current = new T();

        Current.SetParentScope(parentScope);

        if (parentScope != null)
        {
            parentScope.ChildScopes.Add(Current);
        }
    }

    public void PopScope()
    {
        if (Stack.Count &gt; 0)
        {
            Current = Stack.Pop();
        }
    }
}
[/csharp]

And <code>IScopeable</code> is 

[csharp]
public interface IScopeable&lt;T&gt; where T : class, new()
{
    void SetParentScope(T scope);
    List&lt;IScopeable&lt;T&gt;&gt; ChildScopes { get; } 
}
[/csharp]

The concept of a stack based symbol visibility control will get re-used again when we discuss memory spaces (i.e. where a value is in memory).  Memory and scope are very similar, but not the same.  You'll see why later, but needless to say the scope container class gets re-used a few times in different contexts.

<h2>Types</h2>

Every symbol has a type.  Let me show you what my type definition looks like:

[csharp]
public interface IType
{
    String TypeName { get; }
    ExpressionTypes ExpressionType { get; }
    Ast Src { get; set; }
}
[/csharp]

<code>ExpressionTypes</code> is an enum of expression types.  I use this for type checking later.  The <code>Src</code> property holds a reference to the syntax tree that generated this type. For example, if I have a class called <code>foo</code> I will create a type whose <code>TypeName</code> is <code>foo</code> and it's <code>Src</code> property will point to its <code>ClassAst</code> reference.  Later I can get info about the source syntax tree just from the type.

I only have two kinds of types in the entire system. 

<ul>
<li><b>Built in types</b>.  These are types like <code>int</code>, <code>string</code>, etc.  They all share the same class, and the only thing that is different about them is the <code>ExpressionType</code> enum they hold indicating which built in type they are.</li>
<li><b>User defined types</b>. These are classes that the user defines.</li>
</ul>

To make a symbol is easy.  Depending on the syntax tree type I can create the proper symbol type based on its token type.

[csharp]
public static IType CreateSymbolType(Ast astType)
{
    if (astType == null)
    {
        return null;
    }

    Func&lt;IType&gt; op = () =&gt;
        {
            switch (astType.Token.TokenType)
            {
                case TokenType.Int:
                    return new BuiltInType(ExpressionTypes.Int);
                case TokenType.Float:
                    return new BuiltInType(ExpressionTypes.Float);
                case TokenType.Void:
                    return new BuiltInType(ExpressionTypes.Void);
                case TokenType.Infer:
                    return new BuiltInType(ExpressionTypes.Inferred);
                case TokenType.QuotedString:
                case TokenType.String:
                    return new BuiltInType(ExpressionTypes.String);
                case TokenType.Word:
                    return new UserDefinedType(astType.Token.TokenValue);               
                case TokenType.True:
                case TokenType.False:
                    return new BuiltInType(ExpressionTypes.Boolean);
                case TokenType.Method:
                    return new BuiltInType(ExpressionTypes.Method);
            }
            return null;
        };

    var type = op();

    if (type != null)
    {
        type.Src = astType;
    }

    return type;
}
[/csharp]

<h2>Symbols</h2>

A symbol is a pairing of a symbol name and a symbol type.  This means a variable named <code>x</code> declared as type <code>int</code> now gets those two values paired together:

[csharp]
public class Symbol : Scope
{
    public String Name { get; private set; }
    public IType Type { get; private set; }

    public Symbol(String name, IType type)
    {
        Name = name;
        Type = type;
    }

    public Symbol(String name)
    {
        Name = name;
    }
}
[/csharp]

But wait, symbols are also scopes?  They can be.  A class symbol is also a scope and when we're within a class we'll want to define it's symbols within it's own personal space (like I mentioned earlier with the separate scope stacks).  Having a symbol also be a scope makes life really easy when you try and figure out where things are.  

The scope builder is complicated, but it follows the same basic pattern

<ul>
<li>If the current symbol is being declared (variable declaration, method declaration argument definitions, etc) then define the symbol in the appropriate scope.</li>
<li>If we are referencing the symbol, resolve it and make sure it's been declared and is visible in the scope we expect</li>
</ul>

<h2>Defining a symbol</h2>

A simplified version of the variable declration visit function looks like this:

<em>[Note: it's simplified because we need to handle variables that are declared without values, as well as type inferred values, and method assignments, and partial functions, and validating left and right hand side type assignments, etc. There is a lot to it, to be covered later. If you're curious check the github]</em>

[csharp]
public void Visit(VarDeclrAst ast)
{
    if (ast.DeclarationType != null)
    {
        var symbol = ScopeUtil.DefineUserSymbol(ast.DeclarationType, ast.VariableName);

        DefineToScope(symbol);

        ast.AstSymbolType = symbol.Type;
    }
}
[/csharp]

The declaration type is a syntax tree representing the declaration of the variable (int, string, user defined, whatever), and the name is an expression representing a word that the variable is called.

Then we create symbol type and the actual symbol

[csharp]
public static Symbol DefineUserSymbol(Ast ast, Ast name)
{
    IType type = CreateSymbolType(ast);

    return new Symbol(name.Token.TokenValue, type);
}
[/csharp]

Then we define the symbol in the current scope

[csharp]
private void DefineToScope(Symbol symbol)
{    
    Current.Define(symbol);
}
[/csharp]

And finally, we assign the current ast node it's expression type

[csharp]
ast.AstSymbolType = symbol.Type;
[/csharp]

This way each syntax tree can track what type it is. We can use this information to do static type checking later (basically validate that the left hand side and right hand side have the same, or promotable, types). 

<h2>Resolving a symbol</h2>

Resolving is the other way around.  

Here is a simplified version of the visitor for an expression. It first visits the left, then the right.  If the left and right are null then we have a leaf in our syntax tree and we need to resolve what it is.  If it's a built in type, like an integer, it'll get defined as a symbol, otherwise if it's a word (a user defined variable) it'll get resolved:

[csharp]
public void Visit(Expr ast)
{
    if (ast.Left != null)
    {
        ast.Left.Visit(this);
    }

    if (ast.Right != null)
    {
        ast.Right.Visit(this);
    }

    SetScope(ast);

    if (ast.Left == null &amp;&amp; ast.Right == null)
    {
        ast.AstSymbolType = ResolveOrDefine(ast);
    }   
}
[/csharp]

Which calls

[csharp]
/// &lt;summary&gt;
/// Creates a type for built in types or resolves user defined types
/// &lt;/summary&gt;
/// &lt;param name=&quot;ast&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
private IType ResolveOrDefine(Expr ast)
{
    if (ast == null)
    {
        return null;
    }

    switch (ast.Token.TokenType)
    {
        case TokenType.Word: return ResolveType(ast);
    }

    return ScopeUtil.CreateSymbolType(ast);
}
[/csharp]

We've already seen <code>ScopeUtil.CreateSymbolType</code>. Here is a simplified version of <code>ResolveType</code>

[csharp]
private IType ResolveType(Ast ast)
{
    Symbol symbol = Current.Resolve(ast);
    if(symbol != null){
        return symbol.Type;
    }
    
    throw new InvalidSyntax(&quot;Cannot resolve {0}&quot;, ast.Token.TokenValue);
}
[/csharp]

It's a simplified version because it doesn't handle forward references. I'll discuss that in a later post.

Remember the scope rules above.  If a variable isn't visible in a scope, either because it's undefined, or because it's not visible, resolving the type will fail.  

You'll notice the <code>SetScope</code> function. Every syntax tree gets assigned a reference to the scope they came from.  This way everyone knows who can see what.  

<h2>Conclusion</h2>

At this point we have a very basic way of defining symbols, types, and scopes. We've also gone over a way to validate simple scoping rules.  There is a lot more to the scope builder. Now that there is some infrastructure in the next post I'll dig deeper into the scope builder and discuss how we can fix the forward reference problem and do type inference.  Still to come in the scope builder is determining return types and handling how to build partial functions.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3016</wp:post_id>
		<wp:post_date><![CDATA[2013-03-04 14:52:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-04 14:52:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[adding-static-typing-and-scope-validation-into-the-language-part-1]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"7535db1f00326ea7d941a323347758fc";a:2:{s:7:"expires";i:1559692912;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3128;}i:1;a:1:{s:2:"id";i:2735;}i:2;a:1:{s:2:"id";i:3565;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>34</wp:comment_id>
			<wp:comment_author><![CDATA[Adding static typing and scope references, part 3: solving forward references | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/adding-static-typing-and-scope-references-part-3-solving-forward-references/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-05-21 20:30:00]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-05-21 20:30:00]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] an earlier post I gave a brief overview of the scope builder and its jobs. There I mentioned that supporting [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Double encoding: URI and HTML encoding</title>
		<link>https://onoffswitch.net/2013/03/04/double-encoding-uri-and-html-encoding/</link>
		<pubDate>Mon, 04 Mar 2013 16:30:18 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3111</guid>
		<description></description>
		<content:encoded><![CDATA[URL's have specific characters that are special, like <code>%</code> and <code>&</code> that if you need to use as part of your GET URI then you need to <a href="http://www.w3schools.com/tags/ref_urlencode.asp" target="_blank" rel="noopener noreferrer">encode</a> them. For example:

[html]
http://localhost?key=this &amp; that&amp;key2=value2
[/html]

It's obvious that this URL is invalid, <code>this & that</code> has both spaces and a special character <code>&</code>.  In fact, you may have even noticed it's invalid in your browser:

[caption id="attachment_3126" align="alignnone" width="432"]<img src="http://onoffswitch.net/wp-content/uploads/2013/03/invalidurl2.png" alt="Screenshot of what you see above. Yes, I know it&#039;s a little  meta" width="432" height="39" class="size-full wp-image-3126" /> Screenshot of what you see above. Yes, I know it's a little  meta[/caption]

To get around this you can URI encode your URL's which will convert special characters to ASCII:

[html]
http://localhost?key=this%20%26%20that&amp;key2=value2
[/html]

But there is also <a href="http://en.wikipedia.org/wiki/Character_encodings_in_HTML" target="_blank" rel="noopener noreferrer">HTML encoding</a>, which means escaping special HTML characters when you are putting in text into html. For example:

[html]
&lt;p&gt;6 &gt; 7&lt;/p&gt;
[/html]

Doesn't work, since <code>></code> is a special character.  To get around this, you need to HTML encode your text. HTML encoding uses special characters to indicate escaping of text.  HTML encoding the above example would make your text

[html]
&lt;p&gt;6 &amp;gt; 7&lt;/p&gt;
[/html]

But sometimes you want to put in some HTML dynamically into a page that also contains a URL. Here you need to double encode (encode the URI and the HTML).  The ordering here matters. Let's take this HTML text as the source:

[html]
&lt;a href=&quot;me.aspx?Filename=Anton's Document&quot; /&gt;
[/html]

And lets encode it first with HTML then with URI:

[html]
html encoded: &lt;a href=&quot;me.aspx?Filename=Anton&amp;apos;s+Document&quot; /&gt;
uri encoded: &lt;a href=&quot;me.aspx?Filename=Anton%26apos;s+Document&quot; /&gt;
[/html]

But what if we do it the other way around?

[html]
uri encoded: &lt;a href=&quot;me.aspx?Filename=Anton's+Document&quot; /&gt;
html encoded: &lt;a href=&quot;me.aspx?Filename=Anton&amp;apos;s+Document&quot; /&gt;
[/html]

See the difference? Look at where the apostrophe would be

[code]
invalid: %26apos;
valid: &amp;apos;
[/code]

The first example will give you an invalid URL but the second example is the URL you want.  URL and HTML encodings aren't interchangable, they are used for specific scenarios and sometimes need to be used together (in the right order).]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3111</wp:post_id>
		<wp:post_date><![CDATA[2013-03-04 16:30:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-04 16:30:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[double-encoding-uri-and-html-encoding]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_6073140669e937827d5610641b40cdf4]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561033504;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4262;}i:1;a:1:{s:2:"id";i:3942;}i:2;a:1:{s:2:"id";i:4725;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_dcc58c858c7066130a1a8a543b535881]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_da0fe90d451adcefeea5c7bb0f51ab34]]></wp:meta_key>
		<wp:meta_value><![CDATA[{{unknown}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Adding static typing and scope references, part 3: solving forward references</title>
		<link>https://onoffswitch.net/2013/03/07/adding-static-typing-and-scope-references-part-3-solving-forward-references/</link>
		<pubDate>Thu, 07 Mar 2013 08:00:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3128</guid>
		<description></description>
		<content:encoded><![CDATA[In an <a href="http://onoffswitch.net/adding-static-typing-and-scope-validation-into-the-language-part-1/" target="_blank" rel="noopener noreferrer">earlier post</a> I gave a brief overview of the scope builder and its jobs. There I mentioned that supporting forward references required some extra work. In this post I'll talk more about how I solved forward references. 

Here is what I mean by forward references.  <code>func</code> is declared after it's being referenced

[csharp]
string item = func();

string func(){
    return &quot;yes&quot;;
}

print item;
[/csharp]

If we iterate over the program only once from the top down using our visitor pattern based scope builder, when we try and resolve the <code>func</code> method invocation symbol we'll get an error (it hasn't been defined yet). 

Remember that when things are declared (such as methods, classes, or variables) we create a symbol (with a type) in the current scope tree. Later, when we are referencing them, we need to resolve that symbol. Resolution both validates that we can properly see the symbol and gives us information about that symbol (such as its type, which we can use for static typing).  This is important because without forward reference support, we can't tell what the return type of the <code>func()</code> call is, so we can't validate that that it needs to be a string (or something that can be promoted to a string).  

<h2>Maintaining scope references</h2>

But what if instead of iterating over the tree once, we iterate over it twice? The idea is that the first pass defines all your types, and the second pass can now resolve all the types.  

Since I'm going to iterate over the tree twice, I need a way to persist scopes across each iteration regardless of the context of the scope builder.  For that, I added a property on the syntax tree base class to track what its current scope is. Every syntax tree will contain a reference to the scope it came from.  

[csharp]
public abstract class Ast : IAcceptVisitor
{
    // ...

    public Scope CurrentScope { get; set; }

    // ...
}
[/csharp]

The trick here is that I'm leveraging the fact that, in C#, classes are reference types.  For those more familiar with C++ this would be like sharing a pointer across multiple syntax trees all pointing to the same scope object. This means that when one syntax tree defines symbols to the current scope, any other syntax elements that have already been processed, and share the same pointer, will also see the newly added elements. Because of this, when the scope builder is all done, each node in the tree knows what else it can see.  

<h2>The first pass </h2>

Going with the example above

[csharp]
string item = func();
[/csharp]

In the first pass we'll do something like this

<ul>
<li>Create user defined symbol <code>item</code> with type <code>string</code> (a built in type)</li>
<li>Visit the variable declaration value (a function invoke)</li>
<li>In the function invoke visit method, resolve the symbol <code>func</code>. But, this resolving fails and returns null.  We can't resolve <code>func</code> yet because we don't know who or what it is. We can't really do any type validation here yet because the symbol is null</li>
<li>Define <code>func</code> as a method symbol in the same scope (global) that has a return type of type <code>string</code>. Creating symbols was covered in the previous post</li>
<li>Set the variable declaration asts scope to the current so it can see other symbols in its scope</li>
</ul>

Now the scope definitions table looks like this, with all fields defined 

[code]
item  - string
func  - method (returns string)
[/code]

<h2>Persist the scope on the syntax tree</h2>
Let's look at how to set the scope on an ast 

[csharp]
private void SetScope(Ast ast)
{
    if (ast.CurrentScope == null)
    {
        ast.CurrentScope = Current;

        ast.Global = Global;
    }

    if (ast.CurrentScope != null &amp;&amp; ast.CurrentScope.Symbols.Count &lt; Current.Symbols.Count)
    {
        ast.CurrentScope = Current;
    }

    if (ast.Global != null &amp;&amp; ast.Global.Symbols.Count &lt; Global.Symbols.Count)
    {
        ast.Global = Global;
    }
}
[/csharp]

There are a couple conditions here.  First, if the current scope object is null, set it. This should happen on the first pass. Also attach the global scope so that each syntax tree (even if it comes from a class) can look into the global scope.  I mentioned in an earlier post that the syntax tree is the master repo of program related metadata and we're starting to see that here.  The two other if statements are safety checks to make sure that if the current scope has more symbols defined than what the syntax tree sees, make sure to update the syntax trees internal scope blocks. This way we always make sure to have the most up to date symbol defintions.

Once we have the scope set, we can do the second pass on the syntax tree.  This will create a new scope builder and we'll run the tree back through it. It'll do basically the same thing as the first pass, except this time, it's going to look in the previously persisted scope for the variable declaration tree. 

<h2>The second pass and resolving symbols</h2>

First let's look at how the first and second passes are invoked. I invoke them as part of the <code>Start</code> method of the interpreter, so this happens automatically anytime I interpret a program:

[csharp]
var scopeBuilder = new ScopeBuilderVisitor();

var resolver = new ScopeBuilderVisitor(true);

scopeBuilder.Start(ast);

resolver.Start(ast);
[/csharp]

The <code>true</code> passed to the visitor's constructor tells the second pass resolver to resolve types. With regards to our example that I showed above

[csharp]
string item = func();

string func(){
    return &quot;yes&quot;;
}

print item;
[/csharp]

The first time when we hit the variable declaration we'll go through the variable declaration part of the visitor:

[csharp]
public void Visit(VarDeclrAst ast)
{
    // ...

    if (ast.VariableValue != null)
    {
        ast.VariableValue.Visit(this);
    }

    SetScope(ast);

    // ...
}
[/csharp]

Where the variable value is a syntax tree representing a method invocation (<code>func</code>). When the variable value is visited, the function invoke visitor method will do this:

[csharp]
public void Visit(FuncInvoke ast)
{
    // ...

    if(ResolvingTypes)
    {
        ast.AstSymbolType = ResolveType(ast.FunctionName, ast.CurrentScope);
    }

    SetScope(ast);

    // ...
}
[/csharp]

Notice that second parameter, the <code>ast.CurrentScope</code>, which is the trees persisted scope. This is the scope reference from the FIRST pass, and we only ever set it on the first pass, meaning that at this point the variables are all declared.  

<code>ResolveType</code> then looks like this. 

[csharp]
/// &lt;summary&gt;
/// Resolve the target ast type from the current scope, OR give it a scope to use.  
/// Since things can be resolved in two passes (initial scope and forward reference scope)
/// we want to be able to pass in a scope override.  The second value is usually only ever used
/// on the second pass when determining forward references
/// &lt;/summary&gt;
/// &lt;param name=&quot;ast&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;currentScope&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
private IType ResolveType(Ast ast, Scope currentScope = null)
{
    var scopeTrys = new List&lt;Scope&gt; { currentScope, ast.CurrentScope };

    try
    {
        return Current.Resolve(ast).Type;
    }
    catch (Exception ex)
    {
        try
        {
            return ast.CallingScope.Resolve(ast).Type;
        }
        catch
        {
            foreach (var scopeTry in scopeTrys)
            {
                try
                {
                    if (scopeTry == null)
                    {
                        continue;
                    }

                    var resolvedType = scopeTry.Resolve(ast);

                    var allowedFwdReferences = scopeTry.AllowedForwardReferences(ast);

                    if (allowedFwdReferences ||
                        scopeTry.AllowAllForwardReferences ||
                        resolvedType is ClassSymbol ||
                        resolvedType is MethodSymbol)
                    {
                        return resolvedType.Type;
                    }
                }
                catch
                {

                }
            }
        }
    }

    if (ResolvingTypes)
    {
        if (ast.IsPureDynamic)
        {
            return new BuiltInType(ExpressionTypes.Inferred);
        }

        throw new UndefinedElementException(String.Format(&quot;Undefined element {0}&quot;,
                                                                    ast.Token.TokenValue));
    }

    return null;
}
[/csharp]

The basic gist is first try to resolve the type from the current scope (which gets reset every time the scope builder runs over the tree. So even on the second pass, the first line will fail, and thats what we want).  

<code>Current</code> contains the following:

[code]
item  - string
[/code]

<code>func</code> isn't defined yet. <code>Current</code> gets reset each time the scope builder is instantiated, so even though this is the second pass this scope reference still fails. That's OK.  If we are resolving types (i.e. the second pass), we can try an alternate scope.  We can either pass in an alternate scope, or also use the syntax trees previous scope.   For each alternate scope we try and find the symbol we want.  If we use the previously constructed ast scope, which contains

[code]
item  - string
func  - method - string
[/code]

it would have the <code>func</code> symbol defined as a method with return string.  Other alternate scopes can be like the global scope, which we would try if we are within the context of a class. This makes sense because if you are doing this:

[csharp]
class first{
  var secondInstance = new second();
}

class second{
   int x = 0;
}
[/csharp]

When you are inside of the <code>first</code> class and vising the variable declaration, the <code>second</code> class is defined in the global scope, NOT the current class scope. This means resolving the class name from the classes scope will fail, but if we give it an alternate (the global scope) then it will succeed.

Anyways, if we resolve as a forward reference, then we need to determine if we're <em>allowed</em> to see it as a forward reference.  Everything at this point is resolvable (since we've populated the persisted scope reference from the first pass). Method symbols and class symbols are always allowed, and scopes can optionally define forward reference allowance if they need to.  But regular statements aren't allowed to be forward references. We still want this to fail:

[csharp]
int x = y;
int y = 0;
[/csharp]


After all of the resolving, if we still don't have the type, or we weren't allowed to see it as a forward reference, then we've encountered an error and we have to bail.

<em>[Note: I mentioned that I implemented runtime dynamic typing and you can see part of that at the end of the resolve types function. If the syntax tree is deemed to be "purely dynamic" then it will never fail when resolving a type. The type will then be figured out at runtime, which means you get no static analysis on the type ahead of time]</em>

<h2>Updating the AST scope reference</h2>

If we did manage to resolve the type, the next step is to go back and update the type reference in the asts scope declaration:

[csharp]
private void DefineToScope(Ast ast, Symbol symbol)
{
    if (ast.CurrentScope != null &amp;&amp; ast.CurrentScope.Symbols.ContainsKey(symbol.Name))
    {
        Symbol old = ast.CurrentScope.Resolve(symbol.Name);
        if (old.Type == null)
        {
            ast.CurrentScope.Define(symbol);
        }
    }

    Current.Define(symbol);
}
[/csharp]

Notice that first if statement.  What this is saying is that if we have some defined symbol in the scope, but that defined symbol does NOT have a type, then redefine the symbol. All this is doing is filling in the gaps in the <code>ast.CurrentScope</code> so we can use that as the master scope repository later in the interpreter.

<h2>Up next</h2>

Coming up next I'll discuss how I added in partial functions into the scope builder.  After that it'll be time to jump into the interpreter.

<h2>Disclaimer</h2>

As with the other posts, I haven't formally studied language implementation or compiler design. This series is just documenting how I solved problems I encountered while trying to make a working <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">toy language</a> using <a href="http://www.cs.usfca.edu/~parrt/" target="_blank" rel="noopener noreferrer">Terence Parr</a>'s <a href="http://www.amazon.com/Language-Implementation-Patterns-Domain-Specific-Programming/dp/193435645X" target="_blank" rel="noopener noreferrer">Language Implementation Patterns</a> book. If there are things you'd like to share (or correct) leave a comment and let me know!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3128</wp:post_id>
		<wp:post_date><![CDATA[2013-03-07 08:00:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-07 08:00:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[adding-static-typing-and-scope-references-part-3-solving-forward-references]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561171017;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3016;}i:1;a:1:{s:2:"id";i:3164;}i:2;a:1:{s:2:"id";i:3161;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>35</wp:comment_id>
			<wp:comment_author><![CDATA[Adding static typing and scope validation, part 2: type inference and validation | onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/adding-static-typing-and-scope-validation-part-2-type-inference-and-validation/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-03-07 16:45:51]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-03-07 16:45:51]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] [Note: when types are resolved, how symbols are created, and solving forward references is coming in part 3] [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Implementing partial functions</title>
		<link>https://onoffswitch.net/2013/03/11/implementing-partial-functions/</link>
		<pubDate>Mon, 11 Mar 2013 08:00:18 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3161</guid>
		<description></description>
		<content:encoded><![CDATA[This next section I had a lot of fun with, and originally I didn't plan on implementing it at all.  The only reason I did it is because I had a stroke of genius while in the shower one morning. Today, I'm going to talk about how I supported partial functions in my toy programming language.

First let's look at what a partial function looks like in my language.  I took an F# approach where any function whose argument count is less than the declared count becomes a new function (even though F# functions are curried by default but mine are not). For example, in <a href="http://en.wikibooks.org/wiki/Standard_ML_Programming/Types" target="_blank" rel="noopener noreferrer">ML type notation</a> you could have a function of type

[fsharp]
'a -&gt; 'b -&gt; 'c 
[/fsharp]

Which means that it takes something of type a and type b as arguments, and returns a type c. If we pass this function only a <code>'a</code> then it'll return to us a new function of type 

[fsharp]
'b -&gt; 'c
[/fsharp]

Since the first argument has been captured.

Here is an example from one of my unit tests to drive the point home

[csharp]
void func(string printer, int y){
    print printer;
    print y;
}
            
var partial = func('guy');

int x = 1;

partial(x);

partial(2);

var otherPartial = func('girl');

otherPartial(3);
[/csharp]

The <code>partial</code> variable is a function who now expects only 1 argument (since the first string argument to <code>func</code> was already applied).  This outputs

[code]
guy
1
guy
2
girl
3
[/code]

<h2>Manipulating the syntax tree</h2>

If a function invoke AST has fewer arguments than the corresponding method symbol it's paired with then it needs to be partially applied. <em>[Note: I should mention that partial functions and currying are<a href="http://stackoverflow.com/a/10443057/310196" target="_blank" rel="noopener noreferrer"> not technically</a> the same, even though in practice the terminology is interchangeable. I only realized this after I named all the functions, so even if things are called "curry" this and "curry" that, it really means "partially apply"]</em>

[csharp]
public void Visit(FuncInvoke ast)
{
    if (ast.CallingScope != null)
    {
        ast.Arguments.ForEach(arg =&gt; arg.CallingScope = ast.CallingScope);
    }

    ast.Arguments.ForEach(arg =&gt; arg.Visit(this));

    SetScope(ast);

    var functionType = Resolve(ast.FunctionName) as MethodSymbol;

    if (functionType != null &amp;&amp; ast.Arguments.Count &lt; functionType.MethodDeclr.Arguments.Count)
    {
        var curriedMethod = CreateCurriedMethod(ast, functionType);

        curriedMethod.Visit(this);

        var methodSymbol = ScopeUtil.DefineMethod(curriedMethod);

        Current.Define(methodSymbol);

        ast.ConvertedExpression = curriedMethod;
    }
    else if(ResolvingTypes)
    {
        ast.AstSymbolType = ResolveType(ast.FunctionName, ast.CurrentScope);
    }
}
[/csharp]

What I end up doing is creating new hidden syntax tree that represents a closed on syntax tree.  In the example above we'll end up with an anonymous function that looks like this:

[csharp]
void anonymous1(int y){
    string printer = 'guy';
    print printer;
    print y;
}
[/csharp]

The captured argument becomes a variable declaration of the same name as the argument name with the value that was captured!  At this point the rest of the code works as is, because what does it care if it was defined this way by the user or by tree manipulation? It doesn't.  There is one caveat.  I created the concept of "converted expressions" for syntax trees. This means that what used to be a type inferred variable declaration, should now be treated as a method declaration. This is important.  <code>partial</code> (in our original example) is no longer a variable, it is now a method.  When I go to interpret this syntax tree, and use type definitions, I first need to check if the tree has been converted to something else (via the converted expression property).  

<h2>Building out the partial function</h2>

Building the actual partial function isn't that hard.  

<ol>
<li>For each captured argument create a variable declaration syntax tree representing the argument name and the argument value</li>
<li>Resolve the argument symbol. If the passed in argument was a user defined value, we need to validate that the type of that user defined value matches the expected type of the argument we are capturing. </li>
<li>Add the new variable declaration to a list</li>
<li>Create a new method where the variable declarations we just made are prepended to the original methods body</li>
</ol>

From here on out we can treat this method just like any other regularly declared method.  The <code>MethodSymbol</code> type of the original method has a reference to its source syntax tree.  This way we can get access back to the data that this method symbol contains

[csharp]
private LambdaDeclr CreateCurriedMethod(FuncInvoke ast, MethodSymbol functionType)
{
    var srcMethod = functionType.MethodDeclr;

    var fixedAssignments = new List&lt;VarDeclrAst&gt;();

    var count = 0;
    foreach (var argValue in ast.Arguments)
    {
        var srcArg = srcMethod.Arguments[count] as VarDeclrAst;

        var token = new Token(srcArg.DeclarationType.Token.TokenType, argValue.Token.TokenValue);

        var declr = new VarDeclrAst(token, srcArg.Token, new Expr(argValue.Token));

        // if we're creating a partial using a variable then we need to resolve the variable type
        // otherwise we can make a symbol for the literal
        var newArgType = argValue.Token.TokenType == TokenType.Word ? 
                                ast.CurrentScope.Resolve(argValue).Type
                            :   ScopeUtil.CreateSymbolType(argValue);

        // create a symbol type for the target we're invoking on so we can do type checking
        var targetArgType = ScopeUtil.CreateSymbolType(srcArg.DeclarationType);

        if (!TokenUtil.EqualOrPromotable(newArgType, targetArgType))
        {
            throw new InvalidSyntax(String.Format(&quot;Cannot pass argument {0} of type {1} to partial function {2} as argument {3} of type {4}&quot;,
                argValue.Token.TokenValue, 
                newArgType.TypeName, 
                srcMethod.MethodName.Token.TokenValue, 
                srcArg.VariableName.Token.TokenValue,
                targetArgType.TypeName)); 
        }

        fixedAssignments.Add(declr);

        count++;
    }

    var newBody = fixedAssignments.Concat(srcMethod.Body.ScopedStatements).ToList();

    var curriedMethod = new LambdaDeclr(srcMethod.Arguments.Skip(ast.Arguments.Count).ToList(), new ScopeDeclr(newBody));

    SetScope(curriedMethod);

    return curriedMethod;
}
[/csharp]




]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3161</wp:post_id>
		<wp:post_date><![CDATA[2013-03-11 08:00:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-11 08:00:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[implementing-partial-functions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559795293;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3565;}i:1;a:1:{s:2:"id";i:4028;}i:2;a:1:{s:2:"id";i:2020;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Adding static typing and scope validation, part 2: type inference and validation</title>
		<link>https://onoffswitch.net/2013/03/05/adding-static-typing-and-scope-validation-part-2-type-inference-and-validation/</link>
		<pubDate>Tue, 05 Mar 2013 15:53:11 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3164</guid>
		<description></description>
		<content:encoded><![CDATA[This post continues my <a href="http://onoffswitch.net/tag/language-implementation/" target="_blank" rel="noopener noreferrer">series</a> describing how I solved certain problems while creating a <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">toy programming language</a>.  Today I'll discuss static typing and type inference.

<h2>Tracking types</h2>

To have static typing, each syntax tree node needs to track what kind of type it is.  Integers are integers, words are resolved user defined types, quoted strings are strings. But terminals are not the only nodes with types.  Each syntax trees type is derived from its terminals.  For example the expression syntax tree that represents the following:

[csharp]
(1 + 2) == 5
[/csharp]

Is actually this:

[code]
      ==
    /    \
   +      5
 /   \
1     2  
[/code] 

We have a tree that has an expression on the left, and a literal on the right. We need to know what the expression on the lefts type is before we can do anything.  Since the scope builder is depth first and each syntax tree's type is composed of its internal tree types, we'll be guaranteed that by the time we evaluate the type of the <code>==</code> tree we'll know that the left hand side is an integer type.

Even though the left hand side expression is an integer, and the right hand side literal is an integer, the expression is held together with an <code>==</code> token which means it will be a boolean type.  Anything that uses this expression further up the tree can now know that this expression is of type boolean.  This is useful info because we probably want to validate that, for example, <code>if</code> statements predicates only have boolean types. Same with <code>while</code> loops, and parts of a <code>for</code> loop, and <code>else</code>, etc.  

<h2>Type Assignments</h2>

For each expression, while resolving types, I set the current expression trees type to be derived from its branches. If we have a leaf, then either resolve the type (if it is a user defined variable) or create a type describing it:

<em>[Note: when types are resolved, how symbols are created, and solving forward references is coming in <a href="http://onoffswitch.net/adding-static-typing-and-scope-references-part-3-solving-forward-references/" target="_blank" rel="noopener noreferrer">part 3</a>]</em>

[csharp highlight="23"]
public void Visit(Expr ast)
{
    if (ast.Left != null)
    {
        ast.Left.Visit(this);
    }

    if (ast.Right != null)
    {
        ast.Right.Visit(this);
    }

    SetScope(ast);

    if (ast.Left == null &amp;&amp; ast.Right == null)
    {
        ast.AstSymbolType = ResolveOrDefine(ast);
    }
    else
    {
        if (ResolvingTypes)
        {
            ast.AstSymbolType = GetExpressionType(ast.Left, ast.Right, ast.Token);
        }
    }
}
[/csharp]

The expression visit function calls a helper method that takes the left and right trees as well as the current token

[csharp]
/// &lt;summary&gt;
/// Determines user type
/// &lt;/summary&gt;
/// &lt;param name=&quot;left&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;right&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;token&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
private IType GetExpressionType(Ast left, Ast right, Token token)
{
    switch (token.TokenType)
    {
        case TokenType.Ampersand:
        case TokenType.Or:
        case TokenType.GreaterThan:
        case TokenType.Compare:
        case TokenType.LessThan:
        case TokenType.NotCompare:
            return new BuiltInType(ExpressionTypes.Boolean);
                
        case TokenType.Method:
        case TokenType.Infer:
            if (right is MethodDeclr)
            {
                return new BuiltInType(ExpressionTypes.Method, right);
            }

            return right.AstSymbolType;
    }

    if (!ResolvingTypes &amp;&amp; (left.AstSymbolType == null || right.AstSymbolType == null))
    {
        return null;
    }

    if (!TokenUtil.EqualOrPromotable(left.AstSymbolType.ExpressionType, right.AstSymbolType.ExpressionType))
    {
        throw new Exception(&quot;Mismatched types&quot;);
    }

    return left.AstSymbolType;
}
[/csharp]

Each expression knows what kind of type it should have (based on the infix token) and can validate that its left and right branches match accordingly.  Other parts of the scope builder who use these expressions can now determine their types as well (such as function invokes, method declarations, return statements, etc) since the expression is tagged with a type.  Anything that can be used as part of another statement needs to have a type associated to it.

<h2>Type inference and validation</h2>

Type inference now is super easy.  If the left hand side is a type of <code>var</code> we don't try to do anything with it, we'll just give it the same type that the right hand side has.   For example, in the variable declaration syntax tree visitor I have a block that looks kind of like this:

[csharp]
// if its type inferred, determine the declaration by the value's type
if (ast.DeclarationType.Token.TokenType == TokenType.Infer)
{  
    ast.AstSymbolType = ast.VariableValue.AstSymbolType;

    var symbol = ScopeUtil.DefineUserSymbol(ast.AstSymbolType, ast.VariableName);

    DefineToScope(ast, symbol);
}
[/csharp]

Type validation is also easy, all we have to do is check if the right hand side is assignable to the left hand side. To type check any other (non-expression) syntax trees, like <code>print</code>, we can validate that we are expecting the right type (for <code>print</code> we don't want to print a void value).  Each tree contains its type information as the <code>AstSymbolType</code> property that has an enum describing its type that we can use for type checking.

<h2>Inferring method return types</h2>

In my language I decided I would allow functions to be declared with a <code>var</code> type inferred return type. This means I had to infer its type from its return value.  This isn't quite as easy as asking the method declaration tree where its return value is yet, since you have to go find it in the tree.  What I did to find it was, while iterating over the source tree, keep track of if I'm inside of a method.  The scope builder keeps a single heap allocated property called

[csharp]
private MethodDeclr CurrentMethod { get; set; }
[/csharp]

Each time I encounter a method declaration (either by an anonymous lambda or a class method or an inline method), I update this property. I also keep track of what the previous method was on the stack. This way as the scope builder iterates through the tree it always knows what is the current method. When a method is done iterating it'll set <code>CurrentMethod</code> to the last method it knew about (or null if there was no method it was inside of)

To help with tracking return statements, I've also added some extra metadata to the <code>MethodDeclr</code> AST so every method declaration can now access the syntax tree that represents its return statement directly:

[csharp]
public class MethodDeclr : Ast
{
    /// ...

    public ReturnAst ReturnAst { get; private set; }

    /// ...
}
[/csharp]

During the course of tree iteration, if there is a <code>return</code> statement we'll end up hitting the <code>ReturnAst</code> visit method and we can tag the current method's return statement with it:

[csharp]
public void Visit(ReturnAst ast)
{
    if (ast.ReturnExpression != null)
    {
        ast.ReturnExpression.Visit(this);

        ast.AstSymbolType = ast.ReturnExpression.AstSymbolType;

        CurrentMethod.ReturnAst = ast;
    }
}
[/csharp]

Here is my <code>MethodDeclr</code> visit method

[csharp]
public void Visit(MethodDeclr ast)
{
    var previousMethod = CurrentMethod;

    CurrentMethod = ast;

    var symbol = ScopeUtil.DefineMethod(ast);

    Current.Define(symbol);

    ScopeTree.CreateScope();

    ast.Arguments.ForEach(arg =&gt; arg.Visit(this));

    ast.Body.Visit(this);

    SetScope(ast);

    if (symbol.Type.ExpressionType == ExpressionTypes.Inferred)
    {
        if (ast.ReturnAst == null)
        {
            ast.AstSymbolType = new BuiltInType(ExpressionTypes.Void);
        }
        else
        {
            ast.AstSymbolType = ast.ReturnAst.AstSymbolType;
        }
    }
    else
    {
        ast.AstSymbolType = symbol.Type;
    }

    ValidateReturnStatementType(ast, symbol);

    ScopeTree.PopScope();

    CurrentMethod = previousMethod;
}
[/csharp]

Let's trace through it:

<ul>
<li>Lines 3 and 5.  Keep track of the previous method we came from (if any), and set the current method to point to the syntax tree we're on</li>
<li>Lines 7 and 9. Create a method symbol and define it in the current scope. This makes the method visible to anything in the same scope</li>
<li>Line 11. Create a new scope. All internal method arguments and statements are inside of their own scope</li>
<li>Lines 13 and 15. Visit the arguments and body</li>
<li>Line 17. Set the method syntax tree's scope to the current (so now it points to the current scope and can access this later).</li>
<li>Lines 19 through 29. If the method is a type inferred return type, set the methods type to be the same as the return statement types. If there isn't a return statement, make it a void.</li>
<li>Line 32. If its not a type inferred return type, set the type of the method syntax tree to be the same as the symbol we created for it.  </li>
<li>Line 35. <code>ValidateReturnStatement</code> checks to make sure that the declared type matches the return statement type. If we declared a function to return string but we are returning a user object, then that's going to generate a compiler error. >/li>
<li>Line 37. We're done with this method scope so pop the scope off the stack</li>
<li>Line 39. Reset the previous current method to the tracking property</li>
</ul>

Now we've properly validated the method declaration type with its return value, and if we needed to type inferred the method from its return statement.  So, as an example, we can support something like this:

[csharp]
[Test]
public void TestTypeInferFunctionReturn()
{
    var test = @&quot;
               
    var func(){
        return 'test';
    }
                &quot;;

    var ast = (new LanguageParser(new Lexers.Lexer(test)).Parse() as ScopeDeclr);

    var function = ast.ScopedStatements[0] as MethodDeclr;

    new InterpretorVisitor().Start(ast);

    Console.WriteLine(&quot;Inferred return type: &quot; + function.AstSymbolType.ExpressionType);

    Console.WriteLine(&quot;Original declared return expression type: &quot; + function.MethodReturnType);
}
[/csharp]

Which prints out

[code]
Inferred return type: String
Original declared return expression type: Infer: var
[/code]

<h2>Conclusion</h2>

Now the language has static typing and type inference. It doesn't have type promotion, but now that all the types are defined and propagated through the tree its easy to add.  Next I'll talk about forward references and how I solved that problem.  As always, check the <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">github</a> for full source, examples, and tests.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3164</wp:post_id>
		<wp:post_date><![CDATA[2013-03-05 15:53:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-05 15:53:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[adding-static-typing-and-scope-validation-part-2-type-inference-and-validation]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561041570;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3016;}i:1;a:1:{s:2:"id";i:2735;}i:2;a:1:{s:2:"id";i:3128;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Add scheduled task and run even if on battery power</title>
		<link>https://onoffswitch.net/2013/03/05/add-scheduled-task-and-run-even-if-on-battery-power/</link>
		<pubDate>Tue, 05 Mar 2013 22:04:26 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3217</guid>
		<description></description>
		<content:encoded><![CDATA[Just wanted to share a little helpful snippet in case anyone needs it. To add a scheduled task and make sure it starts even when on battery power do this:

[csharp]
using (var taskService = new TaskService())
{
    TaskDefinition task = taskService.NewTask();

    var action = new ExecAction
    {
        Path = &quot;test.exe&quot;,
        Arguments = &quot;&quot;,
        WorkingDirectory = &quot;&quot;
    };

    task.RegistrationInfo.Description = &quot;Test&quot;;

    var trigger = new TimeTrigger(DateTime.Now);
    trigger.Repetition.Interval = TimeSpan.FromMinutes(5);
                
    task.Triggers.Add(trigger);

    task.Actions.Add(action);

    task.Settings.DisallowStartIfOnBatteries = false;
    task.Settings.StopIfGoingOnBatteries = false;

    // Register the task in the root folder
    taskService.RootFolder.RegisterTaskDefinition(&quot;test&quot;, task, TaskCreation.CreateOrUpdate, &quot;SYSTEM&quot;, null, TaskLogonType.ServiceAccount, null);
}
[/csharp]

TaskService is part of <a href="http://taskscheduler.codeplex.com/" target="_blank" rel="noopener noreferrer"><code>Microsoft.Win32.TaskScheduler</code></a>.  

By default when you create a new task <code>DisallowStartIfOnBatteries</code> and <code>StartIfGoingOnBatteries</code> are true, so that's something to keep in mind if you are writing code that can be deployed on a laptop and you must have your scheduled task continue to run.

Quick side note, I personally think negative property names are hard to follow (<code>DisallowStartIfOnBatteries</code>). It's hard to follow when it becomes a double negative. I think it would've been much better to name the property

[code]
AllowStartOnBatteries
[/code]

Especially since it has nothing to do with starting the task when its not on batteries.  It's interesting that the UI control for this doesn't use a double negative to display the context (even though the phrasing is logically inverse). Still, case in point

<img src="http://onoffswitch.net/wp-content/uploads/2013/03/taskScheduler.-600x451.png" alt="taskScheduler." width="600" height="451" class="alignnone size-medium wp-image-3229" />
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3217</wp:post_id>
		<wp:post_date><![CDATA[2013-03-05 22:04:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-05 22:04:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[add-scheduled-task-and-run-even-if-on-battery-power]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="scheduled-tasks"><![CDATA[scheduled tasks]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561852696;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:155;}i:1;a:1:{s:2:"id";i:4394;}i:2;a:1:{s:2:"id";i:265;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Just another brainfuck interpreter</title>
		<link>https://onoffswitch.net/2013/03/06/just-another-brainfuck-interpreter/</link>
		<pubDate>Wed, 06 Mar 2013 00:36:30 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3232</guid>
		<description></description>
		<content:encoded><![CDATA[<h2>Why?</h2>

Honestly, why not? 

<h2>The entry point</h2>

Not much to tell:

[csharp]
static void Main(string[] args)
{
    var parser = new Parser(&quot;++++++++++[&gt;+++++++&gt;++++++++++&gt;+++&gt;+&lt;&lt;&lt;&lt;-]&gt;++.&gt;+.+++++++..+++.&gt;++.&lt;&lt;+++++++++++++++.&gt;.+++.------.--------.&gt;+.&gt;.&quot;);

    var instructions = parser.Instructions;

    var interpreter = new Interpreter(instructions);

    interpreter.Interpret();
}
[/csharp]


<h2>The container classes</h2>

Some data classes and enums:

[csharp]
public enum Tokens
    {
        MoveFwd,
        MoveBack,
        Incr,
        Decr,
        While,
        Print,
        Input,
        WhileEnd,
        WhileStart,
        Unknown
    }

    public class Instruction
    {
        public Tokens Token { get; set; }

        public override string ToString()
        {
            return Token.ToString();
        }
    }

    class While : Instruction
    {
        public While()
        {
            Token = Tokens.While; 
        }

        public List&lt;Instruction&gt; Instructions { get; set; } 
    }
[/csharp]

<h2>A helper function</h2>

A function to translate a character token into a known token

[csharp]
private Tokens GetToken(char input)
{
    switch (input)
    {
        case '+':
            return Tokens.Incr;
        case '-':
            return Tokens.Decr;;
        case '&lt;':
            return Tokens.MoveBack;
        case '&gt;':
            return Tokens.MoveFwd;
        case '.':
            return Tokens.Print;
        case ',':
            return Tokens.Input;
        case '[':
            return Tokens.WhileStart;
        case ']':
            return Tokens.WhileEnd;
    }
    return Tokens.Unknown;
}
[/csharp]

<h2>The parser</h2>

And the entire parser:

[csharp]
public List&lt;Instruction&gt; Instructions { get; private set; }

public Parser(string source)
{
    Instructions = Tokenize(source.Select(GetToken)
                                    .Where(token =&gt; token != Tokens.Unknown)
                                    .ToList()).ToList();
}

IEnumerable&lt;Instruction&gt; Tokenize(IEnumerable&lt;Tokens&gt; input)
{
    var stack = new Stack&lt;While&gt;();

    foreach (var t in input)
    {
        switch (t)
        {
            case Tokens.WhileStart:
                stack.Push(new While {Instructions = new List&lt;Instruction&gt;()});
                break;
            case Tokens.WhileEnd:
                if (stack.Count == 0)
                {
                    throw new Exception(&quot;Found a ] without a matching [&quot;);
                }
                if (stack.Count &gt; 1)
                {
                    var top = stack.Pop();
                    stack.Peek().Instructions.Add(top);
                }
                else
                {
                    yield return stack.Pop();
                }
                break;
            default:
                var instruction = new Instruction {Token = t};
                if (stack.Count &gt; 0)
                {
                    stack.Peek().Instructions.Add(instruction);
                }
                else
                {
                    yield return instruction;
                }
                break;
        }
    }

    if (stack.Count &gt; 0)
    {
        throw new Exception(&quot;Unmatched [ found. Expecting ]&quot;);
    }
}
[/csharp]

I took a different approach to parsing this time than usual. I didn't feel like having to deal with a consumable stream, so I linearly went through the token source. Each time I encountered a while loop start I pushed it onto the while loop stack.  Anytime I had instructions, if I had stuff in the stack, I added it to the instruction list at the top of stack. As I hit while loop end tokens (]), I popped off the stack and yielded the aggregate instruction.  

<h2>The interpreter</h2>

The interpreter is dirt simple too.  We have a 30,000 byte array of memory (per spec from wikipedia).  The rest I think is self explanatory:

[csharp]
public class Interpreter
{
    private readonly byte[] _space = new byte[30000];

    private int _dataPointer;

    private List&lt;Instruction&gt; Instructions { get; set; }
        
    public Interpreter (List&lt;Instruction&gt; instructions)
    {
        Instructions = instructions;
    }

    public void Interpret()
    {
        InterpretImpl(Instructions);
    }

    private void InterpretImpl(IEnumerable&lt;Instruction&gt; instructions)
    {
        foreach(var instruction in instructions){
            switch (instruction.Token)
            {
                case Tokens.Input:
                    _space[_dataPointer] = Convert.ToByte(Console.Read());
                    break;
                case Tokens.Incr:
                    _space[_dataPointer]++;
                    break;
                case Tokens.Decr:
                    _space[_dataPointer]--;
                    break;
                case Tokens.Print:
                    Console.Write(Encoding.ASCII.GetString(new[] { _space[_dataPointer] }));
                    break;
                case Tokens.MoveFwd:
                    _dataPointer++;
                    break;
                case Tokens.MoveBack:
                    _dataPointer--;
                    break;
                case Tokens.While:
                    while (_space[_dataPointer] != 0)
                    {
                        InterpretImpl((instruction as While).Instructions);
                    }
                    break;
            }
        }
    }
}
[/csharp]

<h2>The source</h2>

Read the <a href="https://github.com/devshorts/BrainFuckSharp" target="_blank" rel="noopener noreferrer">source</a>, Luke


]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3232</wp:post_id>
		<wp:post_date><![CDATA[2013-03-06 00:36:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-06 00:36:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[just-another-brainfuck-interpreter]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="brainfuck"><![CDATA[brainfuck]]></category>
		<category domain="post_tag" nicename="interpreter"><![CDATA[interpreter]]></category>
		<category domain="post_tag" nicename="parser"><![CDATA[parser]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[a-brainfuck-interpreter]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560277132;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:6;}i:2;a:1:{s:2:"id";i:3016;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Fixing &quot;Calling LoadLibraryEx on ISAPI filter v4.0.30319 aspnet_filter.dll failed&quot;</title>
		<link>https://onoffswitch.net/2013/03/07/fixing-calling-loadlibraryex-on-isapi-filter-v4-0-30319aspnet_filter-dll-failed/</link>
		<pubDate>Thu, 07 Mar 2013 16:22:10 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3272</guid>
		<description></description>
		<content:encoded><![CDATA[[code wraplines="true"]Calling LoadLibraryEx on ISAPI filter &quot;C:\Windows\Microsoft.NET\Framework\v4.0.30319\\aspnet_filter.dll&quot; failed[/code]

I ran into this adding a new site to my 64 bit machine, and because I haven't had my morning coffee, I forgot to set "enable 32 bit applications" in the app pool.  

If your code is built for 32 bit only (maybe you use mixed mode dll's somewhere or call into native and can't be 64 bit for whatever reason), make sure the app pool of your application is set to 32 bit mode. Otherwise, you get the very descriptive error shown above.

<img src="http://onoffswitch.net/wp-content/uploads/2013/03/iis32bit.-600x434.png" alt="iis32bit." width="600" height="434" class="alignnone size-medium wp-image-3277" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3272</wp:post_id>
		<wp:post_date><![CDATA[2013-03-07 16:22:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-07 16:22:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[fixing-calling-loadlibraryex-on-isapi-filter-v4-0-30319aspnet_filter-dll-failed]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="64-bit"><![CDATA[64 bit]]></category>
		<category domain="post_tag" nicename="iis"><![CDATA[iis]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561366663;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3814;}i:1;a:1:{s:2:"id";i:4914;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>36</wp:comment_id>
			<wp:comment_author><![CDATA[Dan]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[daniel_toomey@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[116.50.58.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-05-15 05:22:30]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-05-15 05:22:30]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks! Very helpful! :-)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>37</wp:comment_id>
			<wp:comment_author><![CDATA[sampath]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[hpmsweere@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[61.245.172.50]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-06-11 23:47:18]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-06-11 23:47:18]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the posting.. finally found the solution ;)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>38</wp:comment_id>
			<wp:comment_author><![CDATA[Nebur]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[noemailhere@noemail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[193.109.234.111]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-06-25 06:41:18]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-06-25 06:41:18]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Very useful! Thanks!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>39</wp:comment_id>
			<wp:comment_author><![CDATA[Arthur]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[arthurzc@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[194.65.37.67]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-28 21:12:41]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-28 21:12:41]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[thanks!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>40</wp:comment_id>
			<wp:comment_author><![CDATA[frinkfree]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[frinkfree@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[74.202.39.3]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-09-23 21:50:48]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-09-23 21:50:48]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Although your solution fixes the immediate problem, I think the root cause of the error is the extraneous backslash near the end of the ISAPI filter entry "C:\Windows\Microsoft.NET\Framework\v4.0.30319\\aspnet_filter.dll". Changing this value to "C:\Windows\Microsoft.NET\Framework\v4.0.30319\aspnet_filter.dll" will fix the error reported by IIS.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>41</wp:comment_id>
			<wp:comment_author><![CDATA[Nirav]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[niravm@demowork.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.conceptinfoway.net/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[119.160.193.154]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-18 10:39:14]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-18 10:39:14]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks appreciate your sharing of knowledge. It has resolved my issue.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>42</wp:comment_id>
			<wp:comment_author><![CDATA[Waleed Alsharkawy]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[ewaleed@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[212.70.33.49]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-30 07:09:36]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-30 07:09:36]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks a lot , you really save my time ,problem solved.
Have a good day and good luck]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>43</wp:comment_id>
			<wp:comment_author><![CDATA[Julian Schlotterbeck]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[julian.schlotterbeck@independis.de]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.independis.de</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[62.157.178.178]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-16 14:32:21]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-16 14:32:21]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks, was looking for this for a while! 
Helpful!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>44</wp:comment_id>
			<wp:comment_author><![CDATA[Parmesh]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[parmesh.a@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.technoxis.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[103.246.65.10]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-03-05 11:46:20]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-03-05 11:46:20]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[thanks a lot. you saved my time.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>45</wp:comment_id>
			<wp:comment_author><![CDATA[Sunsoft Infoway]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[sunsoftinfoway07@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.sunsoftinfoway.com/web-development.html</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[117.213.65.212]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-12 10:19:27]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-12 10:19:27]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[valuable and useful sharing
Thank so much]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>46</wp:comment_id>
			<wp:comment_author><![CDATA[RahulM]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[mergu.rahul@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[114.143.243.87]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-02 12:10:29]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-02 12:10:29]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks... Its works...]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>47</wp:comment_id>
			<wp:comment_author><![CDATA[Şeyda Yılmaz]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[syd.ylmz@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[195.214.144.130]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-10 12:03:30]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-10 12:03:30]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you very much!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>48</wp:comment_id>
			<wp:comment_author><![CDATA[thanks frinkfree]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[gmail@bipins.net]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[65.242.97.254]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-10 19:09:47]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-10 19:09:47]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[removing the extra slash fixed the issue. 

I needed 32bit application set to false.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>40</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>49</wp:comment_id>
			<wp:comment_author><![CDATA[Php Development Company]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[omwebtechnologies37@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.webdesigning-india.com/website-development/php-development/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[122.161.39.57]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-22 07:49:45]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-22 07:49:45]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[great post, thx for the information..]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>50</wp:comment_id>
			<wp:comment_author><![CDATA[Rickkee]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[rick.cassel@racassel.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[71.43.19.115]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-10-21 14:59:04]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-10-21 14:59:04]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[What causes that extra \\ to appear.  Just default load, did not edit it, but with .net update, the \\ appears and wacked the site.  Thanks for the answer, it worked beautiful after 14 hours of troubleshooting, should have come here first!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>51</wp:comment_id>
			<wp:comment_author><![CDATA[Tamil Sooner]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[shravan@iwcsolutions.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.227.252.34]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-03-03 19:52:03]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-03-03 19:52:03]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Worked.!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>52</wp:comment_id>
			<wp:comment_author><![CDATA[Chris]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[amarsden@eastdevon.gov.uk]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[95.131.110.108]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-03-23 11:14:59]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-03-23 11:14:59]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Many thanks - fixed my problem - but last week the site worked fine.  Any idea why (.NET update?) this would break without changing any settings?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>53</wp:comment_id>
			<wp:comment_author><![CDATA[kapil Dev]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kapil.dev8624@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[182.74.195.178]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-03-27 07:10:00]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-03-27 07:10:00]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[HTTP Error 500.0 - Internal Server Error
Calling LoadLibraryEx on ISAPI filter "C:\Windows\Microsoft.NET\Framework\v4.0.30319\\aspnet_filter.dll" failed

Thanks to solve this issues 
multipal slash issues
Framework64\\v4.0.30319\aspnet_filter.dll    To Framework64\v4.0.30319\aspnet_filter.dll 


Solutions:
Hi All,

I got this error, before you do any above stuff, just try below

1. Click on your computer icon in IIS

2. On right side, you see ISAPI filters - Click

3. Pick up the Filter name ASP.Net_4.0_64bit  and its path MIGHT HAVE AN EXTRA \ IN THE PATH,

SO UNABLE TO LOAD THIS DLL,

 %windir%\Microsoft.NET\Framework64\\v4.0.30319\aspnet_filter.dll

4. restart IIS - should resolve the error.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>54</wp:comment_id>
			<wp:comment_author><![CDATA[Haftom G.]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[haftom.gbrmskl@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[197.156.100.49]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-14 08:30:17]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-14 08:30:17]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you very much Anton Kropp and really solved.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>55</wp:comment_id>
			<wp:comment_author><![CDATA[c]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[throneofmight@yahoo.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://xxx</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[192.55.54.40]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-08-25 20:36:23]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-08-25 20:36:23]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the BKM!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>56</wp:comment_id>
			<wp:comment_author><![CDATA[Muhammad Rafi Qureshi]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[mrafi127@yahoo.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[182.181.145.245]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-09-05 10:36:57]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-09-05 10:36:57]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks Sir, its working super fine..]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>57</wp:comment_id>
			<wp:comment_author><![CDATA[Ravinder Singh]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[ctsinghs@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[167.220.236.27]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-12-31 12:56:52]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-12-31 12:56:52]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks a lot .....:)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>58</wp:comment_id>
			<wp:comment_author><![CDATA[Harjeet]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[harjeetpabla2888@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[220.227.15.70]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-25 06:48:55]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-25 06:48:55]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Awesome , its work]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>40</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>59</wp:comment_id>
			<wp:comment_author><![CDATA[vinay]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[venay.maurya@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[170.251.175.165]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-04-12 10:25:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-04-12 10:25:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank you very much... resolved my issue ... :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>60</wp:comment_id>
			<wp:comment_author><![CDATA[Guest]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[Guest@test.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[194.67.29.119]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-04-16 13:59:34]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-04-16 13:59:34]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is the best solution!
It's simple. And genius!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>40</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>61</wp:comment_id>
			<wp:comment_author><![CDATA[Martin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[martin.moser@emineo.ch]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[217.147.217.161]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-07-13 13:02:07]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-07-13 13:02:07]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[OMG - Thank you very much!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech talk: Service stack</title>
		<link>https://onoffswitch.net/2013/03/08/tech-talk-service-stack/</link>
		<pubDate>Fri, 08 Mar 2013 16:25:48 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3285</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk the team and I talked about <a href="http://www.servicestack.net/" target="_blank" rel="noopener noreferrer">ServiceStack</a>. I've heard a lot of hype about it but never really understood what it did or was about. Today, unfortunately, didn't really clear any of that up.

We watched the ServiceStack powerpoint on their site and went to their <a href="https://github.com/ServiceStack/ServiceStack" target="_blank" rel="noopener noreferrer">github</a> to look at the examples. What I got out of all of that is that ServiceStack  is trying to simplify the creation of service oriented architectures (SOA) by providing a complete stack architecture for you to use (an ORM, backing cache/NoSql, routes, serialization, etc). 

The confusion, however, lays in the fact that most other solutions are doing nearly the same thing. MVC has routes and web-api's with GET/POST/PUT/DELETE methods. WCF exposes transport independent RPC. Node.js has routes with auto fill parameters. Everyone is using JSON serialization.  Redis is common as a caching layer. A bunch of ORM's already exist for SQL. Enforcement of lightweight DTO's.  I couldn't really tell what made ServiceStack so much better than anything else.  

ServiceStack boasts of a faster ORM then NHibernate and Entity Framework, as well as the fastest .NET JSON serializer, but the comparison charts were almost 3 years old (a lifetime in technology!).  I did see that ServiceStack was less verbose to get set up making a web-api than MVC4, but the code looked pretty similar. If you rolled an app using any ORM, MVC or WCF, created your own DTO's, and sent everything over JSON then I think you basically have ServiceStack?  Frankly I'm as confused as ever.

The team, I think, feels the same way I do. We didn't have a very spirited conversation because everyone was a little lost as to what the significance of ServiceStack was.  We came out of it thinking that maybe to appreciate ServiceStack's offerings better that we should have a hack day to build a small service oriented application in node.js, MVC4, IIS hosted WCF, and ServiceStack.  

But, from my few hours of looking at the examples I'm not really sold on what is so great about it. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3285</wp:post_id>
		<wp:post_date><![CDATA[2013-03-08 16:25:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-08 16:25:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-service-stack]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="servicestack"><![CDATA[servicestack]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560613195;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:3696;}i:2;a:1:{s:2:"id";i:4939;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Mongoose with TypeScript</title>
		<link>https://onoffswitch.net/2013/03/25/mongoose-with-typescript/</link>
		<pubDate>Mon, 25 Mar 2013 08:00:23 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3344</guid>
		<description></description>
		<content:encoded><![CDATA[Mongoose is a library for node.js that wraps the mongoDB driver.  Since I've been playing with typescript, I wanted to show a short demo of strongly typing mongoose with unit tests run in nodeunit all using typescript.  

<h2>Definitions</h2>

First, I have a collection of definition files that represent mongoose types, nodeunit types, and my own document types (in schemaDef.d.ts).  

<code>/def/all.d.ts</code>

[ts]
///&lt;reference path=&quot;./mongoose.d.ts&quot;/&gt;
///&lt;reference path=&quot;./nodeUnit.d.ts&quot;/&gt;
///&lt;reference path=&quot;./schemaDef.d.ts&quot;/&gt;
[/ts]


The nodeunit definitions <code>/def/nodeUnit.d.ts</code>

[ts]
interface ITest{
    done(): void;
    ok(isGood:Boolean, message?:string):void;
    equal(expected:any, actual:any, message?:string);
}
[/ts]

Here are the basic mongoose definitions in <code>/def/mongoose.d.ts</code>.  I can't guarantee that these types are right, I'm updating them as I go along.  I'm inferring the structure from the documentation and personal experimentation.  As I encounter new mongoose definitions, I can just add them to the appropriate scope.  You can see that I'm also chaining definitions: one definitions functions might return another interface that has other definitions.  This makes it really easy to model the fluent api that mongoose exposes.

[ts]
interface ICallback{
    callback(error:string, item:any): void;
}

interface IEmptyCallback{
    callback() : void;
}

interface IErrorCallback{
    callback(item:string) : void;
}

interface IWhere{
    equals(value:String):IChainable;
    gt(value:String):IChainable;
    lt(value:String):IChainable;
    in(value:String[]):IChainable;
}

interface IChainable{
    exec(item:ICallback) : IChainable;
    populate(...args: any[]) : IChainable;
    select(query:string):IChainable;
    limit(num:Number):IChainable;
    sort(field:String):IChainable;
    where(selector:String):IWhere;
}

interface IMongooseSearchable{
    findOne(item:any, callback:ICallback) : void;
    find(id:string, callback?:ICallback) : IChainable;
    find(propBag:Object, callback?:ICallback) : IChainable;
    remove(item:any, callback:IErrorCallback) : void;  
}

interface IMongooseBase {   
    save(item: IEmptyCallback) : void;
    push(item:IMongooseBase):void;
}
[/ts]

Here is my test schema definition <code>/def/schema.d.ts</code>.  Just for the example I only have a user with a name and an id.

[ts]
interface IUser extends IMongooseBase{
    _id: string;
    name: string;
}
[/ts]

<h2>Modeling the Schema</h2>

First, I need to model the schema using mongoose.  I have a couple of run once variables that are part of the module export. These are things that are not enclosed with an export tag.  They're not global since CommonJS encapsulates everything in a module, but they will run once when the module is created.  This makes it easier to reference in the dbclass.  

To create data with mongoose you need to first instantiate a schema with a property bag. This gives you an instance that you pass to the mongoose model function. The model function gives you a function reference back that you can use to create new model objects.  Since we need to be able to new up objects, we can cast it to an anonymous object that has a <code>new()</code> function that returns the correct type we want (IUser).  Thanks to Bill Ticehurst for answering <a href="http://stackoverflow.com/a/15399536/310196" target="_blank" rel="noopener noreferrer">my question</a> regarding how this is done. The target type for the user function reference is of <code>IMongooseSearchable</code> because we'll be able to do searching/querying on that model using this.  

I've also wrapped the creation of a new user in a helper function that does the appropriate casting that I need. This way, later, I can just call "db.newUser()" to create a new model object without having to worry about casting outside of the db class.

[ts]
///&lt;reference path='../def/all.ts'/&gt;
var mongoose:any = require(&quot;mongoose&quot;);

// Need to provide the same structure in 'mongoose' style format to define.
var userSchema = new mongoose.Schema(
    {  
       name: String
    });


export var User:IMongooseSearchable = &lt;{ new() : IUser; }&gt;mongoose.model('User', userSchema);

export class db{
    init(dbName:string, ignoreFailures:bool){
        if(dbName == null){
            dbName = &quot;test&quot;;
        }

        try{
            mongoose.connect('localhost', dbName);
        }
        catch(e){
            if(!ignoreFailures){
                throw e;
            }
        }
    }

    disconnect(){
        mongoose.disconnect();
    }

    newUser():IUser{
        return &lt;IUser&gt;new User();
    }
}
[/ts]

<h2>Testing</h2>

And now to test it all with with NodeUnit.  Tests are run top to bottom so we can create a test fixture setup as the first function. The downside to this is that it counts as a test itself, but the upside is that we can do a single setup and teardown per group.  NodeUnit lets you define setup and teardown for each test as well (but I'm not showing it here) by exposing <code>setUp</code> and <code>tearDown</code> methods on your exported group object.

As an example, my one query test shows a mongoose save, basic find, and a more complex find with a filter.  Notice that the user object has a save method on it (because it inherits from <code>IMongooseBase</code>, but to query the users we have to reference the function that was given to us by mongoose that represents the schema.  From typescripts perspective though, this is all strongly typed since we've hidden away the casting and weirdness inside of the db class. 

I've also used the definitions I made for nodeunit to get a strongly typed nodeunit test object.  If I needed more properties for assertions I could add them to the definitions file and they'd be available in typescript for me.

[ts]
///&lt;reference path=&quot;../def/all.d.ts&quot;/&gt;
///&lt;reference path=&quot;../storage/schema.ts&quot;/&gt;

import schema = module(&quot;../storage/schema&quot;);

var storage = new schema.db();

export var group = {
    init: (t:ITest) =&gt;{
        storage.init(&quot;test&quot;);
        t.done();
    },

    test: (t:ITest) =&gt;{
        var u = storage.newUser();

        u.name = &quot;test&quot;;

        u.save(()=&gt; {
            schema.User.findOne(u._id, (err, user) =&gt; {
                console.log(user.name);

                schema.User.find(u._id)
                    .where(&quot;_id&quot;).equals(u._id)
                    .exec((err, u1) =&gt; {
                        console.log(u1);

                        t.equal(u1[0].name, u.name);
                        t.done();
                    });
            });
        });
    },

    end: (t:ITest) =&gt;{
        storage.disconnect();
        t.done();
    }
}
[/ts]

<h2>Conclusion</h2>

When working with typescript I found myself frequently checking the compiled javascript to make sure it's what I actually wanted it to be. I found this to workflow to work well when using code that you are typing yourself.  This way you can validate that what is compiled makes sense.

<h2>edit:</h2>

My schema def's have changed since the posting of this post: for my up to date mongoose schema definitions check my <a href="https://github.com/devshorts/trakkit/blob/master/def/mongoose.d.ts">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3344</wp:post_id>
		<wp:post_date><![CDATA[2013-03-25 08:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-25 08:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[mongoose-with-typescript]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="js"><![CDATA[JavaScript]]></category>
		<category domain="post_tag" nicename="mongoose"><![CDATA[mongoose]]></category>
		<category domain="post_tag" nicename="typescript"><![CDATA[typescript]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561781734;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3295;}i:1;a:1:{s:2:"id";i:4028;}i:2;a:1:{s:2:"id";i:3452;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Merging two immutable dictionaries in F#</title>
		<link>https://onoffswitch.net/2013/03/18/merging-two-dictionaries-in-f/</link>
		<pubDate>Mon, 18 Mar 2013 21:20:47 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3367</guid>
		<description></description>
		<content:encoded><![CDATA[If you ever need to merge two immutable dictionaries (maps) that may share the same key, here is how I did it

[fsharp]
let mapMerge group1 group2 appender = 
    group1 |&gt; Seq.fold(fun (acc:Map&lt;'a,'b&gt;) (KeyValue(key, values)) -&gt; 
                      match acc.TryFind key with
                                        | Some items -&gt; Map.add key (appender values items) acc
                                        | None -&gt; Map.add key values acc) group2

// for example, assume map1 and map2 are dictionaries of type
// Map&lt;string, seq&lt;string&gt;&gt;
// then the appender deals with how to merge duplicate keys
let joinMaps = mapMerge map1 map2 Seq.append
[/fsharp]

It doesn't matter which group you treat as the source and which group you treat as the seed since you are creating a new dictionary out of the two. By pre-seeding the fold with one of the dictionaries you know that the accumulator will already have some of the values you want. The map will iterate over the second dictionary (used as the source).  All you need to do is pull out the data from the dictionary if it exists, and if so return a new dictionary that adds the elements to the accumulator again.  By injecting a custom "key conflict" resolver you can merge any kind of item.  For my example, I have a map of type 

[fsharp]
Map&lt;string, seq&lt;string&gt;&gt;
[/fsharp]

Which is why I injected the <code>Seq.append</code> function. The types don't need to be specified in the sequence fold, I just put them there to be explicit for the post.

<code>KeyValuePair</code> is a built in discriminated union that you can use to break up a key value pair tuple that the map iterator gives you.

After figuring this out I realized I could have saved myself a little bit of time if I had just googled it. If I had done that I would've found this <a href="http://stackoverflow.com/questions/3974758/in-f-how-do-you-merge-2-collections-map-instances" target="_blank" rel="noopener noreferrer">great question</a> on stackoverflow, which shows a few other methods and basically exactly what I have (though not all preserve key conflicts).  Hey, great minds think alike, right? 
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3367</wp:post_id>
		<wp:post_date><![CDATA[2013-03-18 21:20:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-18 21:20:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[merging-two-dictionaries-in-f]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="utilities"><![CDATA[Utilities]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560791954;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3536;}i:1;a:1:{s:2:"id";i:4365;}i:2;a:1:{s:2:"id";i:3847;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>62</wp:comment_id>
			<wp:comment_author><![CDATA[abbas shojaee]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[abbas.shojaee@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[130.132.173.194]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-09 22:12:46]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-09 22:12:46]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[indeed]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech talk: Javascript Memory Leaks and JSWhiz</title>
		<link>https://onoffswitch.net/2013/03/21/tech-talk-javascript-memory-leaks-and-jswiz/</link>
		<pubDate>Thu, 21 Mar 2013 18:12:30 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3435</guid>
		<description></description>
		<content:encoded><![CDATA[Todays tech talk revolved around the recently published <a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/40738.pdf" target="_blank" rel="noopener noreferrer">JSWhiz whitepaper</a> from google.  The paper discusses common javascript memory leak patterns. It also goes over how those leaks can be created and how google automated detection of them using <a href="https://developers.google.com/closure/" target="_blank" rel="noopener noreferrer">Closure</a> type annotations.

<h2>JSWhiz</h2>

First Google identified common javascript memory leak patterns, though some of these are described in terms of Closure syntax

<ul>
<li>Create without dispose EventHandler - "<em>Closure objects of type EventHandler provide a simple mechanism to group all events and listeners associated with an object together. This allows for easy removal of listeners when the events being listened to can no longer ﬁre. Unfortunately this is also one of the main sources of leaks</em>"</li>
<li>Setting member to null without removing event listeners. "<em>In JavaScript setting a variable to null is the idiomatic way of providing a hint to the garbage collector that the memory allocated by an object can be reclaimed. But the EventHandler and event listeners attached to the object prevents the garbage collector from disposing of the object and reclaiming memory.</em>" </li>
<li>Undisposed member object has EventHandler as member</li>
<li>Object graveyard (holding onto objects in a persistent structure like a list/dictionary so objects can't ever get collected)</li>
<li>Overwriting EventHandler ﬁeld in derived class. - "<em>Closure implements inheritance using a prototype-based approach [8], which can break the expectations of programmers coming from a more standard OOP language background (such as C++ or Java). For EventHandlers this can cause memory leaks as the overwritten EventHandler cannot be freed [8].</em>"</li>
<li>Unmatched listen/unlisten calls. - "<em>The semantics of the listen and unlisten calls require all parameters to be the same. When the parameters do not match, the event listener is not removed and a reference to objects being listened remains, inhibiting the garbage disposal.</em>"</li>
<li>Local EventHandler - "<em>A local EventHandler instance that does not escape scope, e.g., a locally deﬁned variable that is not assigned to a ﬁeld member, added to an array, or captured in a closure, can not be disposed of later.</em>"</li>
</ul>

<h2>How</h2>

Google's engineers leveraged the fact that they had an annotated AST of javascript (with the Closure compiler) to look these particular patterns and help identify potential leaks.  It's not always as easy as it sounds though, as the paper mentions in one section

<blockquote>Doing the analysis in a ﬂow-insensitive manner is equal to assuming that an eventful object is disposed of if there exists a disposal instruction in the AST tree. This assumption is further necessitated by 1) the dynamic nature of JavaScript, 2) most disposals resulting from runtime events (user actions, that may or may not occur) and 3) the computational demands associated with full program/inter-procedural analysis.</blockquote>

Other pitfalls of their current analysis include

<ul>
<li>Not tracking objects with fully qualified names (such as application.window.toolbar)</li>
<li>Objects that are never returned</li>
<li>Objects not captured in closures</li>
<li>Not checking of double disposal</li>
</ul>

Still, it's an impressive feat.

<h2>Discussion</h2>

After running through the paper we started talking about different garbage collection issues with javascript.  One of the big problems people have encountered is that since the DOM and javascript use reference counting you can easily get yourself into <a href="http://www.ibm.com/developerworks/web/library/wa-memleak/" target="_blank" rel="noopener noreferrer">circular references</a> which would prevent collection.  We talked a bit about generational collection since that's how the <a href="https://developers.google.com/v8/design#garb_coll" target="_blank" rel="noopener noreferrer">V8 engine</a> that chrome and node.js use, and how that can solve circular references by doing path from root detection.  

A couple other neat points came up, such as that the <code>delete</code> keyword <a href="http://buildnewgames.com/garbage-collector-friendly-code/" target="_blank" rel="noopener noreferrer">doesn't actually delete memory</a>. It only deletes properties of an object.  Just to double check that we found ourselves at the <a href="https://developer.mozilla.org/en-US/docs/JavaScript/Reference/Operators/delete" target="_blank" rel="noopener noreferrer">mozilla dev site</a> which had this hilariously appropriate quote on the page for the <code>delete</code> keyword

<blockquote>The delete operator removes a property from an object. Unlike what common beliefs suggests, the delete operator has nothing to do with directly freeing memory (it only does indirectly via breaking references.</blockquote>

<h2>Conclusion</h2>

In the end we decided that these memory leak patterns aren't strictly related to javascript. Lots of the same issues occur with actionscript and other languages (even .NET), where you have strong references in the form of event handlers or closures and things can never be garbage collected.  

The ultimate way to avoid memory leaks is to be cognizant of who uses what, who holds on to what, and what needs to be removed when.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3435</wp:post_id>
		<wp:post_date><![CDATA[2013-03-21 18:12:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-03-21 18:12:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-javascript-memory-leaks-and-jswiz]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="garbage-collection"><![CDATA[garbage collection]]></category>
		<category domain="post_tag" nicename="js"><![CDATA[JavaScript]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558691246;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2635;}i:1;a:1:{s:2:"id";i:4394;}i:2;a:1:{s:2:"id";i:3710;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Advice to young engineers</title>
		<link>https://onoffswitch.net/2013/04/03/advice-young-engineers/</link>
		<pubDate>Wed, 03 Apr 2013 23:27:17 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3477</guid>
		<description></description>
		<content:encoded><![CDATA[I had the opportunity to represent the company I work for at an engineering networking event at the University of Maryland today catered to young engineering students of all disciplines.  The basic idea was to be available for students to ask questions they don't normally get to ask of working professionals such as "<em>what's the day to day like?</em>" [lots of coffee, followed by coding all day], "what advice would you give to someone looking to get into xyz field", etc.

Personally I had a great time being there since as an alum I felt like I could relate to their specific college experience. In this post, I wanted to share a couple of the main points that came up today during my informal discussions with the students.  

<h2>Don't be afraid of problems</h2>

I really wanted to stress this to the people I talked to today.  You can't anticipate every problem you will face in the technical world, and the only real way to succeed in a career is to accept that. The trick is, though, is to know just enough to be able to find the information you want.  If you can't find the info you want, ask someone! Unlike school, group work is encouraged.  On top of that, the things you learn in school won't prepare you for all the real world things you will encounter.  All a good education really gives you is the toolset to help you find the information you need.

Not being afraid of problems means you won't freeze and give up when you're faced with what seems like an insurmountable issue. Break things down into smaller sets; do some research.  Eventually you'll find a solution, or at least be more informed as to why you can't solve a certain problem and hopefully have learned from it.  

Remember, nobody knows the answer to everything, and if they say they do they are lying.

<h2>Work with people you like</h2>

Almost 5/7 of the week (and sometimes more) is spent with a bunch of people at work. If you don't like who you work with that's a problem.  I think recent graduates don't realize that at an interview the interviewer should be selling themselves to the candidate just as much as the candidate to the interviewer.  It has to be a good match, both professionally and personally.  If you come out of an interview and feel like you just talked to the weirdest most uncomfortable person ever, don't work there!  It's natural to be afraid of saying no to a job that was offered, especially when you are starting out.  But, if you can afford to, it's good to be picky.  The people you work with can make all the difference between a place you consider a "job" and a place where you get to practice your hobby all day long and get paid for it.

On top of that, don't work at a place where you won't feel challenged. If you can find a mentor at that place that's even better, because guided growth (especially in the beginning of a career) is invaluable.  

Also, don't worry about any stigma of jumping ship early. Leaving a job after a year isn't a bad thing if it's not a right fit.  Find somewhere else to work. Engineering is a field that is in demand right now, but it's also extremely competitive and constantly changing. The only way to be competitive is to always be learning.

<h2>Interests matter</h2>

For me, when I'm conducting interviews, what really sets people apart is their level of enthusiasm and interest.  You can be the best engineer in the world, but if you don't care about what you work on, or your field, you won't do a good job.  Being enthusiastic about the field you are in is important. If you care about what you do, whether its computer engineering, or biological engineering, or whatever, you should have personal projects you can show. Even just showing you've gone above and beyond basic classwork and done research or internships in an area goes a long way.

I don't think it matters how big or small the personal projects are, what matters is you spent the time independently to do them. People frequently suggest contributing to open source projects, and that's great, if you have time. But if not, small personal projects also show interest and a real drive to learn and do better. 

<h2>Engineering is fun as hell</h2>

I could spend all day doling out advice, but I only had about an hour with the students. In the end, while sometimes the engineering field can be wrought with roadblocks, if you can get past them it's super fun and gratifying to build stuff that works.  Sometimes as a student it's hard to see how all the pieces fit, but they do, and if you persevere through it a career in engineering can be extremely satisfying.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3477</wp:post_id>
		<wp:post_date><![CDATA[2013-04-03 23:27:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-03 23:27:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[advice-young-engineers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="careers"><![CDATA[careers]]></category>
		<category domain="post_tag" nicename="engineering"><![CDATA[engineering]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[advice-young-egineers]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[career-advice-people-starting]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558607516;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4244;}i:1;a:1:{s:2:"id";i:3295;}i:2;a:1:{s:2:"id";i:3524;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>73</wp:comment_id>
			<wp:comment_author><![CDATA[Career Advice for Young Engineers | Automationtechies]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://automationtechies.com/career-advice-for-young-engineers/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[45.56.98.181]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-21 08:50:36]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-21 08:50:36]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Advice to Young Engineers Anton Kropp via Onoffswitch.net [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>74</wp:comment_id>
			<wp:comment_author><![CDATA[Automationtechies]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[mcarty@automationtechies.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.automationtechies.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[173.165.225.141]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-21 21:07:24]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-21 21:07:24]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[We've compiled a list of great career advice for young engineers (found here: http://bit.ly/YoungEngs) and have included your blog post. Thanks for the good points! We hope our engineers find them helpful too!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Users by connections in SignalR</title>
		<link>https://onoffswitch.net/2013/05/13/users-by-connections-in-signalr/</link>
		<pubDate>Mon, 13 May 2013 08:00:09 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=2365</guid>
		<description></description>
		<content:encoded><![CDATA[SignalR gives you events when users connect, disconnect, and reconnect, however the only identifying piece of information you have at this point is their connection ID. Unfortunately it's not very practical to identify all your connected users strictly off their connectionIDs - usually you have some other identifier in your application (userID, email, etc).  

If you are using ASP.NET MVC3, you can access this information from the hub context via <a href="http://stackoverflow.com/questions/7289724/how-can-i-access-the-logged-in-user-from-outside-of-a-controller" target="_blank" rel="noopener noreferrer"><code>Context.User</code></a>, but if you aren't using mvc3 (like a .net to .net client) a good workflow is to have your client identify themselves on connect.  They can call a known <code>Register</code> method on the hub and give you the identifying information of who they are. 

At this point you have your unique identifier along with their connectionID, but you have to manage all their disconnections, reconnections, and multiple connections of the same client yourself.  This post will go through an easy way to manage this client information yourself.

<h2>The Hub</h2>

I like to keep the hub code simple and use it only as an incoming facade. I hand off every request to an internal singleton that I can then leverage and put all the business logic in. This keeps the hub clean.  

[csharp]
[HubName(&quot;hub&quot;)]
public class ServerHub : Hub, IConnected, IDisconnect
{
    private HubProcessor Instance
    {
        get { return HubProcessor.Instance; }
    }

    public ServerHub()
    {
        Instance.Initialize(); 
    }

    public void Register(HubClientPayload payload)
    {
        Instance.Register(payload, Context.ConnectionId);
    }

    public Task Connect()
    {
        return Instance.Connect(Context.ConnectionId); 
    }

    public Task Reconnect(IEnumerable&lt;string&gt; groups)
    {
        return Instance.Reconnect(Context.ConnectionId); 
    }

    public Task Disconnect()
    {
        return Instance.Disconnect(Context.ConnectionId); 
    }
}
[/csharp]

<h2>Hub Processor</h2>

I've put all the business logic of the hub processing into a hub processor singleton that all instances of the hub can access.  When a hub is initialized it calls an Initialize function on the singleton which is only used to force lazy creation on the singleton:

[csharp]
public class HubProcessor : HubProcessorBase&lt;AusUpdaterHub&gt;
{
    #region Data
    
    private static object _lock = new object();
    
    #endregion

    #region Singleton and Constructor

    private static readonly Lazy&lt;HubProcessor&gt; _instance = new Lazy&lt;HubProcessor&gt;();

    public static HubProcessor Instance
    {
        get { return _instance.Value; }
    }

    public void Initialize()
    {
        // force creation of lazy constructor
    }

    #endregion

    // ... business logic handed off from the hub 
}
[/csharp]

The HubProcessor inherits from a base class which only exposes a helper method to get the context for a current hub. This is so we can re-use the base class elsewhere, or if we want to create our own <a href="https://github.com/blinemedical/SignalRToAs3" target="_blank" rel="noopener noreferrer">hub context wrappers</a> we can do that in the base class without affecting how the hub treats a client.

[csharp]
public class HubProcessorBase&lt;T&gt; : IDisposable where T: Hub
{
    protected IHubContext Context
    {
        get
        {
            return GlobalHost.ConnectionManager.GetHubContext&lt;T&gt;();
        }
    }

    protected override void Dispose()
    {
        // for inheritance
    }
}
[/csharp]

<h2>Connect</h2>

When a client connects, we don't know anything about them other than their connection ID.  If we're not using MVC3 then we'll need the client to tell us who they are and give us some meaningful information.  The expectation is that they will register themselves when they successfully connect (which they can know about client side).  

[csharp]
/// &lt;summary&gt;
/// Called by a client when they connect and register
/// &lt;/summary&gt;
/// &lt;param name=&quot;payload&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;connectionId&quot;&gt;&lt;/param&gt;
public void Register(HubClientPayload payload, string connectionId)
{
    try
    {
        lock (_lock)
        {
            List&lt;String&gt; connections;
            if (_registeredClients.TryGetValue(payload.UniqueID, out connections))
            {
                if (!connections.Any(connection =&gt; connectionID == connection))
                {
                    connections.Add(connectionId);
                }
            }
            else
            {
                _registeredClients[payload.UniqueID] = new List&lt;string&gt; { connectionId };
            }
        }
    }
    catch(Exception ex)
    {
        Log.Error(this, &quot;Error registering on hub&quot;, ex);
    }
}
[/csharp]

When a client registers on the hub, the hub passes the input argument (the client payload, which contains unique identifying information) as well as the connectionID to the hub processor.  Now we have a thread safe dictionary that tracks the users unique identifier along with all associated connectionIDs.  This way if the same user is open in multiple tabs, or across multiple .net clients, we can have a central list of connectionIDs to act on.

<h2>Disconnect</h2>

When a client disconnects we'll execute the disconnect function on the singleton which will remove the connection from the connected client list 

[csharp]
/// &lt;summary&gt;
/// Invoked by SignalR when a disconnection is detected
/// &lt;/summary&gt;
/// &lt;param name=&quot;connectionId&quot;&gt;&lt;/param&gt;
public Task Disconnect(string connectionId)
{
    try
    {
        lock (_lock)
        {
            var connections = _registeredClients.Where(c =&gt; c.Value.Any(connection =&gt; connection == connectionId)).FirstOrDefault();

            // if we are tracking a client with this connection 
            // remove it
            if (!CollectionUtil.IsNullOrEmpty(connections.Value))
            {
                connections.Value.Remove(connectionId);

                // if there are no connections for the client, remove the client from the tracking dictionary
                if (CollectionUtil.IsNullOrEmpty(connections.Value))
                {
                    _registeredClients.Remove(connections.Key);
                }
            }
        }
    }
    catch(Exception ex)
    {
        Log.Error(this, &quot;Error on disconnect in hub&quot;, ex);
    }

    return null;
}
[/csharp]


<h2>Reconnections</h2>

Now, what happens if our server goes down but clients are still up? When they come online they'll do a reconnect, not an initial connect.  When clients reconnect we should just invoke back to them to re-register themselves. This way we can quickly rebuild our tracker dictionary of who is out there.  You might want to persist the dictionary and validate who is still connected by the reconnection message, but what happens if a client is slow to reconnect? At what point do we invalidate disconnected clients? I think it's safer to have everyone re-register. You can obviously throttle this by having the re-registration synchronized or batched off if you have a huge number of connected clients.

[csharp]
/// &lt;summary&gt;
/// Invoked by SignalR when a client reconnects to the server
/// &lt;/summary&gt;
/// &lt;param name=&quot;connectionId&quot;&gt;&lt;/param&gt;
public Task Reconnect(string connectionId)
{
    try
    {
        Context.Clients[connectionId].reRegister();
    }
    catch(Exception ex)
    {
        Log.Error(this, &quot;Error re-connecting on hub&quot;, ex);
    }

    return null;
}
[/csharp]

<h2>Invoking</h2>

At this point we have a dictionary keyed off our internal user unique identifier. To do any work all we have to do is lock the registeredClients dictionary, get the list of connectionID's associated to who we want, and execute them on the context. For example:

[csharp]
private void SendTextToUser(string uniqueID, string text)
{
    DispatchToClient(connection =&gt; connection.sendText(text), uniqueID);
}

/// &lt;summary&gt;
/// Execute lambda for each connection associated to a client's unique ID
/// &lt;/summary&gt;
/// &lt;param name=&quot;action&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;uniqueID&quot;&gt;&lt;/param&gt;
private void DispatchToClient(Action&lt;dynamic&gt; action, string uniqueID)
{
    foreach (dynamic connection in GetConnections(uniqueID))
    {
        action(connection);
    }
}

private List&lt;dynamic&gt; GetConnections(string uniqueID)
{
    var connections = new List&lt;dynamic&gt;();
    lock (_lock)
    {
        connections = (from client in _registeredClients
                       let clientID = client.Key
                       let clientConnections = client.Value
                       where clientID == uniqueID
                       from connection in clientConnections
                       select Context.Clients[connection]).ToList();
    }
    return connections;
}
[/csharp]


<h2>Conclusion</h2>

The nice thing about having things set up this way is that as long as there are some connections associated to a user we know that that user is active.  

Disconnects, depending on what long polling transport mechanism SignalR chooses for the client, can sometimes come after a short timeout.  This means that a user can disconnect, and then create a new connection. Maybe they closed the browser window, then opened it back up again. In this scenario for a short time we'll have two connections, but only one is an actual valid connection for the client.  We won't know that the first one disconnected until the timeout expires. But, that's ok, since the dictionary guarantees you can get to at least one connection for the active client. After a short period of time the disconnect message will happen and we can do a cleanup.  

While SignalR does great job of letting broadcast info based on connectionID, we sometimes want to have our own collections pairing connectionIDs to other client identifying information.  With some simple client to server invocations and a threadsafe dictionary we can keep track of all the relevant information stored in an easy to use fashion.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>2365</wp:post_id>
		<wp:post_date><![CDATA[2013-05-13 08:00:09]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-13 08:00:09]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[users-by-connections-in-signalr]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dotnet"><![CDATA[.NET]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="signalr"><![CDATA[SignalR]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561793453;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3392;}i:1;a:1:{s:2:"id";i:4116;}i:2;a:1:{s:2:"id";i:155;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>63</wp:comment_id>
			<wp:comment_author><![CDATA[Arthur]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[arthur.kater@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[1.171.55.77]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-05-27 09:42:45]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-05-27 09:42:45]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice article, but it would be so much better if there was a full source code. There are parts of the code (Connect, data types) that are not present and a bit hard to figure out.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>64</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[206.169.195.21]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-05-28 17:14:43]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-05-28 17:14:43]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Arthur, the only thing that isn't posted is the HubClientPayload which can be whatever you want.  It's just an object that you can pass as a json payload to your service and it represents a DTO.  I seperated the payload from the connectionID passing it to the underlying processor]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>63</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>65</wp:comment_id>
			<wp:comment_author><![CDATA[Brenton Unger]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[brenton.unger@live.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.10.217.87]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-08 20:49:42]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-08 20:49:42]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Agree with Arthur...for example...where is _registeredClients defined? What is it's type?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>66</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.235.7.161]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-09 17:20:23]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-09 17:20:23]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[It's a dictionary of guid - list<string>. It's just keeping g track of which user is logged in with which connection ids.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>65</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>67</wp:comment_id>
			<wp:comment_author><![CDATA[Carlos]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[carlos.lopes@sis-france.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[82.236.231.202]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-29 09:05:14]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-29 09:05:14]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Merci,
Très bel article.
Juste une question 
HubProcessorBase ne devrait-il pas être HubProcessorBase ?
carlos]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>68</wp:comment_id>
			<wp:comment_author><![CDATA[Carlos]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[carlos.lopes@sis-france.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[82.236.231.202]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-29 09:08:31]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-29 09:08:31]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[petit problème avec les ""
public class HubProcessor : HubProcessorBase 'AusUpdaterHub'

HubProcessorBase (AusUpdaterHub ) ne devrait-il pas être HubProcessorBase(ServerHub) ?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>69</wp:comment_id>
			<wp:comment_author><![CDATA[Carlos]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[carlos.lopes@sis-france.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[82.236.231.202]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-29 10:34:17]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-29 10:34:17]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[??
if  (  ! CollectionUtil.IsNullOrEmpty(connections.Value))
 {
                if ( CollectionUtil.IsNullOrEmpty(connections.Value) )
                {
                }
}]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>70</wp:comment_id>
			<wp:comment_author><![CDATA[Cabecinha]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[cabecinha@souloko.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[187.73.168.26]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-24 16:35:07]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-24 16:35:07]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[wheres the full code? its easier to learn with the code in hands.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Command pattern with SignalR</title>
		<link>https://onoffswitch.net/2013/04/15/command-pattern-with-signalr/</link>
		<pubDate>Mon, 15 Apr 2013 08:00:17 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3392</guid>
		<description></description>
		<content:encoded><![CDATA[I'm using SignalR as a long poll mechanism between multiple .NET clients because part of my projects requirements is to have everything over http/https. There's no point in rolling my own socket based long poll since SignalR has already done all the heavy lifting. Unfortunately since the app I work on is distributed I can't upgrade my SignalR version from what I have (0.5.2) since the newer SignalR versions aren't backwards compatabile. This means I have to make do with what this version of SignalR gives me.  

Still, the version I have works great for my purposes and I can't complain.  However, I was getting frustrated at having to constantly update my internal boilerplate whenever I wanted to add a new hub method dispatch.  What I want is to have a single method dispatch and encapsulate all my business logic in a command pattern so that I never have to update my SignalR hub code. Instead I want to only need to add a new command and everything magically gets executed when someone sends a remote action using it.

<h2>The Old Way</h2>

As an example, you can (or used to, since I know SignalR has changed a lot since the version that I have), do this to create a hub proxy connection:

[csharp]
var Connection = new HubConnection(Url);

var Hub = Connection.CreateProxy(&quot;hubName&quot;);

Hub.On&lt;string&gt;(&quot;DynamicMethodCall&quot;, DynamicMethodReference);
[/csharp]

So when the server invokes

[csharp]
client.DynamicMethodCall(&quot;foo&quot;);
[/csharp]

Your client will get <code>foo</code> passed to the <code>DynamicMethodReference</code> function.  

But if you start having a lot of client method calls, then this becomes a pain to update.  Each time you want to dispatch a new method you have to update both your client and your server and its easy to forget to do.  You may be thinking "<em>but its just one server call and one client call, so what, who cares?</em>", and I agree, if you have a simple use case. 

Of course nothing is ever simple, right? In my application, I have a facade hiding the actual SignalR implementation. The outside code doesn't touch the dynamic clients object, since there is a lot of code to manage finding the right client based on a bunch of things like separate keys and who is and isn't connected, etc.  

In addition, when you work in a large application it's always a good idea to separate out 3rd party libraries behind firewalls like that, it makes them easier to update later.  The last thing you want is a spaghetti mess of 3rd party code all over your app that you can't update easily. It's a little more work up front but it pays off later.  The downside is that to make changes to the external API you need to also update the internals. So, being able to have one single entry point and exit point makes things a lot neater in the long run. Less boilerplate means more fun times making things do stuff!

<h2>Commands</h2>

To solve this problem, I wanted to send an <a href="http://en.wikipedia.org/wiki/Command_pattern" target="_blank" rel="noopener noreferrer">encapsulated object</a> that contained my data and code to do the work I wanted. For example, what I wanted to end up with was:

[csharp]
var Connection = new HubConnection(Url);

var Hub = Connection.CreateProxy(&quot;hubName&quot;);

Hub.On&lt;RemoteCommand&gt;(&quot;RemoteCommand&quot;, RemoteCommadHandler);
[/csharp]

Where <code>RemoteCommand</code> would be an abstract base class.  This would let me focus on my business logic and not my interconnect logic.  If I wanted to invoke an echo on a client I could do this:

[csharp]
public class EchoCommand : RemoteCommand
{
    public override void Execute(){
         Console.WriteLine(&quot;Echoecho!&quot;);
    }
}
[/csharp]

And to send it from the server would look like this

[csharp]
client.RemoteCommand(new EchoCommand());
[/csharp]

Later, if I had something else I wanted to invoke all I'd need to do is make a new command and dispatch it on the client again.  

[csharp]
public class LogMeCommand : RemoteCommand
{
    public override void Execute(){
         Log.Debug(&quot;log me happened!&quot;);
    }
}
[/csharp]

[csharp]
client.RemoteCommand(new LogMeCommand ());
[/csharp]

<h2>But...</h2>

I was hoping this would work but originally it didn't.  Obviously the fact that I had registered an abstract base class was preventing it from working.

The first thing I did was change my <code>Hub.On<T></code> code to be of type <code>dynamic</code>. When I did that I was able to execute the registered command, but it came back as a json type.  This tipped me off that the default JSON serializer that SignalR was using didn't serialize the inheritance structure.  

To test this theory out I manually serialized json on the server side, sent a string over the wire, then deserialized the item on the client side. This worked perfectly and maintained all my inheritance structure.

Next step was to find out how to replace the JSON serializer in an old SignalR version.  The SignalR dev's theoretically made  SignalR extremely extensible.  They've exposed a dependency injection hook to register your own serializer.  All they claim you had to do was add

[csharp]
GlobalHost.DependencyResolver.Register(
        typeof (IJsonSerializer),
        () =&gt; JsonSerializer.Create(new JsonSerializerSettings
        {
            TypeNameHandling = TypeNameHandling.All
        }));
[/csharp]

In the <code>Application_Start</code> function of your site (or wherever your app start may be). But, this didn't work for me. Not sure what I was doing wrong, but my endpoints that needed to serialize this base class never got hit. Other endpoints that had basic types for the hub argument worked fine.

<h2>The Solution</h2>

Eventually I settled back on my hacky way of doing it, but at least the hack is hidden in my boilerplate and I wont have to come back to it.

When sending out I do

[csharp]
var serializer = new JsonSerializerSettings { TypeNameHandling = TypeNameHandling.All };
var str = JsonConvert.SerializeObject(command, serializer);
client.RemoteCommand(str);
[/csharp]

And when receiving I do

[csharp]
Hub.On&lt;string&gt;(&quot;RemoteCommand&quot;, item =&gt;
    {
        var rcmd = JsonConvert.DeserializeObject&lt;RemoteCommand&gt;(
                    item,
                    new JsonSerializerSettings
                    {
                        TypeNameHandling = TypeNameHandling.All
                    });
        RemoteCommandRequest(rcmd);
    });   
[/csharp]

And now everything works great.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3392</wp:post_id>
		<wp:post_date><![CDATA[2013-04-15 08:00:17]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-15 08:00:17]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[command-pattern-with-signalr]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="design-patterns"><![CDATA[design patterns]]></category>
		<category domain="post_tag" nicename="json"><![CDATA[json]]></category>
		<category domain="post_tag" nicename="signalr"><![CDATA[SignalR]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561935122;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4091;}i:1;a:1:{s:2:"id";i:4116;}i:2;a:1:{s:2:"id";i:289;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Separation of concerns in node.js</title>
		<link>https://onoffswitch.net/2013/04/29/separation-concerns-node-js/</link>
		<pubDate>Mon, 29 Apr 2013 08:00:23 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3452</guid>
		<description></description>
		<content:encoded><![CDATA[I've been playing with typescript and node.js and I wanted to talk a little about how I've broken up my app source.  It's always good to modularize an application into smaller bits, and while node lets you do a lot, quickly, with just a little bit of code, as your application grows you really can't put all your logic in one big <code>app.ts</code>.

<h2>App.ts</h2>

Instead of the normal application example you see for node projects, I wanted to make it clearer what the initialization of my application does.  My app start is structured like this:

[ts]
/**
 * Module dependencies.
 */

import db = module(&quot;./storage/storageContainer&quot;);
import auth = module(&quot;./auth/oauthDefinitions&quot;);
import requestBase = module(&quot;./routes/requestBase&quot;);

var express = require('express')
    , routes = require('./routes')
    , http = require('http')
    , path = require('path')
    , log = require(&quot;./utils/log.js&quot;)
    , fs = require('fs')
    , passport = require('passport');

var app = express();

class AppEntry{
    constructor(){
        this.initDb();
        this.setupRoutes();
        this.defineOAuth();
        this.startServer();
    }
   
    // initialization functions
}

var application = new AppEntry();
[/ts]


The upside to this kind of simple structure is that it's easy to see what the entrypoint structure is.  Adding new initialization logic is encapsulated and isn't intermingled among route configurations, OAuth authorization code, server start, database initialization, etc.  Having a monolithic app can quickly get into a tangled mess. 

You may have noticed that I didn't pass in any required modules or references to the application. This is because I'm relying on the class initialization closure to capture the variables to keep function signatures clean.  I opted to use a class instead of a module for no particular reason other than I like classes and forgot modules existed when I did this.

<h2>Storage</h2>

Even though I'm using mongoose as my mongoDB ORM, I still have tried to move all the storage logic in special storage classes.  This means that any outside access to storage has to go through classes that wrap the storage calls.  I've mentioned it before, but I think it's always good practice to not entangle an application with specific 3rd party libraries (if you can avoid it).  Also having storage classes means I can hide away internal mongo calls, if necessary, and let me do extra data manipulation outside of the context that wants the data.  

To make accessing the storage classes easy for myself, I have split them up into separate classes based on what they most commonly access. For example, there is a <code>userStorage</code> class, and a <code>trackStorage</code> class, etc.  Each class contains relevant CRUD and helper methods to aggregate the data in forms that I commonly use them.

Unfortunately, the way node works is that in each module you work in, if you wanted access to a storage class you'd have to import each one independently (one import for users, one import for dataPoints, etc).  That's a pain. Instead, I've wrapped the storage classes with a single exported singleton container. 

[ts]
// storageContainer.ts

import schemaImport = module(&quot;./schema&quot;);
import users = module(&quot;./userStorage&quot;);
import tracks = module(&quot;./trackStorage&quot;)


export var storage:schemaImport.db = new schemaImport.db();
export var userStorage:users.userStorage = new users.userStorage();
export var schema = schemaImport;
export var trackStorage:tracks.trackStorage = new tracks.trackStorage();
[/ts]

Anywhere I want access to storage classes, I only need to import one module:

[ts]
import db = module(&quot;./storage/storageContainer&quot;);

// ...

db.userStorage.getUserByUsername(...)
[/ts]

Adding new storage classes and updating the singleton container means I have access to these everywhere I need them without having to worry about importing and instantiating modules.


<h2>Definition files</h2>

Like the storage classes, the same pattern goes for definition files.  I've made a folder called <code>def</code> and created an <code>all.d.ts</code> that just has reference path's to all my other definition mappings.  

[ts]
///&lt;reference path=&quot;./mongoose.d.ts&quot;/&gt;
///&lt;reference path=&quot;./nodeUnit.d.ts&quot;/&gt;
///&lt;reference path=&quot;./schemaDef.d.ts&quot;/&gt;
///&lt;reference path=&quot;./passport.d.ts&quot;/&gt;
[/ts]

Any other file that needs definition mappings can include the one all aggregate. Since it costs nothing and is just a compiler hint, there's no resource hit.

<h2>Routes</h2>

And again, I do the same kind of pattern with routes. I have a folder setup like this:

[code]
routes
├── index.js
├── indexRoutes.ts
├── userRoutes.ts
├── ... etc
[/code]

Where index.js looks like this:

[javascript]
var userRoutes = require(&quot;./userRoutes&quot;);
var indexRoutes = require(&quot;./indexRoutes&quot;);
var trackRoutes = require(&quot;./trackRoutes&quot;);
var partialsRoutes = require(&quot;./partialsRoutes&quot;);

module.exports = function(app){
    new userRoutes.userRoutes(app);
    new indexRoutes.indexRoutes(app);
    new trackRoutes.trackRoutes(app);
    new partialsRoutes.partialsRoutes(app);
};
[/javascript]

From my main application, I import the routes module and pass it the app reference. I know that <a href="http://dailyjs.com/2012/01/26/effective-node-modules/" target="_blank" rel="noopener noreferrer">app is global in a node application</a>, however, I don't like relying on globals. It was just as easy to pass app as an argument and I prefer that flow control.  

[ts]
routes = require('./routes')
routes(app);
[/ts]

In each of the route modules, I then go back to typescript

[ts]
import db = module(&quot;../storage/storageContainer&quot;);

import base = module(&quot;./requestBase&quot;);

export class userRoutes {
    constructor(app:ExpressApplication) {
        var requestUtils = new base.requestBase();

        app.get('/logout', (req:any, res) =&gt; {
            req.logout();
            res.redirect('/');
        });

        app.get(&quot;/users&quot;, requestUtils.ensureAuthenticated, (req, res) =&gt; {
            res.send(req.user.name);
        });
}
[/ts]

So at this point I'm using the definition files from <a href="https://github.com/borisyankov/DefinitelyTyped" target="_blank" rel="noopener noreferrer">DefinitelyTyped</a> for my express application. Also you can see that I'm injecting custom middleware into my routes for things that I want to make sure are authenticated.  The point being that the routes class is now self encapsulated, and we don't need to modify the main entry point to the application anymore.  We update the routes index page and create a route object and that's it.


<h2>Conclusion</h2>

It's been fun playing in node, and while I see myself doing some things the .NET way, I'm also trying to embrace the node way. However, when it comes to module organization the node projects I've seen have been seriously lacking.  While it does mean more boilerplate upfront, I think making sure to split up your files helps maintain a separation of concerns and your project extensible.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3452</wp:post_id>
		<wp:post_date><![CDATA[2013-04-29 08:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-29 08:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[separation-concerns-node-js]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="node-js"><![CDATA[node.js]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="typescript"><![CDATA[typescript]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560893531;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4515;}i:1;a:1:{s:2:"id";i:4028;}i:2;a:1:{s:2:"id";i:2635;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>71</wp:comment_id>
			<wp:comment_author><![CDATA[Kevin]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kevin@mitchell.ch]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[86.135.137.32]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-22 15:20:37]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-22 15:20:37]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,

It looks like you've developed some typescript definition files for libraries like passport that aren't in https://github.com/borisyankov/DefinitelyTyped.  To avoid people duplicating effort do you have any plans to publish these, or submitting them to DefinitelyTyped?  

Thanks]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>72</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-22 15:50:05]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-22 15:50:05]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Kevin, I didn't really do much with regards to passport or monogoose (and I haven't kept up to date with any of the library updates). The mongoose definitions I used are available https://github.com/devshorts/trakkit/blob/master/def/mongoose.d.ts but they are a pretty small subset of what is available. I was just adding as I went. If I revisit the project and add more to it I'll definitely submit them to DefinitelyTyped as it's an excellent resource!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>71</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Capturing mutables in f#</title>
		<link>https://onoffswitch.net/2013/04/08/capturing-mutables-f/</link>
		<pubDate>Mon, 08 Apr 2013 08:00:26 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3497</guid>
		<description></description>
		<content:encoded><![CDATA[I was talking about F# with a coworker recently and we were discussing the merits of a stateless system. Both of us really like the enforcement of having to inject state, and when necessary, returning a new modified copy of state.  Functional languages want you to work with this pattern,  but like with all things software, it's good to be able to break the rules.  This is one of the things I like about F#, you can create mutables and do work imperatively if you need to.  

But, there is a small caveat with mutables: you can't close over them.  Look at the following example:

[fsharp]
let g() = 
    let mutable f = 0;
        
    fun () -&gt; Console.WriteLine f
[/fsharp]

The intent is that calling <code>g()</code>  would give you a new function that writes <code>f</code> to the console.  In C# it would be the same as

[csharp]
public Action g(){
    int f = 0;
    return () =&gt; Console.WriteLine(f);
}
[/csharp]

Both examples look functionally the same, but the F# example actually gives you the following compiler error:

<blockquote>The mutable variable 'f' is used in an invalid way. Mutable variables cannot be captured by closures. Consider eliminating this use of mutation or using a heap-allocated mutable reference cell via 'ref' and '!'.</blockquote>

But, the C# version is totally fine. Why? 

The reason is because F# mutable values are <a href="http://stackoverflow.com/a/4004715/310196" target="_blank" rel="noopener noreferrer">always stack allocated</a>. To close on a variable, the variable needs to be allocated on the heap (or copied by value). This is why you can close on objects that aren't mutable (you close on their reference) and values that aren't mutable (they are closed by value, i.e. copied).  If you closed on a stack allocated type it wouldn't work; stack objects are popped off after the function loses scope. This is the basis of <a href="http://en.wikipedia.org/wiki/Call_stack#Unwinding" target="_blank" rel="noopener noreferrer">stack unwinding</a>.  After the stack is unwound, the reference to the value you closed on would point to garbage!

So why does the C# version work? <code>f</code> looks like a stack allocated value type to me.  The nuance is that the C# compiler actually makes <code>f</code> become a heap allocated value type. Here is a quote from <a href="http://blogs.msdn.com/b/ericlippert/archive/2010/09/30/the-truth-about-value-types.aspx" target="_blank" rel="noopener noreferrer">Eric Lipperts</a> blog explaining this (emphasis mine):

<blockquote>in the Microsoft implementation of C# on the desktop CLR, value types are stored on the stack when the value is a local variable or temporary that <strong>is not a closed-over local variable of a lambda or anonymous method</strong>, and the method body is not an iterator block, and the jitter chooses to not enregister the value.</blockquote>

So C# actually moves the value type to the heap to be declared because it needs to be accessed later via the closure. If you didn't do that, then the value type wouldn't exist when the closure is executed since the stack reference would have been lost (stacks are popped off when functions return).

F#, then, is much stricter about its stack vs heap allocations and opted to not do this magic for you.  I think their decision aligns with the functional philosophy of statelessness; they obviously could have done the magic for you but chose not to.  

Instead, if you do need to return a captured mutable value in a function closure you have to use what is called a <a href="http://msdn.microsoft.com/en-us/library/dd233186.aspx" target="_blank" rel="noopener noreferrer">reference cell</a>. All a reference cell is is a heap allocated mutable variable, which is exactly what you need for returned closures to work.

A modified version of our example that would now work looks like this:

[fsharp]
let g() = 
    let f = ref 0;
        
    fun () -&gt; Console.WriteLine !f


g()()
[/fsharp]

Notice the <code>!</code> which dereferences the cell. This example outputs 

[code]
0
[/code]

Without the <code>!</code>, though, you'd get 

[code]
Microsoft.FSharp.Core.FSharpRef`1[System.Int32]
[/code]

Showing you that <code>f</code> isn't really an int, it's a boxed heap value of an int. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3497</wp:post_id>
		<wp:post_date><![CDATA[2013-04-08 08:00:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-08 08:00:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[capturing-mutables-f]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="closures"><![CDATA[closures]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="heap"><![CDATA[heap]]></category>
		<category domain="post_tag" nicename="stack"><![CDATA[stack]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560183675;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1873;}i:1;a:1:{s:2:"id";i:3723;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>75</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #15, 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/04/15/f-weekly-15-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.8.72]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-14 21:02:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-14 21:02:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Anton Kropp shared &#8220;Capturing mutables in F#&#8220;. [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>76</wp:comment_id>
			<wp:comment_author><![CDATA[Mutable values and closures | My Memory]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://putridparrot.com/blog/mutable-values-and-closures/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[173.254.28.109]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-17 21:24:26]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-17 21:24:26]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] In C# we can do this because the Common Language Runtime (CLR) automatically places variables on the heap if they&#8217;re used as part of a closure as detailed in the post Capturing Mutables in F#. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech talk: Hacking droid</title>
		<link>https://onoffswitch.net/2013/04/04/tech-talk-hacking-droid/</link>
		<pubDate>Thu, 04 Apr 2013 20:20:37 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3500</guid>
		<description></description>
		<content:encoded><![CDATA[Todays tech talk was based off of a blog entry posted <a href="https://www.facebook.com/notes/facebook-engineering/under-the-hood-dalvik-patch-for-facebook-for-android/10151345597798920" target="_blank" rel="noopener noreferrer">by facebook</a> recently where they described the things they needed to do to get their mobile app running on android OS Froyo (v 2.2). 

The gist of the post was that facebook migrated a lot of javascript code to Java and then found themselves in an interesting situation: they were unable to load the app due to the number of methods declared being larger than the method metadata buffer could hold. The buffer, called "LinearAlloc", on this particular droid version was 5MB in size.  Later versions were increased to 8MB which meant that the bug was no longer exhibited.

Facebook's engineers tried to break their application up into multiple droid executable files (dex), tried automatic source transformations to minimize their method calls, tried refactoring some of their code (but were unable to make siginifant headway due to the strong coupling they had), and ended up having to do some memory hacking insanity to find the source of the buffer and replace it with a pointer to another buffer that was larger. This way they could have enough space to store the methods they needed.  They also put in a failsafe where they would scan the entire process memory heap looking for the correct buffer location.

You can see the buffer structure they linked to <a href="https://github.com/android/platform_dalvik/blob/android-2.3.7_r1/vm/LinearAlloc.h#L33" target="_blank" rel="noopener noreferrer">here</a>:

[c]
/*
 * Linear allocation state.  We could tuck this into the start of the
 * allocated region, but that would prevent us from sharing the rest of
 * that first page.
 */
typedef struct LinearAllocHdr {
    int     curOffset;          /* offset where next data goes */
    pthread_mutex_t lock;       /* controls updates to this struct */

    char*   mapAddr;            /* start of mmap()ed region */
    int     mapLength;          /* length of region */
    int     firstOffset;        /* for chasing through */

    short*  writeRefCount;      /* for ENFORCE_READ_ONLY */
} LinearAllocHdr;
[/c]

The team and I had a good time talking about their thought process and whether what they did was a good idea or not as well as critiquing decisions by the <a href="http://en.wikipedia.org/wiki/Dalvik_(software)" target="_blank" rel="noopener noreferrer">dalvik</a> designers. Some ideas and discussions that came up were:

<ol>
<li>If the size of the application is so big the operating system can't load it, should the application be that big? Why not modularize work? If they couldn't modulraize their application because of how they are reliant on some framework, they should have put proxy's to decouple their logic. Tightly coupled code suffers from this probelm</li>
<li>Maybe the internal buffer size was chosen at 5MB to maximize storage space vs estimated application size.  It could have been an arbitrary choice to choose that number, OR, it could have been an extremely specific value based on other internal factors not known to us. Either way, messing with that buffer seems like a bad idea.</li>
<li>Why not deprecate the application for phones running that operating system? (One of our new engineers brought up that froyo isn't that old, and facebook's capital is based on running their application everywhere, unlike Apple who can dictate when to buy new hardware to run whatever software)</li>
<li>Why not have made the internal LinearAlloc buffer dynamic? This got us talking about the memory overhead of having a dynamically resized array (vector) and maybe they chose to not make it dynamic due to performance overhead on an embedded device</li>
</ol>

We all related to them since I'm sure every engineer has gotten the main business logic to work and then suddenly a show stopping, undocumented, strange heisenbug shows up and you have to stop everything and fix that for weeks. 

After that, the team's discussion diverged a little and we got to talking about basic droid development. One of the QA engineers here had dome some droid work before and walked us through a basic application design, describing what is an intent, activity, and how droid views are created.  

All in all, a fun tech talk.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3500</wp:post_id>
		<wp:post_date><![CDATA[2013-04-04 20:20:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-04 20:20:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-hacking-droid]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="droid"><![CDATA[droid]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560892211;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3285;}i:1;a:1:{s:2:"id";i:1268;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Jon Skeet, C#, and Resharper</title>
		<link>https://onoffswitch.net/2013/04/09/jon-skeet-c-resharper/</link>
		<pubDate>Tue, 09 Apr 2013 19:21:04 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3524</guid>
		<description></description>
		<content:encoded><![CDATA[Today, at 1pm EST, the venerable <a href="http://stackoverflow.com/users/22656/jon-skeet" target="_blank" rel="noopener noreferrer">Jon Skeet</a> had a goto meeting webinar sponsored by <a href="http://www.jetbrains.com/" target="_blank" rel="noopener noreferrer">JetBrains</a> reviewing weird and cool stuff about C# and Resharper. For those who aren't in the know, <a href="http://www.jetbrains.com/resharper/" target="_blank" rel="noopener noreferrer">Resharper</a> is a static analysis tool for C# that is pretty much the best thing ever.  Skeet's a great speaker and my entire team at work and I watched the webinar in our conference room while eating lunch.  

I took some notes and wanted to share some of the interesting things that Jon mentioned. You can watch the video <a href="http://blogs.jetbrains.com/dotnet/2013/04/webinar-recording-jon-skeet-inspects-resharper/" target="_blank" rel="noopener noreferrer">here</a>.  It's an hour long and definitely worth viewing.

<h2>Recursive Parameterization</h2>

Skeet talked about how Resharper, and in fact the C# compiler lets you do weird stuff like this:

[csharp]
public class SuperContainer&lt;T&gt;
{
        
}

public class Container&lt;T&gt; : SuperContainer&lt;Container&lt;Container&lt;T&gt;&gt;&gt;
{
}
[/csharp]

Even though this leads itself to recursive parameterization.  Compiling this is just fine though. However, even if its not used in an assembly, if you run unit tests for that assembly you'll get:

<img src="http://onoffswitch.net/wp-content/uploads/2013/04/recursiveParameterization.-600x87.png" alt="recursiveParameterization." width="600" height="87" class="alignnone size-medium wp-image-3525" />

This is because unit tests usually use reflection to test your assemblies. If you don't use a unit test, and you never access it you won't have an issue. The problem, Skeet told me, isn't in the C# compiler, it's that the CLR goes, as Skeet put it, "<em>bang</em>"

<h2>Access to modified closures</h2>

Jon talked about the problem of accessing modified closures and how it's different in C#5 vs previous versions.  The problem is described like this:

[csharp]
var list = new List&lt;Action&gt;();
foreach (var i in Enumerable.Range(0, 10))
{
    list.Add(() =&gt; Console.WriteLine(i));
}
[/csharp]

In C# 4, the variable <code>i</code> is the same reference for each iteration.  This means that when you capture the value in a lambda, you are closing on its reference.  Running this, you are going to get 

[code]
9
9
9
9
9
9
9
9
9
9
[/code]

The C# 4 and earlier solution is to make sure that a new variable is created each time the iteration runs:

[csharp]
var list = new List&lt;Action&gt;();
foreach (var i in Enumerable.Range(0, 10))
{
    int tmp = i;
    list.Add(() =&gt; Console.WriteLine(tmp));
}
[/csharp]

This gives you the right answer. But in C# 5 they changed the handling of foreach internally to give you the expected behavior: you will close on different references each time.  

<h2>Covariance</h2>

Jon then spent a short bit discussing covariance between objects and how you can induce runtime failures, but resharper doesn't warn you about it.  For example, the following code is compilable, but not runnable:

[csharp]
string[] x = new string[10];
object[] o = x;
o[0] = 5; // breaks
[/csharp]

<h2>Statics in generic types</h2>

The next thing Jon talked about was the Resharper warning when you have a static member variable as part of a class with generics.  For example:

[csharp]
public class Foo&lt;T&gt;
{
    public static string Item { get; set; }
}

[Test]
public void StaticTest()
{
    Foo&lt;String&gt;.Item = &quot;a&quot;;
    Console.WriteLine(Foo&lt;String&gt;.Item);

    Foo&lt;int&gt;.Item = &quot;b&quot;;
    Console.WriteLine(Foo&lt;String&gt;.Item);
    Console.WriteLine(Foo&lt;int&gt;.Item);
}
[/csharp]

Which prints out

[code]
a
a
b
[/code]

Interestingly enough, Resharper 7 gives me no warning on using a static item in a templated class.  The problem is really when you think you have a cache or some other static item per class, but its created once per <strong>type</strong>.  This was new info to me so I thought this was pretty cool.

<h2>Virtual method call in constructor</h2>

Jon's mentioned it on twitter before, and it was cool to see him mention it in his webinar, but you can get into very strange things when you call a virtual method from a base constructor.  For example:

[csharp]
public class Base
{
    protected int item;

    protected Base()
    {
        VirtualFunc();                            
    }

    public virtual void VirtualFunc()
    {
        Console.WriteLine(item);
    }

}

public class Derived : Base
{
    public Derived()
    {
        item = 1;

        VirtualFunc();
    }

    public override void VirtualFunc()
    {
        if (item != 1)
        {
            throw new Exception(&quot;Should never do this&quot;);
        }
    }
}
[/csharp]

Which prints out

[code]
System.Exception : Should never do this
[/code]

Basically the base class constructor is called first, so you haven't set the member field in the derived constructor.  This means that if you run into this problem you have no way of assuring that items are initialized, even if they may be set in the constructor.  Resharper, thankfully, gives you a warning about this. So follow it's advice!


<h2> Miscellaneous c# weirdos</h2>

Skeet ended with a spattering of random C# weirdness, like being able to declare a class called <code>var</code> even though <code>var</code> is a keyword.  Also, comparing of doubles can be...well, odd:

[csharp]
[Test]
public void CompareDouble()
{
    Console.WriteLine(double.NaN == double.NaN);

    var x = double.NaN;

    Console.WriteLine(x == x);
}
[/csharp]

Here, Resharper says "hey, just change these values to true, they're always going to be true", but actually this prints out

[code]
False
False
[/code]

What? 

<h2>Conclusion</h2>

Jon quoted an unnamed source that describes the content of the webinar:

<blockquote>You are entering dark places</blockquote>

And I tend to agree.  Thanks for the great presentation Jon and the JetBrains team.

EDIT:

Skeet tweeted his <a href="https://github.com/hhariri/Tidbits" target="_blank" rel="noopener noreferrer">sample solution project</a> that he used in the webinar.  For more samples of C# weird/cool stuff check it out!
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3524</wp:post_id>
		<wp:post_date><![CDATA[2013-04-09 19:21:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-09 19:21:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[jon-skeet-c-resharper]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
		<category domain="post_tag" nicename="resharper"><![CDATA[resharper]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1556356876;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4493;}i:1;a:1:{s:2:"id";i:3656;}i:2;a:1:{s:2:"id";i:3565;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>77</wp:comment_id>
			<wp:comment_author><![CDATA[D Carral]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[dan@dcarral.org]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[212.89.22.4]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-11 11:39:53]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-11 11:39:53]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Ey, thanks a lot for this interesting post! Interesting corner cases here.

Already waiting for the video to watch it from Mr Skeet himself ;)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Building better regular expressions</title>
		<link>https://onoffswitch.net/2013/05/06/composable-regex/</link>
		<pubDate>Mon, 06 May 2013 08:00:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3536</guid>
		<description></description>
		<content:encoded><![CDATA[Every software developer has at one point in time heard the adage

<blockquote>If you have a problem and you think you can solve it with [threads|pointers|regex|etc], now you have two problems</blockquote>

For me, I've always told it with regex (and I think that's the <a href="http://www.codinghorror.com/blog/2008/06/regular-expressions-now-you-have-two-problems.html" target="_blank" rel="noopener noreferrer">official</a> way to do it).  It's not that threads and pointers aren't hard, but more that with proper stylistic choices and with experience, they can be easily manageable and simple to debug.  Regex though, have a tendency to spiral out of control. What starts with something simple always bloats into an enormously difficult to read haze of PERLgasms.

For example, I frequently wonder why in the 21st century why we still deal with a syntax like this:

[code]
(?:(?:\r\n)?[ \t])*(?:(?:(?:[^()&lt;&gt;@,;:\\&quot;.\[\] &#92;&#48;00-&#92;&#48;31]+(?:(?:(?:\r\n)?[ \t]
)+|\Z|(?=[\[&quot;()&lt;&gt;@,;:\\&quot;.\[\]]))|&quot;(?:[^\&quot;\r\\]|\\.|(?:(?:\r\n)?[ \t]))*&quot;(?:(?:
\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\&quot;.\[\] &#92;&#48;00-&#92;&#48;31]+(?:(?:(
?:\r\n)?[ \t])+|\Z|(?=[\[&quot;()&lt;&gt;@,;:\\&quot;.\[\]]))|&quot;(?:[^\&quot;\r\\]|\\.|(?:(?:\r\n)?[ 
\t]))*&quot;(?:(?:\r\n)?[ \t])*))*@(?:(?:\r\n)?[ \t])*(?:[^()&lt;&gt;@,;:\\&quot;.\[\] &#92;&#48;00-&#92;&#48;
31]+(?:(?:(?:\r\n)?[ \t])+|\Z|(?=[\[&quot;()&lt;&gt;@,;:\\&quot;.\[\]]))|\[([^\[\]\r\\]|\\.)*\
](?:(?:\r\n)?[ \t])*)(?:\.(?:(?:\r\n)?[
[/code]

Even the most seasoned engineers couldn't tell me what this did off the bat.  

Still, regex, even for all their annoyances, are an extremely powerful form of <a href="http://en.wikipedia.org/wiki/Deterministic_finite_automaton" target="_blank" rel="noopener noreferrer">deterministic finite automata</a>.  I think it's important to know what they are and how to use them, and when you wield them properly they can make life better.  

<h2>Let's make it better</h2>

I'm also not the only person to have given it some thought.  <a href="http://martinfowler.com/bliki/ComposedRegex.html" target="_blank" rel="noopener noreferrer">Martin Fowler</a> has also discussed the regex composability problem. Fowler says

<blockquote>One of the most powerful tools in writing maintainable code is break large methods into well-named smaller methods - a technique Kent Beck refers to as the Composed Method pattern.</blockquote>

And I absolutely agree. The problem is that writing regex usually starts out this way too.  You pattern match on one subset, then you figure out another pattern match group, and you start to build yourself a long and complex regex string. However, by the end, even you can't figure out what it does. The human eye can't distinguish the logical patterns anymore.  

While others have also tackled the issue, their solutions usually involved special classes and workflows.  I initially thought about building a full fledged DSL, mimicking those solutions, but came up with a simpler approach.  I don't want to replace regex completely, I just want a way to view my composable groups easier.

My regex patterns use white space ignored key value pairs for composable groups, and the very last line of the regex (where all the key value pairs are seperated by newlines) will be the final composed regex.  In the end, the regular expressions now look something like this:

[code]
group1 = [A-f]
group2 = [g-z]

group1|group2
[/code]

And build out into

[code]
[A-f]|[g-z]
[/code]

<h2>The builder</h2>

The code to do this is pretty trivial. It's all simple string manipulation. Let me show you anyways:

[csharp]
public class Regexer
{
	public String Regex { get; private set; }

	public List&lt;String&gt; DebugTrace { get; private set; }

	public Regexer(string regex, bool includesComments = true, bool useDebug = true)
	{
		if (String.IsNullOrEmpty(regex))
		{
			throw new ArgumentNullException(&quot;regex&quot;, &quot;Regex cannot be null&quot;);
		}

		DebugTrace = new List&lt;string&gt;();

		Parse(regex, includesComments, useDebug);
	}

	private void Parse(string regex, bool includeComments, bool useDebug)
	{
		var splits = regex.Split(new[] { &quot;\r\n&quot;, &quot;\n&quot; }, StringSplitOptions.RemoveEmptyEntries);
		if (splits.Count() == 1)
		{
			Regex = splits.First();
			return;
		}

		var groups = (from item in splits.Take(splits.Count() - 1)
		        where !item.StartsWith(&quot;##&quot;)
			let keyValueSplit = item.Split(new[] { '=' }, 2, StringSplitOptions.RemoveEmptyEntries)
			let key = keyValueSplit.First().Trim()
			let value = keyValueSplit.Last().Trim()
		        where !String.IsNullOrEmpty(key)
		        where !String.IsNullOrEmpty(value)
			let regexWithoutComments = includeComments ? value.Split(new[] { &quot;##&quot; }, 2, StringSplitOptions.RemoveEmptyEntries).First().Trim() : value
			select new { Key = key, Value = regexWithoutComments }).ToList();

		var final = splits.Last().Trim();

		Regex = final;

		for (int i = groups.Count - 1; i &gt;= 0; i--)
		{
			var item = groups[i];
			Regex = Regex.Replace(item.Key, item.Value);
			if (useDebug)
			{
				DebugTrace.Add(Regex);
			}
		}
	}
}
[/csharp]

Basically build out all key/value pairs and string replace them going from the bottom of the call group up.  The string replace also supports comments, by adding in "##" followed by whatever you want to comment.     

<h2>Validation of email addresses</h2>

To test this idea, I decided it would be fun to try to validate email addresses. For those who don't' know, the <a href="http://tools.ietf.org/html/rfc3696#page-5" target="_blank" rel="noopener noreferrer">email RFC</a> is absurdly complex.  In fact the initial regex I posted is just a very small snippet of the official <a href="http://ex-parrot.com/~pdw/Mail-RFC822-Address.html" target="_blank" rel="noopener noreferrer">email validation regex</a>.  

I didn't think to account for ALL of those cases, but I was able to get a pretty robust email verifier on almost the first try. Writing regex in human readable groups really does make a difference.

Here is the composed regex I built:

[code]
weirdChars = (!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)
numbers = \d
characters = [A-z]
anyChars = (weirdChars|numbers|characters)
            
lettersFollowedBySingleDot = (anyChars+\.anyChars+)
            
names = anyChars|lettersFollowedBySingleDot
            
onlyQuotableCharacters = @|\s
quotedNames = &quot;&quot;(names|onlyQuotableCharacters)+&quot;&quot;

anyValidStart = (names|quotedNames)+

group = (quotedNames:anyValidStart)|anyValidStart

local = ^(group)

ipv4 = ((\d{1,3}.){3}(\d{1,3}))

ipv6Entry = ([a-f]|[A-F]|[0-9]){4}? ## group of 4 hex values
ipv6 = ((ipv6Entry:){7}?ipv6Entry) ## 8 groups of ipv6 entries

comAddresses = (characters+(\.characters+)*) ## stuff like a.b.c.d etc
domain = (comAddresses|ipv6|ipv4)$ ## this has to be at the end

(local)@(domain)
[/code]

Which compiles to:

[csharp]
(^((&quot;(((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])|(((!|-|\+|\\|\$|\^|~|#|%
|\?|{|}|_|/|=)|\d|[A-z])+\.((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])+)|@|
\s)+&quot;:(((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])|(((!|-|\+|\\|\$|\^|~|#|%
|\?|{|}|_|/|=)|\d|[A-z])+\.((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])+)|&quot;((
(!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])|(((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_
|/|=)|\d|[A-z])+\.((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])+)|@|\s)+&quot;)+)|(
((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])|(((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|
_|/|=)|\d|[A-z])+\.((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])+)|&quot;(((!|-|\+|
\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])|(((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d
|[A-z])+\.((!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)|\d|[A-z])+)|@|\s)+&quot;)+))@((([A-z]+
(\.[A-z]+)*)|((([a-f]|[A-F]|[0-9]){4}?:){7}?([a-f]|[A-F]|[0-9]){4}?)|((\d{1,3}.)
{3}(\d{1,3})))$)
[/csharp]

And passes all of these tests:

[csharp]
[Test]
[TestCase(&quot;someDude@gmail.com&quot;, true)]
[TestCase(&quot;foo&quot;, false)]
[TestCase(&quot;foo@&quot;, false)]
[TestCase(&quot;!@2001:0db8:85a3:0000:0000:8a2e:0370:7334&quot;, true)]
[TestCase(&quot;!@2001:0db8:85a3:0000:0000:8a2e:0370:73345&quot;, false)]
[TestCase(&quot;stupidM0nk3yA0l@aol.net&quot;, true)]
[TestCase(&quot;stupidM0nk3yA0l@aol.net.net&quot;, true)]
[TestCase(&quot;123+guy@domain&quot;, true)]
[TestCase(&quot;@@domain&quot;, false)]
[TestCase(&quot;!@ 2001:0db8:85a3:0000:0000:8a2e:0370:73345&quot;, false)]
[TestCase(@&quot;&quot;&quot;Abc\@def&quot;&quot;@example.com&quot;, true)]
[TestCase(&quot;customer/department=shipping@example.com&quot;, true)]
[TestCase(&quot;$A12345@example.com&quot;, true)]
[TestCase(&quot;!def!xyz%abc@example.com&quot;, true)]
[TestCase(&quot;_somename@example.com&quot;, true)]
[TestCase(&quot;abc+\&quot;foo=123$\&quot;+test@example.com&quot;, true)]
[TestCase(&quot;&quot;, false)]
[TestCase(&quot;john@uk&quot;, true)]
[TestCase(&quot;john.a@uk&quot;, true)]
[TestCase(&quot;john.@uk&quot;, false)]
[TestCase(&quot;john..@uk&quot;, false)]
[TestCase(&quot;.john@uk&quot;, false)]
[TestCase(&quot;\&quot;a group\&quot;:cal@iamcalx.com&quot;, true)]
[TestCase(&quot;\&quot;a group\&quot;:@iamcalx.com&quot;, false)]
[TestCase(&quot;:\&quot;a group\&quot;:cal@iamcalx.com&quot;, false)]
[TestCase(&quot;\&quot;a group\&quot;:cal@192.168.1.1.1&quot;, false)]
[TestCase(&quot;\&quot;a group\&quot;:cal@192.168.1.1&quot;, true)]
[TestCase(&quot;\&quot;a group\&quot;:cal@192.168.1.100&quot;, true)]
[TestCase(&quot;\&quot;a group\&quot;:cal@192.168.1111.100&quot;, false)]
public void Email(string email, bool pass)
{
    var composableRegex = @&quot;

    weirdChars = (!|-|\+|\\|\$|\^|~|#|%|\?|{|}|_|/|=)
    numbers = \d
    characters = [A-z]
    anyChars = (weirdChars|numbers|characters)
            
    lettersFollowedBySingleDot = (anyChars+\.anyChars+)
            
    names = anyChars|lettersFollowedBySingleDot
            
    onlyQuotableCharacters = @|\s
    quotedNames = &quot;&quot;(names|onlyQuotableCharacters)+&quot;&quot;

    anyValidStart = (names|quotedNames)+

    group = (quotedNames:anyValidStart)|anyValidStart

    local = ^(group)

    ipv4 = ((\d{1,3}.){3}(\d{1,3}))

    ipv6Entry = ([a-f]|[A-F]|[0-9]){4}? ## group of 4 hex values
    ipv6 = ((ipv6Entry:){7}?ipv6Entry) ## 8 groups of ipv6 entries

    comAddresses = (characters+(\.characters+)*) ## stuff like a.b.c.d etc
    domain = (comAddresses|ipv6|ipv4)$ ## this has to be at the end

    (local)@(domain)&quot;;

    var regex = new Regexer(composableRegex).Regex;

    Console.Write(regex);

    if (pass)
    {
        Assert.IsTrue(Regex.IsMatch(email, regex));
    }
    else
    {
        Assert.IsFalse(Regex.IsMatch(email, regex));
    }
}
[/csharp]

At one point I had a problem with my ipv4 vs ipv6 validation, but now it was trivial to test. For the domain I could remove both <code>comAddresses</code> and <code>ipv6</code> and deal only with ipv4.  Without having this kind of composable group I'd have to manually edit the enormous regex and it'd be easy to make a parenthesis mistake.  

<h2>Try it out!</h2>

At the suggestion of a coworker, I've uploaded a very small MVC4 test app to generate regex for you using this. Go to <a href="http://composableregex.apphb.com/" target="_blank" rel="noopener noreferrer">composableregex.apphb.com</a> to try it out.  Big thanks to Carlo for helping make it look nice!

I went to <a href="http://www.debuggex.com/" target="_blank" rel="noopener noreferrer">debuggex.com</a> which is a new regular expression visualizer and ran my regex in it.  Here's the breakdown.  FYI, the image is pretty big

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/regex_visualizer.png" rel="attachment wp-att-3690"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/regex_visualizer-216x600.png" alt="regex_visualizer" width="216" height="600" class="alignnone size-medium wp-image-3690" /></a>
<h2>Conclusion</h2>

It's too bad the regex spec doesn't let us build things out like this by default, though it is reasonably easy to build your own regex composer.  While I don't think my solution is maybe the most elegant, or high tech, it does help me minimize mistakes by being able to work on smaller chunks at a time.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3536</wp:post_id>
		<wp:post_date><![CDATA[2013-05-06 08:00:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-06 08:00:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[composable-regex]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="regex"><![CDATA[regex]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561472667;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4343;}i:1;a:1:{s:2:"id";i:4316;}i:2;a:1:{s:2:"id";i:4068;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>78</wp:comment_id>
			<wp:comment_author><![CDATA[Serge Toarca]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[debuggex@toarca.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.debuggex.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.196.191.228]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-06-16 04:40:00]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-06-16 04:40:00]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey Anton,

I'm the creator of Debuggex. Thanks for sharing the link! Does Debuggex appear to you like the image you posted or is it modified? That is, there should be a blue navbar at the top, and the sliders should have tracks that they're on. If it's not modified, can you please let me know what os/browser you're using? I'd like to fix it.

Thanks,
Serge]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>A response to &quot;Ten reasons to not use a functional programming language&quot;</title>
		<link>https://onoffswitch.net/2013/04/22/response-ten-reasons-functional-programming-language/</link>
		<pubDate>Mon, 22 Apr 2013 08:00:43 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3565</guid>
		<description></description>
		<content:encoded><![CDATA[If you haven't read the <a href="http://fsharpforfunandprofit.com/posts/ten-reasons-not-to-use-a-functional-programming-language/" target="_blank" rel="noopener noreferrer">top ten reasons to not use a functional programming language</a>, I think you should. It's a well written post and ironically debunks a lot of the major trepidations people have with functional languages. 

But, I wanted to play devils advocate here. I read a lot of articles on functional and everyone touts a lot of the same reasons to use functional languages, and this post was no different.  What these posts always lack, though, is acknowledgement that functional isn't the end all be all of language solutions.  It has plenty of problems itself, and it's just as easy to critique it using the same ten reasons. That said, I wanted to address a few of my opinions regarding functional programming using the same format as the original article.

<h2>Reason 1: I don't want to follow the latest fad</h2>

The authors point here is that people claim that functional is a fad, but he's right in that it's not. Functional has been around as long as imperative has, in fact <a href="http://en.wikipedia.org/wiki/Alonzo_Church" target="_blank" rel="noopener noreferrer">Alonzo Church</a> pioneered it with the concepts of <a href="http://en.wikipedia.org/wiki/Lambda_calculus" target="_blank" rel="noopener noreferrer">lambda calculus</a>. 

That said, people don't like functional because it's hard to map to their mental model.  I don't know about you, but when I think of doing something 4 times I don't think of a recursive loop with an accumulator, I think of a for loop.  Making the mental jump to functional isn't easy for everyone which is why it has been slow to adopt to the mainstream.  

However, some aspects of functional are reasonably mainstream.  Lambdas, options, first class functions and higher order functions are available in many languages such as Ruby, Scala, C#, JavaScript, Python, etc.  Some even, like Scala, encourage immutable types!  Functional isn't a fad, but pure functional may be.

<h2>Reason 2: I get paid by the line</h2>

The point here is that functional languages are usually syntactically shorter.  An example the author posts is this:

[csharp]
public static class SumOfSquaresHelper
{
   public static int Square(int i)
   {
      return i * i;
   }

   public static int SumOfSquares(int n)
   {
      int sum = 0;
      for (int i = 1; i &lt;= n; i++)
      {
         sum += Square(i);
      }
      return sum;
   }
}
[/csharp]

compared to

[fsharp]
let square x = x * x
let sumOfSquares n = [1..n] |&gt; List.map square |&gt; List.sum
[/fsharp]

But that's cheating.  What if we did this instead:

[csharp]
public int SumOfSquares(int n)
{
    return Enumerable.Range(1, n).Select(i =&gt; i * i).Sum();
}
[/csharp]

And

[fsharp]
let square x = x * x
let sumOfSquares n = [1..n] 
                         |&gt; List.map square 
                         |&gt; List.sum
[/fsharp]

Now who has more lines?  It's all in how you see it. Granted, both are leveraging <a href="http://en.wikipedia.org/wiki/Higher-order_function" target="_blank" rel="noopener noreferrer">higher order functions</a>, but most modern imperative languages support that.  Comparing crappy code with good code is never a comparison.  Terse code can be written in any (almost) language (sorry Java).

<h2>Reason 3: I love me some curly braces</h2>

Personally I don't like space dependent languages since its easy to make scoping mistakes, but that notwithstanding, lets look at some Clojure:

[fsharp]
(defn run-prep-tasks
  [{:keys [prep-tasks] :as project}]
  (doseq [task prep-tasks]
    (let [[task-name &amp; task-args] (if (vector? task) task [task])
          task-name (main/lookup-alias task-name project)]
      (main/apply-task task-name (dissoc project :prep-tasks) task-args)))
[/fsharp]

While functional usually has less curly braces, many functional languages have a whole lot more parenthesis

<h2>Reason 4: I like to see explicit types</h2>

This is a common complaint from people who aren't used to functional, and I can understand, because if someone asked you what the signature below does on first glance what would you say?

[code]
('State -&gt; 'T1 -&gt; 'T2 -&gt; 'State) -&gt; 'State -&gt; 'T1 list -&gt; 'T2 list -&gt; 'State
[/code]

Practiced functional programmers can tell its a function that takes a function (which takes a state, two items, and returns a new state) a seed state, and two lists, and returns a final state. This is the type signature of List.fold2 and its a mouthful!

Compare to the example the author gave:

[csharp]
public IEnumerable&lt;IGrouping&lt;TKey, TSource&gt;&gt; GroupBy&lt;TSource, TKey&gt;(
    IEnumerable&lt;TSource&gt; source,
    Func&lt;TSource, TKey&gt; keySelector
    )
[/csharp]

Immediately at first glance, without caring about the types, you can tell it returns an enumerable, it takes an enumerable, and it takes a function.  At the signature you can even see how the source and the selector map to each other.  Reading the code you get a sense of how things work together. I won't lie, the signature is nasty, and its verbose. Part of me wishes C# had inferred method signatures, but the other part really likes that I can glance at something and get a big picture overview of what is happening.  

On top of that, its easy to make the mistake of passing a function instead of applying a function. Take this example:

[fsharp]
apply one two
[/fsharp]

You might think that we are applying two to one, or maybe one to two, or maybe I am passing in a function called one and an argument called two, or maybe both one and two are functions and are being combined and returned as another function, or maybe I meant to curry the apply function by applying one to two like this:

[fsharp]
apply (one two)
[/fsharp]

It's very easy to make mistakes like this in functional, especially if the type arguments are generic enough.  If the signature for <code>apply</code> is a <code>'a -> 'b -> 'c</code> then you don't know what you meant!  Anyways, this is the complaint people have about implicit vs explicit typing.

<h2>Reason 5: I like to fix bugs</h2>

I like to fix type mismatches AND bugs.

<h2>Reason 6: I live in the debugger</h2>

I still live in the debugger. To say that a language makes it so that if your code compiles it probably works just boggles my mind.  Code can compile fine but be logically completely wrong.   This happens in every language! In fact, debugging in fsharp can be complex because of the pipe operator (see my other post on debugging the pipe operator).

<h2>Reason 7: I don't want to think about every little detail</h2>

I didn't really get this one. The author talks about how by having all the types matched up you suddenly think of all edge conditions.  That's just not true. Like I mentioned above, edge condtions are part of logical flow, not code semantics.  You can have all the types match up and still have edge cases you didn't consider.

<h2>Reason 8: I like to check for nulls</h2>

This one is fun because I've brought this up with a coworker to discuss before. I, personally, really like the option type, but you can still have nulls. What about

[fsharp]
let foo = None;

Option.get foo
[/fsharp]

This results in:

[code]
System.ArgumentException was unhandled
  HResult=-2147024809
  Message=The option value was None
Parameter name: option
  Source=FSharp.Core
  ParamName=option
  StackTrace:
       at Microsoft.FSharp.Core.OptionModule.GetValue[T](FSharpOption`1 option)
       at &lt;StartupCode$FSharpScratch&gt;.$Print.main@() in C:\Projects\Program.fs:line 28
  InnerException: 
[/code]

Oops! So, you still have to match on option discriminated unions for none which means you are still checking for some sort of empty thing.  Instead of having

[code]
if(x != null){
}
[/code]

You start having

[code]
match x with 
 | Some item -&gt;
 | _ -&gt;
[/code]

I'm not saying matching is bad, just saying that its wrong to assume you get no exceptions since there are fewer nulls. 

A safer design pattern is to use the maybe monad, which can easily be built into every object type in C# using extension methods.  I also like the <code>get</code> and <code>getOrElse</code> pattern that Scala has, meaning that you can either get and it's a None type or get and if it's a None return a default.  Much safer.

<h2>Reason 9: I like to use design patterns everywhere</h2>

Design patterns apply to any code of any language.  To just write flat code with no organization or thought to structure is going to break large apps.  Those patterns exist, I'm sure, even in functional applications.  And if they don't, I'd be skeptical of their extensiblity and robustness.  

You always want to segment out 3rd party libraries behind proxies, you want to hide how things are created with factories, and you want to make sure that you interface with abstract classes and interfaces when necessary so you can inject different implementations of things.  In fact, the F# team have videos showing how to do certain <a href="http://channel9.msdn.com/posts/Tao-Liu-F-Design-Patterns" target="_blank" rel="noopener noreferrer">design patterns</a> with F#! 

<h2>Reason 10: It's too mathematical</h2>

This is something I've never heard people mention when complaining about functional, but maybe it's true. I don't know.  I can't comment on this one.

<h2>Conclusion</h2>

I think the article is hilarious and well written, and I am a huge proponent of functional languages. But I find that some things about functional do annoy me.  One of the reasons I really like f# is because you can do imperative work when you need to. That said, I think the big language winners will be languages like C# and Scala that embrace functional paradigms but also let you build with imperative.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3565</wp:post_id>
		<wp:post_date><![CDATA[2013-04-22 08:00:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-22 08:00:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[response-ten-reasons-functional-programming-language]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="functional"><![CDATA[functional]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560931038;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4306;}i:1;a:1:{s:2:"id";i:1828;}i:2;a:1:{s:2:"id";i:7777;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>79</wp:comment_id>
			<wp:comment_author><![CDATA[Passerby]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[horsesacoya@yahoo.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[99.103.166.160]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-23 22:39:45]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-23 22:39:45]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,

This is a very insightful article!  I always enjoy perspectives that challenge my own.  I had to do some research, but hopefully it resulted in some convincing counter-counter-arguments.

<strong>Reason 2: I get paid by the line</strong>
This reason is regarding those who prefer programming in the imperative style.  Trust me, there are those who prefer the original code, for loop and all.  To illustrate my point, here's a comment I recently came across: "I don’t know about you, but when I think of doing something 4 times . . . I think of a for loop." ;)

<strong>Reason 3: I love me some curly braces</strong>
The author specified that he was talking about the ML family of languages.

<strong>Reason 4: I like to see explicit types</strong>
What does the first half of your argument have to do with preferring explicit types?  The second half is just a matter of becoming familiar with the order of operations.  Most times if you mess up it will result in a type error right away.  If you're concerned about ambiguity you can always use parenthesis and explicitly declare the types.

<strong>Reason 5: I like to fix bugs</strong>
Type errors are that big of an issue?  They can be handled and avoided fairly easily.  Either way, isn't it worth dealing with that part of a functional language if you get less bugs?

<strong>Reason 6: I live in the debugger</strong>
In functional languages it's much easier to make illegal states unrepresentable, catching more errors at compile time.

<strong>Reason 7: I don’t want to think about every little detail</strong>
There are many instances where you are forced to handle edge cases.  When pattern matching you have to handle all possible results.  For your option example, you have to handle the case when the result is None.

<strong>Reason 8: I like to check for nulls</strong>
Option.get is unsafe, which is precisely why its use is discouraged.  Fortunately, there are idiomatic ways to safely handle options.  Sadly, this is not the case for nulls.  Also, options are opt-in, meaning you only have to use them when necessary.  Nulls, on the other hand, are attached to every reference type, whether you want them or not.  The maybe monad could be helpful, but it's still not optimal.  Either you have to trust that people will no longer use null, or you still have to validate every object, even ones that you don't expect to be Nothing.  It's possible to mess either up and not get an error at compile time.  Isn't Scala's get just the same as Option.get?  And getOrElse is defaultArg in F#.

<strong>Reason 9: I like to use design patterns everywhere</strong>
Design patterns are used to address design problems.  One could argue that design patterns are less common in functional languages because you run into less design problems.  For example, the strategy pattern requires creating a class for each strategy, while you could just use higher order functions.  

You're grouping C# with Scala?  Isn't C# closer to Java?  Granted, C# has added a lot of nice functional features, but it's missing many crucial ones like immutability by default and non-nullability by default.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>80</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.243.58.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-23 23:38:40]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-23 23:38:40]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey, thanks for the response! I agree with most of your responses, like I said, I was playing devils advocate :). When it comes down to it, I'm all for whatever gets to the code to be as short, readable, and bug free as possible. Functional is definitely a part of that end goal, but I don't think its the only part.

As far as the C# and Scala comment, that was mainly in reference to first class functions and lazy lists (and infinite sequences).  I definitely agree about the non-nullability. What I wouldn't give for an Option...
]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>79</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>81</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #17, 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/04/29/f-weekly-17-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[72.232.7.41]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-28 21:08:15]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-28 21:08:15]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Anton Kropp wrote &#8220;A response to &#8220;Ten reasons to not use a functional programming language&#8221;&#8220;. [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>82</wp:comment_id>
			<wp:comment_author><![CDATA[Kingsley Payne]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kingsleypayne@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[196.209.245.69]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-30 19:00:45]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-30 19:00:45]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[What role does Automatic Generalization play in this argument?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>83</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.193.149.209]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-04-30 19:06:03]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-04-30 19:06:03]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Kingsley, thanks for asking. Keep in mind I'm playing both sides of the field here. I guess the argument here is that by having everything be automatically general makes for confusing type signatures.  Most of the time code isn't general, and you have to specify some of the argument types of functions to be able to do work you want, so making it general just makes it needlessy complex to begin with.  In general,  the argument against making everything automatically generalized is readability and ambiguous type relationships (at first glance). 

That said, it kind of goes without saying that the goal is to make function signatures clean and easy to understand.  It's a smell when you can't at a glance tell what it does by its signature.   But, when everything is auto generalized that does promote you to write reusable and modular code. When done right its a huge bonus. When done wrong, deciphering type mismatches is really annoying.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>82</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech Talk: Text Editors</title>
		<link>https://onoffswitch.net/2013/04/18/tech-talk-text-editors/</link>
		<pubDate>Thu, 18 Apr 2013 15:55:59 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3608</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk was a little less tech but no less important.  We got together and talked about the different text editors that we use and why we like them. 

<h2>JuJu Edit</h2>

<ul>
<li>Pros: We use JuJu at work a lot because it handles enormous files really well.  And when I mean enormous I mean upwards of 50+GB log files.  Other editors choke when its this big but JuJu handles it fine.  On top of that JuJu lets you do custom syntax highlighting, which is great for log files, since now you can highlight Debug, Warn, Info, Errors, etc. It's pretty bare bones but its very lightweight and useful for quick note taking and basic text editing.  Also having the ability to auto reload the file on changes makes it great as a <code>tail</code> replacement.</li>

<li>Cons: JuJu isn't in active development anymore and the documentation is non-existant (setting up the regex to do log line highlighting was a lot of trial and error).  If you set up your regex for highghlitng wrong it can really choke JuJu and make it run super slow. Also, with large files sometimes it can grind to a halt if you need to scroll to an arbitrary position in an enormous file.</li>

<li><a href="http://jujusoft.com/jujuedit/" target="_blank" rel="noopener noreferrer">http://jujusoft.com/jujuedit/</a></li>

</ul>

<h2>EditPad Lite</h2>

<ul>
<li>Pros: The team commented that EditPad Lite handles large files pretty well, has undo/redo support, auto-save, and a multi tab interface to handle lots of open files. </li>

<li>Cons: No syntax highlighting and the multi tab interface. </li>

<li><a href="http://www.editpadlite.com/" target="_blank" rel="noopener noreferrer">http://www.editpadlite.com/</a></li>
</ul>

<h2>EditPad Pro</h2>

<ul>
<li>Pros: On top of what you get with EditPad you also get syntax highlighting which is cool.</li>

<li>Cons: Not much other than its not free</li>

<li><a href="http://www.editpadpro.com/" target="_blank" rel="noopener noreferrer">http://www.editpadpro.com/</a></li>
</ul>

<h2>Notepad++</h2>

<ul>
<li>Pros: Notepad++ is a pretty big winner in the text editor shootout. It has auto-reload on change, plugin support, great find in file support, it's actively maintained. It also lets you do cool block/column selection (great for selecting code without the leading indentations).</li>

<li>Cons: A common complaint was that it's a heavier application than something like JuJu edit. Even with medium sized files (a few hundred MB) the memory usage of the application can soar.  So while this may make a great actual text editor, it makes for a poor log viewer.</li>

<li><a href="http://notepad-plus-plus.org/" target="_blank" rel="noopener noreferrer">http://notepad-plus-plus.org/</a></li>

</ul>

<h2>Intype</h2>

<ul>
<li>Pros: Intype is a new player and it's being actively developed.  It has syntax highlighting and commands related to specific file types.  Also, it lets you handle language fallback fonts, so if you are dealing with different languages this kind of cool.  If the default font doesn't have the character set that a language wants to load then it can fallback to another font, and then another, and another.  Great if you are working with localization.</li>

<li>Cons: None yet!</li>

<li><a href="http://inotai.com/intype/" target="_blank" rel="noopener noreferrer">http://inotai.com/intype/</a></li>
</ul>


<h2>Markdown Pad</h2>

<ul>
<li>Pros: I mentioned this at our talk because I like to take notes in markdown. Markdown Pad also lets you put in github flavorerd markdown for code highlighting, and you can render the final markdown as html (with inline CSS), or as a PDF.  You write markdown in a left panel and it renders the markdown in a right panel, so you can get a preview of what you are writing as you write it. It has a bunch of other really nice features that I like and for note taking I think its better than just a plain text editor.</li>

<li>Cons: To get github flavored markdown you need to pay, also to get a bunch of other neat features (like PDF support) you need to get the pro version. THankfully its reasonably cheap and I've gone ahead and done it.  One thing I'm not too fond of is the delay in rendering markdown. I think this is because they are doing some optimization to handle very large files, but it would be nice if they did it only when a file reached a certain point and not all the time.</li>

<li><a href="http://markdownpad.com/" target="_blank" rel="noopener noreferrer">http://markdownpad.com/</a></li>

</ul>


For those who are wondering, we did discuss vim, emacs, textmate, and scintilla, but didn't spend much time talking about them hence why I didn't include it. I've personally used vim a lot in my linux days but it's not my personal choice.

Text editors are a highly personal choice, so it's great that there are so many tools out there. The paid versions of these apps are usually relatively cheap and if you like a piece of software definitely buy it!  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3608</wp:post_id>
		<wp:post_date><![CDATA[2013-04-18 15:55:59]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-18 15:55:59]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-text-editors]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="markdown"><![CDATA[markdown]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
		<category domain="post_tag" nicename="text-editors"><![CDATA[text editors]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559335486;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4568;}i:1;a:1:{s:2:"id";i:4244;}i:2;a:1:{s:2:"id";i:3536;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Images, memory leaks, GDI+, and the aggregate function</title>
		<link>https://onoffswitch.net/2013/04/24/memory-leaks-images/</link>
		<pubDate>Wed, 24 Apr 2013 00:55:42 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3640</guid>
		<description></description>
		<content:encoded><![CDATA[I ran into a neat C# memory leak today that I wanted to share. It's not often you get a clear undeniable leak in C# and so I really had fun figuring this one out.

Look at this and see if you can spot the leak:

[csharp]
public static class Extensions
{
    public static Image Append(this Image source, Image append)
    {
        var newImage = new Bitmap(source.Width + append.Width, source.Height);
        using (var g = Graphics.FromImage(newImage))
        {
            g.DrawImage(source, 0, 0);

            g.DrawImage(append, source.Width, 0);
        }

        return newImage;
    }
}

private static void Main(string[] args)
{
    var src = @&quot;C:\users\anton\desktop\bigImage.jpg&quot;;

    var images = Enumerable.Repeat(Image.FromFile(src), 25).ToList();

    var appendedImage = images.Aggregate((acc, i) =&gt; acc.Append(i));

    foreach (var image in images)
    {
        image.Dispose();
    }

    appendedImage.Dispose();

    Console.ReadLine();
}
[/csharp]


What this code does is create 25 instances of my bigImage.jpg (6.5MB), and then creates a new image consisting of those 25 images side by side. The aggregate function folds the list into a single image using the <code>Append</code> extension method. This way the accumulator is the new running image.

Since <code>Image</code> is disposable, I've disposed of the source images, as well as the final new image. Should be cool right? 

And yet, even though I have deterministically released resources (the whole point of dispose), I have a memory leak!

<img src="http://onoffswitch.net/wp-content/uploads/2013/04/memLeakImage..png" alt="memLeakImage." width="484" height="95" class="alignnone size-full wp-image-3641" />

Weird... 

What if I get rid of the fold and append everything in one go?

[csharp]
public static Image Append(this Image source, List&lt;Image&gt; appends)
{
    var newImage = new Bitmap(source.Width + appends.Count() * source.Width, source.Height);
    using (var g = Graphics.FromImage(newImage))
    {
        g.DrawImage(source, 0, 0);

        int i = 1;
        foreach (var item in appends)
        {
            g.DrawImage(item, i * source.Width, 0);

            i++;
        }
    }

    return newImage;
}

private static void Main(string[] args)
{
    var src = @&quot;C:\users\anton.kropp\desktop\bigImage.jpg&quot;;

    var images = Enumerable.Repeat(Image.FromFile(src), 25).ToList();

    var appendedImage = images.First().Append(images.Skip(1).ToList());

    foreach (var image in images)
    {
        image.Dispose();
    }

    appendedImage.Dispose();

    Console.ReadLine();
}
[/csharp]

And now when I wait on the read at the end

<img src="http://onoffswitch.net/wp-content/uploads/2013/04/noMemLeakImage..png" alt="noMemLeakImage." width="506" height="44" class="alignnone size-full wp-image-3642" />

No memory leak!

Something with that aggregate function is causing a leak.  Well, when in doubt, go to the source. Here is the decompiled source for the <code>Aggregate</code> overload that uses the first element in the sequence as the seed

[csharp highlight="14"]
[__DynamicallyInvokable]
public static TSource Aggregate&lt;TSource&gt;(this IEnumerable&lt;TSource&gt; source, Func&lt;TSource, TSource, TSource&gt; func)
{
    if (source == null)
        throw Error.ArgumentNull(&quot;source&quot;);
    if (func == null)
        throw Error.ArgumentNull(&quot;func&quot;);
    using (IEnumerator&lt;TSource&gt; enumerator = source.GetEnumerator())
    {
        if (!enumerator.MoveNext())
            throw Error.NoElements();
        TSource source1 = enumerator.Current;
        while (enumerator.MoveNext())
            source1 = func(source1, enumerator.Current);
        return source1;
    }
}
[/csharp]

Once I saw this I spotted the leak right away.  The issue is that each time an item is folded, the function is overwriting the reference to the first element with the aggregation lambda (highlighted).  Since the <code>Image</code> class wraps native GDI+ code, this means that unless you call dispose on each instance that is created, the underlying unmanaged resources are still sitting around!  

The reason the second version works is because I am not making intermediary images via an accumulator. Each image reference that is created is tracked then properly destroyed.

Now, understanding the issue, I was able to mimic the problem outside of the aggregate function:

[csharp]
public static void MemLeak()
{
    var src = @&quot;C:\users\anton.kropp\desktop\bigImage.jpg&quot;;

    Image image1 = null;

    foreach (var i in Enumerable.Range(0, 10))
    {
        image1 = Image.FromFile(src);
    }

    image1.Dispose();

    Console.ReadLine();
}
[/csharp]

Just because it's the same variable, does not mean it's the same reference.  Actually if we modify the code like this we can really drive the point home:

[csharp]
public static void MemLeak()
{
    var src = @&quot;C:\users\anton.kropp\desktop\bigImage.jpg&quot;;

    Image image1 = null;

    Image previousImage;

    foreach (var i in Enumerable.Range(0, 10))
    {
        previousImage = image1;

        image1 = Image.FromFile(src);

        Console.WriteLine(&quot;References equal? &quot; + ReferenceEquals(previousImage, image1));
    }

    image1.Dispose();

    Console.ReadLine();
}

[/csharp]

Which prints out

[code]
References equal? False
References equal? False
References equal? False
References equal? False
References equal? False
References equal? False
References equal? False
References equal? False
References equal? False
References equal? False
[/code]

Which makes sense.

<h2>Conclusion</h2>

What I got out of all of this is that you really need to know where your references are and understand what your temporary values are doing. Also, don't use disposable items as accumulators in an aggregator.

This is where I really regret posting the last article as a response to  functional. If things were immutable by default I wouldn't have run into this problem.  :)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3640</wp:post_id>
		<wp:post_date><![CDATA[2013-04-24 00:55:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-24 00:55:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[memory-leaks-images]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="post_tag" nicename="image"><![CDATA[Image]]></category>
		<category domain="post_tag" nicename="memory-leak"><![CDATA[memory leak]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561976764;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3524;}i:1;a:1:{s:2:"id";i:4028;}i:2;a:1:{s:2:"id";i:3232;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Debugging Serialization Exception: The constructor to deserialize an object was not found.</title>
		<link>https://onoffswitch.net/2013/04/30/debugging-serializationexception-constructor-deserialize-object-type-com-thesilentgroup-fluorine-asobject-found/</link>
		<pubDate>Tue, 30 Apr 2013 20:38:27 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3696</guid>
		<description></description>
		<content:encoded><![CDATA[Today I was debugging an exception that was occuring when remoting a data object between two .NET processes. I  kept getting 

[code]
System.Runtime.Serialization.SerializationException: The constructor to deserialize an object of type 'com.TheSilentGroup.Fluorine.ASObject' was not found.
[/code]

The issue I had was that there was a .NET object that looked like this

[csharp]
public class ItemDto{
  public object Item { get;set; }
}
[/csharp]

Which was used as a <a href="http://en.wikipedia.org/wiki/Covariance_and_contravariance_(computer_science)" target="_blank" rel="noopener noreferrer">covariant</a> store for any item (because everything is an <code>object</code>).  This was needed because the code I was working in leveraged reflection to pull out certain fields at runtime depending on whatever this object type really was.  

But, at some point the <code>ItemDto</code> object was sent to an ActionScript frontend.  Later, the same object came back to .NET and the property <code>Item</code> was now a <code>ASObject</code> type due to <a href="http://www.fluorinefx.com/" target="_blank" rel="noopener noreferrer">Fluorine</a>'s serialization process.  Next, this object had to be serialized to another .NET process, and that's where the exception was happening.

It was obvious that there was an issue with the <code>ASObject</code> but I had assumed it was because the host process (that I was remoting to) didn't have a reference to Fluorine's dll to serialize the ASObject.  But that wasn't it.

Turns out that <code>ASObject</code> inherits from <code>HashTable</code> which implements <code>ISerializable</code>.

<a href="http://onoffswitch.net/wp-content/uploads/2013/04/2013-04-30-16_31_02-JetBrains-dotPeek-1.0-EAP.-Build-1.0.0.png" rel="attachment wp-att-3697"><img src="http://onoffswitch.net/wp-content/uploads/2013/04/2013-04-30-16_31_02-JetBrains-dotPeek-1.0-EAP.-Build-1.0.0.png" alt="HashTable ISerializable" width="888" height="333" class="alignnone size-full wp-image-3697" /></a>

From the <a href="http://msdn.microsoft.com/en-us/library/system.runtime.serialization.iserializable.aspx" target="_blank" rel="noopener noreferrer">MSDN</a>

<blockquote> If a class needs to control its serialization process, it can implement the ISerializable interface. [...] The ISerializable interface <strong>implies a constructor</strong> with the signature constructor (SerializationInfo information, StreamingContext context). At deserialization time, the current constructor is called only after the data in the SerializationInfo has been deserialized by the formatter. In general, this constructor should be protected if the class is not sealed.</blockquote>

So, because an interface can't enforce a constructor signature, you have to make sure to implement this constructor on any object (even derived objects) if they are going to be serialized across .NET boundaries and implement ISerializable.

The solution, for me, was to just not send this object to the UI and back. It didn't need to. But, you can overcome this issue by subclassing your data class, implementing the needed constructor, and calling the base class (in this case HashTable's) constructor for proper serialization.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3696</wp:post_id>
		<wp:post_date><![CDATA[2013-04-30 20:38:27]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-04-30 20:38:27]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[debugging-serializationexception-constructor-deserialize-object-type-com-thesilentgroup-fluorine-asobject-found]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="debugging"><![CDATA[Debugging]]></category>
		<category domain="post_tag" nicename="serialization"><![CDATA[serialization]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560848118;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4456;}i:1;a:1:{s:2:"id";i:4919;}i:2;a:1:{s:2:"id";i:3779;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tech Talk: AngularJS</title>
		<link>https://onoffswitch.net/2013/05/02/tech-talk-angularjs/</link>
		<pubDate>Thu, 02 May 2013 16:40:05 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3710</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk was a continuation on front-end discussions we're having. Last week we talked about typescript (I forgot to write it up) and this week we discussed the basics of <a href="http://angularjs.org/" target="_blank" rel="noopener noreferrer">angular</a>.  Angular is a front-end MVC framework written by google that, at first glance, looks completely different from previous javascript/html development.  The basic gist is to strongly decouple logic into encapsulated modules.   But that's not all there is, there's a lot to it. Angular has a templating engine, dependency injection, double bindings between views and controllers, event dispatching, etc.  

Since it's hopeless to cover all of angular in one blog post I'll just mention a few of the good questions that came up during our discussion

<ul>
<li><em>Can a child component dispatch to a parent component? Let's say something happened in an innner component and you need a parent compoinent to also register that?</em><br/>
Sure, angular provides a function called <code>emit</code> that lets a child component redispatch an event to parent components, and a <code>broadcast</code> method for a parent component to dispatch to all child components.
</li>

<li><em>Why use client side rendering vs server side rendering?</em><br/>
This was an interesting discussion we had.  While we didn't come up with a definite answer, we figure it's partial because browsers can handle more work now, and partially beacuse the trend is to decouple the server from view logic.  Servers are more often being designed as a collection of REST apis and to serve static documents, so the UI can now handle compilation of a view and offload that work onto a client.  So why does server side rendering still exist?  Well, it's the way its always been done and it's easy to do. On top of that it does offload client side work. I guess it's two schools of thought of when to do what.  
</li>

<li><em>How do you profile angular?</em><br/>
This was an interesting discussion.  We didn't know of any particular specific ways to profile angular other than built in profiling tools in browsers and other external html/js profilers, but angular has done a really nice job of exposing almost the entire framework in a <a href="http://docs.angularjs.org/guide/dev_guide.unit-testing" target="_blank" rel="noopener noreferrer">unit testable way</a>.  This means you can step through angular and see where bottlenecks are within the framework.
</li>

<li><em>What if angular stops being developed, can you use a later version of JQuery if their bundled version of JQuery lite stops being updated?</em><br/>
Another good question. You can alias new version of JQuery (apparenlty JQuery already has this feature), but you can also provide a new JQuery object as a provider to any directives if you wanted to.  This way you can have the original JQuery that angular comes with, and you can have the injected secondary JQuery that you can use.
</li>

<li><em>Is it possible to mix server side rendering (with Razor or Jade) with angular client side templating?</em><br/>
Sure, since whatever the server doesn't recognize (in the {{ ... }} syntax) will be rendered in the client side.  
</li>

<li><em>Does angular come with prepackged widgets?</em><br/>
Not really, but it's easy to wrap any other frameworks/libraries widgets within directives.  
</li>

</ul>

<h2>More Info</h2>

Here are some links that talk more about angular

Video Tutorial: AngularJS Fundamentals in 60-ish Minutes: <a href="http://weblogs.asp.net/dwahlin/archive/2013/04/12/video-tutorial-angularjs-fundamentals-in-60-ish-minutes.aspx" target="_blank" rel="noopener noreferrer">http://weblogs.asp.net/dwahlin/archive/2013/04/12/video-tutorial-angularjs-fundamentals-in-60-ish-minutes.aspx</a>

Angular Providers: <a href="http://slides.wesalvaro.com/20121113/#/" target="_blank" rel="noopener noreferrer">http://slides.wesalvaro.com/20121113/#/</a>

AngularJS MTV Meetup: Best Practices (2012/12/11): <a href="http://www.youtube.com/watch?v=ZhfUv0spHCY" target="_blank" rel="noopener noreferrer">http://www.youtube.com/watch?v=ZhfUv0spHCY</a>

Excellent set of short video tutorials on AngularJS (recommended!): <a href="http://www.egghead.io/" target="_blank" rel="noopener noreferrer">http://www.egghead.io/</a>

<a href="http://www.egghead.io/video/HvTZbQ_hUZY" target="_blank" rel="noopener noreferrer">http://www.egghead.io/video/HvTZbQ_hUZY</a>

Code Organization in Large AngularJS and JavaScript Applications: <a href="http://cliffmeyers.com/blog/2013/4/21/code-organization-angularjs-javascript" target="_blank" rel="noopener noreferrer">http://cliffmeyers.com/blog/2013/4/21/code-organization-angularjs-javascript
</a>

Building up AngularJS (Greg Weber yap.TV): <a href="http://www.slideshare.net/antonkropp/angular-js-meetup-20416779">http://www.slideshare.net/antonkropp/angular-js-meetup-20416779</a>
<a href="http://stephanebegaudeau.tumblr.com/post/48776908163/everything-you-need-to-understand-to-start-with" target="_blank" rel="noopener noreferrer">http://stephanebegaudeau.tumblr.com/post/48776908163/everything-you-need-to-understand-to-start-with</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3710</wp:post_id>
		<wp:post_date><![CDATA[2013-05-02 16:40:05]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-02 16:40:05]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-angularjs]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="angularjs"><![CDATA[angularjs]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558720449;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4028;}i:1;a:1:{s:2:"id";i:4515;}i:2;a:1:{s:2:"id";i:3500;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Capturing union values with fparsec</title>
		<link>https://onoffswitch.net/2013/05/02/capturing-union-values-fparsec/</link>
		<pubDate>Thu, 02 May 2013 20:02:06 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3723</guid>
		<description></description>
		<content:encoded><![CDATA[I just started playing with <a href="http://www.quanttec.com/fparsec/" target="_blank" rel="noopener noreferrer">fparsec</a> which is a <a href="http://en.wikipedia.org/wiki/Parser_combinator" target="_blank" rel="noopener noreferrer">parser combinatorics</a> library that lets you create chainable parsers to parse DSL's.  After having built my own parser, lexer, and interpreter, playing with other libraries is really fun, I like seeing how others have done it.  Unlike my mutable parser written in C#, with FParsec the idea is that it will encapsulate the underlying stream state and result into a parser object.  Since F# is mostly immutable, this is how the underlying modified stream state gets captured and passed as a new stream to the next parser.  I actually like this kind of workflow since you don't need to create a grammar which is parsed and creates code for you (which is what ANTLR does).  There's something very appealing to have it be dynamic.  

As a quick example, I was following the <a href="http://www.quanttec.com/fparsec/tutorial.html" target="_blank" rel="noopener noreferrer">tutorial</a> on the fparsec site and wanted to understand how to capture a value to store in a discriminated union. For example, if I have a type

[fsharp]
type Token = 
     | Literal of string
[/fsharp]

How do I get a <code>Literal("foo")</code> created?

All of the examples I saw never looked to instantiate that.  After a bit of poking around I noticed that they were using the <code>|>></code> syntax which is a function that is passed the result value of the capture.  So when you do

[fsharp]
pstring &quot;foo&quot; |&gt;&gt; Literal
[/fsharp]

You've invoked the constructor of the discriminated union similar to this:

[fsharp]
let literal = &quot;foo&quot; |&gt; Literal 
[/fsharp]

Which is equivalent to

[fsharp]
let literal = Literal(&quot;foo&quot;)
[/fsharp]

This is because most of the fparsec functions and overloads give you back a Parser type

[fsharp]
type Parser&lt;'TResult, 'TUserState&gt; = CharStream&lt;'TUserState&gt; -&gt; Reply&lt;'TResult&gt;
[/fsharp]

Which is just an alias for a function that takes a utf16 character stream that holds onto a user state and returns a reply that holds the value you wanted.  If you look at <a href="http://www.quanttec.com/fparsec/reference/charstream.html#CharStream" target="_blank" rel="noopener noreferrer">charstream</a> it looks similar to my simple <a href="https://github.com/devshorts/LanguageCreator/blob/master/Lang/Lexers/TokenizableStreamBase.cs" target="_blank" rel="noopener noreferrer">tokenizer</a>.  The functions <code>|>></code>, <code>>>=</code>, and <code>>>%</code> are all overloads that help you chain parsers and get your result back.  If you are curious you can trace through their types <a href="http://www.quanttec.com/fparsec/reference/primitives.html#members.:62::62::61:" target="_blank" rel="noopener noreferrer">here</a>. 

Now, if you don't need to capture the result value and want to just create an instance of an empty union type then you can use the <code>>>%</code> syntax which will let you return a result:

[fsharp]
let Token = 
    | Null
let nullTest = pstring &quot;null&quot; &gt;&gt;% Null 
[/fsharp]

There are a bunch of overloaded methods and custom operators with fparsec. For example

[fsharp]
let Token = 
    | Null
let nullTest = stringReturn &quot;null&quot; Null
[/fsharp]

Is equivalent to the <code>>>%</code> example.  

It's a little overwhelming trying to figure out how all the combinators are pieced together, but that's part of the fun of learning something new.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3723</wp:post_id>
		<wp:post_date><![CDATA[2013-05-02 20:02:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-02 20:02:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[capturing-union-values-fparsec]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="combinators"><![CDATA[combinators]]></category>
		<category domain="post_tag" nicename="fsharp"><![CDATA[fsharp]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[fparsec]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558731860;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4131;}i:1;a:1:{s:2:"id";i:4068;}i:2;a:1:{s:2:"id";i:4077;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>84</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #18, 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/05/06/f-weekly-18-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[76.74.248.198]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-05-05 21:00:39]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-05-05 21:00:39]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Anton Kropp wrote &#8220;Capturing union values with fparsec&#8220;. [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>The largest mass problem</title>
		<link>https://onoffswitch.net/2013/05/04/largest-mass-problem/</link>
		<pubDate>Sat, 04 May 2013 18:05:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3735</guid>
		<description></description>
		<content:encoded><![CDATA[I was recently asked to write some code to find the largest contiguous group of synonymous elements in a two dimensional array. The idea is that you want to find the largest "land mass" in a problem where you have a game board that looks something like

[code]
L L L W
W L L W
L W W W
W L L W
[/code]

Where <code>L</code> stands for land, and <code>W</code> stands for water.  In this example, the largest land mass would be of size 5. But there are also 2 other land masses, one of size one, and another of size two.  Elements can be contiguous only if their direct adjacent neighbor is the same type, so diagonals don't count.

In general, you can think of the largest mass problem as almost exactly the same as the <a href="http://en.wikipedia.org/wiki/Flood_fill" target="_blank" rel="noopener noreferrer">flood fill problem</a> in image graphics. Except with flood fill, you are given a location and you want to find all the contiguous areas to fill.  Here, you don't know where to start, you have to find all the contiguous areas in the board.

To me, this solution smells of recursion. You need a way to start at a point, and branch in all directions following until you find non land areas.  This way you can go up, down, left and right starting at a point and each direction will return to you what it found.  

<h2>The board</h2>

First, lets define our example board:

[fsharp]
type Earth = 
    | Land
    | Water

let board = array2D [[Land;  Land;  Land;  Water;];
                     [Water; Land;  Land;  Water;];
                     [Land;  Water; Water; Water;];
                     [Water; Land;  Land;  Water;]]
[/fsharp]

<h2>Moving around</h2>

Next, lets define some helper methods. Since I know I'm going to have to branch up, down, left and right.  I also know that I need to cover edge conditions such as when I'm iterating over the board and I am going to step off the board edge (beyond the size of the 2d array).  I'm treating the current position on the board as an integer tuple representing x and y.  

[fsharp]
let moveRight position = 
    let (x,y) = position
    (x + 1, y)

let moveLeft position = 
    let (x,y) = position
    (x - 1, y)

let moveUp position = 
    let (x,y) = position
    (x, y + 1)

let moveDown position = 
    let (x,y) = position
    (x, y - 1)

let xSize board = Array2D.length1 board

let ySize board = Array2D.length2 board

let offBoard position board = 
    let (x,y) = position
    x &lt; 0 || y &lt; 0 || x &gt;= (xSize board) || y &gt;= (ySize board)
[/fsharp]

<h2>Keeping track of where you've been</h2>

I also know that since I'm going to be branching through this board in different recursive iterations, I need to be able to keep track of cells that I've already worked on. This makes sure that one branch (for example going left) doesn't re-process cells that were processed by another branch (like one that went up).  I have two methods here, one to just cons the current position to the previous positions list, and another to help me find if the current position is in a positions list.

[fsharp]
let markPosition position previousSpots = position::previousSpots

let positionExists position list = 
    List.exists(fun pos -&gt; pos = position) list
[/fsharp]

<h2>Did I find one?</h2>

Also, I can create a helper method that tells me if the current position I'm on matches the target type that I want.  

[fsharp]
let positionOnTarget position board target = 
    if offBoard position board then 
        false
    else
        let (x, y) = position
        (Array2D.get board x y) = target
[/fsharp]

You may have noticed that a lot of these helper functions are only one line, and sometimes just wrap another one line built in F# functionality. I like to do it that way for readability sake. 

<h2>Finding masses</h2>

Lets start with what flood fill does. Given a position, find all the contiguous elements. Each time we find a block it returns the block positions it found and the elements it already processed as a tuple

[fsharp]
type Board&lt;'T&gt; = 'T[,]

type X = int

type Y = int

type Position = X * Y

type PositionList = Position list 

type ProcessedPositions = PositionList

type ContiguousPoints = PositionList

type MassFinder = ContiguousPoints * ProcessedPositions

(*
    Looks for a specified contigoius block
    and keeps track of processed positions using a 
    reference cell of a list of positions (supplied by the caller)
*)

let findMassStartingAt (position:Position) (board:Board&lt;'A&gt;) (target:'A) (positionSeed:ProcessedPositions) : MassFinder = 
    let rec findMassStartingAt' position (currentMass:ContiguousPoints, processedList:ProcessedPositions) = 
        
        // if you move off the board return
        if offBoard position board then
            (currentMass, processedList)

        // if you already processed this position then don't do anything
        else if positionExists position processedList then
            (currentMass, processedList)
        else  
            
            // branch out left, up, right, and down and see what you can find
            let up = moveUp position
            let down = moveDown position
            let left = moveLeft position
            let right = moveRight position
            
            let found = positionOnTarget position board target

            match found with 
                | true -&gt;
                    (position::currentMass, position::processedList)
                        |&gt; findMassStartingAt' up 
                        |&gt; findMassStartingAt' down 
                        |&gt; findMassStartingAt' left 
                        |&gt; findMassStartingAt' right 

                | false -&gt; 
                    // if you didn't find anything return the masses that you 
                    // found prevoiusly
                    (currentMass, processedList)

    findMassStartingAt' position ([], positionSeed)
[/fsharp]

Each time the mass function is called it returns the masses it found. This is why up, down, left and right are all being piped a new list telling it what's already been found.  By the end of the entire search the recursive calls have returned all available contiguous blocks starting from the original seed position.  Also instead of passing the board to the inner list I'm leveraging the parent closure to reference the board. 

But, this only finds a mass if we told it where to start.  To search for other masses I opted to brute force the problem and iterate over the entire 2d array, re-using the function that knew how to find a single mass.   To iterate over the 2d array I created the following function

[fsharp]
(* 
   Iterate over each element in a 2d array, passing the x and y
   coordinate and the board, to the supplied function
   which can return an item. The items are all cons together
   and the function returns a new list
*)

let forEachElement (applier:(X -&gt; Y -&gt; Board&lt;'a&gt; -&gt; 'b)) (twoDimArray:Board&lt;'a&gt;) =
    let mutable items = [] 
    for x in 0..(xSize board) do
        for y in 0..(ySize board) do            
            items &lt;- (applier x y twoDimArray)::items
    items
[/fsharp]

Which lets you apply a function to each element and return a new item.  The other Array2D built in functions always created other 2D arrays, but I basically wanted to create a list based on the indexes and not just the element at those indexes.

Now our final contiguous searcher looks like this.  Remember that each block we find returns a <code>MassFinder</code> tuple which is <code>ContiguousPoints * ProcessedPositions</code> so I am just picking out the contiguous blocks with the <code>fst</code> map.

[fsharp]
(*
    Finds all contiguous blocks of the specified type
    and returns a list of lists (each list is the points for a specific
    block)
*)

let getContiguousBlocks board target = 

    // go through each board element and find masses starting at the
    // the current position
    // filter out any positions that found no masses
    let findMass x y board = findMassStartingAt (x, y) board target []

    forEachElement findMass board
        |&gt; List.map fst
        |&gt; List.filter (List.isEmpty &gt;&gt; not)
[/fsharp]

Breaking things up like this also lets us solve the correlary problem of flood fill! I'm passing it an empty list as a seed to say we haven't processed any elements

[fsharp]
(*
    Returns a list of points representing a contigious block 
    of the type that the point was at. 
*)

let floodFillArea (point:Position) (canvas:Board&lt;'T&gt;) =
    let (x, y) = point
    let itemAtPoint = Array2D.get canvas x y
    
    findMassStartingAt point canvas itemAtPoint [] |&gt; fst

[/fsharp]


<h2>The test</h2>

Well, lets try it:

[fsharp]
(* 
    Test functions to run it
*)

let masses = getContiguousBlocks board Land

let largestList = List.maxBy(List.length) masses

let massAt = floodFillArea (2, 2) boardInt

let sizeOfMassAt22 = List.length massAt

System.Console.WriteLine(&quot;Largest mass is &quot; + (List.length largestList).ToString());
System.Console.WriteLine(&quot;Mass size at (2,2) is &quot; + sizeOfMassAt22.ToString());
[/fsharp]

And the answer is... 

[code]
Largest mass is 5
Mass size at (2,2) is 6
[/code]

Where index (2,2) was the large water block in the middle.

Looks like it worked

<h2>But what about stack depth?</h2>

Now that there is a basic working version, we can make this a bit more advanced.  If we use continuation passing style then we can make our multiple branching recursion tail recursive. Here is a rewritten version of the mass finder function but which uses continuations:

[fsharp]
let findMassStartingAt (position:Position) (board:Board&lt;'A&gt;) (target:'A) (positionSeed:ProcessedPositions) : MassFinder = 
    let rec findMassStartingAt' position (currentMass:ContiguousPoints, processedList:ProcessedPositions) cont = 
        
        // if you move off the board return
        if offBoard position board then
            cont((currentMass, processedList))

        // if you already processed this position then don't do anything
        else if positionExists position processedList then
            cont((currentMass, processedList))
        else  
            
            // branch out left, up, right, and down and see what you can find
            let up = moveUp position
            let down = moveDown position
            let left = moveLeft position
            let right = moveRight position
            
            let found = positionOnTarget position board target   

            // track that we processed this element even if we don't find anything
            let updatedProcess = position::processedList

            match found with 
                | true -&gt;                    
                           let massState = (position::currentMass, updatedProcess)

                           findMassStartingAt' up  massState (fun foundMassUp -&gt; 
                           findMassStartingAt' down foundMassUp (fun foundMassDown -&gt;
                           findMassStartingAt' left foundMassDown (fun foundMassLeft -&gt;
                           findMassStartingAt' right foundMassLeft cont))) 

                | false -&gt; 
                    // if you didn't find anything return the masses that you 
                    // found previously
                    cont((currentMass, updatedProcess))


    findMassStartingAt' position ([], positionSeed) id
[/fsharp]

Instead of letting all the recursion bubble and piping that value to the next recursion, now we're capturing what to do when the next recursion is ready to run. By using the closure state we can capture what is the next point to go to, and we know that the next <code>MassFinder</code> that was previously processed will be passed to the continuation at each round. Now there's no worry about stack depth!

If we look at the IL that was generated for the inner recursive function we can really illustrate the point, that the F# compiler has emitted a <a href="http://stackoverflow.com/questions/15864670/generate-tail-call-opcode" target="_blank" rel="noopener noreferrer">tail call opcode</a>:

[csharp highlight="33"]
.method assembly static 
	!!a 'findMassStartingAt\'@176'&lt;A, a&gt; (
		!!A[0..., 0...] board,
		!!A target,
		int32 position_0,
		int32 position_1,
		class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt; currentMass,
		class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt; processedList,
		class [FSharp.Core]Microsoft.FSharp.Core.FSharpFunc`2&lt;class [mscorlib]System.Tuple`2&lt;class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt;, class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt;&gt;, !!a&gt; cont
	) cil managed 
{
	// Method begins at RVA 0x2218
	// Code size 197 (0xc5)
	.maxstack 16
	.locals init (
		[0] class [mscorlib]System.Tuple`2&lt;int32, int32&gt; position,
		[1] class [mscorlib]System.Tuple`2&lt;int32, int32&gt; up,
		[2] class [mscorlib]System.Tuple`2&lt;int32, int32&gt; down,
		[3] class [mscorlib]System.Tuple`2&lt;int32, int32&gt; left,
		[4] class [mscorlib]System.Tuple`2&lt;int32, int32&gt; right
	)

	// loop start
		IL_0000: ldarg.2
		// .. removed ...
		IL_00ad: br IL_0000
	// end loop

	IL_00b2: ldarg.s cont
	IL_00b4: ldarg.s currentMass
	IL_00b6: ldarg.s processedList
	IL_00b8: newobj instance void class [mscorlib]System.Tuple`2&lt;class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt;, class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt;&gt;::.ctor(!0, !1)
	IL_00bd: tail.
	IL_00bf: callvirt instance !1 class [FSharp.Core]Microsoft.FSharp.Core.FSharpFunc`2&lt;class [mscorlib]System.Tuple`2&lt;class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt;, class [FSharp.Core]Microsoft.FSharp.Collections.FSharpList`1&lt;class [mscorlib]System.Tuple`2&lt;int32, int32&gt;&gt;&gt;, !!a&gt;::Invoke(!0)
	IL_00c4: ret
} // end of method Print::'findMassStartingAt\'@176'
[/csharp]

<h2>Choose only non processed elements</h2>

I couldn't help myself so I revisited the code a little.  I didn't want to brute force the entire board, instead I want to selectively choose the first unprocessed position to see if its got a contiguous block of what I want.  

The basic idea is to generate an array reprenseting all of the boards positions. Then find the intersection of the points you've processed vs the available points. Then find the first point NOT in the intersection. If the resulting list is empty we've processed everything.  If we've never processed anything just start from the top left corner.

You could cut out even more work if you cached the creation of the board tuple array elsewhere.

[fsharp]
(*
    Finds all items of list2 that are not in list1
*)

let except list1 list2 = 
    let listContainsElement item = List.exists (fun i -&gt; i = item) list1
    List.filter(fun item -&gt; not (listContainsElement item)) list2

(*
    Find first non processed position
*)

let firstNonProcessedPosition processedList xCount yCount = 
    match processedList with
        | [] -&gt; 
            Some((0, 0))
        | _ -&gt;
            if List.length processedList = (xCount * yCount) then
                None 
            else

                // get an array representing (x, y) tuples of the entire board
                let totalPositions = [0..xCount] |&gt; List.collect (fun x -&gt; [0..yCount] |&gt; List.map (fun y -&gt; (x, y)))

                // set intersections from the total positions array and the entire board
                let intersections = Set.intersect (Set.ofList totalPositions) (Set.ofList processedList)
                                        |&gt; List.ofSeq

                // exclude the intersections from the total list
                let excludes = except intersections totalPositions

                match excludes with 
                    | [] -&gt; None
                    | _ -&gt; Some(List.head excludes)
[/fsharp]

And now we just need to use this new information to feed to the fill function to find our contiguous block of elements. This function is a little more complicated, but not by much. 

[fsharp]
(*
    Finds all contiguous blocks of the specified type
    and returns a list of lists (each list is the points for a specific
    block)
*)
    
let getContiguousBlocks board target = 
    
    let xCount = (xSize board) - 1
    let yCount = (ySize board) - 1

    let rec findBlocks' (blocks, processed:PositionList) = 
        
        let findMass x y board = findMassStartingAt (x, y) board target processed

        // find the first non processed block 
        // and try and find its contigoius area
        // if it isn't a valid area the block it returns will be
        // empty and we can exclude it
        match firstNonProcessedPosition processed xCount yCount with 
            | None -&gt; blocks
            | Some (x, y) -&gt; 
                let (block, processed) = findMass x y board

                findBlocks' ((match block with 
                                | [] -&gt; blocks
                                | _ -&gt; block::blocks), processed)
        
    findBlocks' ([],[])
[/fsharp]

Now, we keep processing until we've processed everyone. At that point, return what we found!

<h2>View the full snippet</h2>

I'm posting the full snippet at <a href="http://fssnip.net/ik" target="_blank" rel="noopener noreferrer">fsharp snippets</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3735</wp:post_id>
		<wp:post_date><![CDATA[2013-05-04 18:05:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-04 18:05:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[largest-mass-problem]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="algorithms"><![CDATA[algorithms]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="flood-fill"><![CDATA[flood fill]]></category>
		<category domain="post_tag" nicename="tail-recursion"><![CDATA[tail recursion]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559119788;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4411;}i:1;a:1:{s:2:"id";i:3899;}i:2;a:1:{s:2:"id";i:1043;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>85</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #18, 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/05/06/f-weekly-18-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[72.232.7.96]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-05-05 21:04:08]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-05-05 21:04:08]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[...] Anton Kropp posted &#8220;The largest mass problem&#8220;. [...]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech Talk: Path finding algorithms</title>
		<link>https://onoffswitch.net/2013/05/09/tech-talk-path-finding-algorithms/</link>
		<pubDate>Thu, 09 May 2013 17:35:38 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3766</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk was about path finding algorithms. The topic was picked because of a recent linked shared to reddit that <a href="http://qiao.github.io/PathFinding.js/visual/" target="_blank" rel="noopener noreferrer">visualized</a> different algorithms. The neat thing about the link is that you can really see how different algorithms and heuristics modify the route.

In general, path finding algorithms are based off a breadth first search. At each iteration while walking through the graph you check the nearest neighbors you update what was the calculated weight of the path to get to that neighbor. If it was cheaper to get to the neighbor via your own node (than whoever visited it previously) you update the neighbors weight to reflect that.  This is pretty much <a href="http://en.wikipedia.org/wiki/Dijkstra's_algorithm" target="_blank" rel="noopener noreferrer">dijsktras algorithm.</a>  Disjkstra gives you the shortest path cost, but not necessarily the shortest path. To find the shortest path you mark each node with who its cheapest parent is (i.e. the node you  need to get to the neighbor).  When you get to the final destination all you have to do is backtrace from the parent reference until you get back to the source.

You can also use a priority queue (implemented as a min heap) to store who is the best neighbor to check next, since each time you check a neighbor you add them to a list of checked (but unvisited) nodes called the "open list".  By ordering the open list with the cheapest neighbor you can more effectively process who to check next.

A couple of neat things we discussed were the differences in algorithms.  Almost all of them were based off of Dijskstra, except instead of just using the weight of the edge to calculate the distance to a node, the other algorithms also used some sort of heuristic to guide the direction of neighbor checking. For example, with A* you add the distance from the neighbor to the target to the total weight of a node.  With best-first, you add the distance from the neighbor to the target, but you also amplify the distance weight by a large factor (making it like a focused A*).  

There are different kinds of <a href="http://lyfat.wordpress.com/2012/05/22/euclidean-vs-chebyshev-vs-manhattan-distance/" target="_blank" rel="noopener noreferrer">distance calculations</a> that you use as the heuristic weight too.  Manhattan distance counts only vertical and horizontal movements (like a taxi in manhattan).  So a diagonal move would cost you two units, since you have to move once horizontally, and once vertically.  Euclidean distance is a vector difference between to the two x,y coordinates.  And Chebyschev distance is basically the maximum of either direction (x, or y). On a graph where all units are one, a euclidean distance of going diagonally is sqrt 2, Chebyschev is 1, and Manhattan is 2.  

We also talked about <a href="http://zerowidth.com/2013/05/05/jump-point-search-explained.html" target="_blank" rel="noopener noreferrer">jump point search</a>, which is completely different.  The idea is to use a set of rules to eliminate nodes you don't need to check.  This makes it much more effective by being able to remove huge swaths of the search space.   The downside here is that it only works if you can check cells beyond the reach of where you are at.  If you need to actually traverse a cell to find its weight you can't really use jump point.

After that we got into a discussion of how 3 dimensional path finding algorithms work.  With real world robotics you can't do a direct BFS of every single point in space around you, it would take forever. So, instead what happens is you probabilistically select N number of points in the real world space. This gives you a random sampling of what is out there in the world view, and you create a graph using that. At that point you can use BFS to find the nearest path and take a step in that direction.  At each step, you build a new graph and try again.  This gives you a discrete set of sample points to work in and generally moves you in the right direction.  

A coworker also mentioned a paper discussing when the target moves.  When the target moves you can treat your graph as changing vector field.  Again, at each time interval you re-evaluate what is the best path to the target and make a move in that direction.  See <a href="http://students.cs.byu.edu/~cs470ta/goodrich/fall2004/lectures/Pfields.pdf" target="_blank" rel="noopener noreferrer">figure 4 (PDF)</a>.

In the end path finding is an extremely interesting subject with lots of ways of doing it.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3766</wp:post_id>
		<wp:post_date><![CDATA[2013-05-09 17:35:38]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-09 17:35:38]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-path-finding-algorithms]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="bfs"><![CDATA[bfs]]></category>
		<category domain="post_tag" nicename="graphs"><![CDATA[graphs]]></category>
		<category domain="post_tag" nicename="path-finding"><![CDATA[path finding]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558686525;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4027;}i:1;a:1:{s:2:"id";i:4783;}i:2;a:1:{s:2:"id";i:3656;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Minimizing the null ref with dynamic proxies</title>
		<link>https://onoffswitch.net/2013/05/20/minimizing-null-ref/</link>
		<pubDate>Mon, 20 May 2013 08:00:33 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3779</guid>
		<description></description>
		<content:encoded><![CDATA[In a production application you frequently can find yourself working with objects that have a large accessor chain like

[code]
student.School.District.Street.Name
[/code]

But when you want to program defensively you need to always do null checks on any reference type.  So your accessing chain looks more like this instead

[csharp]
if (student.School != null)
{
    if (student.School.District != null)
    {
        if (student.School.District.Street != null)
        {
            s += student.School.District.Street.Name;
        }
    }
}
[/csharp]

Which sucks.  Especially since its easy to forget to add a null check, and not to mention it clutters the code up.  Even if you used an option type, you still have to check if it's something or if its nothing, and dealing with huge option chains is just as annoying.

One solution is to use the <a href="http://devtalk.net/csharp/chained-null-checks-and-the-maybe-monad/" target="_blank" rel="noopener noreferrer">maybe monad</a>, which can be implemented using extension methods and lambdas. While this is certainly better, it can still can get unwieldy.  

What I really want is a way to just access the chain, and if any part of it is null for it to return null.  

<h2>The magic of Castle Dynamic Proxy</h2>

This is where the magic of <a href="http://www.castleproject.org/projects/dynamicproxy/" target="_blank" rel="noopener noreferrer">castle dynamic proxy</a> comes into play.  Castle creates <a href="http://www.codeproject.com/Articles/121568/Dynamic-Type-Using-Reflection-Emit#heading0002" target="_blank" rel="noopener noreferrer">runtime byte code</a> that can subclass your class and intercept method calls to it.  This means you can now control what happens each time a method is invoked on your function, both by manipulating the return value and by choosing whether or not to even invoke the function.  Lots of libraries use castle to do neat things, like the <a href="https://code.google.com/p/moq/" target="_blank" rel="noopener noreferrer">moq library</a> from google and <a href="http://nhforge.org/" target="_blank" rel="noopener noreferrer">NHibernate</a>.  

For my purposes, I wanted to create a null safe proxy that lets me safely iterate through the function call chain. Before I dive into it, lets see what the final result is:

[csharp]
var user = new User();

var name = user.NeverNull().School.District.Street.Name.Final();
[/csharp]

At this point <code>name</code> can be either null, or the street name. But since this user never set any of its public properties everything is null, so name here will be null. At this point I can do one null check and move on.

<h2>The start</h2>

<code>NeverNull</code> is an extension method that wraps the invocation target (the thing calling the method) with a new dynamic proxy.    

[csharp]
public static T NeverNull&lt;T&gt;(this T source) where T : class
{
    return (T) _generator.CreateClassProxyWithTarget(typeof(T), new[] { typeof(IUnBoxProxy) }, source, new NeverNullInterceptor(source));
}
[/csharp]

I'm doing a few things here.  First I'm making a proxy that wraps the source object. The proxy will be of the same type as the source. Second, I'm telling castle to also add the <code>IUnBoxProxy</code> interface to the proxy implementation. We'll see why that's used later. All it means is that the proxy that is returned implements not only all the methods of the source, but is also going to be of the <code>IUnBoxProxy</code> interface.  Third, I am telling castle to use a <code>NeverNullInterceptor</code> that holds a reference to the source item.  This interceptor is responsible for manipulating any function calls on the source object.

<h2>The method interceptor</h2>

The interceptor isn't that complicated.  Here is the whole class:

[csharp]
public class NeverNullInterceptor : IInterceptor
{
    private object Source { get; set; }

    public NeverNullInterceptor(object source)
    {
        Source = source;
    }

    public void Intercept(IInvocation invocation)
    {
        try
        {
            if (invocation.Method.DeclaringType == typeof(IUnBoxProxy))
            {
                invocation.ReturnValue = Source;
                return;
            }

            invocation.Proceed();

            var returnItem = Convert.ChangeType(invocation.ReturnValue, invocation.Method.ReturnType);

            if (!PrimitiveTypes.Test(invocation.Method.ReturnType))
            {
                invocation.ReturnValue = invocation.ReturnValue == null
                                             ? ProxyExtensions.NeverNullProxy(invocation.Method.ReturnType)
                                             : ProxyExtensions.NeverNull(returnItem, invocation.Method.ReturnType);
            }
        }
        catch (Exception ex)
        {
            invocation.ReturnValue = null;
        }
    }
}
[/csharp]

The main gist of this class is that whenever a function gets called on a proxy object, the interceptor can capture the function call. We created the specific proxy to be tied to this interceptor as part of the proxy generation.

When a function is captured by the interceptor, the interceptor can choose to invoke the actual underlying function if it wants to (via the proceed method).  After that, the interceptor tests to see if the function return value was null or not. If the value wasn't null, the interceptor then proxies the return value (creating a chain of proxy objects). This means that the next function call in the accessor chain is now also on a proxy!

But, if the return value was null we still need to continue the accessor chain. Unlike the maybe monad, we can't bail in the middle of the call. So, what we do instead is to create an empty proxy of the same type.  This just gives us a way to capture invocations onto what would otherwise be a null object.  Castle can give you a proxy that doesn't wrap any target.   This is what moq does as well.  If anyone calls a function on this proxy, the interceptor's intercept method gets called and we can choose to not proceed with the actual invocation!  There's no underlying wrapped target, it's just the interceptor catching calls. 

In the scenario where the return result is null, here is the function to proxy the type

[csharp]
public static object NeverNullProxy(Type t)
{
    return _generator.CreateClassProxy(t, new[] { typeof(IUnBoxProxy) }, new NeverNullInterceptor(null));
}
[/csharp]

Now, you may notice that I'm passing <code>null</code> to the constructor of the interceptor, but previously I passed a source object to the constructor. This is because I want the interceptor to know what is the underlying proxied target.  This is how I'm going to be able to unbox the final value out of the proxy chain when it's requested.  This is also the reason for the <code>IUnBoxProxy</code> interface we added.

<h2>Getting the value out!</h2>

At this point there is an entire proxy chain set up. Once you enter the proxy chain, all other functions on that object are also proxies.  But at some point you want to get the actual value out, whether its null or not. This is where that special interface comes in. Using an extension method on all object types we can cast the object to the special interface (remembering that the object we're working on is actually a proxy and that it should have implemented the special interface we told it to) and execute a function on it. It really doesn't matter which function, just a function

[csharp]
public static T Final&lt;T&gt;(this T source)
{
    var proxy = (source as IUnBoxProxy);
    if (proxy == null)
    {
        return source;
    }

    return (T)proxy.Value;
}
[/csharp]

Since the proxy is actually a dynamic proxy that was created we get caught back in the interceptor.  This is why this block exists

[csharp]
if (invocation.Method.DeclaringType == typeof(IUnBoxProxy))
{
    invocation.ReturnValue = Source;
    return;
}
[/csharp]

If the declaring type (i.e. the thing calling the function) is of that type (which it is since we explicitly cast it to it) then return the internal stored unboxed proxy.  If the proxy contained null then a null gets returned, otherwise the last thing in the chain gets returned.

I specificailly excluded primitives during the proxy boxing phase since a primitive implies the final ending of the chain. That and castle kept throwing me an error saying that it

[csharp]
Could not load type 'Castle.Proxies.StringProxy' from assembly 'DynamicProxyGenAssembly2, Version=0.0.0.0, Culture=neutral, PublicKeyToken=null' because the parent type is sealed.
[/csharp]

But thats OK since we don't need to proxy primitives in this scenario.

<h2>Performance tests</h2>

Now this is great and all, but if it incurs an enormous performance penalty then we can't really use it.  This is where I ran some unscientific tests.  In a unit test run in release I checked the relative execution time of the following 3 functions:

Create an empty user and use the never null proxy to check a string some amount of times.  The console writeline exists only to make sure the compiler doesn't optimize out unused variables.

[csharp]
private void NullWithProxy(int amount)
{
    var user = new User();

    var s = &quot;na&quot;;
    for (int i = 0; i &lt; amount; i++)
    {
        s += user.NeverNull().School.District.Street.Name.Final() ?? &quot;na&quot;;
    }

    Console.WriteLine(s.FirstOrDefault());
}
[/csharp]

Test a non null object chain with the proxy

[csharp]
 private void TestNonNullWithProxy(int amount)
 {
     var student = new User
     {
         School = new School
         {
             District = new District
             {
                 Street = new Street
                 {
                     Name = &quot;Elm&quot;
                 }
             }
         }
     };

     var s = &quot;na&quot;;
     for (int i = 0; i &lt; amount; i++)
     {
         s += student.NeverNull().School.District.Street.Name.Final();
     }

     Console.WriteLine(s.FirstOrDefault());
 }
[/csharp]

And finally test a bunch of if statements on a non null object

[csharp]
private void NonNullNoProxy(int amount)
{
    var student = new User
            {
                School = new School
                         {
                             District = new District
                                        {
                                            Street = new Street
                                                     {
                                                         Name = &quot;Elm&quot;
                                                     }
                                        }
                         }
            };

    var s = &quot;na&quot;;
    for (int i = 0; i &lt; amount; i++)
    {
        if (student.School != null)
        {
            if (student.School.District != null)
            {
                if (student.School.District.Street != null)
                {
                    s += student.School.District.Street.Name;
                }
            }
        }
    }

    Console.WriteLine(s.FirstOrDefault());
}
[/csharp]

And the results are

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/performanceChart.png" rel="attachment wp-att-3781"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/performanceChart-1024x692.png" alt="performanceChart" width="1024" height="692" class="alignnone size-large wp-image-3781" /></a>

You can see on iteration 1 that there is a big spike in using the proxy. That's because castle has to initially create and then cache dynamic proxies. After that things level out and grow linearly.  While you do incur a penalty hit, its not that far off from regular if checks.  Doing 4 chained proxy checks 5000 times runs about 200 milliseconds, compared to 25 milliseconds with direct if checks. While its 8 times longer, you get the security of knowing you won't accidentally have a null reference exception.  For lower amounts of accessing the time is pretty comparable.

<h2>Conclusion</h2>

Unfortunately a downside to all of this is that castle can only proxy methods and properties that are marked as <code>virtual</code>.  Also I had a lot of difficulty getting proxying of enumerables to work.  I was only able to get it to work with things that are declared as <code>IEnumerable</code> or <code>List</code> but not Dictionary or HashSet or anything else.  If you know how to do this please <a href="http://stackoverflow.com/questions/16525589/interceptor-for-ienumerable-methods" target="_blank" rel="noopener noreferrer">let me know</a>! Because of those limitations I wouldn't suggest using this in a production application. But, maybe, one of these days a language will come out with this built in and I'll be pretty stoked about that.

For full source check out <a href="https://github.com/devshorts/Playground/tree/master/NoNulls" target="_blank" rel="noopener noreferrer">my github</a>.  Also I'd like to thank my coworker <a href="http://fmansoor.wordpress.com/" target="_blank" rel="noopener noreferrer">Faisal</a> for really helping out on this idea.  It was his experience with dynamic proxies that led to this post.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3779</wp:post_id>
		<wp:post_date><![CDATA[2013-05-20 08:00:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-20 08:00:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[minimizing-null-ref]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dynamic-proxy"><![CDATA[dynamic proxy]]></category>
		<category domain="post_tag" nicename="maybe-monad"><![CDATA[maybe monad]]></category>
		<category domain="post_tag" nicename="null"><![CDATA[null]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561365978;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4493;}i:1;a:1:{s:2:"id";i:3565;}i:2;a:1:{s:2:"id";i:3723;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>89</wp:comment_id>
			<wp:comment_author><![CDATA[Avoiding nulls with expression trees | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/avoiding-nulls-expression-trees/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-04-04 04:23:34]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-04-04 04:23:34]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] blogged about this subject before, but I REALLY hate null refs. This is one of the reasons I love F# and other functional languages, [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Streaming video to ios device with custom httphandler in asp.net</title>
		<link>https://onoffswitch.net/2013/05/15/streaming-video-ios-device-custom-httphandler-asp-net/</link>
		<pubDate>Wed, 15 May 2013 21:44:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3803</guid>
		<description></description>
		<content:encoded><![CDATA[I ran into an interesting tidbit just now while trying to dynamically stream a video file using a custom http handler.  The idea here is to bypass the static handler for a file so that I can perform authentication/preprocessing/etc when a user requests a video resource and I don't have to expose a static folder with potentially sensitive resources. 

I had everything working fine on my desktop browser, but when I went to test on my iPhone I got the dreaded play button with a circle crossed out 

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/noVideo.png" rel="attachment wp-att-3804"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/noVideo.png" alt="noVideo" width="338" height="600" class="alignnone size-full wp-image-3804" /></a>

I hate that thing.

Anyways, streaming a file from the static handler worked fine though, so what was the difference? This is where I pulled out charles and checked the response headers.

From the static handler I'd get this:

[code]
HTTP/1.1 200 OK
Content-Type	video/mp4
Last-Modified	Wed, 15 May 2013 20:59:30 GMT
Accept-Ranges	bytes
ETag	&quot;9077fe17af51ce1:0&quot;
Server	Microsoft-IIS/7.5
X-Powered-By	ASP.NET
Date	Wed, 15 May 2013 21:23:41 GMT
Content-Length	2509720
[/code]

And for my dynamic handler I got this:

[code]
HTTP/1.1 200 OK
Cache-Control	private
Content-Type	video/mp4
Server	Microsoft-IIS/7.5
X-AspNet-Version	4.0.30319
X-Powered-By	ASP.NET
Date	Wed, 15 May 2013 21:22:55 GMT
Content-Length	2509720
[/code]

So all that was missing was ETag and Accept-Ranges.  

Turns out ETag is a good thing to have since its a CRC of the file. It tells the client if anything has actually changed or not and helps with caching.  Not a bad thing to have, especially if you use a fast hashing algorithm to generate your signature.

The second thing that was different was the Accept-Ranges header. This turns out to be a way for the client to make certain range requests in case a connection is closed or something fails. From the <a href="http://greenbytes.de/tech/webdav/draft-ietf-httpbis-p5-range-latest.html" target="_blank" rel="noopener noreferrer">spec</a>:

<blockquote>Hypertext Transfer Protocol (HTTP) clients often encounter interrupted data transfers as a result of canceled requests or dropped connections. When a client has stored a partial representation, it is desirable to request the remainder of that representation in a subsequent request rather than transfer the entire representation. <strong>Likewise, devices with limited local storage might benefit from being able to request only a subset of a larger representation, such as a single page of a very large document, or the dimensions of an embedded image</strong>.

This document defines HTTP/1.1 range requests, partial responses, and the multipart/byteranges media type. Range requests are an optional feature of HTTP, designed so that recipients not implementing this feature (or not supporting it for the target resource) can respond as if it is a normal GET request without impacting interoperability. Partial responses are indicated by a distinct status code to not be mistaken for full responses by caches that might not implement the feature.

Although the range request mechanism is designed to allow for extensible range types, this specification only defines requests for byte ranges.</blockquote>

However, it's up the server to tell the client that it supports range requests.  So, once I added this into the header

[csharp]
context.Response.Headers.Add(&quot;Accept-Ranges&quot;, &quot;bytes&quot;);
context.Response.Headers.Add(&quot;ETag&quot;, HashUtil.QuickComputeHash(target));
context.Response.ContentType = &quot;video/mp4&quot;;
context.Response.TransmitFile(target);
[/csharp]


Everything started to work. Now, I'm not actually handling range requests though.  I need to test with a huge file and kill the connection and see what happens.  But, even then, all that requires is <a href="http://stackoverflow.com/questions/4330023/detecting-byte-range-requests-in-net-httphandler" target="_blank" rel="noopener noreferrer">testing the request headers</a> and parsing for the byte range it wants to send out.

Turns out I'm not the only one to see this happen. Check out <a href="http://www.markeverard.com/2011/07/05/serving-videos-to-ios-devices-from-episerver-vpp-folders/" target="_blank" rel="noopener noreferrer">here</a> and <a href="http://dotnetslackers.com/articles/aspnet/Range-Specific-Requests-in-ASP-NET.aspx" target="_blank" rel="noopener noreferrer">here</a> for more info.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3803</wp:post_id>
		<wp:post_date><![CDATA[2013-05-15 21:44:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-15 21:44:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[streaming-video-ios-device-custom-httphandler-asp-net]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="asp-net"><![CDATA[asp.net]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="iphone"><![CDATA[iphone]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="video"><![CDATA[video]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[streaming-h-264-video-ipad-iphone-custom-http-handler-asp-net]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561920465;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4028;}i:1;a:1:{s:2:"id";i:1587;}i:2;a:1:{s:2:"id";i:3524;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>86</wp:comment_id>
			<wp:comment_author><![CDATA[Video Streaming for mobile clients via ASP.NET Web API 2 [Tutorial] - Eloy&#039;s blog - Site Root - StudentGuru]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://studentguru.gr/b/eloy/archive/2015/07/13/video-streaming-for-mobile-clients-via-asp-net-web-api-2-tutorial</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[168.63.54.187]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-13 13:48:20]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-13 13:48:20]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] One blog post that will “untie your hands” is this:  http://onoffswitch.net/streaming-video-ios-device-custom-httphandler-asp-net/ [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>87</wp:comment_id>
			<wp:comment_author><![CDATA[Vhalkyrion]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[lfv_luigi@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[187.174.190.194]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-05-04 17:28:48]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-05-04 17:28:48]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Just what I needed. Thank's man.

The only difference is that I'm using an "ActionResult" in MVC 5, but I was missing the same headers.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Determining 64bit or 32 bit .NET assemblies</title>
		<link>https://onoffswitch.net/2013/05/16/determining-64bit-32-bit-assemblies/</link>
		<pubDate>Thu, 16 May 2013 16:28:26 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3814</guid>
		<description></description>
		<content:encoded><![CDATA[I work on a 64 bit machine but frequently deploy to 32 bit machines. The code I work on though has native hooks so I always need to deploy assembly entry points at 32 bit. This means I am usually paranoid about the build configuration. However, sometimes things slip up and a 64 bit dll gets sent out or an entrypoint is built with <code>ANY CPU</code> set. Usually this is caught on our continuous build server with some cryptic reason for a unit test that should be working is actually failing. 

When this happens, what you'll get is a message like this:

[code]
Unhandled Exception: System.BadImageFormatException: Could not load file or assembly 'Some.dll' or one of its dependencies. An attempt was made to load a program with
 an incorrect format.
   at Test.Program.Run(Args args, Boolean fastStart)
   at Test.ProgramMain(String[] args) in Program.cs:line 36
[/code]

The first thing I do here is to try and figure out which of these dll's is built at the wrong type.  The easiest way I've found to do that is to have a simple app that reflectively loads all the assemblies in a directory and tests their image format:

[csharp]
class Program
{
    static void Main(string[] args)
    {
        if (args.Length != 1)
        {
            Console.WriteLine(&quot;Usage: &lt;directory to test for dlls&gt;&quot;);
            return;
        }

        var dir = args[0];

        Console.WriteLine();
        Console.WriteLine(&quot;This machine is {0}&quot;, Is64BitOperatingSystem ? &quot;64 bit&quot; : &quot;32 bit&quot;);
        Console.WriteLine();

        foreach (var file in Directory.EnumerateFiles(dir))
        {
            if (Path.GetExtension(file) == &quot;.dll&quot; || Path.GetExtension(file) == &quot;.exe&quot;)
            {
                try
                {
                    Assembly assembly = Assembly.ReflectionOnlyLoadFrom(file);
                    PortableExecutableKinds kinds;
                    ImageFileMachine imgFileMachine;
                    assembly.ManifestModule.GetPEKind(out kinds, out imgFileMachine);

                    Console.WriteLine(&quot;{0,-40} - {1,-15} - {2, -10}&quot;, 
                        Path.GetFileName(file),
                        imgFileMachine,
                        kinds);
                }
                catch (Exception ex)
                {
                    var err = &quot;error&quot;;

                    if (ex.Message.Contains(&quot;The module was expected to contain an assembly manifest.&quot;))
                    {
                        err = &quot;native&quot;;
                    }

                    Console.WriteLine(&quot;{0,-40} - {1,-15}&quot;, Path.GetFileName(file), err);
                }
            }
        }
    }

    public static bool Is64BitOperatingSystem
    {
        get
        {
            // Clearly if this is a 64-bit process we must be on a 64-bit OS.
            if (IntPtr.Size == 8)
                return true;
            // Ok, so we are a 32-bit process, but is the OS 64-bit?
            // If we are running under Wow64 than the OS is 64-bit.
            bool isWow64;
            return ModuleContainsFunction(&quot;kernel32.dll&quot;, &quot;IsWow64Process&quot;) &amp;&amp; IsWow64Process(GetCurrentProcess(), out isWow64) &amp;&amp; isWow64;
        }
    }

    static bool ModuleContainsFunction(string moduleName, string methodName)
    {
        IntPtr hModule = GetModuleHandle(moduleName);
        if (hModule != IntPtr.Zero)
            return GetProcAddress(hModule, methodName) != IntPtr.Zero;
        return false;
    }

    [DllImport(&quot;kernel32.dll&quot;, SetLastError = true)]
    [return: MarshalAs(UnmanagedType.Bool)]
    extern static bool IsWow64Process(IntPtr hProcess, [MarshalAs(UnmanagedType.Bool)] out bool isWow64);
    [DllImport(&quot;kernel32.dll&quot;, CharSet = CharSet.Auto, SetLastError = true)]
    extern static IntPtr GetCurrentProcess();
    [DllImport(&quot;kernel32.dll&quot;, CharSet = CharSet.Auto)]
    extern static IntPtr GetModuleHandle(string moduleName);
    [DllImport(&quot;kernel32.dll&quot;, CharSet = CharSet.Ansi, SetLastError = true)]
    extern static IntPtr GetProcAddress(IntPtr hModule, string methodName);
}
[/csharp]

Running the app will print out something like this:

[csharp highlight="16,14"]

This machine is 64 bit

7z.dll                                   - native
7z64.dll                                 - native
antlr.runtime.dll                        - I386            - ILOnly
Local.Common.dll                         - I386            - ILOnly, Required32Bit
BCrypt.Net.dll                           - I386            - ILOnly
ICSharpCode.SharpZipLib.dll              - I386            - ILOnly
log4net.dll                              - I386            - ILOnly
Lucene.Net.dll                           - I386            - ILOnly
nunit.framework.dll                      - I386            - ILOnly
pthreadVC2.dll                           - native
SevenZipSharp.dll                        - I386            - ILOnly
LocalInterop.dll                         - I386            - Required32Bit
swscale-0.dll                            - native
App.exe                                  - I386            - ILOnly
System.Reactive.dll                      - I386            - ILOnly
XmlDiffPatch.dll                         - I386            - ILOnly
[/csharp]

There you go, the application was built at <code>ANY CPU</code>. Anything marked with <code>ILOnly</code> can run on both 64bit and 32bit.  If it is marked as only <code>Required32Bit</code> then it'll only work from a 32 bit process.  Since I'm on a 64 bit machine running an ANY CPU process, the OS attempted to load the app at a 64 bit program format. This means all DLL's it loads have to support either ANY or 64 bit.  Unfortunately, the interop dll is 32 bit only so that's what is causing the error.

If you're wondering how I knew that exception was for native code, the <code>native</code> determination is due to native dll's missing an <a href="http://msdn.microsoft.com/en-us/library/1w45z383(v=vs.100).aspx" target="_blank" rel="noopener noreferrer">assembly manifest</a>. <a href="http://stackoverflow.com/questions/12752828/are-manifest-files-only-for-managed-net-assemblies" target="_blank" rel="noopener noreferrer">Only .NET files</a> contain an assembly manifest so I'm just testing that specific error.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3814</wp:post_id>
		<wp:post_date><![CDATA[2013-05-16 16:28:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-16 16:28:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[determining-64bit-32-bit-assemblies]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="64-bit"><![CDATA[64 bit]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="utilities"><![CDATA[Utilities]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560340851;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3837;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:4800;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>88</wp:comment_id>
			<wp:comment_author><![CDATA[Jeff Webster]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[chryosolo@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://chryosolo.wordpress.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.53.79.170]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-25 15:39:52]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-25 15:39:52]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks, you saved me quite a bit of time!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Bad image format &quot;Invalid access to memory location&quot;</title>
		<link>https://onoffswitch.net/2013/05/16/bad-image-format-invalid-access-memory-location/</link>
		<pubDate>Thu, 16 May 2013 19:14:51 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3837</guid>
		<description></description>
		<content:encoded><![CDATA[Wow, two bad image format posts in one day. So, the previous post talked about debugging 64bit vs 32 bit assemblies. But after that was solved I ran into another issue. This time with the message:

[csharp]
Unhandled Exception: System.BadImageFormatException: Could not load file or assembly 'Interop.dll' or one of its dependencies. Invalid access to memory location. (Exception from HRESULT: 0x800703E6)
   at Program.Program.Run(Args args, Boolean fastStart)
   at Program.Program.Main(String[] args) in C:\Projects\Program.cs:line 36
[/csharp]

Gah, what gives?

It seems that I had an interop DLL that was linking against pthreads. In debug mode, the dll worked fine on a 32 bit machine, but in release mode I'd get the error. The only difference I found was that in debug the dll was being explicity linked against pthreads lib file. Since pthread was also being built as a dynamic library, it's lib file contains information about functions and their address, but no actual code (that's in the dll).  

After working for a while with my buddy <a href="http://fmansoor.wordpress.com/" target="_blank" rel="noopener noreferrer">Faisal</a>, we decided that even though the interop dll was being built and linked properly, when it wasn't being explicity linked to the lib file the function addresses were somehow wrong. What the compiler was actually linking against we never figured out. But, it does all make sense. If the function addresses were wrong, then when it would try and access anything in the dll it could access memory outside of its space and get an exception.  

Once we explicitly linked the library as part of the linker settings of the interop dll everything worked fine.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3837</wp:post_id>
		<wp:post_date><![CDATA[2013-05-16 19:14:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-16 19:14:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[bad-image-format-invalid-access-memory-location]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561330604;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3814;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:4107;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Why \d is slower than [0-9]</title>
		<link>https://onoffswitch.net/2013/05/20/d-slower-0-9/</link>
		<pubDate>Mon, 20 May 2013 15:46:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3841</guid>
		<description></description>
		<content:encoded><![CDATA[I learned an interesting thing today about regular expressions via this <a href="http://stackoverflow.com/questions/16621738/d-less-efficient-than-0-9" target="_blank" rel="noopener noreferrer">stackoverflow</a> question.  <code>\d</code>, commonly used as a shorthand for digits (which we usually think of as <code>0-9</code>) actually checks against all valid <a href="http://www.fileformat.info/info/unicode/category/Nd/list.htm" target="_blank" rel="noopener noreferrer">unicode digits</a>.  

Given that, it makes sense why <code>\d</code> in a regular expression is slower, since it has to check against all possible digit types.  In C# you can limit the regular expression to use ECMAScript standards which doesn't include the full unicode subset of digits.  

While I'm neither the question asker nor answerer, I wanted to share since this is something I didn't know about before.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3841</wp:post_id>
		<wp:post_date><![CDATA[2013-05-20 15:46:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-20 15:46:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[d-slower-0-9]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="regular-expressions"><![CDATA[regular expressions]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554335009;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:4197;}i:2;a:1:{s:2:"id";i:4493;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Threadpooling in netduino</title>
		<link>https://onoffswitch.net/2013/06/17/threadpooling-in-netduino/</link>
		<pubDate>Mon, 17 Jun 2013 08:00:39 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=390</guid>
		<description></description>
		<content:encoded><![CDATA[Sometimes you want to do asynchronous work without holding up your current thread but the work that needs to be done doesn't really warrant <a href="http://msdn.microsoft.com/en-us/magazine/cc163552.aspx#S4">the cost</a> of spinning up a new thread (though what the exact cost is on an embedded environment I'm not sure).  

This where <a href="http://en.wikipedia.org/wiki/Thread_pool_pattern">threadpooling</a> comes into play. A threadpool has a certain amount of pre-spun up threads that you can re-use for actions. You push actions onto the threadpool and when there is an available thread it'll run your action. While threadpools aren't free (you still incur <a href="http://en.wikipedia.org/wiki/Context_switch">context switching</a> and the initial overhead of firing up a thread) you can limit your context switches and minimize thread start/cleanup time by reusing threads.  Threadpooling is a handy feature and C# has built in support for it, but it's <a href="http://netmf.codeplex.com/workitem/78">not in the .net micro framework</a> so I decided to write my own.

For a basic threadpool manager it's really pretty simple. First you start with no threads. When someone queues an action into the threadpool we spin up our first thread. The thread's main body is an infinite loop that waits on a shared mutex. When the mutex is pulsed, threads in the threadpool try and de-queue the queued action. Whoever gets the action then safely executes it.  When the maximum number of threads is created we don't spin any more up and just re-use what exists.  

It's important to note the <code>ManualResetEvent</code>.  This means that the mutex will stay signaled (i.e. wait's will exit immediately) until the queue is empty.  If we used an <code>AutoResetEvent</code> then if you queued up too much too fast the threadpool would miss events that were added while all the threads were running.

[csharp]
public static class ThreadUtil
{
    #region Data

    /// &lt;summary&gt;
    /// Synchronizes thread queue actions
    /// &lt;/summary&gt;
    private static readonly object lockObject = new object();

    /// &lt;summary&gt;
    /// List storing our available threadpool threads
    /// &lt;/summary&gt;
    private static readonly ArrayList _availableThreads = new ArrayList();

    /// &lt;summary&gt;
    /// Queue of actions for our threadpool
    /// &lt;/summary&gt;
    private static readonly Queue _threadActions = new Queue();

    /// &lt;summary&gt;
    /// Wait handle for us to synchronize de-queuing thread actions
    /// &lt;/summary&gt;
    private static readonly ManualResetEvent _threadSynch = new ManualResetEvent(false);

    /// &lt;summary&gt;
    /// Maximum size of our thread pool
    /// &lt;/summary&gt;
    private const int MaxThreads = 3;

    #endregion

    #region Thread Start


    /// &lt;summary&gt;
    /// Starts a new thread with an action
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;start&quot;&gt;&lt;/param&gt;
    public static void Start(ThreadStart start)
    {
        try
        {
            var t = new Thread(start);
            t.Start();
        }
        catch (Exception ex)
        {
            Debug.Print(ex.ToString());
        }
    }

    #endregion

    #region ThreadPooling

    /// &lt;summary&gt;
    /// Queues an action into the threadpool
    /// &lt;/summary&gt;
    /// &lt;param name=&quot;start&quot;&gt;&lt;/param&gt;
    public static void SafeQueueWorkItem(ThreadStart start)
    {
        lock (lockObject)
        {
            _threadActions.Enqueue(start);

            // if we haven't spun all the threads up, create a new one
            // and add it to our available threads

            if (_availableThreads.Count &lt; MaxThreads)
            {
                var t = new Thread(ActionConsumer);
                _availableThreads.Add(t);
                t.Start();
            }

            // pulse all waiting threads
            _threadSynch.Set();
        }
    }

    /// &lt;summary&gt;
    /// Main body of a threadpool thread.  Indefinitely wait until
    /// an action is queued. When an action is de-queued safely execute it
    /// &lt;/summary&gt;
    private static void ActionConsumer()
    {
        while (true)
        {
            // wait on action pulse
            _threadSynch.WaitOne();

            ThreadStart action = null;

            // try and de-queue an action
            lock (lockObject)
            {
                if (_threadActions.Count &gt; 0)
                {
                    action = _threadActions.Dequeue() as ThreadStart;
                }
                else
                {
                    // the queue is empty and we are in a critical section
                    // safely reset the mutex so that everyone waits 
                    // until the next action is queued

                    _threadSynch.Reset();
                }
            }

            // if we got an action execute it
            if (action != null)
            {
                try
                {
                    action();
                }
                catch (Exception ex)
                {
                    Debug.Print(&quot;Unhandled error in thread pool: &quot; + ex);
                }
            }
        }
    }

    #endregion
}
[/csharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>390</wp:post_id>
		<wp:post_date><![CDATA[2013-06-17 08:00:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-17 08:00:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[threadpooling-in-netduino]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="netduino"><![CDATA[netduino]]></category>
		<category domain="post_tag" nicename="threading"><![CDATA[threading]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560398911;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4394;}i:1;a:1:{s:2:"id";i:1587;}i:2;a:1:{s:2:"id";i:365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Byte arrays, typed values, binary reader, and fwrite</title>
		<link>https://onoffswitch.net/2013/05/27/byte-arrays-values-binary-reader-fwrite/</link>
		<pubDate>Mon, 27 May 2013 08:00:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3615</guid>
		<description></description>
		<content:encoded><![CDATA[I was trying to read a binary file created from a native app using the C# BinaryReader class but kept getting weird numbers. When I checked the hex in visual studio I saw that the bytes were backwards from what I expected, indicating endianess issues.  This threw me for a loop since I was writing the file from C++ on the same machine that I was reading the file in C# in.  Also, I wasn't sending any data over the network so I was a little confused. Endianess is usually an issue across machine architectures or over the network.

The issue is that I ran into an endianess problem when writing values byte by byte, versus by using the actual data type of an object.  Let me demonstrate the issue

What happens if I write 65297 (0xFF11) using C++

[c]
#include &quot;stdafx.h&quot;
#include &quot;fstream&quot;

int _tmain(int argc, _TCHAR* argv[])
{
	char buffer[] = { 0xFF, 0x11 };

	auto _stream = fopen(&quot;test2.out&quot;,&quot;wb&quot;);

	fwrite(buffer, 1, sizeof(buffer), _stream);

	fclose(_stream);		
}
[/c]

And read it in using the following C# code

[csharp]
public void ReadBinary()
{
    using (var reader = new BinaryReader(new FileStream(@&quot;test2.out&quot;, FileMode.Open)))
    {                
        // read two bytes and print them out in hex
        foreach (var b  in reader.ReadBytes(2))
        {
            Console.Write(&quot;{0:X}&quot;, b);
        }

        Console.WriteLine();

        // go back to the beginning
        reader.BaseStream.Seek(0, SeekOrigin.Begin);

        // read a two byte short and print it out in hex
        var val = reader.ReadUInt16();

        Console.WriteLine(&quot;{0:X}&quot;, val);   
    }    
}
[/csharp]

What would you expect I get? You might think we get the same thing both times, a 16 bit unsigned integer (2 bytes) and reading two bytes from the file should be the same right? 

Actually, I got

[code]
FF11 &lt;-- reading in two bytes
11FF &lt;-- reading in a two byte short
[/code]

What gives?

Turns out that since I'm on a little endian system (intel x86), when you read data as a typed structure it will always read little endian. The binary reader class in C# reads little endian, and fwrite in C++ will write little endian, as long as you aren't writing a value byte by byte.  

When you write a value byte by byte it doesn't go through the correct endianess conversion. This means that you should make sure to use consistent write semantics. If you are going to write values byte by byte always write them byte by byte. If you are going to use typed data, always write with typed data. If you mix the write paradigms you can get into weird situations where some numbers are "big endian" (by writing it byte by byte), and some other values are little endian (by using typed data).

Here's a good quote from the ibm blog on <a href="http://www.ibm.com/developerworks/aix/library/au-endianc/?ca=drs-" target="_blank" rel="noopener noreferrer">writing endianness independent code</a> summarizing the effect:

<blockquote>Endianness does matter when you use a type cast that depends on a certain endian being in use. </blockquote>

If you do happen to need to write byte by byte, and you want to read values in directly as casted types in C#, you can make use of Jon Skeet's <a href="http://www.yoda.arachsys.com/csharp/miscutil/" target="_blank" rel="noopener noreferrer">MiscUtil</a> which contains a big endian and little endian binary reader/writer class.  By using the big endian reader you can now read files where you wrote them from C++ byte by byte.

Here is a fixed version

[csharp]
using (var reader = new EndianBinaryReader(new BigEndianBitConverter(), new FileStream(@test2.out&quot;, FileMode.Open)))
{
    foreach (var b in reader.ReadBytes(2))
    {
        Console.Write(&quot;{0:X}&quot;, b);
    }

    Console.WriteLine();

    reader.BaseStream.Seek(0, SeekOrigin.Begin);

    var val = reader.ReadUInt16();

    Console.WriteLine(&quot;{0:X}&quot;, val);

}
[/csharp]

Which spits out

[code]
FF11
FF11
[/code]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3615</wp:post_id>
		<wp:post_date><![CDATA[2013-05-27 08:00:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-27 08:00:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[byte-arrays-values-binary-reader-fwrite]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="endianess"><![CDATA[endianess]]></category>
		<category domain="post_tag" nicename="x86"><![CDATA[x86]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561494804;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1268;}i:1;a:1:{s:2:"id";i:4286;}i:2;a:1:{s:2:"id";i:4213;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Trees and continuation passing style</title>
		<link>https://onoffswitch.net/2013/08/05/trees-cps/</link>
		<pubDate>Mon, 05 Aug 2013 08:00:35 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3656</guid>
		<description></description>
		<content:encoded><![CDATA[For no reason in particular I decided to revisit tree traversal as a kind of <a href="http://en.wikipedia.org/wiki/Kata_(programming)" target="_blank" rel="noopener noreferrer">programming kata</a>.  There are two main kinds of tree traversal:

<ul>
<li>Depth first - This is where you go all the way down a tree's branches first before bubbling up to do work. With a tree like below, you'd hit <code>c</code> before doing any work since it's the deepest part of the tree (assuming you iterated left first then right)

[code]
     a
    / \
   b   e
 /  \
c    d
[/code]

</li>
<li>Breadth first - This is where you hit all the nodes at the level you're on before going further. So with the same tree, you'd hit a, then b, then e, then c and d.  </li>

</ul>

Being as I actually hate tree traversal, and having to think about it, I decided that whatever I write better be extensible and clean.

<h2>Depth first</h2>

Here is a simple DFS traversal

[csharp]
private List&lt;T&gt; DepthFirstFlatten&lt;T&gt;(T root, Func&lt;T, List&lt;T&gt;&gt; edgeFunction) where T : class
{
    if (root == null)
    {
        return null;
    }

    var totalNodes = new List&lt;T&gt; { root };

    var edges = edgeFunction(root);

    if (edges != null &amp;&amp; edges.Any())
    {
        foreach (var edge in edges)
        {
            if (edge != null)
            {
                totalNodes.AddRange(DepthFirstFlatten(edge, edgeFunction));
            }
        }
    }

    return totalNodes;
} 

[/csharp]

In this case I'm just flattening the list and using a function to return all the edges. This way I can re-use the same depth algorithm for any kind of graph, not just a tree (assuming acyclic).  To handle cycles you would need to pass the total processed nodes as an accumulator and test if the current node was already processed and if so skip it. 

<h2>Breadth first</h2>

For the BFS, it's very similar, except instead of using recursion it uses the standard iterative way of doing it with a queue:

[csharp]
private List&lt;T&gt; BreadthFlatten&lt;T&gt;(T root, Func&lt;T, List&lt;T&gt;&gt; edgeFunction) where T : class
{
    var queue = new Queue&lt;T&gt;();

    queue.Enqueue(root);

    var allNodes = new List&lt;T&gt;();

    while (queue.Any())
    {
        var head = queue.Dequeue();

        if (head == null)
        {
            continue;
        }

        allNodes.Add(head);

        edgeFunction(head).ForEach(queue.Enqueue );
    }

    return allNodes;
}
[/csharp]

Same kind of deal here.  This one is nice because it's not limited by stack depth.  

Also, for both traversals, if you wanted to you could pass in an action to do work each time a node was processed.  Here is an example using the following tree

[code]
     1
    / \
   2   3
 /  \   \
4    5   6
[/code]

Below is a small class representing a binary tree


[csharp]
class Node&lt;T&gt;
{
    public Node(T data, Node&lt;T&gt; left = null, Node&lt;T&gt; right = null)
    {
        Item = data;
        Left = left;
        Right = right;
    }

    public Node&lt;T&gt; Left { get; set; }
    public Node&lt;T&gt; Right { get; set; }

    public T Item { get; set; }
}
[/csharp]

And our unit test to print out the different traversal types

[csharp]
[Test]
public void DepthFlatten()
{
    var tree = new Node&lt;int&gt;(1,
                            new Node&lt;int&gt;(2, new Node&lt;int&gt;(4), new Node&lt;int&gt;(5)),
                            new Node&lt;int&gt;(3, null, new Node&lt;int&gt;(6)));

    Func&lt;Node&lt;int&gt;, List&lt;Node&lt;int&gt;&gt;&gt; extractor = node =&gt; new List&lt;Node&lt;int&gt;&gt; {node.Left, node.Right};

    Console.WriteLine(&quot;Depth&quot;);
    DepthFirstFlatten(tree, extractor).ForEach(n =&gt; Console.WriteLine(n.Item));

    Console.WriteLine(&quot;Breadth&quot;);
    BreadthFlatten(tree, extractor).ForEach(n =&gt; Console.WriteLine(n.Item)); ;
}
[/csharp]


Which prints out:

[code]
Depth
1
2
4
5
3
6

Breadth
1
2
3
4
5
6
[/code]

<h2>DFS stack agnostic</h2>

We can even change the DFS to not use recursion in this case so that it's agnostic of how deep the tree is.  In this scenario, unlike the BFS, you'd use a stack instead of a queue.  This way you are pushing on the deepest nodes and then immediately processing them. This contrasts with the queue where you enqueue the deepest nodes but process the queue FIFO (first in first out), meaning you process all the nodes at the current depth first before moving to the next depth.

[csharp]
private List&lt;T&gt; DepthFirstFlattenIterative&lt;T&gt;(T root, Func&lt;T, List&lt;T&gt;&gt; edgeFunction) where T : class
{
    var stack = new Stack&lt;T&gt;();

    stack.Push(root);

    var allNodes = new List&lt;T&gt;();

    while (stack.Any())
    {
        var head = stack.Pop();

        if (head == null)
        {
            continue;
        }

        allNodes.Add(head);

        var edges = edgeFunction(head);

        edges.Reverse();
        
        edges.ForEach(stack.Push);
    }

    return allNodes;
} 
[/csharp]

The reverse is only there to be consistent with the left tree descent. Otherwise it goes down the right branch first. This spits out

[csharp]
Depth iterative
1
2
4
5
3
6
[/csharp]

<h2>DFS with continuation passing</h2>

There is yet another way to do tree traversal that is common in functional languages. You can do what is called "continuation passing style". Doing it this way you can actually get tail recursive code while iterating over multiple tree branches.  

Below is some F# code to count the number of nodes in a tree.  The tree I'm using as the sample looks like this

[code]
       1
     /   \ 
   2      3
 /  \      \
4    5      6

[/code]

The total nodes here is 6, which is what you get with the code below.

[fsharp]
open System

type Tree = 
    | Leaf of int
    | Node of int * Tree Option * Tree Option


let countNodes tree = 
    let rec countNodes' treeOpt cont = 
        match treeOpt with 
            | Some tree -&gt; 
                match tree with 
                | Leaf item -&gt; cont 1
                | Node (currentValue, left, right) -&gt;
                    countNodes' left (fun leftCount -&gt;
                                          countNodes' right (fun rightCount -&gt;
                                                                 cont(1 + leftCount + rightCount)))
            | None -&gt; cont 0
                    
    countNodes' tree id


let leftBranch = Node(2, Some(Leaf(4)), Some(Leaf(5)))

let rightBranch = Node(3, None, Some(Leaf(6)))

let tree = Node(1, Some(leftBranch), Some(rightBranch))

let treeNodeCount = countNodes (Some(tree))
[/fsharp]

But what the hell is going on here? It's really not apparent when you first look at it what executes what and when.

The trick here is to pass around a function to each iteration that closes over what the next work should be.  To be fair, its hard to wrap your mind around what is happening, so lets trace this out. I've highlighted each of the continuations and given them an alias so you can see how they are re-used elsewhere. Each time the continuation is called I also show the expanded form following the <code>-&gt;</code>.  

<a href="http://onoffswitch.net/wp-content/uploads/2013/04/2013-04-29-10_55_39-MarkdownPad-2.png" rel="attachment wp-att-3673"><img src="http://onoffswitch.net/wp-content/uploads/2013/04/2013-04-29-10_55_39-MarkdownPad-2-1024x328.png" alt="ContinuationPassing Trace" width="1024" height="328" class="alignnone size-large wp-image-3673" /></a>

You can see how each iteration captures the work to do next.  Eventually the very last work that needs to be done is the first function you passed in as the function seed. In this case, it's the built in <code>id</code> function that returns whatever value is given to it (which turns out to be 6, which is how many nodes are in the tree).  You can see the ordering of the traversal is the exact same as the other DFS traversals earlier, except this time everything is tail recursive.  

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3656</wp:post_id>
		<wp:post_date><![CDATA[2013-08-05 08:00:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-05 08:00:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[trees-cps]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="continuation-passing"><![CDATA[continuation passing]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="graphs"><![CDATA[graphs]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="tail-recursion"><![CDATA[tail recursion]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561472644;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:3016;}i:2;a:1:{s:2:"id";i:4348;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Building an ID3 decision tree</title>
		<link>https://onoffswitch.net/2013/06/03/building-decision-tree/</link>
		<pubDate>Mon, 03 Jun 2013 08:00:02 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3847</guid>
		<description></description>
		<content:encoded><![CDATA[After following <a href="http://clear-lines.com/blog/post/Decision-Tree-classification.aspx" target="_blank" rel="noopener noreferrer">Mathias Brandewinder's</a> series on converting the python from <a href="http://www.manning.com/pharrington/" target="_blank" rel="noopener noreferrer">"Machine Learning in Action"</a> to F#, I decided I'd give the book a try myself. Brandewinder's blog is great and he went through chapter by chapter working through F# conversions.  If you followed his series, this won't be anything new.  Still, I decided to do the same thing as a way to solidify the concepts for myself, and in order to differentiate my posts I am reworking the python code into C#.  For the impatient, the full source is available at my <a href="https://github.com/devshorts/Playground/tree/master/MachineLearning/DecisionTree" target="_blank" rel="noopener noreferrer">github</a>.

This post will discuss the ID3 decision tree algorithm. ID3 is an algorithm that's used to create a decision tree from a sample data set. Once you have the tree, you can then follow the branches of the tree until you reach a leaf and that will give you a classification for your sample.  

For example, lets say you have a dataset like this. Each row represents some animal and its characteristics.

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-11_57_19-MarkdownPad-2.png" rel="attachment wp-att-3851"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-11_57_19-MarkdownPad-2.png" alt="2013-05-25 11_57_19-MarkdownPad 2" width="481" height="228" class="alignnone size-full wp-image-3851" /></a>

The data set clearly tells us what is and isn't a fish.  Using ID3 you can take all your known sample data and build out a tree that would look like this in the end:

[code]
can survive without surfacing: yes
	has flippers: yes
		is fish: yes
	has flippers: no
		is fish: no
can survive without surfacing: no
	is fish: no
[/code]

If we get a piece of data that has features, but no class, to find the class all we have to do is follow the branches till we get a leaf which gives us the final class classifier.

With our sample, we can say <em>Can it survive without surfacing? If yes, then does it have flippers? If no, then its not a fish</em>.  The nice thing about a decision tree like this, is that it can collapse common features for you. Notice how if your animal can't survive without surfacing, you don't even have to ask if it has flippers. It will never be a fish.

<h2>The Data Set</h2>

The data set is what you'll use to build the original tree and it just contains a set of instances. An instance is like what I showed above, it's one piece of data in the set. Each instance also has features (also called axis). A feature would be "Has flippers", or "Can survive without surfacing".  The final output of an instance is its class.  

[csharp]
public class DecisionTreeSet
{
    public List&lt;Instance&gt; Instances { get; set; }
}
[/csharp]

[csharp]
public class Instance
{
    public List&lt;Feature&gt; Features { get; set; }

    public Output Output { get; set; }
}
[/csharp]

[csharp]
public class Feature
{
    public string Value { get; set; }
    public string Axis { get; set; }

    public Feature(string value, string axis)
    {
        Value = value;
        Axis = axis;
    }
}
[/csharp]


<h2>Splitting Features</h2>

Let's imagine we were to generate a decision without a decision tree. We have some input data set and we have the thing we want to classify.  If the thing we are trying to classify matches an existing item in the set (all the feature values match), then that's easy.  But, if the thing we are trying to classify has a mix of features that doesn't match any one item we can use an algorithm like this:

<ol>
<li>Find all instances in the data set that have the same value of a feature.  Discard all other instances (as they won't match)</li>
<li>For the instances you found, remove the feature from their lists. We don't need it anymore, we already matched on it.</li>
<li>If we have only one instance left, or all the instances have the same output class, then we're that class. Otherwise go back to step 1 for the next feature</li>
</ol>

But doing this each time we want to classify an item is going to be expensive.  By precomputing this information, and structuring it in a smart way, we can vastly improve the classification time of a new instance.

Like mentioned in step's 1 and 2, splitting the data set on a feature means returning a new set of instances from the original set who have the target feature value, and then removing that axis from the feature list.   This will come in handy later when we try and determine which feature to create a branch in the decision tree from. Let me show an example using the original data set:

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-11_57_19-MarkdownPad-2.png" rel="attachment wp-att-3851"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-11_57_19-MarkdownPad-2.png" alt="2013-05-25 11_57_19-MarkdownPad 2" width="481" height="228" class="alignnone size-full wp-image-3851" /></a>

If I split on "Can survive without surfacing" with the value of "yes" I'd get:

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/split1.jpg" rel="attachment wp-att-3868"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/split1.jpg" alt="split" width="758" height="220" class="alignnone size-full wp-image-3868" /></a>

So I've returned only the instances that had the value of "yes" in that column, AND I've removed the axis (column) and created a new sub data set.  

Here is how I split the sets in C#

[csharp]
public class DecisionTreeSet
{
	//...
	public DecisionTreeSet Split(Feature feature)
	{
	    return Split(feature.Axis, feature.Value);
	}
	
	public DecisionTreeSet Split(string axis, string value)
	{            
	    return new DecisionTreeSet
	           {
	               Instances = Instances.Select(i =&gt; i.Split(axis, value))
	                                    .Where(i =&gt; i.Features.Any())
	                                    .ToList()
	           };
	}                            
}
[/csharp]

[csharp]
public class Instance
{
    //...

    public Instance Split(string axis, string value)
    {
        var featureSplit = Features.Where(feature =&gt; !feature.IsMatch(axis, value)).ToList();

        // no split happened
        if (featureSplit.Count == Features.Count)
        {
            featureSplit = new List&lt;Feature&gt;();
        }

        return new Instance
               {
                   Output = Output,
                   Features = featureSplit
               };
    }
}
[/csharp]

<h2>Measuring Order</h2>

While we can split the tree up and decide without a tree, we still have to wisely decide how to structure the tree.  The goal of the tree building is to figure out what is the best way to structure the data.  The question we're trying to answer is what feature makes the best branch in order to maximize the information in the tree. In other words, when would you choose to make a branch on "has flippers", vs "can survive without surfacing?".  To figure this out we need a way to measure if we gained information by choosing one branch over the other.

How do you do that though? Well, we can measure the amount of disorder in a set using the <a href="http://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank" rel="noopener noreferrer">shannon entropy</a> formula. Shannon gives you a measurement of how mixed up the data is.  The less mixed up the data you have, the better of a decision you are making on that particular feature.   We're interested not in the disorder of the features, but only in the outputs of a set.  

Calculating the entropy is pretty easy using shannons formula:

<img src="http://onoffswitch.net/wp-content/uploads/2013/05/efdf8c905c0f9dfd78002df6f20edb5d.png" width="337" height="44" class="alignnone" />

[csharp]
public static double Entropy(DecisionTreeSet set)
{
    var total = set.Instances.Count();

    var outputs = set.Instances.Select(i =&gt; i.Output).GroupBy(f =&gt; f.Value).ToList();

    var entropy = 0.0;

    foreach (var target in outputs)
    {
        var probability = (float)target.Count()/total;
        entropy -= probability*Math.Log(probability, 2);
    }

    return entropy;
}
[/csharp]

For each possible output class type (fish/not fish), determine the probability of finding that class type in the total set.  The sum of all those probabilities in the set is your entropy.

<h2>Selecting the best axis to split on</h2>

Now that we have a way to create sub data sets, and a way to measure how well ordered those subsets are, we can measure if these data sets helps us predict the outcome better vs any other split.  Doing a split for each feature's unique values will tell us which feature to split the set on. 

This should make sense, since we want the resulting data sets after a split to have the least amount of entropy (more quickly converging to an answer).  We need to test a split on each unique value of a feature (eg. yes:has flippers, and no:has flippers).  The sum of the entropy of those splits will give us the measure of order of the next level deep in the tree.  

First we figure out what the entropy of the total base set is.  Then get all the unique values for a feature.  In our sample set, "can survive without surfacing" has two unique values: "yes" and "no". Same with "has flippers". By summing the entropy for each split of a feature's unique values we can get the total entropy of the tree for that split.  A large info gain tells us that splitting on an axis (the sum of entropy of splitting on all a features unique values) gave us a more homogenous and uniform final class output. A low info gain tells us that the final class output was still pretty mixed up and not so good.  

[csharp]
public static string SelectBestAxis(DecisionTreeSet set)
{
    var baseEntropy = Entropy(set);

    var bestInfoGain = 0.0;            

    var uniqueFeaturesByAxis = set.UniqueFeatures().GroupBy(i =&gt; i.Axis).ToList();

    string bestAxisSplit = uniqueFeaturesByAxis.First().Key;

    foreach (var axis in uniqueFeaturesByAxis)
    {                
        // calculate the total entropy based on splitting by this axis. The total entropy
        // is the sum of the entropy of each branch that would be created by this split 

        var newEntropy = EntropyForSplitBranches(set, axis.ToList());

        var infoGain = baseEntropy - newEntropy;

        if (infoGain &gt; bestInfoGain)
        {
            bestInfoGain = infoGain;

            bestAxisSplit = axis.Key;
        }
    }

    return bestAxisSplit;
}        

private static double EntropyForSplitBranches(DecisionTreeSet set, IEnumerable&lt;Feature&gt; allPossibleAxisValues)
{
    return (from possibleValue in allPossibleAxisValues 
            select set.Split(possibleValue) into subset 
            let prob = (float) subset.NumberOfInstances/set.NumberOfInstances 
            select prob*Entropy(subset)).Sum();
}
[/csharp]


<h2>Split the samples example</h2>

Lets trace through it with the sample set above.  The original sets base entropy is 0.97095057690894.  

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_36-MarkdownPad-2.png" rel="attachment wp-att-3881"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_36-MarkdownPad-2.png" alt="2013-05-25 13_18_36-MarkdownPad 2" width="234" height="157" class="alignnone size-large wp-image-3881" /></a>

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_29-MarkdownPad-2.png" rel="attachment wp-att-3882"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_29-MarkdownPad-2.png" alt="2013-05-25 13_18_29-MarkdownPad 2" width="241" height="194" class="alignnone size-large wp-image-3882" /></a>

[code]
The entropy for axis &quot;can survive without surfacing&quot; value &quot;yes&quot; is 0.550977512949583
The entropy for axis &quot;can survive without surfacing&quot; value &quot;no&quot; is 0
[/code]

This makes sense, if we split on "no", then both of the "Is Fish" outputs are "no", so the class output is uniform.  This is best kind of entropy! 

Now, the other splits

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_47-MarkdownPad-2.png" rel="attachment wp-att-3879"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_47-MarkdownPad-2.png" alt="2013-05-25 13_18_47-MarkdownPad 2" width="356" height="222" class="alignnone size-full wp-image-3879" /></a>

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_41-MarkdownPad-2.png" rel="attachment wp-att-3880"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-25-13_18_41-MarkdownPad-2.png" alt="2013-05-25 13_18_41-MarkdownPad 2" width="364" height="116" class="alignnone size-large wp-image-3880" /></a>

[code]
The entropy for axis &quot;has flippers&quot; value &quot;yes&quot; is 0.800000011920929
The entropy for axis &quot;has flippers&quot; value &quot;no&quot; is 0
[/code]

So while splitting on "has flippers" with the value of "no" gives us a zero entropy, splitting on the value "yes" gives a higher entropy (0.8) than splitting on "can survive without surfacing" with a value of "yes" (0.5509).  In this case, splitting on "can survive" is a better split.
 
When all is said and done we can figure out which axis is the best to split on.  After splitting on an axis, each potential feature value (yes, or no), becomes a separate branch in the decision tree.  Each branch on a tree represents a new subset, and so you can recursively split the sets until the set only has on instance in it, OR, all the classes in all the instances are the same.

<h2>Building the tree</h2>

Once you have the best axis to split on, building the tree comes together easily.  

A tree is just a leaf, or some branches

[csharp]
public class Tree
{
    public Output Leaf { get; set; }

    public Dictionary&lt;Feature, Tree&gt; Branches { get; set; }
}
[/csharp]

And to build the tree, recurse and build until all instances are of the same class, or all instances only have one feature left

[csharp]
public class DecisionTreeSet
{
    //...
    public Tree BuildTree()
    {
        if (InstancesAreSameClass || Instances.All(f =&gt; f.Features.Count() == 1))
        {
            return LeafTreeForRemainingFeatures();
        }

        var best = Decider.SelectBestAxis(this);

        return SplitByAxis(best);
    }

    private Tree SplitByAxis(string axis)
    {
        if (axis == null)
        {
            return null;
        }

        // split the set on each unique feature value where the feature is 
        // if of the right axis
        var splits = (from feature in UniqueFeatures().Where(a =&gt; a.Axis == axis)
                        select new {splitFeature = feature, set = Split(feature)}).ToList();

        var branches = new Dictionary&lt;Feature, Tree&gt;();

        // for each split, either recursively create a new tree
        // or split the final feature outputs into leaf trees
        foreach (var item in splits)
        {
            branches[item.splitFeature] = item.set.BuildTree();
        }

        return new Tree
                {                       
                    Branches = branches
                };
    }

    private Tree LeafTreeForRemainingFeatures()
    {
        if (InstancesAreSameClass)
        {
            return GroupByClass();
        }

        if (Instances.All(f =&gt; f.Features.Count() == 1))
        {
            return LeafForEachFeature();
        }

        return null;
    }

    private Tree LeafForEachFeature()
    {
        // each feature is the last item
        var branches = new Dictionary&lt;Feature, Tree&gt;();
                        
        foreach (var instance in Instances)
        {
            foreach (var feature in instance.Features)
            {
                if (branches.Any(k =&gt; k.Key.Value == feature.Value))
                {
                    continue;
                }

                branches[feature] = new Tree
                {
                    Leaf = instance.Output
                };
            }
        }
            
        return new Tree
        {
            Branches = branches
        };
    }

    private Tree GroupByClass()
    {
        var groupings = Instances.DistinctBy(i =&gt; i.Output.Value)
                                        .ToDictionary(i =&gt; i.Features.First(), j =&gt; new Tree
                                        {
                                            Leaf = j.Output
                                        });

        if (groupings.Count() &gt; 1)
        {
            return new Tree
            {
                Branches = groupings
            };
        }

        return new Tree
        {
            Leaf = groupings.First().Value.Leaf
        };
    }

    public IEnumerable&lt;Feature&gt; UniqueFeatures()
    {
        return Instances.SelectMany(f =&gt; f.Features).DistinctBy(f =&gt; f.Axis + f.Value).ToList();
    } 
}
[/csharp]

<h2>Testing it</h2>

The examples I've used all are all straight from the Machine Learning in Action book, but we can try on a more realistic data set.  The <a href="http://archive.ics.uci.edu/ml/datasets.html" target="_blank" rel="noopener noreferrer">UCI Machine Repository</a> has a bunch of great test data sets to use.  

Using my decision tree, I've built a classifier using the <a href="http://archive.ics.uci.edu/ml/datasets/Car+Evaluation" target="_blank" rel="noopener noreferrer">car evaluation</a> data set.  

First, lets parse the data set. I made a generic line reader class that can handle emitting data sets for static files

[csharp]
public abstract class LineReader : IParser
{
    public DecisionTreeSet Parse(string file)
    {
        var set = new DecisionTreeSet();

        set.Instances = new List&lt;Instance&gt;();

        using (var stream = new StreamReader(file))
        {
            while (!stream.EndOfStream)
            {
                var line = stream.ReadLine();
                set.Instances.Add(ParseLine(line));
            }
        }

        return set;
    }

    protected abstract Instance ParseLine(string line);
}
[/csharp]

And implement the line parser for this set

[csharp]
public class Car : LineReader
{
    protected override Instance ParseLine(string line)
    {
        var splits = line.Split(new[] { ',' }, StringSplitOptions.RemoveEmptyEntries);

        var buying = splits[0];
        var maintence = splits[1];
        var doors = splits[2];
        var people = splits[3];
        var lugBoot = splits[4];
        var safety = splits[5];

        return new Instance
        {
            Output = new Output(splits[6], &quot;car acceptability&quot;),
            Features = new List&lt;Feature&gt;
                              {
                                  new Feature(buying, &quot;buying&quot;),
                                  new Feature(maintence, &quot;maintence&quot;),
                                  new Feature(doors, &quot;doors&quot;),
                                  new Feature(people, &quot;people&quot;),
                                  new Feature(lugBoot, &quot;lugboot&quot;),
                                  new Feature(safety, &quot;safety&quot;)
                              }
        };
    }
}
[/csharp]

Finally to build the tree

[csharp]
[Test]
public void TestCar()
{
    var file = @&quot;..\..\..\Assets\CarEvaluation\car.data&quot;;
    var set = new Car().Parse(file);

    var tree = set.BuildTree();

    tree.DisplayTree();

    foreach (var instance in set.Instances)
    {
        Assert.That(Tree.ProcessInstance(tree, instance).Value, Is.EqualTo(instance.Output.Value));
    }
}
[/csharp]

Which gives us a pretty big tree. <code>Tree.ProcessInstance</code> takes a tree and a sample instance and returns you the class.  The test runs through all the sample data and validates that the tree returns the appropriate output.

Feel free to <a href="https://github.com/devshorts/Playground/tree/master/MachineLearning/DecisionTree" target="_blank" rel="noopener noreferrer">check out the code</a> and run it to see the actual tree

<h2>Notes</h2>

When you use a decision tree you don't want to have to rebuild the tree each time you use it, so it's nice that the tree is easily serialzable. If you train the tree on a large data set you can store it and deserialize it whenever you need to use it.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3847</wp:post_id>
		<wp:post_date><![CDATA[2013-06-03 08:00:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-03 08:00:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-decision-tree]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="algorithms"><![CDATA[algorithms]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560643978;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4275;}i:1;a:1:{s:2:"id";i:4126;}i:2;a:1:{s:2:"id";i:3656;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>90</wp:comment_id>
			<wp:comment_author><![CDATA[Kien]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[kiennv.it@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://h-pet.blogspot.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[113.20.117.243]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-03-27 15:56:49]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-03-27 15:56:49]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I'm learning about decision trees and the need to write code in C #, I have the actual database, I need to code a demo of this decision tree algorithm, you can help me?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Ordered Consumable</title>
		<link>https://onoffswitch.net/2013/06/24/ordered-consumable/</link>
		<pubDate>Mon, 24 Jun 2013 08:00:23 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3899</guid>
		<description></description>
		<content:encoded><![CDATA[I had the need for a specific collection type where I would only ever process an element once, but be able to arbitrarily jump around and process different elements. Once a jump happened, the elements would be processed in circular order: continue to the end, then loop around to the beginning and process any remaining items.

The use case that prompted this is I have an image generator that creates snapshots out of a video on demand. However, I need to be able to seek in the video and create snapshots at the seeked point in time.  Once all the snapshots are created I don't need to create them again, it's just a one time processing, but the snapshot generation has to follow the users actions.  This also means that if a user seeks to a point in time where a snapshot was already generated, the snapshot generation doesn't need to re-process that image, but it should start processing any pending images linearlly in time nearby where the user went to.

For example, imagine you have a video that's an hour long.  You start creating images at time 0, then time 1, then time 2, etc.  At time 10, the user seeks to time 390 (6.5 minutes). We should jump and create image 390, then start 391, then 392, etc. If a user goes back to time 0, we don't need to process time 0 again, but should jump to time 11 (0 through 10 were already processed). Then time 12, then time 13, etc.

Here's a simplified example of what I want. The arrow points to the item to be processed.  

Start with a regular list

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/start.jpg" rel="attachment wp-att-3908"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/start-1024x141.jpg" alt="start" width="1024" height="141" class="alignnone size-large wp-image-3908" /></a>

Consume 1, move to 2.

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/2.jpg" rel="attachment wp-att-3907"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/2-1024x141.jpg" alt="2" width="1024" height="141" class="alignnone size-large wp-image-3907" /></a>

Consume 2, but then jump to 13.

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/shiftTo13.jpg" rel="attachment wp-att-3906"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/shiftTo13-1024x141.jpg" alt="shiftTo13" width="1024" height="141" class="alignnone size-large wp-image-3906" /></a>

Consume 13, reach the end and wrap around. 1, and 2 are already processed, next to process is 3
 
<a href="http://onoffswitch.net/wp-content/uploads/2013/05/3.jpg" rel="attachment wp-att-3905"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/3-1024x141.jpg" alt="3" width="1024" height="141" class="alignnone size-large wp-image-3905" /></a>

Consume 3, move to 4

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/4.jpg" rel="attachment wp-att-3904"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/4-1024x141.jpg" alt="4" width="1024" height="141" class="alignnone size-large wp-image-3904" /></a>

Consume 4, move to 5

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/5.jpg" rel="attachment wp-att-3903"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/5-1024x141.jpg" alt="5" width="1024" height="141" class="alignnone size-large wp-image-3903" /></a>

Jump to 10, consume, and move to 11 

<a href="http://onoffswitch.net/wp-content/uploads/2013/05/101.jpg" rel="attachment wp-att-3912"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/101-1024x141.jpg" alt="10" width="1024" height="141" class="alignnone size-large wp-image-3912" /></a>

Until the whole list is processed. Once the list is processed you can't consume anything else.

Here is a unit test to demonstrate in code:

[csharp]
[Test]
public void TestConsuming()
{
    var l = new List&lt;int&gt; { 1, 2, 3, 4, 5, 6, 7, 8, 9 };

    var consumable = new OrderedConsumable&lt;int&gt;(l);

    consumable.SetAsHead(item =&gt; item == 8);

    var sort = consumable.Take(4).ToList();

    Assert.IsTrue(sort.First() == 8);
    Assert.IsTrue(sort.Last() == 2);

    consumable.SetAsHead(item =&gt; item == 8);

    sort = consumable.Take(40).ToList();

    Assert.IsTrue(sort.First() == 3);
    Assert.IsTrue(sort.Last() == 7);
}
[/csharp]

The easiest way to implement this was to wrap a list with a custom IEnumerable.  The underlying enumerator can track if an item is processed or not, and if so find the next item to emit.

Here's the IEnumerable

[csharp]
public class OrderedConsumable&lt;T&gt; : IEnumerable&lt;T&gt;
{
    public IList&lt;T&gt; List { get; set; }

    private OrderedConsumableEnumerator&lt;T&gt; Enumerator { get; set; }

    public OrderedConsumable(IList&lt;T&gt; list)
    {
        List = list;

        Enumerator = new OrderedConsumableEnumerator&lt;T&gt;(List);
    }

    public bool SetAsHead(Func&lt;T, bool&gt; selector)
    {
        var item = List.FirstOrDefault(selector);

        var indx = List.IndexOf(item);

        if (indx != -1)
        {
            var alreadyProcessedIndex = Enumerator.HasProcessed(indx);

            Enumerator.SetIndexTo(indx);

            return !alreadyProcessedIndex;
        }

        return false;
    }

    public void Unmark(T item)
    {
        var idx = List.IndexOf(item);

        if (idx != -1)
        {
            Enumerator.ReProcess(idx);
        }
    }

    public void Reset()
    {
        Enumerator.Reset();
    }

    public Boolean CompletelyConsumed
    {
        get { return Enumerator.CompletelyConsumed; }
    }

    public IEnumerator&lt;T&gt; GetEnumerator()
    {
        return Enumerator;
    }

    IEnumerator IEnumerable.GetEnumerator()
    {
        return GetEnumerator();
    }
}
[/csharp]

It's basically just a wrapper over the enumerator. The enumerator does all the real work:

[csharp]
public class OrderedConsumableEnumerator&lt;T&gt; : IEnumerator&lt;T&gt;
{
    private readonly object _locker = new object();

    private IList&lt;T&gt; List { get; set; }

    private int Index { get; set; }

    private List&lt;Boolean&gt; Processed { get; set; }

    private int Length { get; set; }

    private int ConsumedCount { get; set; }

    public OrderedConsumableEnumerator(IList&lt;T&gt; list)
    {
        List = list;
        Index = 0;

        ConsumedCount = 0;

        Length = List.Count;

        Processed = List.Select(f =&gt; false).ToList();

    }

    public void Dispose()
    {

    }

    public bool MoveNext()
    {
        lock (_locker)
        {
            if (!CompletelyConsumed)
            {
                while (Processed[Index % Length])
                {
                    Index++;

                    if (Index &gt;= Length)
                    {
                        Index = 0;
                    }
                }

                return true;
            }

            return false;
        }
    }



    public void Reset()
    {
        lock (_locker)
        {
            Index = -1;

            Processed = Processed.Select(i =&gt; false).ToList();
        }
    }

    public void SetIndexTo(int indx)
    {
        lock (_locker)
        {
            Index = indx;
        }
    }

    public T Current
    {
        get
        {
            lock (_locker)
            {
                Processed[Index] = true;
                ConsumedCount++;
                return List[Index];
            }
        }
    }

    object IEnumerator.Current
    {
        get { return Current; }
    }

    public Boolean CompletelyConsumed
    {
        get { return ConsumedCount == Length; }
    }

    public void ReProcess(int idx)
    {
        lock (_locker)
        {
            Processed[idx] = false;
            ConsumedCount--;
        }
    }

    public bool HasProcessed(int indx)
    {
        lock (_locker)
        {
            return Processed[indx];
        }
    }
}
[/csharp]

The enumerator uses a boolean array (the same size as the input list) to track if something is processed or not.  When someone calls <code>Current</code> it marks the current index as processed and returns the value.  When  <code>MoveNext</code> is executed, we just need to find the next unprocessed element in the processed list and set the underlying index to that element. 

While using a linked list might have been more space and time efficient to track what is next (since iterating over the while loop if the array gets fragmented can be inefficient), I needed a way to "unmark" if something was processed. For example, if I had consumed an item but needed to re-consume it, I needed a way to unmark that it was consumed. It also needed to maintain its order in the list. If I used a linked list I'd lose that ordering.

Anyways, the final use case looked something like this.  The generator doesn't care what the order is, and when someone seeks they just update the internal head pointer.

[csharp]
public void GenerateImages()
{
    OrderedConsumable = new OrderedConsumable&lt;int&gt;(SecondsToProcessList));
	
    foreach (var second in OrderedConsumable)
    {
        MakeImageForSecond(second);
    }
}    
 
public void Seek(int targetSeconds)
{
    OrderedConsumable.SetAsHead(seconds =&gt; targetSeconds == seconds);
}
[/csharp]

The nice thing is here, if I try to iterate over the consumable again, if everything is consumed nothing will return.  This ensures that I don't reprocess anything that was already processed.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3899</wp:post_id>
		<wp:post_date><![CDATA[2013-06-24 08:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-24 08:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[ordered-consumable]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="data-structure"><![CDATA[data structure]]></category>
		<category domain="post_tag" nicename="enumerable"><![CDATA[enumerable]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554944619;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4892;}i:1;a:1:{s:2:"id";i:3640;}i:2;a:1:{s:2:"id";i:6;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tech Talk: Sorting of ratings</title>
		<link>https://onoffswitch.net/2013/05/30/tech-talk-sorting-ratings/</link>
		<pubDate>Thu, 30 May 2013 15:35:37 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3924</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk discussed different ways to sort ratings system. The topic revolved around a <a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html" target="_blank" rel="noopener noreferrer">blog post</a> we discovered a while ago breaking down different problems with star based sorts.

The article describes a few problems:

<h2>Rating type: Good - Bad</h2>

The issue here is that when you use only the difference in positive vs negative ratings, you get skewed results to highly popular (but also maybe highly disliked) items. For example, an item that has upvotes of 200, but downvotes of 50 would have a score of 150.  However, an item who has 125 upvotes and no downvotes would be technically scored lower here.  The team and I agreed that this isn't a good way of sorting a rating, since the abscense of negatives is a stronger indication of a positive review.  I think most people actually do this kind of analysis in their mind: if something is highly rated in both positive and negative, then even though it may be popular, it also could be problematic.  

<h2>Rating type: Good / (Good + Bad)</h2>

In this scenario, you're taking the average positive reviews of an item.  The article breaks this down pretty clearly when it shows how amazon rates an item only 1 review as higher rated than an item with 500 reviews.  Like with the previous example, people intuitively know to discount an item with only a few votes: its not statistically significant enough.

<h2>Rating type: Lower bound of Wilson score confidence interval for a Bernoulli parameter</h2>

I won't begin to explain what this is, since I don't fully understand all the math behind it, but from what we were able to discern this type of rating system uses a Bernoulli distribution and even outs the weights for both large samples and small samples.  It would've been nice to see a comparison of how this would work with the amazon or urban dictionary example, since without that its hard to visualize how this affects the sort.  Also I'm not sure how well (or even at all) this works on a rating scale though (1-5 for example), since the article makes the assumption of a binomial distribution (either yes or no).  

<h2>Rating type: Baysenian Estimation</h2>

In our research for this discussion we also stumbled on <a href="http://wiki.answers.com/Q/What_does_true_Bayesian_estimate_mean_in_connection_with_the_IMDb_Top_250_ratings" target="_blank" rel="noopener noreferrer">how IMDB does their ratings system</a>.  In this kind of estimation IMDB weights the actual rating score by the number of ratings. So instead of normalizing, or removing outliers, they adjust the score to lean more towards the average movie rating.  This way it takes an exceptional amount of up votes to get something to be rated 10 out of 10 since the score is trying to be weighted towards the average (by both the regular average of the movie AND by the number of votes).  

<h2>Conclusion</h2>

Our conclusion was that the best way to give meaningful data to a user is to allow for many different kinds of ranking systems.  It seems obvious to me that there isn't just one kind of sort that gives a user meaningful data.  Different people interpret data differently and are looking for different meanings in the data, so a truly robust system would be able to provide a couple different ways of rating sorts.  In the end, here's a relevant xkcd:

[caption id="attachment_3933" align="aligncenter" width="201"]<a href="http://xkcd.com/937/"><img src="http://onoffswitch.net/wp-content/uploads/2013/05/tornadoguard.png" alt="From xkcd http://xkcd.com/937/" width="201" height="386" class="size-full wp-image-3933" /></a> From xkcd http://xkcd.com/937/[/caption]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3924</wp:post_id>
		<wp:post_date><![CDATA[2013-05-30 15:35:37]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-05-30 15:35:37]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-sorting-ratings]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561943268;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4191;}i:1;a:1:{s:2:"id";i:3500;}i:2;a:1:{s:2:"id";i:7777;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Automatic fogbugz triage with naive bayes</title>
		<link>https://onoffswitch.net/2013/06/10/fogbugz-priority-prediction-naive-bayes/</link>
		<pubDate>Mon, 10 Jun 2013 08:00:39 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3942</guid>
		<description></description>
		<content:encoded><![CDATA[At my work we use fogbugz for our bugtracker and over the history of our company's lifetime we have tens of thousands of cases.  I was thinking recently that this is an interesting repository of historical data and I wanted to see what I could do with it.  What if I was able to predict, to some degree of acuracy, who the case would be assigned to based soley on the case title?  What about area? Or priority? Being able to predict who a case gets assigned to could alleviate a big time burden on the bug triager.

Thankfully, I'm reading "Machine Learning In Action" and came across the naive bayes classifier, which seemed a good fit for me to use to try and categorize cases based on their titles.  Naive bayes is most famously used as part of spam filtering algorithms.  The general idea is you train the classifier with some known documents to seed the algorithm.  Once you have a trained data set you can run new documents through it to see what they classify as (spam or not spam).

For those who've never used Fogbugz, let me illustrate the data that's available to me. I've highlighted a few areas I'm going to use.  The title is what we're going to use as the prediction value (highlighted blue), and the other red highlights are categories I want to predict (area, priority, and who the case is assigned to).

<img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-30-19_42_05-FogBugz1.png" alt="2013-05-30 19_42_05-FogBugz" width="793" height="480" class="aligncenter size-full wp-image-3965" />

For the impatient, full source code of my bayes classifier is available on my <a href="https://github.com/devshorts/Playground/tree/master/MachineLearning/NaiveBayes" target="_blank" rel="noopener noreferrer">github</a>.

<h2>Conditional probability</h2>

Conditional probability describes the probability of an item given you already know something about it.  Formally it's described in the syntax of P(A | B), which is pronounced as "probability of A given B".  A good example is provided for in the machine learning book. Imagine you have 7 marbles.  3 white marbles, and 4 black marbles. Whats the probability of a white marble? It's 3/7.  How about a black marble? It's 4/7.

Now imagine you introduce two buckets: a blue bucket and a red bucket.  In the red bucket, you have 2 white marbles and 2 black marbles. In the blue bucket you have 1 white marble and 2 black marbles.  Whats the probability of getting a white marble from the blue bucket? It's 1/3. There is only one white marble in the blue bucket, and 3 total marbles.  So, <em>P(white marble | blue bucket)</em> is 1/3. 

<img src="http://onoffswitch.net/wp-content/uploads/2013/05/buckets.jpg" alt="buckets" width="321" height="144" class="aligncenter size-full wp-image-3952" />

<h2>Bayes Formula</h2>

This doesn't really help though.  What you really want is to be able to calculate <em>P(red bucket | white marble)</em>. This is where bayes rule comes into play:

<img src="http://onoffswitch.net/wp-content/uploads/2013/05/d92e290c66d423e4798a22a3690cbd31.png" width="216" height="48" class="alignnone" />

This formula describes how items and their conditions relate (marbles and buckets).

<h2>Conditional Independence</h2>

Naive bayes is called naive because it assumes that each occurrence of an item is just as likely as any other.  Getting a white marble isn't dependent on first getting a black marble. To put it another way, the word "delicious" is just as likely to be next to "sandwich" as it is to "stupid".  It's not really the case. In reality "delicious" is much more likely to be next to "sandwich" than "stupid".

The naive portion is important to note, because it allows us to use the following property of conditionally independent data:

<img src="http://onoffswitch.net/wp-content/uploads/2013/05/2013-05-31-14_09_56-www.asz_.ymmf_.hu_talata_math_probability_sections_section_2_6.png" alt="Independent product formula" width="354" height="61" class="aligncenter size-full wp-image-3980" />

What this formula means is that the probability of one thing AND another thing is the probability of each multiplied together. This applies to us since if the text is composed of words, and words are conditionally independent, then we can use the above property to determine the probability of text. In other words, you can expand <em>P(text | spam)</em> to be 

[code]
text = word1 ∪ word2 ∪ word3 ∪ ... ∪ wordN

P(text | spam) = P(word1 | spam)*P(word2 | spam)...*P(wordN | spam)
[/code]

The probability of the entire text is the probability of each word multiplied together.

<h2>Naive Bayes Algorithm Training</h2>

The goal of training the classifier is to find out what is the probability of a word in a particular class. If we're going to classify the assigned user based on case text, then we need to know what is the probability of a word when the assigned user is "<em>anton kropp</em>", and what is the probability when it is "<em>other developer</em>".

To train the classifier, you need to have some documents whose class you know. For my example, I need to have a bunch of cases already made who are properly assigned.  Using the training documents you first need to find out what are all the available words in the space.  For this I tokenized the case titles into words without any special tokens (?, !, -, <, >, etc) and I removed <a href="http://www.webconfs.com/stop-words.php" target="_blank" rel="noopener noreferrer">commmon stop words</a>.  The word space is the unique set of all the words in the training documents.

The next idea is that you treat this word space as a vector.  If I am training the data on two cases with titles "<em>this is broken</em>" and "<em>users don't work</em>",  then the vector will be:

[code]
[this | is | broken | users | dont | work]
[/code]

Where the word "this" is in the 0 position, "is" is in position 1, etc.  Next what we need to do is correlate each document's word occurrence to the training word space vector.  If a word exists it'll get incremented in the right position.  For example, with title "<em>this is broken</em>", it's vector will look like

[code]
[1 1 1 0 0 0]
[/code]

Indicating it had 1 occurrence of "<em>this</em>", 1 of "<em>is</em>", and 1 of "<em>broken</em>", but zero occurrences of "<em>users</em>", "<em>dont</em>" and "<em>work</em>".  The second title will have

[code]
[0 0 0 1 1 1]
[/code]

Next we need to do is go through every class (the assigned users, for example, such as "<em>anton kropp</em>", and "<em>other developer</em>") and divide the count of each word by the total number of words found. This gives you the probability that a certain word occurs in a certain class.

<h2>Work through an example</h2>

So lets say I have a document class that represents what I'm using to train my data. The document class is generic, it doesn't really care what it's representing.

[csharp]
public class Class
{
    public string Name { get; set; }       
}

public class Document
{
    public List&lt;String&gt; Words { get; set; }

    public Class Class { get; set; }
}
[/csharp]

In my example a document is the case. The words represent the tokenized case title, and the class is the thing I am classifying ("<em>anton kropp</em>" or "<em>other developer</em>").

To build the unique set of vocab in the training space is easy

[csharp]
/// &lt;summary&gt;
/// Create a unique set of words in the vocab space
/// &lt;/summary&gt;
/// &lt;param name=&quot;documents&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static List&lt;string&gt; Vocab(IEnumerable&lt;Document&gt; documents)
{
    return new HashSet&lt;string&gt;(documents.SelectMany(doc =&gt; doc.Words)).ToList();
}
[/csharp]

Now to train the data.  We group the documents by class, then create a vector representing the occurance of each word in the space for all the documents in that class. I'm initializing the first vector to use all 1's instead of 0's since we are going to multiply the probability of each word together. If any probability is zero (i.e a word wasn't found), then we'll get a zero for the probability, which isn't really true.  

I'm using the <a href="http://mathnetnumerics.codeplex.com/" target="_blank" rel="noopener noreferrer">Math.Net numerics</a> vector library to do the vector processing.

[csharp]
public static TrainedData TrainBayes(List&lt;Document&gt; trainingDocuments)
{
    var vocab = VocabBuilder.Vocab(trainingDocuments);

    var classes = trainingDocuments.GroupBy(doc =&gt; doc.Class.Name).ToList();

    var classProbabilities = new List&lt;ClassProbability&gt;();

    foreach (var @class in classes)
    {
        Vector&lt;double&gt; countPerWordInVocabSpace = DenseVector.Create(vocab.Count, i =&gt; 1);

        countPerWordInVocabSpace = @class.Select(doc =&gt; doc.VocabListVector(vocab))
                                         .Aggregate(countPerWordInVocabSpace, (current, docVocabVector) =&gt; current.Add(docVocabVector));

        var totalWordsFound = 2 + countPerWordInVocabSpace.Sum();

        var probablityVector = DenseVector.OfEnumerable(countPerWordInVocabSpace.Select(i =&gt; Math.Log(i/totalWordsFound)));

        // create an easy to read list of words and its probablity
        var probabilityPerWord = probablityVector.Zip(vocab, (occurence, word) =&gt; new WordProbablity { Probability = occurence, Word = word })
                                                 .ToList();

        var probabilityOfClass = 1.0 / classes.Count();

        classProbabilities.Add(new ClassProbability
        {
            Class = @class.First().Class,
            ProbablityOfClass = probabilityOfClass,
            ProbablitiesList = probabilityPerWord,
            ProbabilityVector = probablityVector
        });
    }

    return new TrainedData
    {
        Probabilities = classProbabilities,
        Vocab = vocab
    };
} 
[/csharp]

You may notice the logarithm stuff going on. That's to prevent <a href="http://stackoverflow.com/a/9342513/310196" target="_blank" rel="noopener noreferrer">numeric underflow</a>. Since probabilities will be small decimals, multiplying them all will make them smaller. By taking the logarithm we can maintain the same relative shape of the function, but they get shifted into a different number space. Now multiplying them won't give us underflow.

<h2>Classifying</h2>

To classify, we need to multiply the vocab vector of a document by each classes probability vector and add the probabilities up.  Whatever class gives us the highest probability is the class we predict we are

[csharp]
public static Class Classify(Document document, TrainedData trainedData)
{
    var vocabVector = document.VocabListVector(trainedData.Vocab);

    var highestProbability = double.MinValue;

    Class classified = null;

    foreach (var @class in trainedData.Probabilities)
    {
        var probablityOfWordsInClass = vocabVector.PointwiseMultiply(@class.ProbabilityVector).Sum();

        var probablity = probablityOfWordsInClass + Math.Log(@class.ProbablityOfClass);

        if (probablity &gt; highestProbability)
        {
            highestProbability = probablity;

            classified = @class.Class;
        }
    }

    return classified;
}
[/csharp]

<h2>Success rate</h2>

The next step is to test the success rate.  If we have a bunch of documents that are already categorized we can run them back through the classifier.  At that point all that's required is to see if the classifier classifies a document properly or not.  The success rate is important, because if we have a low success rate of classifying documents then either the training set was poor, or the classifier needs other tuning.  Without testing the success rate you can't know how accurate your predictions would be. 

<h2>Back to fogubgz</h2>

Remember, I want to see if I can auto triage cases.  To do that I need to train the bayes classifier with some known data.  Thankfully, I have over 50,000 cases to use as a my training set.  Fogbugz makes it easy to pull data back since they have an exposed <a href="http://www.fogcreek.com/fogbugz/docs/70/topics/advanced/api.html" target="_blank" rel="noopener noreferrer">web api</a> you can hook into that returns back neatly formatted XML. First log in:

[code]
curl &quot;http://fogbugz.com/api.asp?cmd=logon&amp;email=myEmail@company.com&amp;password=naivebayes&quot;
[/code]

Which gives you a token as a response

[code]
&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;response&gt;
    &lt;token&gt;
       &lt;![CDATA[5lrnq9c34cjg6h01flg3coe5etj2gg]]&gt;
    &lt;/token&gt;
&lt;/response&gt;
[/code]

Next it's easy to query for a bunch of raw cases and pipe it to some file

[code]
curl &quot;http://fogbugz.com/api.asp?cmd=search&amp;q=openedby:anton&amp;cols=sTitle,sProject,sArea,sPersonAssignedTo,sPriority,events,ixPersonOpenedBy,ixPersonResolvedBy&amp;max=1500&amp;token=5lrnq9c34cjg6h01flg3coe5etj2gg&quot; &gt; anton.xml
[/code]

This pulls back 1500 cases that I opened with the colums of title, project, area, assigned to, priority, all the edit events, who opened it, and who resolved it.  I chose 1500 only to not have to wait to pull back all 50,000+ cases (though I probably could have).  Also I am limiting my searches to individual users who have opened cases. By doing this I may be able to get some insight in what kind of cases people make. Maybe cases I make are more likely to be assigned to myself. By limiting my training set to a specific category like this I'm inherently creating a pivot.

After parsing the xml, all you  need to do is transform your fogbugz case to a generic document (that the naive bayes classifier I wrote wants). <code>ToDoc</code> lets me adjust what is the class (in this scenario who the case is assigned to) and what is the predictor text (the case title).  Generalizing the transform lets me reapply the same runner for different combinations of input.

[csharp]
public void TestParser(string path)
{                      
    var parser = new FogbugzXmlParser();

    var cases = parser.GetCases(path);

    Func&lt;Case, Document&gt; caseTransform =
        i =&gt;
        i.ToDoc(@case =&gt; @case.AssignedTo,
                @case =&gt; @case.Title);
    
    ProcessCases(cases, caseTransform);
}

private void ProcessCases(List&lt;Case&gt; cases, Func&lt;Case, Document&gt; caseTransform)
{
    var total = cases.Count();

    var trainingSet = cases.Take((int)(total * (3.0 / 4))).Select(caseTransform).ToList();

    var validationSet = cases.Skip((int)(total * (3.0 / 4))).Select(caseTransform).ToList();

    var trainedData = NaiveBayes.TrainBayes(trainingSet);

    var successRate = 0.0;

    foreach (var @case in validationSet)
    {
        if (NaiveBayes.Classify(@case, trainedData).Name == @case.Class.Name)
        {
            successRate++;
        }
    }

    successRate = successRate / validationSet.Count();

    foreach (var type in trainedData.Probabilities)
    {
        Console.WriteLine(type.Class.Name);
        Console.WriteLine(&quot;--------------------&quot;);

        type.Top(10).ForEach(i =&gt; Console.WriteLine(&quot;[{1:0.00}] {0}&quot;, i.Word, i.Probability));

        Console.WriteLine();
        Console.WriteLine();
    }

    Console.WriteLine(&quot;Prediction success rate is {0:0.00}%&quot;, successRate * 100);
}
[/csharp]

<h2>Conclusion</h2>

While I can't publish our internal bug information, I found that the classifier ranged in success from as low as 30% to as high as 80% depending on which user I pulled data back from. Personally, cases I opened had a 72.27% success rate classifying their assignment based soley on case title.  That's not too bad!  It'd be neat to tie this classifier into a fogbugz plugin and generate a first pass auto triage report for our triager to use.  When they've reassigned cases we can re-train the set periodically and hopefully get better results.

Full source including bayes classifier and fogbugz parsing available <a href="https://github.com/devshorts/Playground/tree/master/MachineLearning/NaiveBayes" target="_blank" rel="noopener noreferrer">here</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3942</wp:post_id>
		<wp:post_date><![CDATA[2013-06-10 08:00:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-10 08:00:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[fogbugz-priority-prediction-naive-bayes]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="post_tag" nicename="classification"><![CDATA[classification]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="fogbugz"><![CDATA[fogbugz]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
		<category domain="post_tag" nicename="naive-bayes"><![CDATA[naive bayes]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558918892;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4027;}i:1;a:1:{s:2:"id";i:4170;}i:2;a:1:{s:2:"id";i:3847;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Working on a long term svn branch</title>
		<link>https://onoffswitch.net/2013/06/05/working-long-term-svn-branch/</link>
		<pubDate>Wed, 05 Jun 2013 17:13:50 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4000</guid>
		<description></description>
		<content:encoded><![CDATA[I work on a reasonably small team and for the most part everyone works in trunk. But it can happen where you need to switch over to a long term feature branch (more than a week or two) that can last sometimes months.  The problem here is that your branch can easily diverge from trunk. If the intent is that the feature branch will eventually become the master (trunk) then you should merge the feature branch frequently.  For me, this method has worked really well.

Merging often lets you take the trunk fixes that happen and you manually resolve any conflicts as they come in.  Since the feature branch is going to be the final thing (when the feature is done), svn needs to know how to deal with these conflicts. It's <em>much</em> better to deal with them as they come in, rather than try to integrate a feature branch after months of work only to see an svn merge with hundreds of conflicts.

The problem with resolving those conflicts later is that contextually you can't remember what they were doing anymore. If you have a conflict that spans 2 or 3 files, it's easy to get lost in what needs to be discarded, what needs to be modified, and what needs to be resolved with local or repo changes.  This just means that your QA is going to absolutely hate you because nobody is confident that the merge was complete: something could be missing, or a logical piece isn't right.  By merging frequently from trunk into the branch you make svn's job easier. It knows how to resolve potential conflicts because you already did it.  

You can take this one step further and do the same thing with multiple feature branches. Lets say you have a setup like this:

<img src="http://onoffswitch.net/wp-content/uploads/2013/06/svn1.jpg" alt="svn" width="354" height="491" class="aligncenter size-full wp-image-4006" />

You have two feature branches and trunk.  Periodically you should merge the first branch from trunk (I do this every monday morning). Then periodically also merge the second branch <em>from the first branch</em>.  When the first branch is done, you can easily reintegrate it.

After you integrate the first branch, you can start to merge the second branch back off of trunk

<img src="http://onoffswitch.net/wp-content/uploads/2013/06/svn21.jpg" alt="svn2" width="354" height="491" class="aligncenter size-full wp-image-4007" />

Eventually when the second branch is done, you can reintegrate it back into trunk and you won't have any conflicts.  

The important thing here is to do your due diligence in making sure all the conflicts and merges are properly done and done often.  Don't wait till the last minute to do this, it can be time consuming but it's a lot easier to do this upfront then all at the end.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4000</wp:post_id>
		<wp:post_date><![CDATA[2013-06-05 17:13:50]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-05 17:13:50]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[working-long-term-svn-branch]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="branches"><![CDATA[branches]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
		<category domain="post_tag" nicename="svn"><![CDATA[svn]]></category>
		<category domain="post_tag" nicename="version-control"><![CDATA[version control]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559886835;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4892;}i:1;a:1:{s:2:"id";i:3847;}i:2;a:1:{s:2:"id";i:3367;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tech talk: B-Trees</title>
		<link>https://onoffswitch.net/2013/06/07/tech-talk-b-trees/</link>
		<pubDate>Fri, 07 Jun 2013 23:20:51 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4011</guid>
		<description></description>
		<content:encoded><![CDATA[Yesterdays tech talk was on b-trees. B-trees are an interesting tree data structure that are used to minimize disk read access.  Also, since they are self balancing, and optimized for sequential reads and inserts, they're really good for file systems and databases.  CouchDB, MongoDB, SQLite, SQL Server and other datbases all use either a b-tree or a b+ tree as their data indexes, so it was interesting to discuss b-tree properties.

<h2>Disk io optimizations</h2>

A big part of the need for b-trees is disk io optimizations. The need to optimize for disk reads comes from the fact that disk io is slow. Imagine you have to make thousands of reads off disk to try and find a certain piece of data. The rotational delay in a platter drive to read a certain disk block can be up to 20 milliseconds.  If you have to read hundreds, or even thousands of blocks of data to find something then your search speed can now range in the hundreds of milliseconds, which is certainly noticeable to a user.

A good way of alleviating some of this disk read time is to have an optimized index that tells you where to go look for your data. So instead of reading blocks off disk to search for data, you can read a much smaller index which points to which disk block the data you want is. Then all you have to do is go find the disk block and do one final search inside of that block to find your data.  

<h2>Insertion and deletion</h2>

Deletion in a b-tree is cheap and easy.  Once you find the data in the index that should be deleted you mark that index location as deleted.  Periodically you can maybe purge the data. This is why in many databases when you do a delete the database size doesn't actually get any smaller. With SQLite, for example, you have to issue a <code>vacuum</code> command which rebuilds the index.   Only at this point does the database get smaller.

Inserts, however, are more complicated. Since the tree needs to be optimized for sequential linear reads, you don't want to use a dense storage for your data. This would mean every time you needed to add something you would have to shift all your data around. Instead, its cheaper and faster to be sparse: leave empty space for yourself to add things to the index.  

<h2>Rebalancing</h2>

What happens when a block is full? At this point b-trees recursively split themselves.  Theoretically, they take the median of the values and create a new split node from that.  Half the data goes into the left branch from that node, and half from the right.  Look at this trace through from <a href="http://en.wikipedia.org/wiki/B-tree" target="_blank" rel="noopener noreferrer">wikipedia</a>

<img src="http://onoffswitch.net/wp-content/uploads/2013/06/B_tree_insertion_example.png" width="180" height="391" class="alignnone" />

Here you can see the sparseness of the arrays as well as the final split (the last trace) when 6 (the median) is chosen as the split point and 5 and 7 are moved to separate sparse leaves off the 6 branch.

<h2>B+ Trees</h2>

B+ trees, in comparison to b-trees, keep key/value pair data only in the leaf nodes.  B-Tree's keep key value pair data at each point in the index.  You can see that visualized here

[caption width="569" align="aligncenter"]<img src="http://onoffswitch.net/wp-content/uploads/2013/06/image002.jpg" width="569" height="247" class /> source: http://www.mec.ac.in/resources/notes/notes/ds/bplus.htm[/caption]

<h2>When would you make a B-Tree?</h2>

After the discussion of the theory behind b-trees a great question came up: when would you ever use one directly? The conclusion we reached was that you probably wouldn't. If you had so much data you needed to leverage the b tree's disk access properties, you'd probably dump your data into a database that already implemented the tree. You wouldn't really want to build a b-tree from scratch, since there are a lot of optimizations that can be done.  For example, oracle apparently <a href="http://dba.stackexchange.com/questions/9963/b-tree-node-split-strategy-in-sql-server-for-monotonically-increasing-value" target="_blank" rel="noopener noreferrer">doesn't do a 50-50 data split</a> when re-balancing the tree. It instead does a 90-10 split when it detects sequential inserts.

<h2>More reading</h2>

Animated trace of the b-tree algorithm: <a href="http://ats.oka.nu/b-tree/b-tree.html" target="_blank" rel="noopener noreferrer">http://ats.oka.nu/b-tree/b-tree.html</a>

CouchDB explanation of their b-tree implementation: <a href="http://guide.couchdb.org/editions/1/fr/btree.html" target="_blank" rel="noopener noreferrer">http://guide.couchdb.org/editions/1/fr/btree.html</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4011</wp:post_id>
		<wp:post_date><![CDATA[2013-06-07 23:20:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-07 23:20:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-b-trees]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="b-tree"><![CDATA[b-tree]]></category>
		<category domain="post_tag" nicename="databases"><![CDATA[databases]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1555847166;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1043;}i:1;a:1:{s:2:"id";i:3161;}i:2;a:1:{s:2:"id";i:4945;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Qconn NYC 2013</title>
		<link>https://onoffswitch.net/2013/06/12/qconn-nyc-2013/</link>
		<pubDate>Wed, 12 Jun 2013 15:10:10 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4024</guid>
		<description></description>
		<content:encoded><![CDATA[If anyone is at qconn this year come find me (I'm wearing an adult swim hoodie)! There won't be a tech talk this week since I'm busy at the conf but things will return back to normal next week.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4024</wp:post_id>
		<wp:post_date><![CDATA[2013-06-12 15:10:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-06-12 15:10:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[qconn-nyc-2013]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561296656;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3295;}i:1;a:1:{s:2:"id";i:4077;}i:2;a:1:{s:2:"id";i:4575;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Linear separability and the boundary of wx+b</title>
		<link>https://onoffswitch.net/2013/07/01/meaning-wtxb/</link>
		<pubDate>Mon, 01 Jul 2013 08:00:01 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4027</guid>
		<description></description>
		<content:encoded><![CDATA[In machine learning, everyone talks about weights and activations, often in conjunction with a formula of the form <code><strong>w</strong><strong>x</strong>+b</code>.  While reading <a href="http://www.manning.com/pharrington/" target="_blank" rel="noopener noreferrer">machine learning in action</a> I frequently saw this formula but didn't really understand what it meant. Obviously its a line of some sort, but what does the line mean? Where does w come from? I was able to muddle past this for decision trees, and naive bayes, but when I got to support vector machines I was pretty confused. I wasn't able to follow the math and conceptually things got muddled.

At this point, I switched over to a different book, machine learning an algorithmic perspective. 

<a href="http://www.amazon.com/Machine-Learning-Algorithmic-Perspective-Recognition/dp/1420067184/ref=zg_bs_tab_pd_mg_2" target="_blank" rel="noopener noreferrer"><img src="http://onoffswitch.net/wp-content/uploads/2013/06/41o11gP3WpL._BO2204203200_PIsitb-sticker-arrow-clickTopRight35-76_AA300_SH20_OU01_.jpg" alt="41o11gP3WpL._BO2,204,203,200_PIsitb-sticker-arrow-click,TopRight,35,-76_AA300_SH20_OU01_.jpg" width="300" height="300" class="aligncenter size-full wp-image-4037" /></a>

Here, the book starts with a discussion on neural networks, which are directly tied to the equation of <code><strong>w</strong><strong>x</strong>+b</code> and the concept of weights. I think this book is much better than the other one I was reading. The author does an excellent job of describing the big picture of what each algorithm is, followed by how the math is derived. This helped put things in perspective for me and let me peek into the workings of each algorithm without any glossed over magic.

<h2>Neural networks</h2>

In a neural network, you take some set of inputs, multiply them by some value, and if the sum of all the inputs times that value is greater than some threshold (defined by a function) then the neuron fires.  The thing that you multiply the input by is called the weight.  The threshold is determined by an activation function.

[caption width="600" align="alignnone"]<img src="http://onoffswitch.net/wp-content/uploads/2013/06/600px-ArtificialNeuronModel_english.png" width="600" height="285" class="alignnone" /> Source image http://en.wikibooks.org[/caption]

But, lets say the inputs to all these nodes is zero, but you want the neuron to fire. Zero times any weights is zero so the node never can fire. This is why in neural networks you introduce a bias node. This bias node always has the value of one, but it also has its own weight. This bias node can offset inputs that are all zero that should trigger an activation.

[caption width="553" align="alignnone"]<img src="http://onoffswitch.net/wp-content/uploads/2013/06/NN2.png" width="553" height="279" class /> Source image http://www.codeproject.com/Articles/16419/AI-Neural-Network-for-beginners-Part-1-of-3[/caption]

A common way of calculating the activation of a neural network is to use matrix multiplication.  Remembering that a neuron fires if the input times the weights is above some value, to find the sum you can take a row vector of the inputs and multiply it by a column vector of the weights.  This gives you a single value that you can pass to a function that determines whether you want to fire or not.

<img src="http://onoffswitch.net/wp-content/uploads/2013/06/a2913ed385299a6e53da3f2e7c68d2fe.png" width="582" height="109" class="alignnone" />

In this way you can think of a neural network as a basic classifier. Given some input data it either fires or it doesn't.  If the input data doesn't cause a firing then the class can be thought of 0, and if it does fire then the output class can be thought of as 1.

Now it makes sense why the <strong>w</strong> and the <strong>x</strong> are bold. Bold arguments, in math formulas, represent vectors and not scalars. 

<h2>Classifiers</h2>

But how does this line relate to classifiers? The point of classifiers is, given some input feature data, to determine what kind of thing it is.  With a binary classifier, you want to find if the input data classifies as a one or a zero.  

Here is an image representing the margin in a support vector machine

[caption width="220" align="alignnone"]<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/2/2a/Svm_max_sep_hyperplane_with_margin.png/220px-Svm_max_sep_hyperplane_with_margin.png" width="220" height="237" class /> Source wikipedia[/caption]

The points represent different instances, the color of the point tells you what class it is, and the x1 and x2 coordinates represent features. The point at (x1, x2) is an instance that maps to that feature set. 

The line represents the point where if you took an x1 and x2 feature set,  multiplied by and summed some fixed weights (so x1*w1 + x2*w2 + b),  offset it by a determined bias, then everything above the line activates (neuron fires), and everything below the line doesn't activate.  This means that the line is the delineation of what is one class vs what is another class.  

What confused me here is that there is a discussion of w but the axis are only in relation to x1 and x2.  Really what that means is that

[code]f(x1, x2) = w1*x1+w2*x2 + b[/code]

This is the ideal line that represents classification.  <strong>w</strong> is a fixed vector here, the output function varies with the feature input. The b of the function represents the weight of the bias node: it's independent of the input data.

<strong>wx</strong>+b defined a linearly separable classifier. In two dimensional space this is a line, but you can also classify in any other space. If your input has 3 features you can create a separating plane, and in more dimensions you can create a hyperplane. This is the basis of kernel functions (mapping feature sets that aren't linearly separable in one space into a feature set that is linearly separable in another space: ie projection).

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4027</wp:post_id>
		<wp:post_date><![CDATA[2013-07-01 08:00:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-01 08:00:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[meaning-wtxb]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="discussion"><![CDATA[Discussion]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
		<category domain="post_tag" nicename="neural-networks"><![CDATA[neural networks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561917902;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3615;}i:1;a:1:{s:2:"id";i:3942;}i:2;a:1:{s:2:"id";i:4170;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Locale parser with fparsec</title>
		<link>https://onoffswitch.net/2013/07/07/locale-parser-fparsec/</link>
		<pubDate>Sun, 07 Jul 2013 08:00:22 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4068</guid>
		<description></description>
		<content:encoded><![CDATA[Localizing an application consists of extracting out user directed text and managing it outside of hardcoded strings in your code. This lets you tweak strings without having to recompile, and if done properly, allows you to support multiple languages. Localizing is no easy task, it messes up spacing, formatting, name/date other cultural information, but thats a separate issue. The crux of localizing is text.  

But, who just uses bare text to display things to the user? Usually you want to have text be a little dynamic.  Something like 

[code]
Hello {user}! Welcome!
[/code]

Here, <code>user</code> will be some sort of dynamic property.  To support this, your locale files need a way to handle arguments.  

One way of storing contents in a locale file is like this:

[code]
ExampleText = Some Text {argName:argType} other text etc
            = This is on a seperate newline
UserLoginText = ... 
[/code]

This consists of an identifier, followed by an equals sign, followed by some text with arguments of the form {x:y}.  To make a new line you have a new line with an equals sign and you continue your text.  When you reach a string with an identifier, you have a new locale element.

But you can also have comments, like 

[code]
# this is a comment, ignore me!
[/code]

And to throw a monkey wrench in the problem, you can also have arguments with no types, of the form <code>{argName}</code>.

The end goal, is to be able to reference your locale contents in code, something like

[code]
Locale.ExampleText (&quot;foo&quot;);
[/code]

But to get to the point where you can reference this you need to translate your locale files into something workable, kind of like a syntax tree.  If you have a working syntax tree of your locale files you can generate strongly typed locale code for you to use in your application.

<h2>The data</h2>

To parse a locale file of this format I used fparsec. One reason was that it already handles lookaheads and backtracking, and another reason is that I wanted to play with it :)

Going with a data first design, I thought about what I wanted to my final output to be and came up with 3 discriminated unions that look like this:

[fsharp]
type Arg = 
    | WithType of string * string
    | NoType of string

type LocaleContents = 
    | Argument of Arg
    | Text of string
    | Line of LocaleContents list
    | Comment of string
    | NewLine

type Locale =
    | Entry of string * LocaleContents
    | IgnoreEntry of LocaleContents
[/fsharp]


<h2>Utilities</h2>

The next step was to build out some common utilities that I can use. I knew I'd need to be able to parse a phrase, a single word, and know when things are between brackets:

[fsharp]
(*
    Utilities
*)

let brackets = isAnyOf ['{';'}']
    
 (* non new line space *)  
let regSpace = manySatisfy (isAnyOf [' ';'\t'])
  
(* any string literal that is charcaters *)
let phrase = many1Chars (satisfy (isNoneOf ['{';'\n']))
  
let singleWord = many1Chars (satisfy isDigit &lt;|&gt; satisfy isLetter &lt;|&gt; satisfy (isAnyOf ['_';'-']))

(* utility method to set between parsers space agnostic *)
let between x y p = pstring x &gt;&gt;. regSpace &gt;&gt;. p .&gt;&gt; regSpace .&gt;&gt; pstring y
[/fsharp]

Fparsec comes with a lot of great functions and parser combinators to create robust parsers. The idea is to combine parser functions from smaller parsers into larger parsers.  I liked working with it because it felt like dealing directly with a grammar.

<h2>Arguments</h2>

Now that I was able to parse words, phrases, and I could seperate out newlines from spaces, lets tackle an argument:

[fsharp]
(*
    Arguments of {a:b} or {a}
*)

let argDelim = pstring &quot;:&quot;

let argumentNoType = singleWord |&gt;&gt; NoType

let argumentWithType = singleWord .&gt;&gt;.? (argDelim &gt;&gt;. singleWord) |&gt;&gt; WithType

let arg = (argumentWithType &lt;|&gt; argumentNoType) |&gt; between &quot;{&quot; &quot;}&quot; |&gt;&gt; Argument
[/fsharp]

The <code>.>>.?</code> combinator says to apply both combinators results as a tuple, but if it fails to backtrack to the state of the previous parser.  Also, the <code><|></code> combinator lets you apply parsers as alternatives, so either of the parsers can be applied.

<h2>Text elements</h2>

Next up is text elements. This is the contents after the = of the identifier, but not including arguments.  For example, if our locale entry is

[code]
UserLogin = Hey! Whats up?
          = new lineezzz
[/code]

We want to match on "Hey! Whats up?", followed by an explict newline, followed by "new lineeezz"

[fsharp]
(*
    Text Elements
*)

let textElement = phrase |&gt;&gt; Text

let newLine = (unicodeNewline &gt;&gt;? regSpace &gt;&gt;? pstring &quot;=&quot;) &gt;&gt;% NewLine

let line = many (regSpace &gt;&gt;? (arg &lt;|&gt; textElement &lt;|&gt; newLine)) |&gt;&gt; Line
[/fsharp]

Remembering that a phrase is any text except for a start bracket and a newline, we can parse all text up to an argument.  New lines are a new line, followed by some space (maybe), followed by an equal sign.  Since the newline doesn't contain any data we care about from the parser we can ignore the output and just assign the result to the union type <code>NewLine</code> using the <code>>>%</code> operator.  

But a line is an aggregation of new lines, arguments, and phrases, so we can use the fparsec <code>many</code> operator, along with the 3 alternatives (arguments, text elements, and new lines) to build out an actual line.  

<h2>An Entry</h2>

Since we have arguments, new lines, and text set up, we can finally put it all together.  What I need now is to match when we have an identifier ("UserLogin"), an equals sign, followed by a line.

[fsharp]
(*
    Entries
*)

let delim = regSpace &gt;&gt;. pstring &quot;=&quot; .&gt;&gt; regSpace

let identifier = regSpace &gt;&gt;. singleWord .&gt;&gt; delim

let localeElement = unicodeSpaces &gt;&gt;? (identifier .&gt;&gt;. line .&gt;&gt; skipRestOfLine true) |&gt;&gt; Entry
[/fsharp]

This gives you a tuple of identifier * line, representing your entire locale element.

<h2>Comments</h2>

But we also have to account for comments.  Thankfully those are pretty easy

[fsharp]
(*
    Comments
*)

let comment = pstring &quot;#&quot; &gt;&gt;. restOfLine false |&gt;&gt; Comment

let commentElement = unicodeSpaces &gt;&gt;? comment |&gt;&gt; IgnoreEntry
[/fsharp]

This says if you match a "#" then take the rest of the line (but leave the newline since other parsers will handle that).  We might as well maintain the comment information so we can pipe that result to the <code>IgnoreEntry</code> union type.

<h2>Running the parser</h2>

And now we just have to piece together comments, locale elements, and run the parser

[fsharp]
(*
    Full Locale
*)

let locale = many (commentElement &lt;|&gt; localeElement) .&gt;&gt; eof

let test input = match run locale input with
                    | Success(r,_,_) -&gt; r
                    | Failure(r,_,_) -&gt; 
                            Console.WriteLine r
                            raise (Error(r))
[/fsharp]

<h2>Example</h2>

Lets try it out. Here is my sample locale:

[code]
UserLogin = {user}! Whats up!
		  = You rock, thanks for logging

UserLogout = {firstName:string}, {lastName:string}...why you gotta go? We were just getting to know you 		 
[/code]

And running it in fsi

[fsharp]
&gt; test &quot;UserLogin = {user}! Whats up!
		  = You rock, thanks for logging

UserLogout = {firstName:string}, {lastName:string}...why you gotta go? We were just getting to know you 		 &quot;;;

val it : Locale list =
  [Entry (&quot;UserLogin&quot;, Line [Argument (NoType &quot;user&quot;); Text &quot;! Whats up!&quot;; NewLine; Text &quot;You rock, thanks for logging&quot;]);
   Entry (&quot;UserLogout&quot;, Line
        [Argument (WithType (&quot;firstName&quot;,&quot;string&quot;)); Text &quot;, &quot;;
         Argument (WithType (&quot;lastName&quot;,&quot;string&quot;));
         Text &quot;...why you gotta go? We were just getting to know you 		 &quot;])]
[/fsharp]

And now its easy to iterate and manipulate the data! 
 
<h2>Source</h2>

Full source available at <a href="https://github.com/devshorts/Locale-parser/blob/master/FParsecCombinators/Program.fs">my github</a>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4068</wp:post_id>
		<wp:post_date><![CDATA[2013-07-07 08:00:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-07 08:00:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[locale-parser-fparsec]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="fparsec"><![CDATA[fparsec]]></category>
		<category domain="post_tag" nicename="fsharp"><![CDATA[fsharp]]></category>
		<category domain="post_tag" nicename="language-implementation"><![CDATA[language implementation]]></category>
		<category domain="post_tag" nicename="parser"><![CDATA[parser]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554392642;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3723;}i:1;a:1:{s:2:"id";i:4131;}i:2;a:1:{s:2:"id";i:4077;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Reworking my language parser with fparsec</title>
		<link>https://onoffswitch.net/2013/07/15/reworking-language-parser-fparsec/</link>
		<pubDate>Mon, 15 Jul 2013 08:00:21 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4077</guid>
		<description></description>
		<content:encoded><![CDATA[Since I was playing with fparsec last week, I decided to redo (or mostly) the parser for my homebrew language that I've previously posted about.  Using fparsec made the parser surprisingly succinct and expressive.  In fact I was able to do most of this in an afternoon, which is impressive considering<a href="http://onoffswitch.net/a-handrolled-language-parser/" target="_blank" rel="noopener noreferrer"> my last C# attempt</a> took 2 weeks to hammer out.

As always, it starts with the data

[fsharp]
type Op = 
    | Plus
    | Minus
    | GreaterThan
    | LessThan
    | Mult
    | Divide
    | Carrot
       
type Ast =     
    | Statement of Ast    
    | Expression of Ex    
    | Function of string option * Argument list option * Ast
    | Scope of Ast list option
    | Class of Ex * Ast
    | Conditional of Ex * Ast * Ast option 
    | WhileLoop of Ex * Ast
    | ForLoop of Ast * Ex * Ex * Ast    
    | Call of string * Argument list option
    | Assign of Ex * Ex
and Ex =
    | Single of Ast
    | Full of Ex * Op * Ex
    | Float of float
    | Int of int
    | Literal of string
    | Variable of string
and Argument = 
    | Element of Ex
[/fsharp]

<h2>Operators</h2>

Parsing operators is trivial

[fsharp]
let plus = pstring &quot;+&quot; &gt;&gt;% Plus

let minus = pstring &quot;-&quot; &gt;&gt;% Minus

let divide = pstring &quot;/&quot; &gt;&gt;% Divide

let mult = pstring &quot;*&quot; &gt;&gt;% Mult

let carrot = pstring &quot;^&quot; &gt;&gt;% Carrot

let gt = pstring &quot;&gt;&quot; &gt;&gt;% GreaterThan

let lt = pstring  &quot;&lt;&quot; &gt;&gt;% LessThan

let op = spaces &gt;&gt;. choice[plus;minus;divide;mult;carrot;gt;lt]
[/fsharp]

<h2>Expressions</h2>

But what was great was parsing expressions. These were complicated because I had to avoid left recursion, and in my C# parser I had a lot of edge conditions and had to deal with special backtracking. It was a nightmare.  With FParsec you can create forward recursive parsers, basically you create a dummy variable that you use as the recursive parser. Later you populate a tied reference to it with what are the available recursive parser implementations.  

[fsharp]
// create a forward reference 
// the expr is what we'll use in our parser combinators
// the exprImpl we'll populate with all the recursive options later
let expr, exprImpl = createParserForwardedToRef()

let expression1 = spaces &gt;&gt;? choice[floatNum;intNum;literal;variable] 

let between a b p = pstring a &gt;&gt;. p .&gt;&gt; pstring b

let bracketExpression = expr |&gt; between &quot;(&quot; &quot;)&quot;

let lhExpression = choice[expression1; bracketExpression]

let expressionOperation = lhExpression                           &gt;&gt;=? fun operandL -&gt;
                          op                                     &gt;&gt;=? fun operator -&gt;
                          choice[expr;bracketExpression]         &gt;&gt;=? fun operandR -&gt;
                          preturn (operandL, operator, operandR) |&gt;&gt; Full 

do exprImpl := spaces &gt;&gt;. choice[attempt expressionOperation;
                                 attempt bracketExpression; 
                                 expression1]
[/fsharp]

<code>expression1</code> is a type of expression that is just a single element.  <code>expressionOperation</code> is an expression that has an operator inbetween two expressions.  To avoid left recursion, the left hand side of an expression is limited to either expressions of single elements, or expressions encapsulated in parenthesis.  Then the right hand side can be either a parenthesis expression, or a regular expression.   You'll notice that <code>expr</code> isn't actually defined yet when its used here on line 16. It's a placeholder for the recursive parser, which is tied to the mutable reference cell (exprImpl) that I populate after I've defined all the parsers. This lets you define a parser that can actually call itself recursively!  Neat.

The <code>>>=?</code> operator applies the result to the following function, but if it fails, backtracks to the beginning of that parsers state. 

<h2>Scope</h2>

I defined a scope as any valid statements between two curly brackets.

[fsharp]
let funcInners, funcInnersImpl = createParserForwardedToRef()

let scope = parse{
    do! spaces
    do! skipStringCI &quot;{&quot;
    do! spaces
    let! text = opt funcInners
    do! spaces
    do! skipStringCI &quot;}&quot;    
    do! spaces
    return Scope(text)
}
[/fsharp]

The forward parser here is going to be populated at the very end of my parser, since it will allow for any kind of statement like while loops, for loops, conditionals, assignment, etc.  All the scope parser cares about is that it got stuff between curly brackets.  Using this we can leverage it anywhere else that has curly brackets. Later on in the program I set this up:

[fsharp]
(* things that can be in functions *)

do funcInnersImpl := many1 (spaces &gt;&gt;? choice [scope; func; statement])
[/fsharp]

Which allows scopes, statements (delineated by semicolons), or other functions, to appear inside of a function or scope. So you can see how a scope and function can be recursive (by containing other scopes and functions inside of them).

Anyways, lets parse a function

[fsharp]
let innerArgs = sepEndBy1 (expr |&gt;&gt; Element) (pstring &quot;,&quot;)
let arguments = innerArgs |&gt; between &quot;(&quot; &quot;)&quot;

let func = parse {
    do! skipStringCI &quot;func&quot;
    do! spaces
    let! name = opt (many1Chars (satisfy isAsciiLetter))
    let! arguments = opt arguments
    do! spaces 
    do! skipStringCI &quot;-&gt;&quot;
    let! scope = scope
    return Function(name, arguments, scope)
}
[/fsharp]

<h2>Conditionals</h2>

Conditionals were fun, because you can have an if statement, an if/else, or an if/elseif, or an if/elseif/.../else combo.  In my previous C# parser I covered each type independently so I had a lot of extra overlap, but this time I wanted to see if I could create an aggregate parser combinator to handle all these scenarios for me in one.

[fsharp]
let conditionalParser, conditionalParserImpl = createParserForwardedToRef()

let ifBlock = parse{
    do! skipStringCI &quot;if&quot;
    let! condition = expr |&gt; between &quot;(&quot; &quot;)&quot;
    do! spaces
    let! onTrue = scope
    do! spaces

    let elseKeyword = skipStringCI &quot;else&quot; .&gt;&gt; spaces

    let elseParse = parse{
        do! elseKeyword
        let! onFalse = scope
        return (condition, onTrue, Some(onFalse)) |&gt; Conditional
    }

    let elseIfParse = parse{
        do! elseKeyword
        let! onFalse = conditionalParser
        return (condition, onTrue, Some(onFalse)) |&gt; Conditional
    }

    let noElseParse = parse{        
        return (condition, onTrue, None) |&gt; Conditional
    }

    let! result = choice[attempt elseIfParse;elseParse;noElseParse]
    return result
}
[/fsharp]

This time, I created a recursive parser that optionally removes an else statement and captures the scope, or removes the else element and calls back into the if parser, or just terminates (so an if with no else).  The final result is a 3 way alternative that the if block can evaluate.  

<h2>Loops</h2>

Here is a while loop

[fsharp]
let whileLoop = (pstring &quot;while&quot; &gt;&gt;. spaces) &gt;&gt;. (expr |&gt; between &quot;(&quot; &quot;)&quot;) &gt;&gt;= fun predicate -&gt;
                scope &gt;&gt;= fun body -&gt;
                preturn (WhileLoop(predicate, body))
[/fsharp]

And here is a for loop

[fsharp]
let assign = parse{
    let! ex = expr
    do! spaces
    do! skipStringCI &quot;=&quot;
    do! spaces
    let! assignEx = expr
    do! spaces    
    return (ex, assignEx) |&gt; Assign
}

let forLoop = 
    let startCondition = assign .&gt;&gt; pstring &quot;;&quot;
    let predicate = expr .&gt;&gt; pstring &quot;;&quot;
    let endCondition = expr 
    let forKeyword = pstring &quot;for&quot; .&gt;&gt; spaces

    let forItems = tuple3 startCondition predicate endCondition |&gt; between &quot;(&quot; &quot;)&quot;

    forKeyword &gt;&gt;. forItems .&gt;&gt;. scope &gt;&gt;= fun ((start, predicate, end), body) -&gt;
        preturn (start, predicate, end, body) |&gt;&gt; ForLoop
[/fsharp]

Which gives you a result like this

[code]
test &quot;for(x=1;y&lt;z;y+1){}&quot;;;
val it : Ast list =
  [ForLoop
     (Assign (Variable &quot;x&quot;,Float 1.0),
      Full (Variable &quot;y&quot;,LessThan,Variable &quot;z&quot;),
      Full (Variable &quot;y&quot;,Plus,Float 1.0),Scope null)]
[/code]

<h2>Conclusion</h2>

My fiance is probably pissed that I spent 4th of july working on parser combinators, but fparsec is just too fun not to.  I really can't wait for an opportunity to use this for some production code, since I'm extremely happy with fparsecs abilities and the experience of working in it.  

For the full parser check out this <a href="http://fssnip.net/iJ" target="_blank" rel="noopener noreferrer">fsharp snippet</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4077</wp:post_id>
		<wp:post_date><![CDATA[2013-07-15 08:00:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-15 08:00:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[reworking-language-parser-fparsec]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="abstract-syntax-trees"><![CDATA[abstract syntax trees]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="fparsec"><![CDATA[fparsec]]></category>
		<category domain="post_tag" nicename="fsharp"><![CDATA[fsharp]]></category>
		<category domain="post_tag" nicename="parsing"><![CDATA[parsing]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560221353;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:3016;}i:2;a:1:{s:2:"id";i:4365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>91</wp:comment_id>
			<wp:comment_author><![CDATA[Munir Husseini]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[m.husseini@gmx.de]]></wp:comment_author_email>
			<wp:comment_author_url>http://mhusseini.wordpress.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[84.63.117.55]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-04-03 22:50:08]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-04-03 22:50:08]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[A very helpful example. Thank your for the great post.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>SignalR on ios and a single domain</title>
		<link>https://onoffswitch.net/2013/07/22/signalr-ios-single-domain/</link>
		<pubDate>Mon, 22 Jul 2013 08:00:21 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4091</guid>
		<description></description>
		<content:encoded><![CDATA[Safari on ios has a limitation that you can only have one concurrent request to a particular domain at a time.  Normally this is fine, since once a request completes the next one that is queued up fires off. But what if you are using a realtime persistent connection library like signalR? In this case your one allowed connection is held up with the signalR request. If you're not on a mac or linux and you use windows 7 or earlier you can't use websockets so you're stuck using http.  Most suggestions involve buying a second domain, but sometimes thats not possible, especially if your application is a distributable web app that can run on client machines. You can't expect clients to have to buy a second domain just so your realtime push works.

A nice solution, posted about in <a href="https://github.com/SignalR/SignalR/issues/1406" target="_blank" rel="noopener noreferrer">this github tracker</a> following the issue is to configure your signalR's long poll mechanism to work in short bursts.  Instead of having a single persistent connection, you can tell the client to use the long poll transport (using typescript here)

[ts]
var signalRConfig:def.IHubConfiguration = commonUtils.MiscUtil.userIsIphone() ? 
    {                                                                           
        transport: &quot;longPolling&quot;                                                
    } : {};                                                                     
                                                                                
// we can't do signalR push with an iphone since                                
// iphones only allow for single connections, so having the                     
// forever frame interferes with other resource loading                         
$.connection.hub.start(signalRConfig, () =&gt; console.log(&quot;connected&quot;));          
[/ts]

And on the server, in Global.asax.cs, configure the long poll bursts

[csharp]
protected void Application_Start(object sender, EventArgs e)
{
    GlobalHost.Configuration.ConnectionTimeout = TimeSpan.FromMilliseconds(1000);
    LongPollingTransport.LongPollDelay = 5000;
    RouteTable.Routes.MapHubs();
}
[/csharp]

But when I did this, per the tracker suggestion, I saw that I was still getting bursts that lasted 5 seconds.  This was too long for me, since for 5 seconds you can't do anything else. If you are doing a lot of dynamic calls (making AJAX requests, or other service calls) then 5 seconds really holds up the application.

When in doubt, read the source.  I'm unfortunately using a really old version of signalR because I'm tied to that version due to how my application is distributed (and that signalR versions aren't backwards compatible), so this advice may not apply to later versions. But, I found what the issue was.  

SignalR uses a heartbeat to check when things are disconnected or timed out:

[csharp]
_timer = new Timer(Beat,
                   null,
                   _configurationManager.HeartBeatInterval,
                   _configurationManager.HeartBeatInterval);

//....

private void Beat(object state)
{
    //...
        foreach (var metadata in _connections.Values)
        {
            if (metadata.Connection.IsAlive)
            {
                CheckTimeoutAndKeepAlive(metadata);
            }
            else
            {
                // Check if we need to disconnect this connection
                CheckDisconnect(metadata);
            }
        }
    /...
}
[/csharp]

The issue here is that the heartbeat is initialized to 10 seconds

[csharp]
public DefaultConfigurationManager()
{
    ConnectionTimeout = TimeSpan.FromSeconds(110);
    DisconnectTimeout = TimeSpan.FromSeconds(20);
    HeartBeatInterval = TimeSpan.FromSeconds(10);
    KeepAlive = TimeSpan.FromSeconds(30);
}
[/csharp]

So, that means that no matter what you set the disconnect and connection timeouts to be, they can't be any more granular than 10 seconds.  If you remember from signal processing the <a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem" target="_blank" rel="noopener noreferrer">shannon-nyquist sampling theorum</a>

<img src="http://onoffswitch.net/wp-content/uploads/2013/07/953b4b6e51335f67619cad644c437858.png" width="85" height="27" class="alignnone" />

You know that your sampling frequency needs to be twice the target frequency.  Fancy words for sample more to get more granular

The final fix I needed to add was

[csharp highlight="4"]
protected void Application_Start(object sender, EventArgs e)
{
    GlobalHost.Configuration.ConnectionTimeout = TimeSpan.FromMilliseconds(1000);
    GlobalHost.Configuration.HeartBeatInterval = TimeSpan.FromSeconds(GlobalHost.Configuration.ConnectionTimeout.TotalSeconds/2);
    LongPollingTransport.LongPollDelay = 5000;
    RouteTable.Routes.MapHubs();
}
[/csharp]

Now that the heartbeat interval is twice as fast as the connection timeout, I properly get 1 second bursts followed by 5 seconds of down time:


<img src="http://onoffswitch.net/wp-content/uploads/2013/07/2013-07-09-11_27_08-Charles-3.png" alt="2013-07-09 11_27_08-Charles 3" width="620" height="161" class="aligncenter size-full wp-image-4092" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4091</wp:post_id>
		<wp:post_date><![CDATA[2013-07-22 08:00:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-22 08:00:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[signalr-ios-single-domain]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ios"><![CDATA[ios]]></category>
		<category domain="post_tag" nicename="realtime"><![CDATA[realtime]]></category>
		<category domain="post_tag" nicename="signalr"><![CDATA[SignalR]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1555086534;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3392;}i:1;a:1:{s:2:"id";i:289;}i:2;a:1:{s:2:"id";i:2365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tech talk:  CLR Memory Diagnostics</title>
		<link>https://onoffswitch.net/2013/07/18/tech-talk-clr-memory-diagnostics/</link>
		<pubDate>Thu, 18 Jul 2013 15:35:36 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4107</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk we discussed the recent release from Microsoft of <a href="http://blogs.msdn.com/b/dotnet/archive/2013/05/01/net-crash-dump-and-live-process-inspection.aspx" target="_blank" rel="noopener noreferrer">ClrMD</a> that lets you attach and debug processes using an exposed API. You used to be able to do this in WinDbg using the SOS plugin, but now they've wrapped SOS in a managed dll that you can use to inspect CLR process information. The nice thing about this is you can now automate debugging inspections.  It's now as easy as 

[csharp]
int pid = Process.GetProcessesByName(&quot;TestApplication&quot;)[0].Id;

using (DataTarget dataTarget = DataTarget.AttachToProcess(pid, 5000))
{
    string dacLocation = dataTarget.ClrVersions[0].TryGetDacLocation();
    ClrRuntime runtime = dataTarget.CreateRuntime(dacLocation);

    ClrHeap heap = runtime.GetHeap();

    foreach (ulong obj in heap.EnumerateObjects())
    {
         ClrType type = heap.GetObjectType(obj);
         ulong size = type.GetSize(obj);
         Console.WriteLine(&quot;{0,12:X} {1,8:n0} {2}&quot;, obj, size, type.Name);
    }
}
[/csharp]


ClrMD lets you take stack snapshots of running threads, iterate through all objects in the heap and get their values out, show all loaded modules and more.  If you combine it with <a href="http://scriptcs.net/" target="_blank" rel="noopener noreferrer">ScriptCS</a> you've got a really powerful debugging tool. What I liked about ClrMD is that you can use the same API to attach to running processes (in modes where you can pause the app while running, or run without pausing the attached app) as you can with process dumps.  

While it is nice to be able to inspect the heap and stacks, I found that it's not totally trivial to get object information. Since all your queries return pointers to values on the heap (unless its a primitive object), you need to recursively go through the entire object reference to find all the details.  If you have the source, symbols, and a crash dump I think it's easier to just toss it into visual studio to get this information. Still, if you don't have access to this and need to investigate or automate error detection in a low level fashion, then this is an amazing tool.

For more reading check out

<a href="https://nuget.org/packages/Microsoft.Diagnostics.Runtime" target="_blank" rel="noopener noreferrer">https://nuget.org/packages/Microsoft.Diagnostics.Runtime</a>

<a href="http://www.infoq.com/news/2013/06/microsoft-diagnostics-runtime" target="_blank" rel="noopener noreferrer">http://www.infoq.com/news/2013/06/microsoft-diagnostics-runtime</a>

<a href="http://www.piotrwalat.net/clr-diagnostics-with-clrmd-and-scriptcs-repl-scriptcs-clrdiagnostics/" target="_blank" rel="noopener noreferrer">http://www.piotrwalat.net/clr-diagnostics-with-clrmd-and-scriptcs-repl-scriptcs-clrdiagnostics/</a>

<a href="http://www.programmingtidbits.com/post/2013/05/09/CLR-Memory-Diagnostics-Released.aspx" target="_blank" rel="noopener noreferrer">http://www.programmingtidbits.com/post/2013/05/09/CLR-Memory-Diagnostics-Released.aspx</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4107</wp:post_id>
		<wp:post_date><![CDATA[2013-07-18 15:35:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-18 15:35:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-clr-memory-diagnostics]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="clr"><![CDATA[clr]]></category>
		<category domain="post_tag" nicename="debugging"><![CDATA[Debugging]]></category>
		<category domain="post_tag" nicename="heaps"><![CDATA[heaps]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559337772;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4596;}i:1;a:1:{s:2:"id";i:4463;}i:2;a:1:{s:2:"id";i:3497;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Strongly typing SignalR</title>
		<link>https://onoffswitch.net/2013/07/29/strongly-typing-signalr/</link>
		<pubDate>Mon, 29 Jul 2013 08:00:43 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4116</guid>
		<description></description>
		<content:encoded><![CDATA[I'm a big fan of strong typing.  If you can leverage the compiler to give you an error (or warning) before you deploy code, all the better. That means you won't, ideally, push a bug into the field.  So I have a big problem with frameworks and libraries that rely on dynamic objects, or even worse, stringly typing thing.  Don't get me wrong, sometimes dynamics are the only way to solve the problem, but whenever I run into one I'm always afraid that I'm going to get a runtime error since I don't <em>really</em> know what I'm acting on till later.

In this post, I'm going to discuss strongly typing signalR.  For the impatient, I have a <a href="http://strongsignalr.apphb.com/" target="_blank" rel="noopener noreferrer">working demo up</a>, as well as the code posted on my <a href="https://github.com/devshorts/StronglyTypedSignalr" target="_blank" rel="noopener noreferrer">github</a>.

That said, I've written about signalR before so I won't rehash that, but signalR uses dynamic objects heavily to give you the flexibility of "invoking" whatever method you want on the client side.  In my first forays using signalR I went with this iconic chat example:

[csharp]
public void Send(string message)
{
    // Call the addMessage method on all clients            
	Clients.All.addMessage(message);
}
[/csharp]

<code>Clients.All</code> is a dynamic object, and <code>addMessage</code> is going to be a registered handler in the javascript side.   Because it's dynamic, a small typo can cause your client side invocation to never succeed. You won't get an error, just nothing will happen. That's almost even worse than getting an exception! 

But, if we know a little about the signalR internals (which we can since signalR is open source), we can solve all these issues with almost no extra code.  

First, lets start with defining what we want to do:

[csharp]
public interface IJsMethods
{
    void PrintString(string msg);
}
[/csharp]

We'll say that "PrintString" is an available javascript method to call and it has some specific arguments to use. Inside of our signalR hub, the goal is going to be to be able to do this:

[csharp]
AllClients.PrintString(&quot;Everyone gets the time! &quot; + DateTime.Now.ToString())
[/csharp]

Which should invoke a <code>printString</code> method in javascript with a string parameter.

If we change the interface later, we should get compile time errors and we can be confident that we'll be invoking the right things on the client.  

Back to knowing a little about the signalR internals.  If you inspect the type of <code>Clients.All</code> (or look at the signalR source), you'll see that it actually resolves at runtime to be of type <code>ClientProxy</code> which implements <code>IClientProxy</code>.  This makes our lives pretty easy, since we can write an interceptor for <code>IClientProxy</code> and do the invocation of the client side javascript for us.

[csharp]
public static class HubExtensions
{
    private static readonly ProxyGenerator Generator = new ProxyGenerator();

    public static T AsStrongHub&lt;T&gt;(this IClientProxy source)
    {
        return (T)Generator.CreateInterfaceProxyWithoutTarget(typeof(T), new StrongClientProxy(source));
    }
}

public class StrongClientProxy : IInterceptor
{
    public IClientProxy Source { get; set; }

    public StrongClientProxy(IClientProxy source)
    {
        Source = source;
    }

    public void Intercept(IInvocation invocation)
    {
        var methodName = StringUtil.FirstLower(invocation.Method.Name);

        Source.Invoke(methodName, invocation.Arguments);
    }
}
[/csharp]

And we can call this from our hub using:

[csharp]
private IJsMethods AllClients
{
    get { return (Clients.All as ClientProxy).AsStrongHub&lt;IJsMethods&gt;(); }
}
[/csharp]

The interceptor will take the name of the interface defined method that is being acted on, make the first letter lowercase, and pass in the arguments to the client proxy source reference that it contains.  When you do a <code>Clients.All.foo()</code> signalR does the exact same thing inside at runtime, we're just moving this to be wrapped by the dynamic proxy.

If you want to act on a specific client, the type is slightly different but it also implements <code>IClientProxy</code>:

[csharp]
private IJsMethods CurrentClient
{
     get
     {
         return (Clients.Client(Context.ConnectionId) as ConnectionIdProxy).AsStrongHub&lt;IJsMethods&gt;();
     }
} 
[/csharp]

<h2>Conclusion</h2>

While this doesn't touch the client side of things, you can easily fix that problem. Imagine tagging the interface with an attribute and auto generating signalR javascript client side wireups.  Now you can manage all your sends and receives in one place, have them be strongly typed, and set yourself up for robust and safe code generation of boring boilerplate! 

Like mentioned above, a full working project is available on my <a href="https://github.com/devshorts/StronglyTypedSignalr" target="_blank" rel="noopener noreferrer">github</a> and you can see a running example at <a href="http://strongsignalr.apphb.com/" target="_blank" rel="noopener noreferrer">appharbour</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4116</wp:post_id>
		<wp:post_date><![CDATA[2013-07-29 08:00:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-29 08:00:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[strongly-typing-signalr]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dynamic-proxy"><![CDATA[dynamic proxy]]></category>
		<category domain="post_tag" nicename="signalr"><![CDATA[SignalR]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560802337;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3392;}i:1;a:1:{s:2:"id";i:289;}i:2;a:1:{s:2:"id";i:4091;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>92</wp:comment_id>
			<wp:comment_author><![CDATA[SignalR + WCF + SOLID: Ways an SOA WCF Notification Service can use SignalR | .NET, Silverlight, and Prism]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://dotnetsilverlightprism.wordpress.com/2013/11/11/signalr-wcf-solid-ways-an-soa-wcf-notification-service-can-use-signalr/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.9.105]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-11-11 16:30:49]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-11-11 16:30:49]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] result in silently failing broken code, plus other ideas to fix this problem.  See his blog at http://onoffswitch.net/strongly-typing-signalr/  for more info and code [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>93</wp:comment_id>
			<wp:comment_author><![CDATA[Stephan Ryer]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[stephanryer@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.inmobile.dk</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[87.104.183.195]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-12-20 18:00:21]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-12-20 18:00:21]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thankyou for sharing this - as a big fan of strong types, I think this is awesome!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>94</wp:comment_id>
			<wp:comment_author><![CDATA[Eddy]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[ekennway8@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[212.227.35.68]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-09-16 08:55:58]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-09-16 08:55:58]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great post, but ii've found a much simpler solution which supports also strongly typed server hub invocation.

Take a look at https://github.com/Gandalis/SignalR.Client.TypedHubProxy or https://www.nuget.org/packages/SignalR.Client.TypedHubProxy]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>93</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>F# and Machine learning Meetup in DC</title>
		<link>https://onoffswitch.net/2013/07/25/f-machine-learning-meetup-dc/</link>
		<pubDate>Thu, 25 Jul 2013 19:42:29 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4126</guid>
		<description></description>
		<content:encoded><![CDATA[As you may have figured out, I like F# and I like functional languages.  At some point I tweeted to the f# community lamenting that there was a dearth of F# meetups in the DC area.  Lo and behold, tons of people replied saying they'd be interested in forming one, and some notable speakers piped up and said they'd come and speak if I set something up.

So, If any of my readers live in the DC metro area, I'm organizing an F# meetup featuring <a href="http://clear-lines.com/blog/" target="_blank" rel="noopener noreferrer">Mathias Brandewinder</a>. We'll be doing a hands on F# and machine learning coding dojo which should be a whole buttload of fun.  Here's the official blurb:


<blockquote>Machine Learning is the art of writing programs that get better at performing a task as they gain experience, without being explicitly programmed to do so. Feed your program more data, and it will get smarter at handling new situations.

Some machine learning algorithms use fairly advanced math, but simple approaches can be surprisingly effective. In this Session, we'll take a classic Machine Learning challenge from Kaggle.com, automatically recognizing hand-written digits (http://www.kaggle.com/c/digit-recognizer), and build a classifier, from scratch, using F#. So bring your laptop, and let's see how smart we can make our machines!

This session will be organized as an interactive workshop. Come over, and learn yourself a Machine Learning and F# for great good! No prior experience with Machine Learning required, and F# beginners are very welcome - it will be a great opportunity to see F# in action, and why it's awesome.

To get the most from the session please try and bring a laptop along with F# installed (ideally either MonoDevelop or Visual Studio Web Express/Full Edition).

Mathias Brandewinder has been writing software in C# for 7+ years, and loving every minute of it, except maybe for a few release days. He is an F# MVP, enjoys arguing about code and how to make it better, and gets very excited when discussing TDD or F#. His other professional interests are applied math and probability. If you want to know more about him, you can check out his blog at www.clear-lines.com/blog or find him on Twitter as @Brandewinder.</blockquote>

For more info go RSVP at <a href="http://www.meetup.com/F-meetup-in-Dupont-Circle/events/131370292/" target="_blank" rel="noopener noreferrer">meetup.com</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4126</wp:post_id>
		<wp:post_date><![CDATA[2013-07-25 19:42:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-07-25 19:42:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[f-machine-learning-meetup-dc]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dc"><![CDATA[DC]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
		<category domain="post_tag" nicename="meetup"><![CDATA[meetup]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559969648;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4209;}i:1;a:1:{s:2:"id";i:4170;}i:2;a:1:{s:2:"id";i:4275;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Parse whatever with your own parser combinator</title>
		<link>https://onoffswitch.net/2013/08/19/parsing-csvs-parser-combinator/</link>
		<pubDate>Mon, 19 Aug 2013 08:00:32 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4131</guid>
		<description></description>
		<content:encoded><![CDATA[In a few recent posts I talked about playing with fparsec to parse data into usable syntax trees.  But, even after all the time spent fiddling with it, I really didn't fully understand how combinators actually worked. With that in mind, I decided to build a version of fparsec from scratch. What better way to understand something than to build it yourself?  I had one personal stipulation, and that was to not look at the fparsec source.  To be fair, I cheated with one function (the very first one) so I kind of cheated a lot, but I didn't peek at anything else, promise.

<h2>Combinators</h2>

The principle behind combinators is that they are a way to take two functions and combine them into another function.  Functional programming is chock full of this pattern. In general, you can combine any function to get any other function, but what makes a combinator powerful is when you combine a function, with another function, and get the same signature as the first function.  Now you can recursively combine functions together.  

For example, lets say I have a function defined like this:

[fsharp]
let parser = state -&gt; result * state
[/fsharp]

So it takes some input state, and gives you some sort of result along with a new state.

To make this useful though, I need to get the result and do something else with it.  So, lets say I have another function that takes a result, and gives you a function that takes a new state and returns a new result. 

[fsharp]
let applier = result -&gt; (state -&gt; result * state)
[/fsharp]

Is it possible to combine these two somehow? Sure:

[fsharp]
let combiner parser applier =
   fun state -&gt;
     match parser state with
      | (Some(result), newState) -&gt; 
             let nextParser = applier result
             nextParser newState
      | (None, newState) -&gt; (None, newState)
[/fsharp]

What's the function signature of this combiner function?

[fsharp]
(state -&gt; result * state) -&gt; (result -&gt; (state -&gt; result * state)) -&gt; (state -&gt; result * state)
[/fsharp]

That's kind of a mouthful, so lets add a type alias:

[fsharp]
type Parser = state -&gt; result * state
[/fsharp]

Now what is the type signature?

[fsharp]
Parser -&gt; (result -&gt; Parser) -&gt; Parser
[/fsharp]

That's a lot better.

We've just defined a way to take some function that takes a state and returns a result, combine it with something that takes a result and returns a new function, and it gives you a NEW parser.  So you've combined the two things together to create the same kind of thing as the first thing!

What's neat about this is you can use this basic <code>combiner</code> function to build up parsers that do small work.

<h2>Define a simple parser</h2>

Building a parser function is easy, it can be anything. Remembering the signature:

[fsharp]
state -&gt; result * state
[/fsharp]

Here is something that parsers a single character from a string:

[fsharp]
let oneCharParser =
       fun (state:string) -&gt;
          let firstChar = state.Chars(0)
          let remainingState = state.Substring(0, state.Length - 1)
          (Some(firstChar), remainingState)
[/fsharp]

You can create more complex parsers too, maybe one that takes a regular expression or matches on a specific string.  Anything you want.

<h2>Building on the combiner</h2>

Now that there is a combiner, and a way to define a parser, lets build on that. First lets alias the <code>combiner</code> function I first wrote out to make it a little easier to use:

[fsharp]
let (&gt;&gt;=) current next = combiner current next
[/fsharp]

This operator, mimics the syntax from fparsec (and haskells parsec).  Next, lets make a combinator function that takes two parsers, and returns the result of the second parser (ignoring the result of the first):

[fsharp]
let (&gt;&gt;.)  parser1 parser2  =
    parser1 &gt;&gt;= fun firstResult -&gt; 
    parser2 &gt;&gt;= fun secondResult -&gt; 
         secondResult 
[/fsharp]

But wait, that won't really work.  Remember that the combiners second argument wants a function that takes a result and returns a parser (which is a function that takes a state and returns a result) [i.e. of the signature <code>result -> (state -> result * state)</code>.

If you look closely, we aren't returning a parser at the end of this, we are just returning a value (i.e. just <code>result</code>).  We need some sort of way to return a value as a parser.  Hmm, fparsec has this and its called <code>preturn</code>. Let's add this too:

[fsharp]
let preturn value = fun state -&gt; (Some(value), state)
[/fsharp]

Not so bad. Now lets tie it in:

[fsharp]
let (&gt;&gt;.)  parser1 parser2  = 
        parser1 &gt;&gt;= fun firstResult -&gt; 
        parser2 &gt;&gt;= fun secondResult -&gt; 
        preturn secondResult 
[/fsharp]

Awesome, now the magic that is the F# type inference system is happy!

But, we can do so much more.  If all we need to do is create custom operators that leverage the combiner function we can create functions that:

<ul>
<li>Takes two parsers and returns the first (<code>.>></code>)</li>
<li>Takes two parsers and returns the second (<code>>>.</code>)</li>
<li>Takes two parsers and returns a tuple of the result (<code>.>>.</code>)</li>
<li>Takes a parser and preturns its value into a parameterized discriminated union type (<code>|>></code>)</li>
<li>Takes a parser and preturns its value into a non parameterized discriminated union type (<code>|>>%</code>)</li>
</ul>

<h2>State</h2>

In FParsec and other combinators, the state is a character stream. But when building out my own parsec clone I saw no reason that the state had to be tied to a specific type.  Parser states share a few things in common:

<ul>
<li>Consume and return a consumed value</li>
<li>Backtrack to a position</li>
<li>Test if the state contains a predicate</li>
<li>Know if they are empty</li>
</ul>

When building my parser I kept this in mind and made the combinator library work on a general state interface that I called <code>IStreamP</code>

[fsharp]
type IStreamP&lt;'StateType, 'ConsumeType&gt; =        
    abstract member state : 'StateType
    abstract member consume : int -&gt; 'ConsumeType option * IStreamP&lt;'StateType, 'ConsumeType&gt;
    abstract member backtrack : unit -&gt; unit
    abstract member hasMore : unit -&gt; bool
[/fsharp]

Using this interface I was able to implement a string parser state, as well as a binary parser state.  Both states can be reused with all the combinator functions which is part of what makes parser combinators so robust.  You get to mix language functionality with the grammar you are parsing.

<h2>An example</h2>

Now that I have all the basic building blocks, lets try parsing a CSV. The bulk of the work is being able to parse a string.  But to parse a string, we have to see if the state matches something.  So lets start with that. The combinator I wrote has a generic match function that you inject a predicate to:

[fsharp]
let matcher eval target =         
    fun currentState -&gt; 
        match eval currentState target with
            | Some(amount) -&gt; currentState.consume amount                    
            | None         -&gt; (None, currentState)
[/fsharp]

The signature of the eval function is:

[fsharp]
state -&gt; 'a -&gt; int
[/fsharp]

So an evaluator function takes the current state, and some sort of target (maybe you are trying to match on a specific string) and if the predicate returns some integer amount, the state consumes the amount the predicate told it to take.  A simple way of doing this is to see if the beginning of the string matches what you want to take:

[fsharp]
type ParseState = State&lt;string, string&gt;

let private getStringStream (state:ParseState) = (state :?&gt; StringStreamP)

let private startsWith (input:ParseState) target = (input |&gt; getStringStream).startsWith input target

let matchStr str = matcher startsWith str
[/fsharp]

Basically its just getting a starts with expression match function from the state class. The idea here is that each state class can contain its own predicates to match on, so you don't have to mix stuff between a binary state parser and a string state parser.  

And just to show what the startsWith function looks like:

[fsharp]
member x.startsWith (inputStream:IStreamP&lt;string, string&gt;) target =
      if String.IsNullOrEmpty inputStream.state then None
      else if inputStream.state.StartsWith target then 
          Some target.Length
      else None

[/fsharp]

Building on this we can create matches that match using regex, or do other work.  Taking kind of a leap of faith here, let me show the finished CSV parser (full source is on my <a href="https://github.com/devshorts/ParsecClone" target="_blank" rel="noopener noreferrer">github</a>)

[fsharp]
let delimType = &quot;,&quot;

let(|DelimMatch|EscapedType|Other|) i = 
    if i = &quot;\\&quot; || i =&quot;\&quot;&quot; then EscapedType
    else if i = delimType then DelimMatch
    else Other

let delim&lt;'a&gt; = matchStr delimType

let quote  = matchStr &quot;\&quot;&quot;

let validNormalChars = function
                        | EscapedType                                
                        | DelimMatch -&gt; false
                        | rest -&gt; not (isNewLine rest)

let inQuotesChars  = function                                 
                        | &quot;\&quot;&quot; -&gt; false
                        | _ -&gt; true

let unescape = function
                    | &quot;n&quot; -&gt; &quot;\n&quot;
                    | &quot;r&quot; -&gt; &quot;\r&quot;
                    | &quot;t&quot; -&gt; &quot;\t&quot;                     
                    | c   -&gt; c

let quoteStrings = (many (satisfy (inQuotesChars) any)) &gt;&gt;= foldChars

let escapedChar&lt;'a&gt; = matchStr &quot;\\&quot; &gt;&gt;. (anyOf matchStr [delimType; &quot;\&quot;&quot;;&quot;n&quot;;&quot;r&quot;;&quot;t&quot;] |&gt;&gt; unescape)
    
let normal&lt;'a&gt; = satisfy validNormalChars any 

let normalAndEscaped = many (normal &lt;|&gt; escapedChar) &gt;&gt;= foldChars

let literal&lt;'a&gt; = between quote quoteStrings quote

let csvElement = ws &gt;&gt;. (literal &lt;|&gt; normalAndEscaped)

let listItem&lt;'a&gt; = delim &gt;&gt;. opt csvElement

let elements&lt;'a&gt; = csvElement .&lt;?&gt;&gt;. many listItem

let lines&lt;'a&gt; = many (elements |&gt; sepBy &lt;| newline) .&gt;&gt; eof
[/fsharp]

It should look very similiar to fparsec, but slightly different. For example, the [code].&lt;?&gt;&gt;.[/code] operator takes an item parser, and an item list parser, and optionally applies both the item and the list. If the list returns any results it preturns the first item with the item list, otherwise just returns the first item.  

<h2>Another example</h2>

Just to demonstrate the power of decoupling the combinator logic from the state/stream logic, lets use the same combinator functions on a binary stream:

If we implement a new binary state stream, it might look like this:

[fsharp]
type BinStream (state:Stream) =     
    let startPos = state.Position
    
    interface IStreamP&lt;Stream, byte[]&gt;  with       
        member x.state = state     

        member x.consume (count) = 
            let mutable bytes = Array.init count (fun i -&gt; byte(0))
            state.Read(bytes, 0, count) |&gt; ignore
            
            (Some(bytes), new BinStream(state) :&gt; IStreamP&lt;Stream, byte[]&gt; )

        member x.backtrack () = state.Seek(startPos, SeekOrigin.Begin) |&gt; ignore

        member x.hasMore () = state.Position &lt;&gt; state.Length
            
        
    member x.streamCanBeConsumed (state:IStreamP&lt;Stream, byte[]&gt; ) count =                 
        if (int)state.state.Position + (int)count &lt;= (int)state.state.Length then
            Some(count)
        else 
            None
[/fsharp]

And we can define a whole bunch of basic parsers to work with this stream:

[fsharp]
module BinParser = 
    
    let private byteToInt (b:byte) = System.Convert.ToInt32(b)
    let private toInt16 v = System.BitConverter.ToInt16(v, 0)
    let private toInt32 v = System.BitConverter.ToInt32(v, 0)
    let private toInt64 v = System.BitConverter.ToInt64(v, 0)
    
    type ParseState = State&lt;Stream, byte[]&gt;
    
    let private getBinStream (state:ParseState) = (state :?&gt; BinStream)

    let private streamCanBeConsumed (state:ParseState) count  = (state |&gt; getBinStream).streamCanBeConsumed state count
    
    let private binMatch (num:int) = matcher streamCanBeConsumed num        

    let byteN&lt;'a&gt; = binMatch 

    let byte1&lt;'a&gt; = byteN 1 &gt;&gt;= fun b1 -&gt; preturn b1.[0]  

    let byte2&lt;'a&gt; = byteN 2  
    
    let byte3&lt;'a&gt; = byteN 3

    let byte4&lt;'a&gt; = byteN 4

    let int16&lt;'a&gt; = byte2 |&gt;&gt; toInt16
    
    let int32&lt;'a&gt; = byte4 |&gt;&gt; toInt32

    let int64&lt;'a&gt; = byteN 8 |&gt;&gt; toInt64

    let intB&lt;'a&gt; = byte1 |&gt;&gt; byteToInt
[/fsharp]

And here is a unit test to show how it might be used:

[fsharp]
[&lt;Test&gt;]
let ``test reading two sets of 4 bytes``() = 
    let bytes = [|0;1;2;3;4;5;6;7;8|] |&gt; Array.map byte

    let stream = new MemoryStream(bytes)   

    let binaryStream = new BinStream(stream) 

    let parser = manyN 2 byte4

    let result = test binaryStream parser 
    
    result |&gt; should equal [[|0;1;2;3|];[|4;5;6;7|]]
[/fsharp]

<h2>Conclusion</h2>

What I like about combinators is that you build on the smallest blocks.  And, unlike parser generators, you can mix language constructs with your grammar.  Unfortunately debugging combinators is extremely difficult, since each combinator is a function that is composed of other functions.  When you build a complex grammar up from those blocks, its easy to get lost in which function you are in and where you came from.  The up side is that you can easily test against each building block independently.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4131</wp:post_id>
		<wp:post_date><![CDATA[2013-08-19 08:00:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-19 08:00:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[parsing-csvs-parser-combinator]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ast"><![CDATA[ast]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="combinators"><![CDATA[combinators]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="fparsec"><![CDATA[fparsec]]></category>
		<category domain="post_tag" nicename="parsing"><![CDATA[parsing]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558681316;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4068;}i:1;a:1:{s:2:"id";i:3723;}i:2;a:1:{s:2:"id";i:4213;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>125</wp:comment_id>
			<wp:comment_author><![CDATA[ParsecClone on nuget | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/parsecclone-nuget/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-25 16:10:42]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-25 16:10:42]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] I published the first version of ParsecClone to nuget. I blogged recently about creating my own parser combinator and it&#8217;s come along pretty well. While [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>F# class getter fun</title>
		<link>https://onoffswitch.net/2013/08/14/f-class-getter-fun/</link>
		<pubDate>Wed, 14 Aug 2013 16:21:49 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4167</guid>
		<description></description>
		<content:encoded><![CDATA[I was playing with Neo4J (following a recent post I stumbled upon by <a href="http://sergeytihon.wordpress.com/2013/03/27/using-neo4j-graph-db-with-f/" target="_blank" rel="noopener noreferrer">Sergey Tihon</a>), and had everything wired up and ready to test out, but when I tried running my code I kept getting errors saying that I hadn't connected to the neo4j database. This puzzled me because I had clearly called connect, but every time I tried to access my connection object I got an error.

The issue was that I didn't realize that f# class members are always deferred. It makes sense that they are after I traced through it, but I couldn't spot the bug for the life of me at first.

My code looked like this:

[fsharp]
module Connection = 
    type Connection (dbUrl) = 

        member x.client = new GraphClient(new Uri(dbUrl))

        member x.create item = x.client.Create item

        member x.connect() = 
            x.client.Connect()
            x
[/fsharp]

If I had more experience with F# I probably would have spotted this right away, but it took me a while to figure out what was going on. The issue here is 

[fsharp highlight="4"]
module Connection = 
    type Connection (dbUrl) = 

        member x.client = new GraphClient(new Uri(dbUrl))

        member x.create item = x.client.Create item

        member x.connect() = 
            x.client.Connect()
            x
[/fsharp]

Which compiles into

[csharp highlight="15"]
  [AutoOpen]
  [CompilationMapping(SourceConstructFlags.Module)]
  public static class Connection
  {
    [CompilationMapping(SourceConstructFlags.ObjectType)]
    [Serializable]
    public class Connection
    {
      internal string dbUrl;

      public GraphClient client
      {
        get
        {
          return new GraphClient(new Uri(this.dbUrl));
        }
      }

      public Connection(string dbUrl)
      {
        Connection.Connection connection = this;
        this.dbUrl = dbUrl;
      }

      public NodeReference&lt;a&gt; create&lt;a&gt;(a item) where a : class
      {
        return GraphClientExtensions.Create&lt;a&gt;((IGraphClient) this.client, item, new IRelationshipAllowingParticipantNode&lt;a&gt;[0]);
      }

      public Connection.Connection connect()
      {
        this.client.Connect();
        return this;
      }
    }
  }
[/csharp]

Clear as day now. Each time you call the property it returns a new instance. I had assumed that since the member wasn't a function that it would be a property, not an auto wrapped getter.  

The fix was easy:

[fsharp]
module Connection = 
    type Connection (dbUrl) = 

        let graphConnection = new GraphClient(new Uri(dbUrl))

        member x.client = graphConnection

        member x.create item = x.client.Create item

        member x.connect() = 
            x.client.Connect()
            x
[/fsharp]

Which now generates

[csharp]
  [AutoOpen]
  [CompilationMapping(SourceConstructFlags.Module)]
  public static class Connection
  {
    [CompilationMapping(SourceConstructFlags.ObjectType)]
    [Serializable]
    public class Connection
    {
      internal GraphClient graphConnection;

      public GraphClient client
      {
        get
        {
          return this.graphConnection;
        }
      }

      public Connection(string dbUrl)
      {
        Connection.Connection connection = this;
        this.graphConnection = new GraphClient(new Uri(dbUrl));
      }

      public NodeReference&lt;a&gt; create&lt;a&gt;(a item) where a : class
      {
        return GraphClientExtensions.Create&lt;a&gt;((IGraphClient) this.client, item, new IRelationshipAllowingParticipantNode&lt;a&gt;[0]);
      }

      public Connection.Connection connect()
      {
        this.client.Connect();
        return this;
      }
    }
  }
[/csharp]

That's more like it

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4167</wp:post_id>
		<wp:post_date><![CDATA[2013-08-14 16:21:49]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-14 16:21:49]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[f-class-getter-fun]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="post_tag" nicename="classes"><![CDATA[classes]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="initialization"><![CDATA[initialization]]></category>
		<category domain="post_tag" nicename="neo4j"><![CDATA[neo4j]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558960558;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4463;}i:1;a:1:{s:2:"id";i:4244;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>95</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #33 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/08/19/f-weekly-33-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[76.74.248.177]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-18 21:16:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-18 21:16:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp wrote &#8220;F# class getter fun&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>96</wp:comment_id>
			<wp:comment_author><![CDATA[Trixie]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[p8g86v1e@mail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://ayngcbwjlrp.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[188.143.234.155]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-07-19 11:25:45]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-07-19 11:25:45]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I was really confused, and this answered all my quostiens.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Coding Dojo: a gentle introduction to Machine Learning with F# review</title>
		<link>https://onoffswitch.net/2013/08/19/coding-dojo-gentle-introduction-machine-learning-f-review/</link>
		<pubDate>Mon, 19 Aug 2013 00:56:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4170</guid>
		<description></description>
		<content:encoded><![CDATA[Recently I organized an <a href="http://www.meetup.com/F-meetup-in-Dupont-Circle/" target="_blank" rel="noopener noreferrer">F# meetup in DC</a>, and for our first event we brought in a wonderful speaker (Mathias Brandewinder) who's topic was called: "<em>Coding Dojo: a gentle introduction to Machine Learning with F#</em>".  

I was certainly a little nervous about our first meetup, but a ton of great people came out: from experienced F# users, to people who had used other functional languages (like OCaml), to people with no functional experience.  The goal of the meetup was to write a k-nearest neighbors classifier for a previously posted <a href="http://www.kaggle.com/c/digit-recognizer/data" target="_blank" rel="noopener noreferrer">kaggle</a> exercise to classify pixellated numbers. 

[caption width="600" align="alignnone"]<img src="https://pbs.twimg.com/media/BR1-jWLCUAEO6E3.jpg" width="600" height="450" class /> Mathias introducing F#[/caption]

Mathias did a great job of breaking people up into groups and then explaining what is machine learning and the criteria of the project in a surprsingly short time period. I think people were a little scared of jumping in since he only talked for about 10 to 15 minutes, but in place of a long lecture Mathias had a really well put together guided document that encouraged users to play and interact with F#.  

The first step was to create an F# project and to download his fsx gist.  The gist was broken down into 7 steps where each step walked a user through the basics of F# and machine learning to build their classifier.  For example, one step was how to execute lines in F# interactive. Another step was explaining the map function.  Another step talked about how to read a file and parse a csv. And yet another discussed distance functions and converting raw data into records.  

[caption width="600" align="alignnone"]<img src="https://pbs.twimg.com/media/BR0tUFACcAAJRre.jpg" width="600" height="450" class /> The meetup group[/caption]

In the end, if you followed his steps, in a span of under 2 hours, even a novice could end up with a fully working classifier! The classifier's accuracy, by default, was about 94.4%. Not too bad.  

I wanted to share my version of his classifer which is based off of Mathias' well guided steps.

[fsharp]
open System
open System.IO
 
type Number = { Label: string; Pixels: int[] }

let splitLine (line:String) = line.Split([|','|])

let extract file = File.ReadAllLines file |&gt; Array.map splitLine

let strippedHeaders (arr:'a[]) = arr.[1..]

let convertToInt (str:string) = Convert.ToInt32 str

let lineToInt arr = Array.map convertToInt arr

let linesAsInts = Array.map lineToInt 

let toNum (line:int[]) = {Label = line.[0].ToString(); Pixels = line.[1..] }

let convertToNum lines = Array.map toNum lines

let dist (a:int) (b:int) = (a-b)*(a-b)

let arrayDist = Array.map2 dist
 
let totalDist a b = arrayDist a b |&gt; Array.reduce (+)

let train file = 
    extract file 
        |&gt; strippedHeaders
        |&gt; linesAsInts
        |&gt; convertToNum        

let kNNSet trainingSet pixels k =
    trainingSet 
        |&gt; Array.map (fun i -&gt; (i.Label, totalDist i.Pixels pixels)) 
        |&gt; Array.sortBy (fun (label, dist) -&gt; dist)
        |&gt; fun sorted -&gt; sorted.[0..(k - 1)]          
    
let classify trainingSet pixels k = 
    kNNSet trainingSet pixels k
        |&gt; Array.toSeq
        |&gt; Seq.groupBy (fun (label, dist) -&gt; label)
        |&gt; Seq.maxBy (fun (label, items) -&gt; Seq.length items)
        |&gt; fun (label, items) -&gt; label

let accuracy trainingSet validationSet k = 
    Array.map (fun i -&gt; 
        let result = classify trainingSet i.Pixels k
        result = i.Label) validationSet
        |&gt; Array.map(fun i -&gt; if i = true then 1 else 0)
        |&gt; Array.sum 
        |&gt; fun sum -&gt; (double)sum / (double)(Array.length validationSet)
        |&gt; fun acc -&gt; (int)(acc * 100.0)
    
let training = train @&quot;C:\Projects\Personal2\DcDojo\DcDojo\trainingsample.csv&quot;
let validation = train @&quot;C:\Projects\Personal2\DcDojo\DcDojo\validationsample.csv&quot;                       
[/fsharp]

Had I written this without following his steps I probably would have inlined a lot of the simple helper functions, but I wanted to show how Mathias really brought the "<em>start small, build big</em>" mentality to the project.  This is something that really works well in functional languages and I think all the meetup participants picked up on that.

Another meetup participant (my coworker Sam) <a href="http://tech.blinemedical.com/machine-learning-with-f-and-c-side-by-side/" target="_blank" rel="noopener noreferrer">also posted</a> his kNN classifier, so go check it out and worked through it with a side by side C# example which was cool.

If you get a chance to see Mathias during his <a href="http://www.clear-lines.com/blog/" target="_blank" rel="noopener noreferrer">summer of F# tour</a> you should! While DC was on the tail end of the trip, Boston and Detroit still are on the agenda.

--
Edit:

Here is a youtube of a portion of the dojo:

<iframe width="420" height="315" src="//www.youtube.com/embed/MW_Km-vr1eE" frameborder="0" allowfullscreen></iframe>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4170</wp:post_id>
		<wp:post_date><![CDATA[2013-08-19 00:56:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-19 00:56:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[coding-dojo-gentle-introduction-machine-learning-f-review]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="machine-learning"><![CDATA[machine learning]]></category>
		<category domain="post_tag" nicename="meetup"><![CDATA[meetup]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558691217;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4126;}i:1;a:1:{s:2:"id";i:4275;}i:2;a:1:{s:2:"id";i:4209;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>126</wp:comment_id>
			<wp:comment_author><![CDATA[B-Line Medical | Machine Learning with F# and C# side-by-side]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://tech.blinemedical.com/machine-learning-with-f-and-c-side-by-side/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.63.197.20]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-19 18:55:25]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-19 18:55:25]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] This was my first foray into F#. For a more experienced take on the same tutorial, see Anton&#8217;s blog post here. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>127</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #34 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/08/26/f-weekly-34-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.8.201]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-08-25 21:01:48]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-08-25 21:01:48]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp wrote &#8220;Coding Dojo: a gentle introduction to Machine Learning with F# review&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Angular with typescript architecture</title>
		<link>https://onoffswitch.net/2013/09/16/angular-typescript-architecture/</link>
		<pubDate>Mon, 16 Sep 2013 08:00:32 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4028</guid>
		<description></description>
		<content:encoded><![CDATA[Bear with me, this is going to be a long post.

For the past several months I've been working on a production single page application written with AngularJS and TypeScript, and I wanted to share how myself and my team have architected the application.  In case you were wondering, the app was written using typescript 0.8.3 and not 0.9.1 (which is out now with generics).  

In general, I was really unhappy with a lot of the AngularJS examples I had found on the internet when researching how to structure the application, since they all looked flimsy and poorly constructed.  While the examples found online were easy to read and follow, they clearly wouldn't work with an actual large application.

I had several goals:

<ul>
<li>I didn't want to have to constantly register directives/filters/controllers/etc with Angular everytime I added something</li>
<li>I didn't want to have to update my main index.html page with any references to new files as I worked on them</li>
<li>I wanted to avoid string typing as much as possible by centralizing all string references to angular components</li>
<li>I wanted everything testable</li>
<li>I didn't want to inline directive html templates, I wanted them in in separate html components</li>
<li>I wanted one file for each class</li>
<li>I wanted everything strongly typed</li>
</ul>

Anything less than these requirements, I felt, would compromise maintainability and extensibility for the future, especially if the app had more than one developer working on it.

<h2>Folder structure</h2>

Starting off, my folder structure looks like this:

[code]
app
├──components
├──css
├──img
├──js
    ├──common
    ├──controllers
    ├──data
       ├──locale
    ├──def
       ├──local
       ├──vendor
    ├──directives
    ├──filters
    ├──models
    ├──services
    ├──app.ts
    ├──_all.d.ts
├──locales
├──partials
├──tests
    ├──js
       ├──unitTests
          ├──controllres
          ├──directives
          ├──filters
          ├──models
          ├──services
       ├──e2e tests       
├──index.html
[/code]

<ul>
<li>components - This is where all the html templates for directives go</li>
<li>css - All scss files and the final compiled app.css which is what is linked to from index.html</li>
<li>img - All statically served image files</li>
<li>js - All typescript files broken down into individually subsections.  Also the definitions folder <code>def</code> will be seperated out from any local custom definitions if necessary, and all vendor definitions (such as angular, rxjs, jquery, etc). Also, <code>app.ts</code> (which will be discussed later) is the main app bootloader. <code>_all.d.ts</code> is an aggregate typescript def file that has all the references to the .ts files in the application</li>
<li>locales - This is where source .properties files go that can be used to <a href="https://github.com/devshorts/TypescriptLocaleGenerator" target="_blank" rel="noopener noreferrer">auto generate typescript locale data</a></li>
<li>partials - This is where main angular page entrypoints go.  These are the initial partials that are loaded as part of an <code>ng-view</code> change</li>
<li> tests - all unit tests, and e2e tests (using angular e2e scenario testing)</li>
</ul>

<h2>Wrapping Angular in OO</h2>

There's no reason you can't wrap the angularJS object initializers that are used to define directives, filters, controllers, etc, into strongly typed local classes.  My team and I built a bunch of templates to use with IntelliJ Idea which made creating new directives/controllers/filters/services/etc extremely easy.  


<h2>Controllers</h2>

Let me demonstrate an example controller:

[ts]
/// &lt;reference path=&quot;../_all.d.ts&quot; /&gt; 

module devshorts.app.controllers {

    import sharedModel = common.model;

    export interface ITest extends ng.IScope{

    }

    export class Test extends ControllerBase {

        public static $inject:string[] = [
            sharedModel.AngularGlobal.$SCOPE
        ];

        constructor(private $scope:ITest) {
            super(arguments, Test.$inject);
        }
    }
}
[/ts]

There are a few things going on here.  First, there is a single aggregate definitions file that is always included.  This means we don't need to pollute each file with definitions.  The <code>_all.d.ts</code> is also auto generated using a dependency builder we wrote (posted on my <a href="https://github.com/devshorts/TypeScript-Dependency-Builder" target="_blank" rel="noopener noreferrer">github</a>). It finds all <code>.d.ts</code> files based on a dependency configuration and auto populates the all.d.ts for you. 

Second, notice that the controller scope is typed with an interface.  By making sure we use an interface we can guarantee we won't try to access a field in the UI that we didn't explicity expose.  

Third, since for controllers I'm using the <code>$inject</code> annotation, we're making sure to not hardcode any angular strings.  Everything is centralized in an <code>AngularGlobal</code> class:

[ts]
export class AngularGlobal {
        public static $SCOPE = &quot;$scope&quot;;
        public static $COOKIE_STORE = &quot;$cookieStore&quot;;
        public static NG_COOKIES = &quot;ngCookies&quot;;
        ... etc ...
    }
[/ts]

Also, you'll notice that in the constructor of the controller there is a call to the base class with the inject arguments.  The base class is going to validate that the arguments passed to the controller match the items in the $inject array.  This way we can get fail fast runtime errors if we ask to inject something, but didn't wire up the constructor right:

[ts]
export class ControllerBase {                                                                                                                          
                                                                                                                                                       
    definedArguments(args:any):string[] {                                                                                                              
        var functionText = args.callee.toString();                                                                                                     
        var foundArgs = /\(([^)]+)/.exec(functionText);                                                                                                
        if (foundArgs[1]) {                                                                                                                            
            return foundArgs[1].split(/\s*,\s*/);                                                                                                      
        }                                                                                                                                              
                                                                                                                                                       
        return [];                                                                                                                                     
    };                                                                                                                                                 
                                                                                                                                                       
    constructor(args:any, injection:string[]){                                                                                                         
                                                                                                                                                       
        var expectedInjections = _.zip(this.definedArguments(args), injection);                                                                        
                                                                                                                                                       
        _.each(expectedInjections, val =&gt; {                                                                                                            
            var injectionId = val[0];                                                                                                                  
            var argument:string = val[1];                                                                                                              
            if(argument == null){                                                                                                                      
                throw &quot;missing injection id.  Argument for &quot; + injectionId + &quot; is undefined. Make sure to add the ID as part of the $inject function&quot;; 
            }                                                                                                                                          
        })                                                                                                                                             
    }                                                                                                                                                  
                                                                                                                                                      
[/ts]

So, why does moving the constructor to its own class work? Well, look at what angular wants for a controller:

[javascript]
myApp.controller('GreetingCtrl', ['$scope', function($scope) {
    $scope.greeting = 'Hola!';
}]);
[/javascript]

And look at what typescript will generate for this class:

[javascript highlight="8,9,10,11,15"]
var devshorts;
(function (devshorts) {
    (function (app) {
        (function (controllers) {
            var sharedModel = common.model;
            var Test = (function (_super) {
                __extends(Test, _super);
                function Test($scope) {
                    _super.call(this, arguments, Test.$inject);
                    this.$scope = $scope;
                }
                Test.$inject = [
                    sharedModel.AngularGlobal.$SCOPE
                ];
                return Test;
            })(ControllerBase);
            controllers.Test = Test;            
        })(app.controllers || (app.controllers = {}));
        var controllers = app.controllers;
    })(devshorts.app || (devshorts.app = {}));
    var app = devshorts.app;
})(devshorts || (devshorts = {}))
[/javascript]

Disregarding all the wrapping for namespaces, you can see that the constructor for Test is just a function that takes a $scope. So, it's the exact same thing.  Now you can wire up your routes in such a way to pair partials with controllers like this:

[ts]
$routeProvider.when('/test', 
                    this.getRoute(relativePath(&quot;partials/test.html&quot;), 
                    devshorts.app.controllers.Test));
[/ts]

<h2>Auto registration of services, directives, filters, and models</h2>

For our purposes, we did something slightly different with services, models, directives and filters. The reason being that controllers are always "registered" with angular via the routing.  However, services, models, directives, and filters are usually registered with separate angular modules that the main app depends on.  In this scenario, since it's common to create lots of directives, etc, we didn't want to have to manually register anything.

Let me show the model template we have:

[ts]
/// &lt;reference path=&quot;../_all.d.ts&quot; /&gt;

module devshorts.app.models {
    'use strict';

    export interface ITestModel {

    }

    export class TestModel implements ITestModel {

        public static ID:string = &quot;TestModel&quot;;

        public static injection():any[] {
            return [ AngularGlobal.HTTP, 
                     httpService =&gt; new TestModel(httpService) ];
        }
    }
}
[/ts]

Here there is a static <code>ID</code> which is the name of the model, and a static <code>injection</code> function that returns the array notation for injection.  In the array notation the last line is the function that gets called by angular with the relevant injectable types.  We preferred array notation vs $inject notation since array notation is safe to minimize.  The name <code>httpService</code> at this point doesn't matter, since we asked angular for it using the static <code>AngularGlobal</code> class discussed above.

The <code>ID</code> and <code>injection</code> functions are important, and I'll show why in a second.

Below is how we've bootstrapped angular in the index.html page

[javascript]
$(document).ready(function(){                                        
     var main = new devshorts.app.Main(ipadGlobals.available);

     // when all is done, execute bootstrap angular application      
     angular.bootstrap(document, [NG_GLOBAL.APP_NAME]);              
 });                                                                                                                           
[/javascript]

But what is this main class?  This is a class that handles all the registration, routing, and other AngularJS setup we need. 

[ts]                                                                                                                                    
export class Main implements IMain {                                                                                                
                                                                                                                                    
    public app:ng.IModule = angular.module(NG_GLOBAL.APP_NAME,                                                                      
                                            [                                                                                       
                                                NG_GLOBAL.APP_DIRECTIVES,                                                           
                                                NG_GLOBAL.APP_SERVICES,                                                             
                                                NG_GLOBAL.APP_MODELS,                                                               
                                                NG_GLOBAL.APP_PROVIDERS,                                                            
                                                NG_GLOBAL.APP_FILTERS,                                                              
                                                'ui.directives',                                                                    
                                                'ngMobile'                                                                          
                                            ]);                                                                                     
                                                                                                                                    
    public directives:ng.IModule = angular.module(NG_GLOBAL.APP_DIRECTIVES, []);                                                    
                                                                                                                                    
    public services:ng.IModule = angular.module(NG_GLOBAL.APP_SERVICES, []);                                                        
                                                                                                                                    
    public models:ng.IModule = angular.module(NG_GLOBAL.APP_MODELS, [sharedModel.AngularGlobal.NG_COOKIES]);                        
                                                                                                                                    
    public providers:ng.IModule = angular.module(NG_GLOBAL.APP_PROVIDERS,[]);                                                       
                                                                                                                                    
    public filters:ng.IModule = angular.module(NG_GLOBAL.APP_FILTERS, []);                                                          
                                                                                                                                                                                                                                                                     
    constructor(private isAvailable:bool) {                                                                                         
        this.route(this.getApp());                                                                                                  
                                                                                                                                    
        this.wireFactories();                                                                                                       
                                                                                                                                    
        this.configureProviders();                                                                                                  
                                                                                                                                    
        this.configureHttpInterceptors(this.getApp());                                                                              
    }                                                                                                                               
                                                                                                                                    

    ... other methods ...
[/ts]

Again, the app name and other dependency names are all centralized in a static class.  The interesting part here is the <code>wireFactories</code> method which looks like this:

[ts]
wireFactories(){          
    this.wireServices();  
    this.wireDirectives();
    this.wireModels();    
    this.wireFilters();   
}                         
[/ts]

Lets look at <code>wireModels</code>

[ts]
wireModels(){                                                   
    this.wire(devshorts.app.models, this.models.factory);
}                                                               
[/ts]

And finally looking at <code>wire</code>

[ts]
/***                                                                                                                 
 * We are simulating doing something like this:                                                                      
 *                                                                                                                   
 *      this.directives.directive(directives.DynamicView.ID, directives.DynamicView.injection());                    
 *                                                                                                                   
 * The &quot;this.directives.directive&quot; is the function we want to call on which is the registration function             
 *                                                                                                                   
 * Since the ID and injection function are statically defined in our classes and MUST be defined for                 
 * our angular injection to work, we can type them temporarily here in this function                                 
 *                                                                                                                   
 * This way if we add new items to any namespace that have an injection function and an ID we will                   
 * automatically register them to the right angular module         *                                                 
 *                                                                                                                   
 * @param namespace                                                                                                  
 * @param registrator                                                                                                
 * @param byPass                                                                                                     
 */                                                                                                                  
  wire(namespace:any, registrator:(string, Function) =&gt; ng.IModule , byPass?:(s) =&gt; bool){                           
    for(var key in namespace){                                                                                       
        try{                                                                                                         
            if(byPass != null &amp;&amp; byPass(key)) {                                                                      
                continue;                                                                                            
            }                                                                                                        
                                                                                                                     
            var injector = &lt;IInjectable&gt;(namespace[key]);                                                            
                                                                                                                     
            if(injector.ID &amp;&amp; injector.injection){                                                                   
                registrator(injector.ID, injector.injection());                                                      
            }                                                                                                        
        }                                                                                                            
        catch(ex){                                                                                                   
            console.log(ex);                                                                                         
        }                                                                                                            
    }                                                                                                                                                                                                                                     
}                                                                                                                    
[/ts]

Since namespaces in javascript are just objects with properties, we can make an assumption that anything under the <code>devshorts.app.models</code> namespace is a model if it has a static <code>ID</code> and a static <code>injection</code> function.  If it does, then we can register that class with the correct angular module.  

Now we never have to worry about wiring up directives, filters, models, or services since at runtime they are auto wired for us.  This gives application development a more native feel, just by adding the file means it exists and is available for injection.

<h2>Merging the files</h2>

I've read about people promoting writing everything in javascript into one file, since they don't want to have the overhead of loading hundreds of .js files on load. I vehemently disagree here.  From a development standpoint you should have one class per file. It makes it easier to split up your code, move things around, and to properly organize your application.  However, in a production environment you aboslutely should distribute a single merged file. But that's trivial.  I linked to it earlier on, but the dependency tool I had also populates index.html with the appropriate script references for all .js files it finds (that are configured for it to find). If you are interested, go check out the <a href="https://github.com/devshorts/TypeScript-Dependency-Builder" target="_blank" rel="noopener noreferrer">typescript dependency builder</a> (which I will probably blog about again later).

Assuming that your index.html page has all the relevant js files in the head, then it's easy to write a simple script to merge all the files into one and serve that up when the application is loaded in production.  In debug mode, you can serve up all the independent files, it's just a matter of toggling your node config or your web.config (in an asp.net application). 

As an example, here is an F#/C# MSBuild task class to do that for you:

[csharp collapse="true"] 
public class JsMerger : Task
{
    public string IndexPage { get; set; }
    public List&lt;string&gt; JsFiles { get; set; }
    public String OutputFile { get; set; }

    public override bool Execute()
    {           
        try
        {                
            if (File.Exists(OutputFile))
            {
                File.Delete(OutputFile);
            }

            var filesToProcess = GetFilesToProcess()
                                    .Where(NotOutputOrMinFile)
                                    .Select(f =&gt; new JsData
                                    {
                                        FileName = f,
                                        FileContents = File.ReadAllText(f)
                                    });

            using (var output = new StreamWriter(OutputFile))
            {
                foreach (var file in filesToProcess)
                {
                    output.WriteLine(&quot;/*!&quot; + Path.GetFileName(file.FileName) + &quot;*/&quot;);
                    output.WriteLine(file.FileContents);
                    output.WriteLine(Environment.NewLine);
                }
            }
            return true;
        }
        catch (Exception ex)
        {
            Console.WriteLine(ex);
            return false;
        }
    }

    private bool NotOutputOrMinFile(string f)
    {
        var path = f;

        var check = OutputFile.Replace(&quot;.js&quot;, &quot;&quot;);

        return (path != check + &quot;.js&quot;) &amp;&amp; (path != check + &quot;.min.js&quot;);
    }

    private IEnumerable&lt;string&gt; GetFilesToProcess()
    {            
        if (!String.IsNullOrEmpty(IndexPage))
        {
            foreach (var jsFile in JsRetriever.getJsFiles(IndexPage))
            {
                yield return jsFile;
            }
        }

        if (JsFiles != null &amp;&amp; JsFiles.Count &gt; 0)
        {
            foreach (var f in JsFiles)
            {
                yield return f;
            }
        }
    }
}
[/csharp]

Which calls into the following F# script extractor

[fsharp collapse="true"]
namespace MergeJsFiles

open HtmlAgilityPack 
open System.Text.RegularExpressions
open System.IO
open System

module JsRetriever =
    
    let stripHtml (text:string) = 
        try   
            let mutable target = text
                     
            let regex = [
                &quot;&lt;script\s*&quot;, &quot;&quot;;            
                &quot;\&quot;?\s*type\s*=\s*\&quot;\s*text/javascript\s*\&quot;\s*&quot;, &quot;&quot;;                 
                &quot;&lt;/script&gt;&quot;, &quot;&quot;;
                &quot;src\s*=\s*&quot;, &quot;&quot;
                &quot;\&quot;&quot;, &quot;&quot;;
                &quot;&gt;&quot;, &quot;&quot;;
                &quot;&lt;/&quot;,&quot;&quot;
                &quot;&lt;&quot;,&quot;&quot;

            ] 
                
            for (pattern, replacement) in regex do
                    target &lt;- Regex.Replace(target,pattern,replacement).Trim()

            target                 
        with
            | ex -&gt; 
                Console.WriteLine (&quot;Error handling &quot; + text + &quot;, &quot; + ex.ToString())
                &quot;&quot;          

    let convertToAbsolute parent path =
        try            
            Path.Combine(Path.GetDirectoryName(parent), path) |&gt; Path.GetFullPath
        with
            | ex -&gt; 
                Console.WriteLine (&quot;Error handling &quot; + path)
                &quot;&quot;
        

    let endsOn ext file = 
        Path.GetExtension(file) = ext
            
    let getJsFiles (defaultAspxPath:string) = 
        let doc = new HtmlDocument()

        doc.Load defaultAspxPath

        doc.DocumentNode.SelectNodes &quot;/html/head/script/@src&quot; 
            |&gt; Seq.map (fun i -&gt; i.OuterHtml) 
            |&gt; Seq.map stripHtml            
            |&gt; Seq.map (convertToAbsolute defaultAspxPath)
            |&gt; Seq.filter (endsOn &quot;.js&quot;)

[/fsharp]

<h2>Conclusion</h2>

Organizing an application is hard, and everyone has their own style, but proper application organization and architecture is critical to being able to scale your codebase. Also, sometimes to make the development experience pleasant, you need to invest the time to build tools to automate boring tasks for you. Until we spent the time to create the dependency tools, working with angular and typescript was a real headache, but now it's an absolute joy.

There are more things I haven't covered, such as how we dealt with filters, localization, model and service aggregation (for easy injection), and http interceptors but I'll save those for another post.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4028</wp:post_id>
		<wp:post_date><![CDATA[2013-09-16 08:00:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-16 08:00:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[angular-typescript-architecture]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="angularjs"><![CDATA[angularjs]]></category>
		<category domain="post_tag" nicename="architecture"><![CDATA[architecture]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="typescript"><![CDATA[typescript]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561704481;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3710;}i:1;a:1:{s:2:"id";i:3295;}i:2;a:1:{s:2:"id";i:3452;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>97</wp:comment_id>
			<wp:comment_author><![CDATA[James Seppi]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[james.seppi@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[168.39.0.194]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-09-16 13:42:45]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-09-16 13:42:45]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Sweet article, Anton.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>98</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-17 14:35:50]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-17 14:35:50]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hello, Anton, can You, please, share code example for this article?
Thank You.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>99</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-22 07:47:44]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-22 07:47:44]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hello, Anton. I have one more question: where object "_" comes from in ControllerBase class?
Thank You.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>100</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[211.130.2.8]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 00:04:05]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 00:04:05]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Oleg, _ comes from underscore.js which was included in my index page head script block. http://underscorejs.org/]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>99</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>101</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 11:27:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 11:27:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank You, Anton, i get that.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>100</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>102</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 11:34:42]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 11:34:42]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Next I want to ask is: How can I use dependency injection for this kind of class from Your article
    export class TestModel implements ITestModel {
 
        public static ID:string = "TestModel";
 
        public static injection():any[] {
            return [ () =&gt; new TestModel() ];
        }
    }

How can i inject $http services, for an instance?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>103</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 12:22:12]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 12:22:12]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I mean how can I inject $http service into my custom service or model using this kind of architecture?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>102</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>104</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 12:44:54]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 12:44:54]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I guess I get it either. I should use somethink like
public static injection():any[] {
    return [ ($http) =&gt; new TestModel($http) ];
}
Am I right?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>103</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>105</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[211.130.2.8]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 13:02:43]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 13:02:43]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yes and no. Doing it this way you are relying on naming, so angular knows that $http is the http service. This is a bad idea cause you can't minimize your code. Instead do something like this (forgive any typos, I'm in tokyo right now typing this on my phone:

[AngularGlobal.HTTP, 
h => new Test(h)]

Where AngularGlobal could be a static class and the value HTTP maps the angular id of the http service (the actual value I think is string "$http"). The variable "h" will be given the injected http service. I intentionally named it a single letter to demonstrate that using this method the naming convention no longer matters and is now safe for minimization. 

If you have more questions let me know, I am back next week and will have time to post more detailed examples in gists or on my github.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>104</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>106</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[211.130.2.8]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-23 13:07:56]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-23 13:07:56]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Now it may be clearer what the static "ID" value is on models/directives/services, etc that I defined. You reference the string by the static class so this way you centralize all strings into one location. each model, whatever, will define its own dependencies.

By defining the dependencies and the final construction function in static properties you can auto wire them up with angular if you follow a specific convention (I used ID and "injection")]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>105</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>107</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-24 07:56:56]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-24 07:56:56]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thank You very much, Anton, it was very helpful!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>105</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>108</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-24 08:06:31]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-24 08:06:31]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hello, Anton. I have one more question, not related to this article but to some thing i'm currently working on. May be You know how to register in angular dynamically loaded(via requireJS for an instance) controllers, directives, services, etc?
Thank You.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>109</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.243.58.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-10-29 17:13:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-10-29 17:13:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Oleg, sorry, I have no idea there.  I'll ask one of my coworkers, he may be of some help. I'll get back to you]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>108</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>110</wp:comment_id>
			<wp:comment_author><![CDATA[Waynoss]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[wayne.david.harris@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[124.186.106.220]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-11-10 00:47:24]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-11-10 00:47:24]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is very cool, Anton.
Coming from Actionscript3 Flash/Flex background this is exactly what i was looking.
Cheers]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>111</wp:comment_id>
			<wp:comment_author><![CDATA[Matthew Blott]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[matthew.blott@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[81.129.10.9]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-12-23 17:45:43]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-12-23 17:45:43]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[This is excellent. I agree it would be great if the source code was available - it would be even better to create template and wrap it as a Node package or for VS users Nuget.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>98</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>112</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[198.228.201.144]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-12-23 18:05:19]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-12-23 18:05:19]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Not a bad idea. I'll see if I can't put something together]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>111</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>113</wp:comment_id>
			<wp:comment_author><![CDATA[ashd]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[ashdowning@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[74.192.243.28]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-10 03:54:28]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-10 03:54:28]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[github please! ;)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>114</wp:comment_id>
			<wp:comment_author><![CDATA[Rad]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[radoslav@everestkc.net]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[68.117.119.32]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-14 07:56:16]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-14 07:56:16]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great post. Will you please provide a sample as promised. 
Thanks in advance]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>115</wp:comment_id>
			<wp:comment_author><![CDATA[write angularJS code using typescript | DiscVentionsTech]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://discventionstech.wordpress.com/2014/01/19/write-angularjs-code-using-typescript/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.8.65]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-18 21:03:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-18 21:03:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] http://onoffswitch.net/angular-typescript-architecture/ [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>116</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-27 15:30:42]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-27 15:30:42]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hello, Anton. Can You, please, advise me about unit test for controllers / services etc, written using this approach.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>117</wp:comment_id>
			<wp:comment_author><![CDATA[Oleg]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[molfarr@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[91.216.240.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-27 15:32:44]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-27 15:32:44]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[advice*]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>116</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>118</wp:comment_id>
			<wp:comment_author><![CDATA[Vijay]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[vijquick@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[105.236.26.104]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-17 07:01:52]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-17 07:01:52]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,

Wonderful article, so many things to learn for the guys starting with creating new projects. Please can you share the source code for this sample. It's going to worth a lot.
Thanks
Vijay]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>119</wp:comment_id>
			<wp:comment_author><![CDATA[Soham Dasgupta]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[soham1.dasgupta@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[122.169.238.164]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-13 11:27:28]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-13 11:27:28]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Can u please share the source code for the entire application structure. Will be of great help.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>120</wp:comment_id>
			<wp:comment_author><![CDATA[Richard H]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[richardh@cetaris.com]]></wp:comment_author_email>
			<wp:comment_author_url>https://twitter.com/rwhepburn</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[206.210.105.57]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-14 17:54:15]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-14 17:54:15]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[According to the Angular help docs (https://docs.angularjs.org/api/ng/function/angular.bootstrap), the Protractor based end-to-end tests cannot use the <code>angular.bootstrap(...)</code> function (which you use in index.html to manually bootstrap the application). They must use <code>ngApp</code>. 

This approach could be problematic for those that also use Protractor for their e2e tests. 

Are you using Protractor and if so, did you have to use any workarounds?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>121</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[206.169.195.21]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-16 20:20:39]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-16 20:20:39]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Richard, this may have changed since I was working with angular.  In reality we probably didn't need to be manually bootstrapping angular at all and could get away with the ngApp directive.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>120</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>122</wp:comment_id>
			<wp:comment_author><![CDATA[Carl in 't Veld]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[carl@intveld.nl]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.carlintveld.nl</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[31.201.152.162]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-05 05:04:29]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-05 05:04:29]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,

Wonderful post you have created here! I am currently fiddling with your bits, and I wonder what the best way would be to correctly inject the underscope dependency into ControllerBase. There is an UnderscoreStatic interface in the  underscore.d.ts type declaration. So now I have my controller requesting an object with this interface so it can be passed towards the ControllerBase class. For the implementation I have found the angular-module underscore-angularized with the same bower-package name. One should really prevent polluting the global namespace :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>123</wp:comment_id>
			<wp:comment_author><![CDATA[Samko]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[samko8888@yahoo.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[99.249.62.5]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-01-10 18:47:56]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-01-10 18:47:56]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,
I see that others have asked about the code download, but I do not see any links for it yet. Have you been able to set an example and have it downloadable?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>124</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[64.202.160.73]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-01-14 01:33:57]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-01-14 01:33:57]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Unfortunately I left the company I was working for that had this architecture set up and have since lost the code. Won't be happening. Sorry everyone!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>123</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Tech talk: Pattern matching</title>
		<link>https://onoffswitch.net/2013/08/22/tech-talk-pattern-matching/</link>
		<pubDate>Thu, 22 Aug 2013 16:00:40 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4191</guid>
		<description></description>
		<content:encoded><![CDATA[Today's tech talk was about functional pattern matching.  This was a really fun one since I've been sort of "evangelizing" functional programming at work, and it was a blast seeing everyone ask poignant and intersting questions regarding pattern matching.  

What spurred the conversation today was a question my boss asked me which was "how is pattern matching actually compiled?" which led me to find <a href="http://www.codeproject.com/Articles/520869/A-Simple-Overview-on-How-Pattern-Match-Compiles" target="_blank" rel="noopener noreferrer">this blog post</a> describing different ways the f# compiler compiles pattern matching.  The short of it is that the compiler generates a souped up switch statement where it checks each pattern in order. Sometimes it does a good job, sometimes it doesn't, but that's OK.

In the process of researching for the tech talk I came across a great paper entitled <a href="http://wiki.ifs.hsr.ch/SemProgAnTr/files/PatternMatchingInScala.pdf" target="_blank" rel="noopener noreferrer">Pattern Matching in Scala</a> which discussed, obviously, pattern matching in Scala, but also talked about F#, Haskell, and Erlang pattern matching. The interesting thing to me here is how Scala got around comparing classes instead of just algebraic data types.  Scala makes you implement classes as specific <code>case</code> classes when you want to be able to match on them, and also you have to implement <code>apply</code> and <code>unapply</code> methods which effectively "box" and "unbox" your pattern.  

I don't have much experience with Scala (I skimmed a Scala book and wrote a hello world, but that's it), but I am familiar with how F# handled this scenario which is via Active Patterns. I like this since you can mix and match active patterns to provide your own custom way to "compare" items.  

An example I used in our talk today was

[fsharp]
let (|Pattern1|_|) i = 
    if i = 0 then Some(Pattern1) else None
 
let (|Pattern2|_|) i = 
    if i.ToString() = &quot;yo mamma!&quot; then Some(Pattern2) else None
    
let activePatternTest () = 
    let x = 0
    match x with 
        | Pattern1 -&gt; printf &quot;pattern1&quot;
        | Pattern2 -&gt; printf &quot;pattern2&quot;
        | _ -&gt; printf &quot;something else&quot;
[/fsharp]

Which really drives the point home that you can do custom work in your pattern match and hide it away from the user. Another, more real world, example is how I matched on regular expressions in my <a href="https://github.com/devshorts/ParsecClone" target="_blank" rel="noopener noreferrer">parsec clone</a> project

[fsharp]
let (|RegexStr|_|) (pattern:string) (input:IStreamP&lt;string, string&gt;) =
        if String.IsNullOrEmpty input.state then None
        else
            let m = Regex.Match(input.state, &quot;^(&quot; + pattern + &quot;)&quot;, RegexOptions.Singleline)
            if m.Success then 
                Some ([ for g in m.Groups -&gt; g.Value ]
                            |&gt; List.filter (String.IsNullOrEmpty &gt;&gt; not)
                            |&gt; List.head) 
            else 
                None
[/fsharp]

Which can be used to hide away regular expression pattern matching.  The usage of this would now be:

[fsharp]
member x.regexMatch (input:IStreamP&lt;string, string&gt;) target = 
        if String.IsNullOrEmpty input.state then None
        else 
            match input with 
                | RegexStr target result -&gt; Some(result.Length)
                | _ -&gt; None
[/fsharp]

Nice and clean, just the way I like it.

Anyways, pattern matching is a really powerful construct and it's a shame that it's not available in many OO languages.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4191</wp:post_id>
		<wp:post_date><![CDATA[2013-08-22 16:00:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-22 16:00:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tech-talk-pattern-matching]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="pattern-matching"><![CDATA[pattern matching]]></category>
		<category domain="post_tag" nicename="tech-talk"><![CDATA[tech talk]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560233663;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3565;}i:1;a:1:{s:2:"id";i:4961;}i:2;a:1:{s:2:"id";i:4881;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Implementing the game &quot;Arithmetic&quot;</title>
		<link>https://onoffswitch.net/2013/08/24/implementing-game-arithmetic/</link>
		<pubDate>Sat, 24 Aug 2013 17:52:55 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4197</guid>
		<description></description>
		<content:encoded><![CDATA[There is a subreddit on reddit called <a href="http://www.reddit.com/r/dailyprogrammer" target="_blank" rel="noopener noreferrer">/r/dailyprogrammer</a> and while they don't actually post exercises daily, they do sometimes post neat questions that are fun to solve.  About a week ago, they posted <a href="http://www.reddit.com/r/dailyprogrammer/comments/1k7s7p/081313_challenge_135_easy_arithmetic_equations/" target="_blank" rel="noopener noreferrer">a problem</a> that I solved with F# that I wanted to share.  For the impatient, my full source is available at <a href="http://fssnip.net/jy" target="_blank" rel="noopener noreferrer">this fssnip</a>.

The description is as follows:


<blockquote>Unix[2] , the famous multitasking and multi-user operating system, has several standards that defines Unix commands, system calls, subroutines, files, etc. Specifically within Version 7[3] (though this is included in many other Unix standards), there is a game called "arithmetic". To quote the Man Page[4] :

Arithmetic types out simple arithmetic problems, and waits for an answer to be typed in. If the answer
is correct, it types back "Right!", and a new problem. If the answer is wrong, it replies "What?", and
waits for another answer. Every twenty problems, it publishes statistics on correctness and the time
required to answer.

Your goal is to implement this game, with some slight changes, to make this an [Easy]-level challenge. You will only have to use three arithmetic operators (addition, subtraction, multiplication) with four integers. An example equation you are to generate is "2 x 4 + 2 - 5".
Author: nint22</blockquote>

<h2>The cheating solution</h2>

The cheating solution is to use a dynamic string evaluator. For example, dynamic languages such as ruby, python, and javascript have an <code>eval</code> function where you can pass a string to it and it will give you the result of the evaluation.  That's no fun and I think that defeats the purpose of the exercise. Someone, somewhere, had to actually write what the eval function does.

However, I did leverage a .NET version of the dynamic evaluation (via the <code>DataTable</code> class) to validate my solution in a unit test.

<h2>The F# solution</h2>

Whenever I see arbitrary boundaries, I tend to ignore them. My solution works for any size of expression, so unlike some of the other entries that did a lot of specific handling for only four integers, I tested mine up to 1000 terms.   The basic principle is my solution generates an expression tree to represent the random expression. To evaluate the expression AST, it evaluates portions of the tree based on a defined set of order of operations.  In this way you can partially apply an operation and get a new tree if you want.  When evaluating, you have to deal with the fact that some operations have the same weight, such as <code>+</code> and <code>-</code> so they are evaluated left to right.   
<h2>The data types</h2>

First, the data types:

[fsharp]
type Operation = 
    | Mult 
    | Sub
    | Add
    override this.ToString() = 
        match this with  
            | Mult -&gt; &quot;*&quot; 
            | Sub -&gt; &quot;-&quot; 
            | Add -&gt; &quot;+&quot;
    member this.evaluate = 
        match this with  
            | Mult -&gt; (*) 
            | Sub -&gt; (-) 
            | Add -&gt; (+)

let orderofOps = [[Mult];[Add;Sub]]
[/fsharp]

I've created a union type defining the available operations, how to print them out, and what their actual evaluated operation is.  The nice thing in F# is that functions are first class. For example, returning <code>(*)</code> returns a function of signature <code>int -> int -> int</code>.  

Also I've defined an order of operations list list.  Items in inner lists have the same operation precedence (Add and Sub), and the outer list defines what has to happen first.  This way multiplication is evaluated first, then addition and subtraction gets evaluated left to right.

Next is the expression definition. Anyone who's ever worked with syntax tree's should recognize this union pattern:

[fsharp]
type Expression = 
    | Terminal of int    
    | Expr of Expression * Operation * Expression
    
[/fsharp]

Since it's the idiomatic form of an expression tree. 

<h2>Random expressions and numbers</h2>

Lets generate some randomness. This defines random numbers and random operations.

[fsharp]
let rand = new System.Random()
    
let randNum min max = rand.Next(min, max)
    
let randomOperation () = 
    match randNum 0 2 with 
    | 0 -&gt; Mult 
    | 1 -&gt; Sub  
    | _ -&gt; Add
[/fsharp]

Now I can generate a random expression, where each term is within a min and max range, and the expression is of a passed in length

[fsharp]
let rec randomExpression min max length = 
    match length with 
        | 0 -&gt; Terminal(randNum min max)
        | _ -&gt; Expr(Terminal(randNum min max), randomOperation(), randomExpression min max (length - 1))
    
[/fsharp]

<h2>Display an expression</h2>

It'd also be useful to pretty print our expression

[fsharp]
let rec display = function
        | Terminal(i) -&gt; i.ToString()
        | Expr(left, op, right) -&gt; 
             String.Format(&quot;{0} {1} {2}&quot;, display left, op, display right)        
[/fsharp]

This outputs an expression printed like

[code]
8 - 6 * 8 - 5 * 9 * 9 
[/code]

<h2>Tree Evaluation</h2>

The last thing we need to do is actually evaluate the tree.  Let's break down some of the work into active patterns. I love using active patterns to help hide away complex match statements.

[fsharp]
let (|TermWithExpression|_|) predicate expr  = 
       match expr with 
        | Expr(Terminal(left), targetOp, Expr(Terminal(right), o, next)) 
            when predicate targetOp -&gt; 
               Expr(Terminal(targetOp.evaluate left right), o, next) |&gt; Some
        | _ -&gt; None
[/fsharp]

If the operator passes a predicate and folds the left and right terms into a new terminal if the expression has a left terminal and a right expression. Something of the form:

[fsharp]
Expr(Terminal(2), Mult, Expr(Terminal(3), Add, Terminal(4)))
[/fsharp]

The next thing is if we have an expression that is composed of just two terminals. 

[fsharp]
Expr(Terminal(6), Add, Terminal(4))
[/fsharp]

Again, if the operator passes a predicate we'll fold the two terminals into a new terminal.

[fsharp]
let (|TermWithTerm|_|) predicate expr = 
    match expr with 
        | Expr(Terminal(item), targetOp, Terminal(item2)) 
                when predicate targetOp -&gt; 
                    Terminal(targetOp.evaluate item item2) |&gt; Some
        | _ -&gt; None
[/fsharp]


Finally, lets tie it all into one function

[fsharp]
let foldExpr expr opsInPrecedence = 
    let rec foldExpr' expr = 
        let shouldEvalOperator o = List.exists (fun i -&gt; i = o) opsInPrecedence

        match expr with 
            | TermWithExpression shouldEvalOperator output -&gt; foldExpr' output    
            | TermWithTerm shouldEvalOperator output -&gt; output
            | Expr(left, o, right) -&gt; Expr(foldExpr' left, o, foldExpr' right)            
            | Terminal(i) -&gt; Terminal(i)

    foldExpr' expr
[/fsharp]

<h2>Testing it</h2>

I heard a great quote from <a href="http://anton.kovalyov.net/about/" target="_blank" rel="noopener noreferrer">Anton Kovalyov</a>, creator of <a href="http://www.jshint.com/" target="_blank" rel="noopener noreferrer">JSHint</a>, at <a href="https://qconnewyork.com/" target="_blank" rel="noopener noreferrer">QConn NYC</a> 2013: "<em>if it's not tested, it's broken</em>", so here is a test to validate the code:

[fsharp]
[&lt;Test&gt;]
let arithmeticTest() =
 
    let dt = new DataTable()    

    for i in [0..100] do
        let randomExpr = randomExpression 0 10 5

        let validationResult = dt.Compute(display randomExpr, &quot;&quot;).ToString() |&gt; Convert.ToInt32
    
        let result = eval randomExpr
            
        printfn &quot;%s = %d = %d&quot; (display randomExpr) validationResult (match result with Terminal(x) -&gt; x)
    
        result |&gt; should equal &lt;| Terminal(validationResult)
[/fsharp]

This code uses the evaluate capability of the .NET database (found via this <a href="http://stackoverflow.com/questions/333737/c-sharp-evaluating-string-342-yield-int-18" target="_blank" rel="noopener noreferrer">stackoverflow</a> post) to evaluate the displayed random expression and compare the result to my evaluation of the expression.  


<h2>Division</h2>

In the original problem description, division was left out to avoid having to deal with divide by zero, but I think that would be pretty easy to handle.  The expression folding can know if the right hand term is a zero and the operator is a division, and in that case it can return a <code>None</code> solution. So the folding should be modified to return an expression Option type. ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4197</wp:post_id>
		<wp:post_date><![CDATA[2013-08-24 17:52:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-24 17:52:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[implementing-game-arithmetic]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="arithmetic"><![CDATA[arithmetic]]></category>
		<category domain="post_tag" nicename="ast"><![CDATA[ast]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="expression-tree"><![CDATA[expression tree]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="reddit"><![CDATA[reddit]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560829492;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2020;}i:1;a:1:{s:2:"id";i:4244;}i:2;a:1:{s:2:"id";i:4262;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Machine learning from disaster</title>
		<link>https://onoffswitch.net/2013/08/24/machine-learning-disaster/</link>
		<pubDate>Sat, 24 Aug 2013 17:57:40 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4209</guid>
		<description></description>
		<content:encoded><![CDATA[If any of my readers are in the DC/MD/VA area you should all come to the next DC F# meetup that I'm organizing on september 16th (monday). The topic this time is machine learning from disaster, and we'll get to find out who lives and dies on the Titanic! We're bringing in guest speaker <a href="http://trelford.com/blog/" target="_blank" rel="noopener noreferrer">Phil Trelford</a> so you know its going to be awesome!  Phil is in the DC area on his way to the <a href="skillsmatter.com/event/scala/progressive-f-tutorials-nyc" target="_blank" rel="noopener noreferrer">F# skills matters conference</a> in NYC a few days later.   I won't be there but I expect that it will be top notch since all the big F# players are there (such as Don Syme and Tomas Petricek)!.

For more info check out our <a href="http://www.meetup.com/DC-fsharp/events/135766752/" target="_blank" rel="noopener noreferrer">meetup page</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4209</wp:post_id>
		<wp:post_date><![CDATA[2013-08-24 17:57:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-24 17:57:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[machine-learning-disaster]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559849919;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4275;}i:1;a:1:{s:2:"id";i:4126;}i:2;a:1:{s:2:"id";i:4170;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>ParsecClone on nuget</title>
		<link>https://onoffswitch.net/2013/08/25/parsecclone-nuget/</link>
		<pubDate>Sun, 25 Aug 2013 16:10:04 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4213</guid>
		<description></description>
		<content:encoded><![CDATA[Today I published the first version of <a href="https://github.com/devshorts/ParsecClone" target="_blank" rel="noopener noreferrer">ParsecClone</a> to <a href="https://www.nuget.org/packages/ParsecClone/" target="_blank" rel="noopener noreferrer">nuget</a>. I <a href="http://onoffswitch.net/parsing-csvs-parser-combinator/" target="_blank" rel="noopener noreferrer">blogged</a> recently about creating my own parser combinator and it's come along pretty well. While <a href="http://www.quanttec.com/fparsec/" target="_blank" rel="noopener noreferrer">FParsec</a> is more performant and better optimized, mine has other advantages (such as being able to work on arbitrary consumption streams such as binary or bit level) and work directly on strings with regex instead of character by character.  Though I wouldn't recommend using ParsecClone for production string parsing if you have big data sets, since the string parsing isn't streamed. It works directly on a string.  That's still on the todo list, however the binary parsing does work on streams.

Things included:

<ul>
<li>All your favorite parsec style operators: <code><|></code>, <code>>>.</code>, <code>.>></code>, <code>|>></code>, etc. I won't list them all since there are a lot.</li>
<li>String parsing.  Match on full string terms, do regular expression parsing, inverted regular expressions, etc. I have a full working CSV parser written in ParsecClone</li>
<li>Binary parsing. Do byte level parsing with endianness conversion for reading byte arrays, floats, ints, unsigned ints, longs, etc.  </li>
<li>Bit level parsing.  Capture a byte array from the byte parsing stream and then reprocess it with bit level parsing. Extract any bit, fold bits to numbers, get a list of zero and ones representing the bits you captured. Works for any size byte array (though converting to int will only work for up to 32 bit captures).</li>

The fun thing about ParsecClone is you can now parse anything you want as long as you create a streamable container. The combinator libraries don't care what they are consuming, just that they are combining and consuming. This made it easy to support strings, bytes, and bits, all as separate consumption containers.

Anyways, maybe someone will find it useful, as I don't think there are any binary combinator libraries out there for F# other than this one.  I'd love to get feedback if anyone does use it!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4213</wp:post_id>
		<wp:post_date><![CDATA[2013-08-25 16:10:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-08-25 16:10:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[parsecclone-nuget]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="combinator"><![CDATA[combinator]]></category>
		<category domain="post_tag" nicename="nuget"><![CDATA[nuget]]></category>
		<category domain="post_tag" nicename="parser"><![CDATA[parser]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554986094;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4131;}i:1;a:1:{s:2:"id";i:2735;}i:2;a:1:{s:2:"id";i:4077;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>128</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #35 2013 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2013/09/02/f-weekly-35-2013/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.135.48.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-09-01 21:01:00]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-09-01 21:01:00]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp published &#8220;ParsecClone on nuget&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Thinking about haskell functors in .net</title>
		<link>https://onoffswitch.net/2013/09/09/thinking-haskell-functors-net/</link>
		<pubDate>Mon, 09 Sep 2013 20:06:52 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4226</guid>
		<description></description>
		<content:encoded><![CDATA[I've been teaching myself haskell lately and came across an interesting language feature called functors. Functors are a way of describing a transformation when you have a boxed container. They have a generic signature of

[fsharp]
('a -&gt; 'b) -&gt; f 'a -&gt; f 'b
[/fsharp]

Where <code>f</code> isn't a "function", it's a type that contains the type of <code>'a</code>.

The idea is you can write custom map functions for types that act as generic containers. Generic containers are things like lists, an option type, or other things that <em>hold</em> something. By itself a <code>list</code> is nothing, it has to be a list OF something.  Not to get sidetracked too much, but these kinds of boxes are called Monads.  

Anyways, let's do this in C# by assuming that we have a box type that holds something.

[csharp]

public class Box&lt;T&gt;
{
    public T Data { get; set; }   
}

var boxes = new List&lt;Box&lt;string&gt;&gt;();

IEnumerable&lt;string&gt; boxNames  = boxes.Select(box =&gt; box.Data);

[/csharp]

We have a type <code>Box</code> and a list of <code>boxes</code>.  Then we <code>Select</code> (or map) a box's inner data into another list.  We could extract the projection into a separate function too:

[csharp]
public string BoxString(Box&lt;string&gt; p)
{
    return p.Data;
}
[/csharp]

The type signature of this function is 

[csharp]
Box-&gt; string
[/csharp]

But wouldn't it be nice to be able to do work on a boxes data without having to explicity project it out?  Like, maybe define a way so that if you pass in a box, and a function that works on a string, it'll automatically unbox the data and apply the function to its data. 

For example something like this (but this won't compile obviously)

[csharp]
public String AddExclamation(String input){
   return input + &quot;!&quot;;
}

IEnumerable&lt;Box&lt;string&gt;&gt; boxes = new List&lt;Box&lt;string&gt;&gt;();

IEnumerable&lt;string&gt; boxStringsExclamation = boxes.Select(AddExclamation);
[/csharp]

In C# we have to add the projection step (which in this case is overloaded):

[csharp]
public String AddExclamation(Box&lt;String&gt; p){
   return AddExclamation(p.Data);
}
[/csharp]

In F# you have to do basically the same thing:

[fsharp]
type Box&lt;'T&gt; = { Data: 'T }

let boxes = List.init 10 (fun i -&gt; { Data= i.ToString() })

let boxStrings = List.map (fun i -&gt; i.Data) boxes
[/fsharp]

But in Haskell, you can define this projection as part of the type by saying it is an instance of the <code>Functor</code> type class.  When you make a generic type an instance of the functor type class you can define how maps work on the insides of that class.  

[fsharp]
data Box a = Data a deriving (Show)

instance Functor Box where
    fmap f (Data inside) = Data(f inside)    

main =
    print $ fmap (++&quot;... your name!&quot;) (Data &quot;my name&quot;)
[/fsharp]

This outputs

[code]
Data &quot;my name... your name!&quot;
[/code]


Here I have a box that contains a value, and it has a value.  Then I can define how a box behaves when someone maps over it. As long as the type of the box contents matches the type of the projection, the call to <code>fmap</code> works.   
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4226</wp:post_id>
		<wp:post_date><![CDATA[2013-09-09 20:06:52]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-09 20:06:52]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[thinking-haskell-functors-net]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="functors"><![CDATA[functors]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558842116;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4348;}i:1;a:1:{s:2:"id";i:4725;}i:2;a:1:{s:2:"id";i:4262;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>129</wp:comment_id>
			<wp:comment_author><![CDATA[A functor is not a box | 神刀安全网]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.shellsec.com/news/15527.html</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[133.242.174.56]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-05-01 23:02:10]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-05-01 23:02:10]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] box”. Examples of this can be found here , here , here and here , but also here and here , here , here , here and some more here (and that’s just the first two pages of Google’s results for ‘A [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Seq.unfold and creating bit masks</title>
		<link>https://onoffswitch.net/2013/09/09/seq-unfold-creating-bit-masks/</link>
		<pubDate>Mon, 09 Sep 2013 20:28:22 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4237</guid>
		<description></description>
		<content:encoded><![CDATA[In the course of working on  <a href="https://github.com/devshorts/ParsecClone" target="_blank" rel="noopener noreferrer">ParsecClone</a> I needed some code that could take in an arbitrary byte array and convert it to a corresponding bit array. The idea is if I have an array of 

[fsharp]
[|byte 0xFF;byte 0x01|]
[/fsharp]

Then I should get

[fsharp]
[|1;1;1;1;1;1;1;0;0;0;0;0;0;0;1|]
[/fsharp]

I've done plenty of bit slingin' in my day, and the trick is just to apply a sequence of bit masks to each byte and collect all the bit arrays.  In other languages this is always a little bit of a pain, but in F# the solution was amazingly elegant

<h2>Data</h2>
As with anything F#, I like to start with the data

[fsharp]
type Bit = 
    | One
    | Zero
    override this.ToString() =
        match this with 
            | One -&gt; &quot;1&quot;
            | Zero -&gt; &quot;0&quot;
[/fsharp]

<h2>Make bit masks</h2>

Now lets generate the meat and potatoes of this: a sequence of bit masks

[fsharp]
let bitMasks = Seq.unfold (fun bitIndex -&gt; Some((byte(pown 2 bitIndex), bitIndex), bitIndex + 1)) 0
                        |&gt; Seq.take 8
                        |&gt; Seq.toList
                        |&gt; List.rev
[/fsharp]

While a <code>fold</code> takes a list and a seed and returns a single accumulated item, an <code>unfold</code> takes a seed and generates a list.  For those not familiar, <code>unfold</code> takes a function of the signature

[fsharp]
(State -&gt; ('a * State) option) -&gt; State -&gt; Seq&lt;'a&gt;
[/fsharp]

Unfold takes a function with an argument that is the state, and returns an <code>item * state</code> option tuple.  The first element of the option is the element to be emitted in the sequence. The second item is the <em>next</em> state.  If you return <code>None</code> instead of <code>Some</code> the infinite sequence will end. You can see that my state is the exponent n of <em>2^n</em> which gives you the bit mask. The first iteration is 2^0, then 2^1, then 2^2, etc.  By reversing it, I now have a bitmask that look like this:

[fsharp]
[2^7; 2^6; 2^5; 2^4; 2^3; 2^2; 2^1; 2^0]
[/fsharp]

<h2>Byte to Bits</h2>

The next thing is to apply the bitmask to a byte.

[fsharp]
let byteToBitArray b = 
        List.map (fun (bitMask, bitPosition) -&gt; 
                    if (b &amp;&amp;&amp; bitMask) &gt;&gt;&gt; bitPosition = byte(0) then Zero
                    else One) bitMasks
[/fsharp]

The unusual thing here is that bitwise and is the <code>&&&</code> operator and bitwise shift is the </code>>>></code> operator.  Not that weird, but different from other langauges. 

<h2>Bytes to Bits</h2>

All that's left is applying the byteToBitArray function to byte array to get a bit array

[fsharp]
let bytesToBits (bytes:byte[]) =  
    bytes 
        |&gt; Array.toList
        |&gt; List.map byteToBitArray
        |&gt; List.collect id
        |&gt; List.toArray
[/fsharp]

And now to test it in fsi

[fsharp]
&gt; bytesToBits [|byte 0xFF;byte 0x01|];;
val it : Bit [] =
  [|One; One; One; One; One; One; One; One; Zero; Zero; Zero; Zero; Zero; Zero;
    Zero; One|]
[/fsharp]

<h2>Bits To UInt</h2>

We can even take a bit array and create a uint now too

[fsharp]
let bitsToUInt (bits:Bit[])  = 
   let positions = Array.zip bits (Array.rev [|0..Array.length bits - 1|])

   Array.fold (fun acc (bit, index) -&gt; 
                   match bit with 
                       | Zero -&gt; acc
                       | One -&gt; acc + (pown 2 index)) 0 positions
[/fsharp]

First I zip the bit array with each position in the bit array. Then we just need to fold over the array and add the accumulator to <em>2^n</em> if the bit is a one.  

[fsharp]
&gt; [|One; One; One; One; One; One; One; Zero;|] |&gt; bitsToUInt;;
val it : int = 254
[/fsharp]

<h2>Conclusion</h2>

I really enjoyed working with the higher order functions that F# provides to make a simple and robust conversion.  Working with strongly typed data felt more robust than dealing with just integers.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4237</wp:post_id>
		<wp:post_date><![CDATA[2013-09-09 20:28:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-09 20:28:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[seq-unfold-creating-bit-masks]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="bits"><![CDATA[bits]]></category>
		<category domain="post_tag" nicename="byte"><![CDATA[byte]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559923232;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4213;}i:1;a:1:{s:2:"id";i:3615;}i:2;a:1:{s:2:"id";i:4286;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Review of my first time experience with haskell editors</title>
		<link>https://onoffswitch.net/2013/09/30/time-experience-haskell-editors/</link>
		<pubDate>Mon, 30 Sep 2013 08:00:39 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4244</guid>
		<description></description>
		<content:encoded><![CDATA[When you start learning a new language the first hurdle to overcome is how to edit, compile, and debug an application.  In my professional career I rely heavily on visual studio and intellij IDEA as my two IDE workhorses. Things just work with them.  I use visual studio for C#, C++, and F# development and IDEA for everything else (including scala, typescript, javascript, sass, ruby, and python).  

IDEA had a haskell plugin but it didn't work and caused exceptions in intellij using intellij 12+.  Since my main ide's wouldn't work with haskell I took to researching what I could use.

<h2>Requirements</h2>

While some people frown on the idea of an IDE, I personally like them. To quote <a href="https://twitter.com/headinthebox/status/379847787619184640" target="_blank" rel="noopener noreferrer">Erik Meijer</a> 

<blockquote>I am hooked on autocomplete. When I type xs "DOT" it is a cry for help what to pick, map, filter, flatMap. Don't know upfront.</blockquote>

Not only that, but I want the build system hidden away, I want immediate type checking and error highlighting. I want code navigation, syntax highlighting, and an integrated debugger.  I want all that and I don't want to have to spend more than 30 seconds getting started. The reason being is that I have problems to solve! The focus should be on the task at hand, not fiddling with an editor.

In college I used VIM and while it was excellent at what it did, I found that it really wasn't for me. Switching between the command mode and the edit mode was annoying, and I really just want to use the mouse sometimes.  I also tried  EMACS, and while it did the job, I think the learning curve was too high without enough "<em>oo! that's cool!</em>" moments to keep me going.  If I did a lot of terminal work (especially remote) then mastering these tools is a must, but I don't. I know enough to do editing when I have to, but I don't want to develop applications in that environment.  When you find a good IDE (whether its a souped up editor or not) your productivity level skyrockets.

<h2>Getting Haskell working</h2>

Even though I'm on a windows machine I still like to use unix utilities. I have a collection of unix tools like ls, grep, sort, etc.  Turns out this is kind of a problem when installing Haskell. You need to have the official Gnu Utils for wget, tar, and gzip otherwise certain installations won't work. Also if you have tortoise GIT installed on your machine and in your path, some other unix utils are also available.  To get Haskell working properly I had to make sure the GNU utils were first in the path before any of the other tools.  

On top of that, I wasn't able to get the cabal package for Hoogle to install on windows. About a week later, when I was trying to get Haskell up and running again I found <a href="https://code.google.com/p/ndmitchell/issues/detail?id=619" target="_blank" rel="noopener noreferrer">this post</a> which mentioned that they had just fixed a windows build problem.  

<h2>Leksah</h2>

Once haskell was built, I turned to finding an IDE. My first google pointed me to Leksah, which at initially like exactly what I wanted.  It had auto completion, error checking, debugging, etc. And it had a sizzlin dark theme that I thought was cool.  I installed the 2013 Haskell platform (which contains GHC 7.6.3) and tried to run the Leksah build I got from their site.  Being a Haskell novice, I didn't know that you had to run Leksah that is compiled against the GHC version you have, so nothing worked! Leksah loaded, but I was immediately bombared with questions about workspaces, cabal files, modules, etc.  This was overwhelming. I just wanted to type in some haskell and run it.  

Once I figured that all out though, I couldn't get the project to debug or any of the haskell modules to load. Auto complete also wouldn't work.  

Frustrated, I spent 2 days searching for solutions. I eventually realized I needed the right version of Leksah and found a beta build posted on in the Leksah google forums.  Unfortunately this had other issues. I again couldn't debug (clicking the debug button enabled and then immediately disabled), the GTK skin looked wonky, and right clicking opened menus 20 pixels above from where the mouse actually was.  

Given all this, I gave up on Leksah.

<h2>SublimeText</h2>

The next step was sublime text with the sublime text haskell plugin.  I was skeptical here since sublime text is really just a fancy text editor, but people swore by it so I gave it a shot.  Here I had better luck getting things to work, but I was still unhappy.  For a person new to Haskell, the exploratory aspect just wasn't there. There's no integration with GHCi for debugging, and I couldn't search packages for what I wanted.  Auto complete was faulty at best, it wouldn't pick up functions in other files and wouldn't prompt me half the time. 

Still, it looked sharp and loaded fast. I was a big fan of the REPL plugin, but loading things into the REPL was kind of a pain.  Also I liked all the hot keys, adding inferred types was easy, checking types was reasonably easy, but the lack of a good code navigation and proper auto completion irked me.

<strong>EDIT:</strong> I originally wrote this a few weeks ago even though it was just published today, and since then the REPL loading was fixed and so were a bunch of other bugs. In the end I've actually been using sublime text 2 for most of the small project editing, even though I liked the robustness of EclipseFP a lot.

<h2>EclipseFP</h2>

EclipseFP is where I finally hit my stride.  Almost immediately everything worked. Debugging was great, code navigation, syntax highlighting, code time errors, etc.  Unfortunately I couldn't get the hoogle panel to work but the developer was incredibly responsive and worked me through the issue (and updated the plugin to work with the new eclipse version "Kepler").  I also enjoyed the fact that working in a file auto-loaded it into GHCi REPL so I could edit then test my functions quicker.  On top of that, the developer recently submitted a pull request to the eclipse theme plugin so new dark themes will be available soon!  

One thing I do wish is that the REPL had syntax highlighting like the sublimeText REPL did, but that's OK.  

<h2>Conclusion</h2>

In the end, while I can see how people more familiar with Haskell would choose the lightweight editor route (such as sublime), people new to the language really need a way to get up and running fast.  Without that, it's easy to get turned off from trudging through and learning a new language.  A good IDE helps a user explore and automates a lot of the boring nastiness that comes with real development.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4244</wp:post_id>
		<wp:post_date><![CDATA[2013-09-30 08:00:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-30 08:00:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[time-experience-haskell-editors]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="discussion"><![CDATA[Discussion]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="post_tag" nicename="ide"><![CDATA[ide]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561721293;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4327;}i:1;a:1:{s:2:"id";i:4725;}i:2;a:1:{s:2:"id";i:4262;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>130</wp:comment_id>
			<wp:comment_author><![CDATA[felix]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[felixSchlitter@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://felixschlitter.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[125.238.182.162]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-23 06:39:25]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-23 06:39:25]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great post. I'm at the same point where you were, just looking at options and evaluating. Not too fond of the idea of using Eclipse but it may be worth while for the exploration factor. I'm using vim now but nothing really works, at least not out of the box.. Lots of fiddling]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>131</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.56.225.186]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-23 14:30:51]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-23 14:30:51]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Felix, in another post I talk about getting sublime text to work with haskell. Check that out. Since the post I've been using sublime text and I wrote a grunt scaffold that sets up a cabalalized project with unit tests too]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>130</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>132</wp:comment_id>
			<wp:comment_author><![CDATA[felix]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[felixSchlitter@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://felixschlitter.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[125.238.182.162]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-23 16:21:33]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-23 16:21:33]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Link? I cannot navigate these sorts of blogs for the life of me. Sorry for being dense.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>133</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[206.169.195.21]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-23 16:41:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-23 16:41:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[No problem! I was on my phone when I posted earlier so I didn't have the link :). Here it is http://onoffswitch.net/started-haskell/]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>132</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Till functions</title>
		<link>https://onoffswitch.net/2013/09/19/functions/</link>
		<pubDate>Thu, 19 Sep 2013 20:56:55 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4254</guid>
		<description></description>
		<content:encoded><![CDATA[Just wanted to share a couple little functions that I was playing with since it made my code terse and readable. At first I needed a way to fold a function until a predicate. This way I could stop and didn't have to continue through the whole list.  Then I needed to be able to do the same kind of thing but choosing all elements up until a predicate.

<h2>Folding</h2>

First, folding. I wanted to be able to get all the characters up until white space. For example:

[fsharp]
let (++) a b = a.ToString() + b.ToString()

let upToSpaces str = foldTill Char.IsWhiteSpace (++) &quot;&quot; str
[/fsharp]

Which led me to write the following fold function. Granted it's not lazy evaluated, but for me that was OK.

[fsharp]
let foldTill check predicate seed list= 
    let rec foldTill' acc = function
        | [] -&gt; acc
        | (h::t) -&gt; match check h with 
                        | false -&gt; foldTill' (predicate acc h) t
                        | true -&gt; acc
    foldTill' seed list
[/fsharp]


Running this gives 

[fsharp]
&gt; upToSpaces &quot;abcdef gh&quot;;;
val it : string = &quot;abcdef&quot;
[/fsharp]

Here's a more general way of doing it for sequences. Granted it has mutable state, but its hidden in the function and never leaks.  This is very similar to how fold is implemented in <a href="https://github.com/fsharp/fsharp/blob/master/src/fsharp/FSharp.Core/seq.fs#L1042">F# core</a>, I just added the extra check before it calls into the fold predicate

[fsharp]
let foldTill check predicate seed (source:seq&lt;'a&gt;) =     
    let finished = ref false     
    use e = source.GetEnumerator() 
    let mutable state = seed 
    while e.MoveNext() &amp;&amp; not !finished do
        match check e.Current with 
            | false -&gt; state &lt;- predicate state e.Current
            | true -&gt; finished := true
    state       
[/fsharp]

Anyways, fun!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4254</wp:post_id>
		<wp:post_date><![CDATA[2013-09-19 20:56:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-19 20:56:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[functions]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="folds"><![CDATA[folds]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1557131303;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4365;}i:1;a:1:{s:2:"id";i:4131;}i:2;a:1:{s:2:"id";i:4077;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>The Arrow operator</title>
		<link>https://onoffswitch.net/2013/10/14/arrow-operator/</link>
		<pubDate>Mon, 14 Oct 2013 08:00:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4262</guid>
		<description></description>
		<content:encoded><![CDATA[Continuing my journey in functional programming, I decided to try doing the <a href="http://www.haskell.org/haskellwiki/99_questions/" target="_blank" rel="noopener noreferrer">99 haskell problems</a> to wean my way into haskell.  I've found this to be a lot of fun since they give you the answers to each problem and, even though I have functional experience, the <em>haskell</em> way is sometimes very different from what I would have expected. 

For example, I discovered <a href="http://en.wikipedia.org/wiki/Arrow_(computer_science)" target="_blank" rel="noopener noreferrer">Arrows</a> via the following problem:

<blockquote>Run-length encoding of a list. Implement the so-called run-length encoding data compression method. Consecutive duplicates of elements are encoded as lists (N E) where N is the number of duplicates of the element E.

Example:

* (encode '(a a a a b c c a a d e e e e))
((4 A) (1 B) (2 C) (2 A) (1 D)(4 E))
Example in Haskell:

encode "aaaabccaadeeee"
[(4,'a'),(1,'b'),(2,'c'),(2,'a'),(1,'d'),(4,'e')]</blockquote>

My initial solution I did the way I'd probably write it in F#:

[haskell]
encode :: Eq a =&gt; [a] -&gt; [(Int, a)]
encode a = map (\x -&gt; (length x, head x)) . group $ a
[/haskell]

But, one of the alternate solutions to the problem was cleaner and used an operator I'd never seen

[haskell]
encode :: Eq a =&gt; [a] -&gt; [(Int, a)]
encode a = map (length &amp;&amp;&amp; head) . group $ a
[/haskell]

What the hell was <code>&&&</code>?  If we break down the function <code>length &&& head</code> a little we'll see it has a signature of 

[haskell]
(length &amp;&amp;&amp; head) :: [a] -&gt; (Int, a)
[/haskell]

The function takes a list and will apply the first function to the list as the first element of the tuple (length) and then apply the second function to the list and that'll give you the second element of the tuple.  

Turns out this is part of the <code>Control.Arrow</code> module and defines an interesting way to combine logical steps using tuples and several basic transformation functions. There is a great intro to arrows on the <a href="http://www.haskell.org/haskellwiki/Arrow_tutorial" target="_blank" rel="noopener noreferrer">haskell wiki</a> which I followed and ended up simulating in F#.  

<h2>Translating to F#</h2>

A quick translation attempt turned out the following small module

[fsharp]
module Arrow =     
    let split x = (x, x)
    let combine f (x, y) = f x y
    let first f (a, b) = (f a, b)
    let second f (a, b) = (a, f b)
    let onTuple f g = first f &gt;&gt; second g
    let onSingle f g =  split &gt;&gt; (onTuple f g)
    let (.***.) = onTuple
    let (.&amp;&amp;&amp;.) = onSingle
    let onSingleCombine op f g = (onSingle f g) &gt;&gt; combine op
[/fsharp]

To break down what's going on here, we can follow an example posted in the haskell wiki. 

[fsharp]
let div2 x = x / 2
let m3p1 x = 3*x + 1

let example =  onSingleCombine (+) div2 m3p1 8
[/fsharp]

What this is really doing is:

<ul>
<li>Split 8 into (8, 8)</li>
<li>Apply the first function to the first element (<code>div2 8</code>) with a resulting tuple of (4, 8)</li>
<li>Apply the second function to the second element (<code>m3p1 8</code>) with a resulting tuple of (4, 25)</li>
<li>Apply the combiner <code>(+)</code> to the resulting tuple which gives the answer of <code>29</code></li>
</ul>

While I think the example is a little contrived, it's really cool how you can leverage arrows to do this kind of sequencing work.  I'll certainly find use for this!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4262</wp:post_id>
		<wp:post_date><![CDATA[2013-10-14 08:00:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-10-14 08:00:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[arrow-operator]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="arrows"><![CDATA[Arrows]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561173373;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4725;}i:1;a:1:{s:2:"id";i:4320;}i:2;a:1:{s:2:"id";i:4348;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>134</wp:comment_id>
			<wp:comment_author><![CDATA[Gusty]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[gustavo.perezleon@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[212.249.12.50]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-11-06 12:39:17]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-11-06 12:39:17]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Arrows are also implemented in FsControl, you can play with them in <a href="https://github.com/gmpl/FSharpPlus" title="F#+" rel="nofollow">]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>135</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.243.58.180]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2013-11-06 13:24:15]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2013-11-06 13:24:15]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Neat, I think there is also an arrow implementation in fsharpx]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>134</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Machine Learning with disaster video posted</title>
		<link>https://onoffswitch.net/2013/09/25/machine-learning-disaster-video-posted/</link>
		<pubDate>Wed, 25 Sep 2013 22:06:39 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4275</guid>
		<description></description>
		<content:encoded><![CDATA[A few weeks ago we had our second <a href="http://www.meetup.com/DC-fsharp/" target="_blank" rel="noopener noreferrer">DC F# meetup</a> with speaker <a href="http://trelford.com/blog/" target="_blank" rel="noopener noreferrer">Phil Trelford</a> where he led a hands on session introducing decision trees.  The goal of meetup was to see how good of a predictor we could make of who would live and die on the titanic.  <a href="http://www.kaggle.com/c/titanic-gettingStarted" target="_blank" rel="noopener noreferrer">Kaggle</a> has an excellent data set that shows age, sex, ticket price, cabin number, class, and a bunch of other useful features describing Titanic passengers.  

Phil followed <a href="www.clear-lines.com/blog/" target="_blank" rel="noopener noreferrer">Mathias</a>' format and had an excellent .fsx script that walked everyone through it.  I think the best predictor that someone made was close to 84%, though it  was surprisingly difficult to exceed that in the short period of time that we had to work on it.  I'd implemented my own shannon entropy based ID3 decision tree in C# so this wasn't my first foray into decision tree's, but the compactness of the tree in F# was great to see.  On top of that Phil extended the tree to test not just features, but also combinations of features by having the tree accept functions describing features.  This was cool and something I hadn't thought of. By the end you sort of had built out a small DSL describing the feature relationships of what you were trying to test. I like it when a problem domain devolves into a series of small DSL like functions!

If anyone is interested Phil let us post all of his slides and information on our <a href="https://github.com/DCFsharp/Machine-Learning-From-Disaster" target="_blank" rel="noopener noreferrer">github</a>. Anyways, here is the video of the session! 

<iframe width="420" height="315" src="//www.youtube.com/embed/kh9WjKAG4Jk" frameborder="0" allowfullscreen></iframe>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4275</wp:post_id>
		<wp:post_date><![CDATA[2013-09-25 22:06:39]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-09-25 22:06:39]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[machine-learning-disaster-video-posted]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="machine-learing"><![CDATA[machine learing]]></category>
		<category domain="post_tag" nicename="meetup"><![CDATA[meetup]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560307503;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4209;}i:1;a:1:{s:2:"id";i:4170;}i:2;a:1:{s:2:"id";i:3847;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Reading socket commands</title>
		<link>https://onoffswitch.net/2013/10/29/reading-socket-commands/</link>
		<pubDate>Tue, 29 Oct 2013 17:00:43 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4286</guid>
		<description></description>
		<content:encoded><![CDATA[A few weeks ago I was working on a sample application that would simulate a complex state machine. The idea is that there is one control room, and many slave rooms, where each slave room has its own state.  The control room can dispatch a state advance or state reverse to any room or collection of rooms, as well as query room states, and other room metadata.

But to do this I need a way to get commands from the control room in order to know what to do.  In my application clients were connected via tcp sockets and I wanted commands to be newline seperated. This made it easy to test out via a local telnet (I didn't need to design any binary protocol). 

<h2>The socket</h2>

You can never assume you've read what you want off a socket, since you're only ever guaranteed 1 or more bytes when a read succeeds.  This means you need to continue to read until you've read however much you expected.

[fsharp]
/// Listens on a tcp client and returns a seq&lt;byte[]&gt; of all
/// found data
let rec private listenOnClient (client:TcpClient) = 
    seq {            
        let stream = client.GetStream()

        let bytes = Array.create 4096 (byte 0)
        let read = stream.Read(bytes, 0, 4096)
        if read &gt; 0 then 
            yield bytes.[0..read - 1]
        yield! listenOnClient client
    }

[/fsharp]

This function yields a seq of byte arrays each time the socket succeeds in a read.  I'm reading only up to a 4096 buffer and leveraging F# array slicing to return the bytes that were actually read.  After a read, the function calls itself and continues to yield byte arrays forever.

<h2>Converting byte arrays to strings</h2>

The next step is taking those byte arrays and creating statements out of them. This means piecing them together and determining where newlines are. For example, if you read packets like 

[code]
Th
is is a comm
an
d\n
[/code]

It should really be handled like

[code]
This is a command\n
[/code]

To do this, I first map the bytes to utf8 strings, and use a string builder to aggregate lines.  By using the string split function, I can tell (by empty entries) where newlines appeared, and whether or not a final terminating newline exists.  For any statements that are terminated by a newline I can yield the entire command.

[fsharp]
/// Reads off the client socket and aggregates commands that are seperated by newlines
let packets (client:TcpClient) : seq&lt;string&gt; = 
    let filterEmpty =  Seq.filter ((&lt;&gt;) String.Empty)
    seq {
            let builder = new StringBuilder()
            for str in client |&gt; listenOnClient |&gt; Seq.map System.Text.ASCIIEncoding.UTF8.GetString do

                let wordsWithBlanks = (builder.ToString() + str).Split([|'\r'; '\n'|]) 
            
                builder.Clear() |&gt; ignore

                // this means we got a newline following the last string so we have a 
                // group of totally valid commands
                if Seq.last wordsWithBlanks = String.Empty then
                    for entry in wordsWithBlanks |&gt; filterEmpty do yield entry
                else
                    // we didn't get a complete final command, so process all the other ones
                    let nonEmpties = wordsWithBlanks |&gt; filterEmpty

                    builder.Append (Seq.last nonEmpties) |&gt; ignore
                    
                    for entry in (Seq.take (Seq.length nonEmpties - 1) nonEmpties) do 
                        yield entry
    }
[/fsharp]

<h2>Listening for commands</h2>

Now it's easy to leverage this function

[fsharp highlight="8"]
let rec private listenForControlCommands (agentRepo:AgentRepo) client =         
    async {
        let postFlip mailbox msg = post msg mailbox
        let postToControl = postFlip agentRepo.Control

        do! Async.SwitchToNewThread() 
        try
            for message in client |&gt; packets do                                       
                match message with                         
                    | AdvanceCmd roomNum        -&gt; postToControl &lt;| ControlInterfaceMsg.Advance roomNum
                    | ReverseCmd roomNum        -&gt; postToControl &lt;| ControlInterfaceMsg.Reverse roomNum                                 
                    | StartPreview roomNum      -&gt; postToControl &lt;| ControlInterfaceMsg.StartPreview roomNum
                    | StartStreaming roomNum    -&gt; postToControl &lt;| ControlInterfaceMsg.StartStreaming roomNum
                    | Record roomNum            -&gt; postToControl &lt;| ControlInterfaceMsg.Record roomNum
                    | ResetRoom roomNum         -&gt; postToControl &lt;| ControlInterfaceMsg.Reset roomNum
                    | QueryRoom roomNum         -&gt; do! agentRepo |&gt; queryRoom roomNum client
                    | _                         -&gt; postToControl &lt;| ControlInterfaceMsg.Broadcast (&quot;Unknown control sequence &quot; + message)
        with
            | exn -&gt; postToControl (ControlInterfaceMsg.Disconnect client)
    }
[/fsharp]

Where the messages are matched with active patterns that parse the strings such as

[fsharp]
let (|AdvanceCmd|_|) (str:string) = 
    if str.StartsWith(&quot;advance &quot;) then 
        str.Replace(&quot;advance &quot;,&quot;&quot;).Trim() |&gt; Convert.ToInt32 |&gt; Some
    else None
[/fsharp]

The great thing about this is you hide all the string handling and deal only with strongly typed, high level patterns.  Adding new commands is just a matter of creating a new active pattern and updating the message match in the <code>listenForControlCommands</code> function.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4286</wp:post_id>
		<wp:post_date><![CDATA[2013-10-29 17:00:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-10-29 17:00:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[reading-socket-commands]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="active-patterns"><![CDATA[active patterns]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="sockets"><![CDATA[Sockets]]></category>
		<category domain="post_tag" nicename="tcp"><![CDATA[TCP]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561407950;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1268;}i:1;a:1:{s:2:"id";i:1587;}i:2;a:1:{s:2:"id";i:4737;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Checking if a socket is connected</title>
		<link>https://onoffswitch.net/2013/12/16/checking-socket-connected/</link>
		<pubDate>Mon, 16 Dec 2013 08:00:29 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4296</guid>
		<description></description>
		<content:encoded><![CDATA[Testing if a socket is still open isn't as easy at it sounds.  Anyone who has ever dealt with socket programming knows this is hassle.  The general pattern is to poll on the socket to see if its still available, usually by sitting in an infinite loop. However, with f# this can be done more elegantly using async and some decoupled functions.

First lets write an async function that monitors a socket and returns true if its connected or false if its not.  The polling interval is set to 10 microseconds. This is because if you set the interval to a negative integer (representing to wait indefinitely for a response), it won't return until data is written to the socket. If, however, you have a very short poll time, you will be able to detect when the socket is closed without having to write data to it.

[fsharp]
/// Async worker to say whether a socket is connected or not
let isConnected (client:TcpClient) =         
    async { 
        return  
            try
                if client.Client.Poll(10, SelectMode.SelectWrite) &amp;&amp; not &lt;| client.Client.Poll(10, SelectMode.SelectError)  then
                    let checkConn = Array.create 1 (byte 0)
                    if client.Client.Receive(checkConn, SocketFlags.Peek) = 0 then
                        false
                    else 
                        true
                else
                    false
            with
                | exn -&gt; false
    }
[/fsharp]

Next we can create a simple monitor function to check on a predicate, and if the predicate returns false (i.e not connected) it'll execute an action. Otherwise it'll call itself and monitor the socket again.  This is important since the poll will exit once it determines that the socket is connected or not. 

[fsharp]
let rec monitor predicate onAction client  = 
    async {
        let! isConnected = predicate client
        if not isConnected then
            onAction client
        else 
            return! monitor predicate onAction client
    }
[/fsharp]

Then, to use the monitor function all you have to do is something like this

[fsharp]
monitor isConnected onDisconnect client |&gt; Async.Start
[/fsharp]

Where <code>monitor</code> is the generic async monitor function, <code>isConnected</code> is the async socket poll, <code>onDisconnect</code> is a function to call when a client disconnects, and <code>client</code> is a tcpClient socket connection.  The great thing about this is that you don't need to have a seperate thread for each open connection to act as a monitor/handler.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4296</wp:post_id>
		<wp:post_date><![CDATA[2013-12-16 08:00:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-12-16 08:00:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[checking-socket-connected]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="sockets"><![CDATA[Sockets]]></category>
		<category domain="post_tag" nicename="tcp"><![CDATA[TCP]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561771227;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4286;}i:1;a:1:{s:2:"id";i:1268;}i:2;a:1:{s:2:"id";i:1587;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>136</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #4, 2014 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2014/01/26/f-weekly-4-2014/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.38.21]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-26 20:49:04]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-26 20:49:04]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp wrote &#8220;Checking if a socket is connected&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>24 hour time ranges</title>
		<link>https://onoffswitch.net/2013/12/02/24-hour-time-ranges/</link>
		<pubDate>Mon, 02 Dec 2013 08:00:23 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4302</guid>
		<description></description>
		<content:encoded><![CDATA[Dealing with time is hard, it's really easy to make a mistake.  Whenever I'm faced with a problem that deals with time I tend to spend an inordinate amount of time making sure I'm doing things right.  

Today I ran into a situation where I needed to be able to calculate durations and ranges from the current time compared to 24 hour block time.  The current time however has the full date, but the 24 hour times are just relative.  For example, if the current time is 17:00, and the range is 15:00 to 1:00, then I want to say that the current time is within the range.  Also, lets say I have the current time is 17:00 but my range is 1:00 to 5:00. I want to know how far it is from now to the start of the 24 hour range.  The ranges though, don't have date information, it's just generic time.  

It took a bit of thinking but here is what I got. First, checking if a time is in the 24 hour range.  Here we need to know what kind of range boundaries we have. The first check checks a normal boundary, where the start time is less than the end time. If that's the case then we can do a pretty easy range check.  The second case checks if the range is an overnight boundary condition. In that case it needs to know if the current time is greater than the start OR if the current time is less than the end. But that OR can only work if the range is in overnight mode.

[csharp]
/// &lt;summary&gt;
/// Checks if the current time falls within a 24 hour range
/// the date/year/month etc of the comparison dates WILL not be checked
/// only the TimeOfDay is checked.
/// 
/// For example if the time is 17:00, and we check if we are in the range of
/// 15:00 and 1:00 then the return will be true.
/// &lt;/summary&gt;
/// &lt;param name=&quot;time&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;dtStart&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;dtEnd&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static bool IsIn24HourRange(this DateTime time, DateTime dtStart, DateTime dtEnd)
{
    if (dtStart.TimeOfDay &lt; dtEnd.TimeOfDay &amp;&amp; time.TimeOfDay &lt; dtEnd.TimeOfDay &amp;&amp; time.TimeOfDay &gt; dtStart.TimeOfDay)
    {
        return true;
    }
    
    if (dtStart.TimeOfDay &gt; dtEnd.TimeOfDay &amp;&amp; (time.TimeOfDay &lt; dtEnd.TimeOfDay || time.TimeOfDay &gt; dtStart.TimeOfDay))
    {
        return true;
    }

    return false;
}

[/csharp]

To be paranoid, here is a unit test for it

[csharp]
[TestCase(15, 3, true)]
[TestCase(15, 16, false)]
[TestCase(2, 1, true)]
[TestCase(1, 2, false)]
public void TestIsIn24HourRange(int startHour, int endHour, bool valid)
{
    var dtStart = new DateTime(1, 1, 1, startHour, 0, 0);
    var dtEnd = new DateTime(1, 1, 1, endHour, 0, 0);

    var n = new DateTime(1999, 12, 9, 17, 0, 29);

    Assert.True(n.IsIn24HourRange(dtStart, dtEnd) == valid);
}
[/csharp]

Next up is calculating the time offset from one of these generic times.  Since the time that is passed in has no relevant date information, you can't just do a simple subtraction on the times.  You first have to normalize the time to be relative to the date.

[csharp]
/// &lt;summary&gt;
/// Determines the time range from the time to the the 24 hour time.
/// 
/// For example, if now is 17:00, and the end time is passed in (regardless of date)
/// to be 2:00, then the duration will be 540 minutes.  If the time is now 17:00 and the
/// passed in time is 18:00, the duration will be 60 minutes.
/// &lt;/summary&gt;
/// &lt;param name=&quot;time&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;end&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static TimeSpan DurationFrom24HourRange(this DateTime time, DateTime end)
{
    var normalizedTime = new DateTime(time.Ticks).Trim(TimeSpan.TicksPerDay).Add(end.TimeOfDay);

    if (time.TimeOfDay &gt; end.TimeOfDay)
    {
        var newTime = normalizedTime.AddDays(1);

        return newTime - time;
    }

    return normalizedTime - time;
}
[/csharp]

The trim function can truncate a date to different granularities:

[csharp]
/// &lt;summary&gt;
/// Usage: 
/// DateTime.Now.Trim(TimeSpan.TicksPerDay));
/// DateTime.Now.Trim(TimeSpan.TicksPerHour));
/// DateTime.Now.Trim(TimeSpan.TicksPerMillisecond));
/// DateTime.Now.Trim(TimeSpan.TicksPerMinute));
/// DateTime.Now.Trim(TimeSpan.TicksPerSecond));
/// &lt;/summary&gt;
/// &lt;param name=&quot;date&quot;&gt;&lt;/param&gt;
/// &lt;param name=&quot;roundTicks&quot;&gt;&lt;/param&gt;
/// &lt;returns&gt;&lt;/returns&gt;
public static DateTime Trim(this DateTime date, long roundTicks)
{
    return new DateTime(date.Ticks - date.Ticks % roundTicks);
}
[/csharp]

Again this involves an overnight boundary check. If the current time is greater than the passed in time, then it means the passed in time is in the next day. At that point we need to just add a day to the truncated (normalized) date and perform a timespan difference.  Otherwise, it's all part of the current day and we can do a regular difference.

As usual, here's the unit test

[csharp]
[TestCase(18, 60)]
[TestCase(2, 540)]
[TestCase(17, 0)]
[TestCase(0, 420)]
public void DurationFrom24Range(int startHour, int totalMinutes)
{
    var dtStart = new DateTime(1, 1, 1, startHour, 0, 0);

    var time = TimeSpan.FromMinutes(totalMinutes);

    var n = new DateTime(1999, 12, 9, 17, 0, 0);

    Assert.True(n.DurationFrom24HourRange(dtStart) == time);
}
[/csharp]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4302</wp:post_id>
		<wp:post_date><![CDATA[2013-12-02 08:00:23]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-12-02 08:00:23]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[24-hour-time-ranges]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="date"><![CDATA[date]]></category>
		<category domain="post_tag" nicename="time"><![CDATA[time]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560668063;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3128;}i:1;a:1:{s:2:"id";i:3803;}i:2;a:1:{s:2:"id";i:2274;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Java lambdas</title>
		<link>https://onoffswitch.net/2013/11/18/java-lambdas/</link>
		<pubDate>Mon, 18 Nov 2013 08:00:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4306</guid>
		<description></description>
		<content:encoded><![CDATA[I'm not a java person. I've never used it in production, nor have I spent any real time with it outside of my professional work.  However, when a language dawns upon lambdas I am drawn to try out their implementation.  I've long since despised Java for the reasons of verbosity, lack of real closures or events, type erasure in generics, and an over obsession with anonymous classes, so I've shied away from doing anything in it. 

Still, at least the Java world is trying.  While I'd love to just ignore the fact that Java exists, I can't.  Lots of interesting projects are done in Java, and a lot of world class tools are written in Java, so from a professional standpoint it'd be good for me to know it.

In the past when I looked into how past Java utilities did "functional" it never felt natural to me.  People have suggested <a href="https://code.google.com/p/lambdaj/" target="_blank" rel="noopener noreferrer">LambdaJ</a> or google's <a href="https://code.google.com/p/guava-libraries/" target="_blank" rel="noopener noreferrer">Guava</a>, but Guava even goes so far to say to <em>not</em> use their functional approach.  LambdaJ's is just as verbose as anything else, and in benchmarks its shown to be at least 2 times slower! Not a good sign.

But the coming of Java 8 I think a lot of these problems will be solved.  Not to mention I'm sure that projects such as <a href="https://github.com/Netflix/RxJava" target="_blank" rel="noopener noreferrer">RxJava</a> are ecstatically waiting for this since it will make their (and any other reactive users) lives a whooole lot better.

Anyways, I present to you my first Java program in 5 years:

[java]
import java.util.List;
import java.util.concurrent.ExecutionException;

import static java.util.Arrays.asList;

public class Main{
    public static void main(String[] arsg) throws InterruptedException, ExecutionException {
        List&lt;String&gt; strings = asList(&quot;foo&quot;, &quot;bar&quot;, &quot;baz&quot;);

        strings.forEach(System.out::println);

        ThreadUtil.queueToPool(() -&gt; {
            System.out.println(&quot;In the damn thread&quot;);

            return &quot;foo&quot;;
        }).get();

        System.out.println(&quot;done&quot;);
    }
}
[/java]

[java]
import java.util.concurrent.Callable;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;

public class ThreadUtil{
    private static final ExecutorService _threadPoolExecutor = Executors.newCachedThreadPool();

    public static Thread run(Runnable r){
        Thread thread = new Thread(r);

        r.run();

        return thread;
    }

    public static Future&lt;?&gt; queueToPool(Callable&lt;?&gt; r){
        return _threadPoolExecutor.submit(r);
    }
}
[/java]

The program does nothing useful. I just wanted to see what it was like to write some java lambda code, and I was pleasantly surprised.  Even though lambda's in Java 8 aren't actually first class functions, but are in fact wrappers on interfaces that are tagged with the <code>@FunctionalInterface</code> attribute. For example, look at <code>Supplier<T></code> (which is analgous to <code>Action<T></code> in C#, i.e a function with no arguments that when executed returns a value)

[java]
@FunctionalInterface
public interface Supplier&lt;T&gt; {

    /**
     * Gets a result.
     *
     * @return a result
     */
    T get();
}
[/java]

Java's lambda magic looks to rely on the fact that if an interface has the attribute, then it can be auto converted into a lambda as long as there is only one function that is not implemented.  You could, however, treat the interface the old fashioned java way: create an anonymous class that implements <code>get</code> and execute .get().  Still, why would you want to?  

To demonstrate the interface to function mapping you can see both the <em>new way</em> and the <em>old way</em> of doing the same things

[java]
Supplier&lt;String&gt; newWay = () -&gt; &quot;new way&quot;;        
                                                  
Supplier&lt;String&gt; oldWay = new Supplier&lt;String&gt;() {
    @Override                                     
    public String get() {                         
        return &quot;old way&quot;;                         
    }                                             
};                                                                                           
[/java]

My only gripe here is that the supplier is still an interface. It's not a first class function, meaning I can't just execute <code>newWay()</code>. I have to do <code>newWay.get()</code> which seems stupid at first. But, there is a reason.

The reason is that you can now have <code>default</code> implementation in interfaces, meaning that you can create an interface instance that has a bunch of stuff defined but create one off lambda overrides of another method. This is pretty neat. Look at this example:

[java]
@FunctionalInterface
public interface TestMethods{
    public void doWork();

    default void doOtherWork(){
        System.out.println(&quot;do other work default method&quot;);
    }
}
[/java]

Now I can either do the old fashioned way (creating an anonymous class and implementing <code>doWork</code>) or I can create an instance that is assigned a lambda, which does the same thing:

[java]
TestMethods m = () -&gt; System.out.println(&quot;creating the do work method at instantation&quot;);  
                                                            
m.doWork();                                                 
m.doOtherWork();                                            
[/java]

If you have more than one undefined method in a functional interface the compiler will bitch at you, rightfully so.

Anyways, it looks like the Java world is finally growing up!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4306</wp:post_id>
		<wp:post_date><![CDATA[2013-11-18 08:00:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-11-18 08:00:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[java-lambdas]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="lambdas"><![CDATA[lambdas]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560197513;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4627;}i:1;a:1:{s:2:"id";i:4596;}i:2;a:1:{s:2:"id";i:4316;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Building LINQ in Java</title>
		<link>https://onoffswitch.net/2013/12/30/building-linq-java/</link>
		<pubDate>Mon, 30 Dec 2013 08:00:34 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4316</guid>
		<description></description>
		<content:encoded><![CDATA[Now that Java 8 has lambdas, I decided to check out what kind of lazy collection support their streams functionality had.  It had some cool stuff, like 

<ul>
<li>map</li>
<li>filter</li>
<li>flatMap</li>
<li>distinct</li>
<li>sorted</li>
<li>limit (i.e. take)</li>
<li>skip</li>
<li>reduce (i.e fold)</li>
<li>min</li>
<li>max</li>
<li>any</li>
<li>all</li>
<li>generate (for building infinite lists)</li>
</ul>

Not too bad.  But, there are some problems, as far as I see it.  First, you can't extend it. With .NET they solved the lambda problem with extension methods, but Java oddly chose "defender" methods, which lets you safely update an interface declaration, but ONLY the author can do it, not any consumer.  That sucks.  I want other fun functions like 

<ul>
<li>first</li>
<li>last</li>
<li>nth</li>
<li>windowed</li>
<li>intersperse</li>
<li>intercalate</li>
<li>tails</li>
<li>intersect</li>
<li>zip</li>
<li>groupRuns</li>
</ul>

And pretty much any other F#/Haskell list function. Why not? If we're going to go with lazy evaluated lists we might as well have all the fun that comes with them.  There is another problem with the Java 8 streams API, unless I'm mistaken, there is no way to <em>end</em> an infinite stream generated using the <code>generate</code> function.  That really sucks since not every lazy generated stream is infinite. 

So, I decided to try my hand and rebuilding LINQ in Java 8.  For the impatient, the full source, tests, and benchmarks are available at my <a href="https://github.com/devshorts/JEnumerable" target="_blank" rel="noopener noreferrer">github</a>.

<h2>Iterator chains</h2>

The basic idea here is to create an iterator for each type of processing you want to do. If you want to do a map function, you should create an iterator that wraps a source. When the iterator consumes from the underlying source it will emit a projected element.  How do you do this? Well, you can use a fluent API that returns a new enumerable class that wraps a specific iterator.  For example

[java]
private Function&lt;Iterable&lt;TSource&gt;, Iterator&lt;TSource&gt;&gt; iteratorGenerator;

public static &lt;TSource&gt; Enumerable&lt;TSource&gt; init(Iterable&lt;TSource&gt; source){
    return new Enumerable&lt;&gt;(_ig -&gt; new EnumerableIterator&lt;&gt;(source));      
}                                                                          

protected Enumerable(Function&lt;Iterable&lt;TSource&gt;, Iterator&lt;TSource&gt;&gt; iteratorGenerator) {
    this.iteratorGenerator = iteratorGenerator;                                         
}                                                                                       

@Override                                
public Iterator&lt;TSource&gt; iterator() {    
    return iteratorGenerator.apply(this);
}                                        
       
// The underlying iterator

public class EnumerableIterator&lt;TSource&gt; implements Iterator&lt;TSource&gt; {
    protected Iterator&lt;TSource&gt; source;
    private Iterable&lt;TSource&gt; input;

    public EnumerableIterator(Iterable&lt;TSource&gt; input){
        this.input = input;

        reset();
    }

    protected void reset(){
        source = input.iterator();
    }

    @Override
    public boolean hasNext() {
        return source.hasNext();
    }

    @Override
    public TSource next() {
        return (TSource)source.next();
    }
}                                  
[/java]

Lets first look at the underlying iterator. It does nothing other than iterate over the source. That's pretty simple.  

The thing that wraps it is a <code>Enumerable</code> class that takes a function that, when given an Iterable, returns a new iterator.  Then it just exposes that iterator.  In general, thats the whole thing.  The iterator is wrapped in a function so that we can request a new iterator each time someone tries to iterate over this enumerable. That matches what .NET does; if someone tries to re-iterate an enumerable you get a new iterator and start over. 

<h2>Take</h2>

Let's look at a simple iterator that takes only N elements.

[java]
public class TakeIterator&lt;TSource&gt; extends EnumerableIterator&lt;TSource&gt; {
    private int takeNum;

    public TakeIterator(Iterable&lt;TSource&gt; results, int n) {
        super(results);
        takeNum = n;
    }

    @Override
    public boolean hasNext() {
        return source.hasNext() &amp;&amp; takeNum &gt; 0;
    }

    @Override
    public TSource next(){
        takeNum--;
        return source.next();
    }
}
[/java]

And to create an instance of enumerable that uses this

[java]
public Enumerable&lt;TSource&gt; take(int n){                                    
    return enumerableWithIterator(source -&gt; new TakeIterator&lt;&gt;(source, n));
}                                                                          

private &lt;TResult&gt; Enumerable&lt;TResult&gt; enumerableWithIterator(Function&lt;Iterable&lt;TSource&gt;, Iterator&lt;TResult&gt;&gt; generator){
    return new Enumerable&lt;&gt;(_ig -&gt; generator.apply(this));                                                             
}                                                                                                                      
[/java]

Basically I return a new enumerable with a lazy evaluated function that gives the iterator its underlying Iterator source.  By returning a new enumerable each time we can effectively chain the iterators together. Nothing is evaluated until someone tries to get the next value.  

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4316</wp:post_id>
		<wp:post_date><![CDATA[2013-12-30 08:00:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-12-30 08:00:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-linq-java]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="iterator"><![CDATA[iterator]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="lambda"><![CDATA[lambda]]></category>
		<category domain="post_tag" nicename="lazy"><![CDATA[lazy]]></category>
		<category domain="post_tag" nicename="linq"><![CDATA[linq]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559686498;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4306;}i:1;a:1:{s:2:"id";i:4355;}i:2;a:1:{s:2:"id";i:4862;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>137</wp:comment_id>
			<wp:comment_author><![CDATA[Building LINQ in Java pt 2 | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/building-linq-java-pt-2/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-24 08:01:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-24 08:01:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] my last post I discussed building a static class that worked as the fluent interface exposing different iterator [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>F# utilities in haskell</title>
		<link>https://onoffswitch.net/2013/12/02/f-utilities-haskell/</link>
		<pubDate>Mon, 02 Dec 2013 19:52:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4320</guid>
		<description></description>
		<content:encoded><![CDATA[Slowly I am getting more familiar with Haskell, but there are some things that really irk me.  For example, a lot of the point free functions are right to left, instead of left to right. Coming from an F# background this drives me nuts. I want to see what happens first <em>first</em> not <em>last</em>. 

For example, if we wanted to do <code>(x+2)+3</code> in f#

[fsharp]
let chained = (+) 2 &gt;&gt; (+) 3
[/fsharp]

Compare to haskell:

[haskell]
chained :: Integer -&gt; Integer
chained = (+3) . (+2) 
[/haskell]

In haskell, the +2 is given the argument 3, then that value is given to +3. In f# you work left to right, which I think is more readable.

Anyways, this is an easy problem to solve by defining a cusotm infix operator

[haskell]
(&gt;&gt;&gt;) :: (a -&gt; b) -&gt; (b -&gt; c) -&gt; (a -&gt; c)
(&gt;&gt;&gt;) a b = b . a
[/haskell]

Now we can do the same combinations as in F#.

Another thing that bugs me is pipe operator in haskell. I want to be able to pipe using the <code>|></code> operator left to right (as in subject followed by verb) instead of the way haskell does it with <code>$</code>which is verb followed by subject.  

Again, easy fix though

[haskell]
(|&gt;) :: t -&gt; (t -&gt; b) -&gt; b
(|&gt;) a b = b $ a
[/haskell]

Now we can do <code>1 |> (+1)</code> and get <code>2</code>. Fun!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4320</wp:post_id>
		<wp:post_date><![CDATA[2013-12-02 19:52:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-12-02 19:52:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[f-utilities-haskell]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="utilities"><![CDATA[Utilities]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554338514;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4262;}i:1;a:1:{s:2:"id";i:4244;}i:2;a:1:{s:2:"id";i:4725;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>138</wp:comment_id>
			<wp:comment_author><![CDATA[Bartosz Milewski]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[bartosz@relisoft.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://BartoszMilewski.com</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[98.232.93.65]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-01-12 22:57:12]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-01-12 22:57:12]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Elm, which is an FRP version of Haskell also uses the |&gt; operator for piping. But the way to get used to the dot operator's direction is to read it as "after." 

    chained = (+3) . (+2) 

would be read as "add 3 after adding 2."]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>139</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[jpaugh@gmx.us]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[216.96.4.117]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-10-08 20:34:35]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-10-08 20:34:35]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Haskell's style is based on function composition in mathematics. Needless to say, there are strong reasons to keep that convention.

However, when you're programming with monads, Haskell allows monadic composition in both directions with (&gt;&gt;=) and (=&lt;&lt;).]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Getting started with haskell</title>
		<link>https://onoffswitch.net/2014/01/13/started-haskell/</link>
		<pubDate>Mon, 13 Jan 2014 08:00:10 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4327</guid>
		<description></description>
		<content:encoded><![CDATA[I wanted to share how I've finally settled on my haskell development environment and how I got it set up, since the process in the end wasn't that trivial.   Hopefully anyone else starting in haskell can avoid the annoyances and pitfalls that I ran into and get up and running (and doing haskell) quickly.

<h2>Getting haskell</h2>

First, download the haskell platform from <a href="http://www.haskell.org/platform/">haskell.org</a>.  This is pretty easy. At this point you should have <code>ghc</code> (the compiler) and <code>ghci</code> (the interactive REPL) installed and in your path.

Aslo at this point you should have <code>cabal</code> installed. Cabal is haskells package manager. It's like ruby gems, or .NET nuget, or node's NPM (gah, so many!).  

<h2>Get sublime text</h2>

As much of a visual studio fanboy that I am, I have to say that using sublimetext for haskell has turned out to be really nice.  Most of the haskellers I asked on twitter also use sublime text so it has a very supportive and active community.  If you have sublime text already, install the <a href="https://github.com/SublimeHaskell/SublimeHaskell" target="_blank" rel="noopener noreferrer">sublime haskell plugin</a> via sublimes package manager.  

SublimeHaskell leverages command line haskell tools, like <code>hdevtools</code> and <code>ghc-mod</code> to give you language completion, documentation, type inference (and type completion). I highly recommend these, since it makes development a lot easier.  

On windows you may run into issues that you can't install the <code>unix-2.7</code> package. That's OK.  You don't need to install it from cygwin, contrary to lots of stack overflow answers.  Instead, go to the following <a href="https://github.com/mvoidex/hdevtools" target="_blank" rel="noopener noreferrer">fork</a> of the <code>hdevtools</code> project. Go to the project folder and do

[code]
&gt; cabal configure
&gt; cabal build
&gt; cabal install
[/code]

Basically this just builds the project from source, then installs it into your cabal package path (which is for your user).  Hdevtools gives access to type information for files on the command line, so it's really important to get this working, otherwise your type inference in sublimetext won't work.

<h2>Get sublime load file to REPL plugin</h2>

This <a href="https://github.com/laughedelic/LoadFileToRepl" target="_blank" rel="noopener noreferrer">plugin</a> is really handy, it will auto load your module file into the REPL.  

<h2>If you have problems...</h2>

I had some issues getting all this to work on windows. Cabal would fail trying to install certain packages giving gzip errors. If you run into that problem, chances are you have conflicting mingw/cygwin or gnu utils in your path. I suggest removing them and getting the official gnu32 utils (which solves this problem). More info can be found at this <a href="http://stackoverflow.com/questions/7523151/hoogle-data-on-windows" target="_blank" rel="noopener noreferrer">stack overflow question</a>.

I also had issues where I had too many things in my path and the haskell installer (when adding itself to the path) pushed out some path items. Windows has a path length limit so if things get weird, just check that what you <em>think</em> should be there actually is.

<h2>Create a project</h2>

Creating a haskell project, for a novice, isn't trivial. You need to initialize a project directory with <code>cabal</code>, configure a bunch of crap in the <code>project.cabal</code> file, and then re-configure cabal. If you want to have unit testing set up with auto-discoverable tests (like you can do in Java or C# with test attributes), you have to one step further and create a seperate test runner and add a bunch of magic haskell preprocessor tags.  I can never remember how to do any of this and I find it endlessly frustrating every time I have to make a new project.  Some IDE's like EclipseFP automatically set all this up for you, but if you go the sublime route you're on your own.  

However, I took the time to write a <a href="https://github.com/devshorts/grunt-init-haskell-test" target="_blank" rel="noopener noreferrer">grunt scaffolding task</a> that will automatically create your cabalized project for you, AND set up your unit testing infrastructure so you don't have to think about it. For those not familiar, grunt is a javascript build/automation tool that leverages node.  I went with grunt to write the scaffolding since I wanted to try something new, and it looked easy (I'm a sucker for easy things).  

Once you check out the project and put it in your grunt home directory, you can type <code>grunt-init haskell-test</code> and it'll prompt you for a project name, create a cabal file and project folders (with src and test directories), create a test runner, and create an initial unit testing file with a sample test in it.  After that it'll automatically run <code>cabal configure --enable-tests</code> and you are good to go! To compile it type <code>cabal build</code> and to run it type <code>cabal test</code> (or sublime text will do this all for you since the sublime plugin works with cabal created projects).

<h2>Understanding the cabal file</h2>

A few things that got me when starting with haskell is understanding the cabal file. The cabal file is like a <code>.proj</code> file for f# or c#, or your maven <code>pom.xml</code> for Java.  It describes dependencies, where the source directories are, etc.  When you add dependencies to new libraries you'll need to update this file.  

<h2>Adding new tests</h2>

Unfortunately the grunt task I wrote only initializes the scaffold. It won't set up boilerpate for new unit tests. However, at this point just copy the unit test file you have, update the unit test main (<code>TestMain.hs</code>) to import the new unit test fixture module, and everything will be cool.  Note, using HTF there are some conditions.  Functions that are prefixed with <code>test_</code> must be of type <code>Assertion</code>, there needs to be a special preprocessor macro at the top of the file, quickcheck functions are prefixed with <code>prop_</code>, etc. All of that is listed <a href="http://hackage.haskell.org/package/HTF-0.5.0.0/docs/Test-Framework-Tutorial.html" target="_blank" rel="noopener noreferrer">here</a>, and everything is already set up with the grunt scaffold.

<h2>Conclusion</h2>

At this point you should be up and running with syntax highlighting, type inference, syntax completion, error highlighting, linting, unit testing, REPL, and debuggging (via the REPL).  All things that you want from a full fledged programming experience. I'm not going to lie, visual studio is still an infinitely more pleasurable experience, BUT once the main project boilerplate was removed working in haskell is much more enjoyable.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4327</wp:post_id>
		<wp:post_date><![CDATA[2014-01-13 08:00:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-01-13 08:00:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[started-haskell]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="post_tag" nicename="projects"><![CDATA[projects]]></category>
		<category domain="post_tag" nicename="setup"><![CDATA[setup]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561000134;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4244;}i:1;a:1:{s:2:"id";i:4725;}i:2;a:1:{s:2:"id";i:4348;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>144</wp:comment_id>
			<wp:comment_author><![CDATA[Jonathan]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[jonathan@hansfords.net]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[84.92.149.4]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-10-26 17:10:34]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-10-26 17:10:34]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[New to Haskell, git and cabal, but have installed the Haskell Platform for Windows and Sublime Text 3, and downloaded the forked hdevtools Zip file which I am trying to install. I was getting 3 dependency errors and have managed to resolve 2 of them, however I still get:

C:\hdevtools-master&gt;cabal configure
Resolving dependencies...
Configuring hdevtools-0.1.0.5...
cabal: At least the following dependencies are missing:
process ==1.1.*

I have tried to install process but get:

C:\hdevtools-master&gt;cabal install process
Resolving dependencies...
All the requested packages are already installed:
process-1.2.0.0
Use --reinstall if you want to reinstall anyway.

Using "ghc-pkg list" confirms I already have process-1.2.0.0 installed. What do I need to do to complete this install?

Thanks,

Jonathan]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Pulling back all repos of a github user</title>
		<link>https://onoffswitch.net/2013/12/10/pulling-repos-github-user/</link>
		<pubDate>Tue, 10 Dec 2013 21:20:13 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4332</guid>
		<description></description>
		<content:encoded><![CDATA[I recently had to relinquish my trusty dev machine (my work laptop) since I got a new job, and as such am relegated to using my old mac laptop at home for development until I either find a new personal dev machine or get a new work laptop.  For those who don't know, I'm leaving the DC area and moving to Seattle to work for Amazon, so that's pretty cool! Downside is that it's Java and Java kind of sucks, but I can still do f#, haskell, and all the other fun stuff on the side.

Anyways, since I'm setting up my home dev environment I wanted to pull back all my github repos in one go. If I only had a few of them I would've just cloned them by hand, but I have almost 30 repos, which puts me in the realm of wanting to automate it.

As any good engineer does, I did a quick google and found that someone had written a <a href="http://addyosmani.com/blog/backing-up-a-github-account/" target="_blank" rel="noopener noreferrer">ruby script</a> to clone all of a users repos using the github API. However, the script is outdated and the github API has changed. It no longer uses YAML, but now JSON, and the URL's are all different.

So, here is the updated script:

[ruby]
#!/usr/bin/env ruby

require &quot;json&quot;
require &quot;open-uri&quot;

username = &quot;devshorts&quot;

url = &quot;https://api.github.com/users/#{username}/repos&quot;

JSON.parse(open(url).read).map{|repo|
    repo_url = repo[&quot;ssh_url&quot;]
	
    puts &quot;discovered repository: #{repo_url} ... backing up ...&quot;
    	
    system &quot;git clone #{repo_url}&quot;
}
[/ruby]

Unlike the original script, this will clone to the current directory you are in using the same name of each repo (so no renaming it during the clone).

There are a ton of other backup options, but this was fun and simple (and a good way to get me back into using vim)

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4332</wp:post_id>
		<wp:post_date><![CDATA[2013-12-10 21:20:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2013-12-10 21:20:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[pulling-repos-github-user]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="git"><![CDATA[git]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561055272;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:4316;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A daily programmer - nuts and bolts</title>
		<link>https://onoffswitch.net/2014/01/20/daily-programmer/</link>
		<pubDate>Mon, 20 Jan 2014 08:00:13 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4343</guid>
		<description></description>
		<content:encoded><![CDATA[I've mentioned <a href="http://www.reddit.com/r/dailyprogrammer">r/dailyprogrammer</a> in <a href="http://onoffswitch.net/?s=daily+programmer" target="_blank" rel="noopener noreferrer">previous posts</a>, since I think they are fun little problems to solve when I have time on my hands.  They're also great problem sets to do when learning a new language. 

This time around I decided to do an <a href="http://www.reddit.com/r/dailyprogrammer/comments/1sob1e/121113_challenge_144_easy_nuts_bolts/" target="_blank" rel="noopener noreferrer">easy one</a> with haskell.  

<h2>Nuts and bolts problem description</h2>

The goal is stated as:

<blockquote>You have just been hired at a local home improvement store to help compute the proper costs of inventory. The current prices are out of date and wrong; you have to figure out which items need to be re-labeled with the correct price.

You will be first given a list of item-names and their current price. You will then be given another list of the same item-names but with the correct price. You must then print a list of items that have changed, and by how much.</blockquote>

The formal inputs and outputs:

<blockquote>Input Description
The first line of input will be an integer N, which is for the number of rows in each list. Each list has N-lines of two space-delimited strings: the first string will be the unique item name (without spaces), the second string will be the price (in whole-integer cents). The second list, following the same format, will have the same unique item-names, but with the correct price. Note that the lists may not be in the same order!
Output Description

For each item that has had its price changed, print a row with the item name and the price difference (in cents). Print the sign of the change (e.g. '+' for a growth in price, or '-' for a loss in price). Order does not matter for output.</blockquote>

And the sample input/output:


<blockquote>Sample Input 1
4
CarriageBolt 45
Eyebolt 50
Washer 120
Rivet 10
CarriageBolt 45
Eyebolt 45
Washer 140
Rivet 10

Sample Output 1
Eyebolt -5
Washer +20</blockquote>

<h2>My haskell solution</h2>

And here is my haskell solution

[haskell]
module Temp where

import Control.Monad
import Data.List

data Item = Item { name :: String, price :: Integer } 
				deriving (Show, Read, Ord, Eq)

strToLine :: String -&gt; Item
strToLine str = Item name (read price)
	where
		name:price:_ = words str 

formatPair :: (Item, Item) -&gt; [Char]
formatPair (busted, actual) = format
	where
		diff = price actual - price busted
		direction = if diff &gt; 0 then &quot;+&quot; else &quot;-&quot;
		format = name busted ++ &quot; &quot; ++ direction ++ show (abs diff) 
		
getPairs :: IO [(Item, Item)]
getPairs = do
	n &lt;- readLn
	let readGroup = fmap (sort . map strToLine) (replicateM n getLine)
	old &lt;- readGroup
	new &lt;- readGroup
	let busted = filter (\(a,b) -&gt; a /= b) $ zip old new
	return $ busted

printPairs :: IO [(Item, Item)] -&gt; IO [String]
printPairs pairs = fmap (map formatPair) pairs
[/haskell]

I had a lot of fun with this one, since it really forced me to understand and utilize <code>fmap</code> given that you had to deal with being in the <code>IO</code> monad.  I also liked being "forced" to separate the IO from the pure. I say forced in quotes because it's really not that helpful to do all your work in the IO function; it's not reusable.   

Also I found that by sticking to strongly typed data I had a more difficult time than if I had just leveraged the fact that the input was really a key value pair.  However, the engineer in me knows that things could change, and I hate taking shortcuts. By strongly typing the input data and separating out the parsing function from the code that does filtering and formatting, we could extend the problem set to include other fields without having to jump back to the IO code.  

Anyways, things are getting easier with haskell, but I'm still struggling with leveraging all the available libraries and constructs.  I guess that just comes with time and practice.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4343</wp:post_id>
		<wp:post_date><![CDATA[2014-01-20 08:00:13]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-01-20 08:00:13]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[daily-programmer]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dailyprogrammer"><![CDATA[dailyprogrammer]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="post_tag" nicename="io"><![CDATA[IO]]></category>
		<category domain="post_tag" nicename="monad"><![CDATA[monad]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560993964;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4316;}i:1;a:1:{s:2:"id";i:4262;}i:2;a:1:{s:2:"id";i:4327;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Multiple SignalR clients and ASMX service calls from the same application</title>
		<link>https://onoffswitch.net/2014/07/07/multiple-signalr-clients-asmx-service-calls-application/</link>
		<pubDate>Mon, 07 Jul 2014 08:00:47 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://tech.blinemedical.com/?p=289</guid>
		<description></description>
		<content:encoded><![CDATA[I was writing a test application to simulate what multiple signalR clients to a server would act like. The clients were triggered by the server and then would initiate a sequence of asmx web service calls back to the server using a legacy web service. This way I was using signalR as a triggering mechanism and not as a data transport. For my purpose this worked out great.

I had coupled the asmx calling code into a test class for a signalR client, so each class was responsible for its internal signalR connection as well as its outgoing asmx calls. When I had one class everything worked great. But the moment I had two classes running (i.e 2 signalR connections and 2+ asmx connections) everything locked up. I couldn't figure out what was going on. The signalR clients had connected but all the asmx calls stopped making it through, and eventually I got errors like this:

[csharp]
System.Net.WebException: The operation has timed out
at System.Net.HttpWebRequest.GetRequestStream(TransportContext&amp; context)
at System.Net.HttpWebRequest.GetRequestStream()
at System.Web.Services.Protocols.SoapHttpClientProtocol.Invoke(String methodName, Object[] parameters)
[/csharp]

At first I thought maybe the server was rejecting calls or was overloaded somehow. It didn't make any sense since it was only 4 connections, but I was testing against a server with other live connections so I figured I'd rule that option out. But when I looked at the IIS logs of the server I didn't even see any incoming connections. No 503 errors were generated, nothing indicated that the calls were even making it outbound from the client.

I decided to step back a little and took out the signalR client code and used an Rx observable timer to fire off asmx calls every 100 milliseconds in multiple threads trying to simulate a server load but everything worked fine. The moment I tied signalR back in with more than 1 client everything stopped.

After that I started to get curious if there was some sort of outbound connection limit. I spawned two instances of my test application, each with a single signalR client and the asmx services firing, and each app worked. So whatever was going on was definitely process bound, and not operating system bound.

It took a lot of reading about IIS settings and other blog entries to finally find what I was looking for. Turns out that each outgoing http call is handled by a <a href="http://msdn.microsoft.com/en-us/library/system.net.servicepoint.aspx"><code>ServicePoint</code></a> class. From msdn:
<blockquote>The ServicePoint class handles connections to an Internet resource based on the host information passed in the resource's Uniform Resource Identifier (URI). The initial connection to the resource determines the information that the ServicePoint object maintains, which is then shared by all subsequent requests to that resource.

ServicePoint objects are managed by the ServicePointManager class and are created, if necessary, by the ServicePointManager.FindServicePoint method. ServicePoint objects are never created directly but are always created and managed by the ServicePointManager class. The maximum number of ServicePoint objects that can be created is set by the ServicePointManager.MaxServicePoints property.</blockquote>
In the <code>ServicePointManager</code> there is a property called <code>DefaultConnectionLimit</code> which had this tidbit of a comment
<blockquote>The maximum number of concurrent connections allowed by a ServicePoint object. The default value is 2.</blockquote>
Turns out since I was connecting all the signalR clients and the asmx calls through the same server uri it was all using the same ServicePoint object which limited the connections to 2. When I ran just the asmx calls they didn't block because they were short bursts, when one completed the next one in the queue was handled, but when I had the two persistent signalR http connections I had used all the available resources and nothing would complete.

I added

[csharp]ServicePointManager.DefaultConnectionLimit = 10;[/csharp]

to the start of my application and everything worked out great.

Why they've limited the connection to 2 I'm not sure. I think its limited to minimize target server loads and optimize the usual fire and forget use cases, though with more people going towards long polling and persistent connections I think this will come up more often.

For more reading check out <a href="http://blogs.msdn.com/b/jpsanders/archive/2009/05/20/understanding-maxservicepointidletime-and-defaultconnectionlimit.aspx">this great post</a> about default connection limit and max service point idle time property settings.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>289</wp:post_id>
		<wp:post_date><![CDATA[2014-07-07 08:00:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-07-07 08:00:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[multiple-signalr-clients-asmx-service-calls-application]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="signalr"><![CDATA[SignalR]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1556647018;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3392;}i:1;a:1:{s:2:"id";i:4091;}i:2;a:1:{s:2:"id";i:2365;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Sometimes you have to fail hard</title>
		<link>https://onoffswitch.net/2014/08/04/fail-hard/</link>
		<pubDate>Mon, 04 Aug 2014 08:00:00 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=3295</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>This was a post I wrote in the middle of 2013 but never published. I wanted to share this since it's a common story across all technologies and developers of all skill levels. Sometimes things really just don't work. As a post-script, I did come back to this project and had a lot of success.  When in doubt, let time figure it out :)</blockquote>

<hr/>

For the last couple of weeks, I've been trying my hand at the node.js ecosystem.  I had an app idea but I wanted to make sure I chose tech stacks wisely. There's no better way to get familiar with different stacks than to get your hands dirty and try them all, so that's what I did.  

Sometimes when you start with a new language or platform things come easy, you can blaze a burning trail writing great software.  You're like an extension of the computer, everything just works and works great. Sometimes, though, like this time for me, you putter and stall and hit roadblocks at every corner.

At times it feels like a waste of time.  And at times I'm frustrated. I keep saying to myself "I'm better than this! Why am I stuck??". But, even though I haven't made any real progress in my app idea, I have learned tons about node.js and its accompanying workflow.  Frequently, failing is where you really learn the most. It's easy to forget that, even when you are ground to a halt, you are still learning.  As long as you are learning then it's not time wasted.

Just to prove the point, let me recant some of my recent failings.  

<h2>Sequelize</h2>

I started my exploration with just regular node.js.  I set up some routes and everything was cool.  Then it was time to add in a backing store.  At first I wanted to try using a MySql ORM (because in the past I've always done SQL by hand and I wanted to do something different).  I tried out <a href="http://www.sequelizejs.com/" target="_blank" rel="noopener noreferrer">sequelize</a> but found that not only did it not support transactions (and apparently transaction support in node.js is a pain since you need to write a connection pool to manage concurrent MySql connections), but it also never set up any actual foreign keys in the schema. This means you can easily corrupt your data even if the database would have prevented you from doing that.   While, I did have a copy working, I didn't particularly like the workflow so I started over. To be fair, the sequelize developer was extremely helpful and responsive on twitter, and maybe I'll try this library again in the future (when transaction support is added).

<h2>Mongoose</h2>

After sequelize, I switched over to using <a href="http://mongoosejs.com/" target="_blank" rel="noopener noreferrer">mongoose</a> with <a href="http://www.mongodb.org/" target="_blank" rel="noopener noreferrer">mongoDb</a>. My only history with using document based stores is with Lucene, but in that situation I was storing actual documents and using full text searching for it.  I spent a couple days reading up on mongo and mongoose and I had a quick document example up. I was able to insert and query users and related data pretty easily. Then I started to think about how to properly structure my schema for the app I wanted to write in such a way that working with the data was a pleasure, and at the same time maximized performance and throughput. This stumped me.  Researching NoSQL schema design patterns led me down to understanding about linked vs embedded documents, map/reduce with mongo, populating embedded documents with mongoose, different query types and syntax, etc.  Embedding too much meant that I had to search through my document to find an inner document.  Linking too much meant that I had to make lots of extra data calls.  Duplicating too much meant I ran the risk of out of sync data.  I still haven't quite settled on a good schema, so I took another step back and tried a different approach.

<h2>Typescript</h2>

Then I decided to give <a href="http://www.typescriptlang.org/" target="_blank" rel="noopener noreferrer">typescript</a> a try. Since I wasn't making good progress maybe doing things with typescript would help the ideas gel. At first, again, this was great. I got strong typing, succinct lambdas syntax, cleaner classes and functions, etc.  Since I was doing so well, I thought that maybe I'd try and strongly type the parts of mongoose that I had working for my app.  Here I hit another roadblock.  I wanted to <a href="http://typescript.codeplex.com/discussions/436705">map a function proxy</a> that mongoose gives you to a strongly typed class declaration.  This lead me down to reading about ambient declarations in typescript, poring over the typescript spec, furiously searching every typescript stackoverflow post and blog out there. I also had to learn how modules are loaded with <a href="http://www.commonjs.org/" target="_blank" rel="noopener noreferrer">CommonJS</a>.  On top of all of that, I ran into a problem with running unit tests using <a href="https://github.com/caolan/nodeunit" target="_blank" rel="noopener noreferrer">nodeunit</a> with typescript (though I finally did figure this out, the trick is to export a variable argument that has references to your testing class functions).  At one point I even managed to <a href="http://stackoverflow.com/questions/15398787/create-newable-class-from-library" target="_blank" rel="noopener noreferrer">crash the typescript compiler</a>!

<h2>What's next?</h2>

I'm at the point now where I have learned a lot, some things still don't work, but I need to sit back and take a break.  It's disheartening failing to make progress at every turn you make, but that's the way you learn. Without major failures you don't come to appreciate the nuances of how things work and how things are pieced together.  I'll come back to this project in a few weeks and probably feel a whole lot better about it. In the end, what I've been reminded of, is that there's no shame in sometimes failing hard. Really really hard.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>3295</wp:post_id>
		<wp:post_date><![CDATA[2014-08-04 08:00:00]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-08-04 08:00:00]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[fail-hard]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="mongodb"><![CDATA[mongodb]]></category>
		<category domain="post_tag" nicename="mongoose"><![CDATA[mongoose]]></category>
		<category domain="post_tag" nicename="mysql"><![CDATA[MySql]]></category>
		<category domain="post_tag" nicename="node-js"><![CDATA[node.js]]></category>
		<category domain="post_tag" nicename="nosql"><![CDATA[NoSql]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
		<category domain="post_tag" nicename="typescript"><![CDATA[typescript]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561696411;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2635;}i:1;a:1:{s:2:"id";i:3477;}i:2;a:1:{s:2:"id";i:4244;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>140</wp:comment_id>
			<wp:comment_author><![CDATA[Paul]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[paul.diez@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[99.232.133.182]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-10-14 18:32:41]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-10-14 18:32:41]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Anton, thanks for posting this! I'm in a very similar situation. I'm been a PHP developer for a while now and figured I should start to expanded my horizons and build a project using a new server side language. After your roadblocks and experience with nodeJS have you considered trying out Rails or Python?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>141</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[206.169.195.21]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-10-14 20:57:10]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-10-14 20:57:10]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey Paul, I haven't really given rails or python a serious go yet. I came back to node after I wrote this and had a few really successful projects.  I think you should give it a chance.  

The post was really meant to just describe my experience that sometimes diving head first into new tech can be infuriating and a little demoralizing, but that with a few weeks break inbetween you can come back and succeed :)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>140</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>142</wp:comment_id>
			<wp:comment_author><![CDATA[johaness]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[josecarlos.barros@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[201.215.114.2]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-06-28 02:55:19]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-06-28 02:55:19]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Mongodb is a very bad idea, really. But using sequelize, postgres, node.js aren't, they are indeed good pieces of software.
Your problem should be these mongo.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>143</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[64.202.160.73]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-07-08 00:18:18]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-07-08 00:18:18]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Actually I've had lots of great success with mongo since then.  If you use the right write concern and set up your shard keys properly, mongo works wonderfully. Most people have issues with mongo because they neglect to set up the proper shard keys, indexes, and use a write concern that isn't appropriate for their application]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>142</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Filter on deep object properties in angularjs</title>
		<link>https://onoffswitch.net/2014/01/27/filter-deep-object-properties-angularjs/</link>
		<pubDate>Mon, 27 Jan 2014 08:00:25 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4340</guid>
		<description></description>
		<content:encoded><![CDATA[AngularJS provides a neat way of filtering arrays in an <code>ng-repeat</code> tag by piping elements to a built in <code>filter</code> filter which will filter based on a predicate.  Doing it this way you can filter items based on a function, or an expression (evaluated to a literal), or by an object. 

When filtering by an object you can pass in a javascript object that represents key value items that should match whats in the backing data element.  For example:

[js]
$scope.elements = [{ foo: &quot;bar&quot; }, { foo: &quot;biz&quot; }]

--

&lt;div ng-repeat=&quot;foos in elements | filter: { foo: &quot;bar&quot; }&quot;&gt;
  {{ foos.foo }} matches &quot;bar&quot;
&lt;/div&gt;
[/js]

Here I filtered all the objects whose foo property matches the value "bar". But what if I have a non-trivial object?  Something with lots of nested objects?  I found that passing in the object to the filter was both unweidly, and error prone. I wanted something simple where I could write out how to dereference it, like <code>obj.property.otherItem.target</code>.

Thankfully this is pretty easy to write:

[js]
function initFilters(app){
    app.filter('property', property);
}

function property(){
    function parseString(input){
        return input.split(&quot;.&quot;);
    }

    function getValue(element, propertyArray){
        var value = element;

        _.forEach(propertyArray, function(property){
            value = value[property];
        });

        return value;
    }

    return function (array, propertyString, target){
        var properties = parseString(propertyString);

        return _.filter(array, function(item){
            return getValue(item, properties) == target;
        });
    }
}
[/js]

And can be used in your html like this:

[js]
&lt;ul&gt;
    only failed: &lt;input type=&quot;checkbox&quot;
                      ng-model=&quot;onlyFailed&quot;
                      ng-init=&quot;onlyFailed=false&quot;/&gt;

    &lt;li ng-repeat=&quot;entry in data.entries | property:'test.status.pass':!onlyFailed&quot;&gt;
        &lt;test-entry test=&quot;entry.test&quot;&gt;&lt;/test-entry&gt;
    &lt;/li&gt;
&lt;/ul&gt;
[/js]
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4340</wp:post_id>
		<wp:post_date><![CDATA[2014-01-27 08:00:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-01-27 08:00:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[filter-deep-object-properties-angularjs]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="angularjs"><![CDATA[angularjs]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="js"><![CDATA[JavaScript]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561519402;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4028;}i:1;a:1:{s:2:"id";i:3710;}i:2;a:1:{s:2:"id";i:265;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>145</wp:comment_id>
			<wp:comment_author><![CDATA[André Simões]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[andrejnsimoes@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[70.39.231.100]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-11 12:38:55]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-11 12:38:55]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice. Just a suggestion: jsfiddle with it would be great.
Thanks]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>146</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[54.240.196.185]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-12 17:28:18]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-12 17:28:18]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Andre, here is a <a href="http://jsfiddle.net/MbeR5/1/" rel="nofollow">fiddle</a> demonstrating it]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>145</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>147</wp:comment_id>
			<wp:comment_author><![CDATA[Dave Sanderling]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[dsanderling@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[69.30.63.150]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-12 19:33:40]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-12 19:33:40]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey, this is awesome.  Saved me some time, thank you!  I was wondering why angular wasn't filtering on deep properties.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>148</wp:comment_id>
			<wp:comment_author><![CDATA[Gio Zunino]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[giovanni.zunino@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[176.198.112.20]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-17 11:28:33]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-17 11:28:33]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great work Anton!
This issue costed me a couple of hours after upgrading Angular from 1.2.9 to 1.2.12

It shouldn't be necessary anymore since the latest Angular release (1.2.13) from last friday (February 14)
<a href="https://github.com/angular/angular.js/blob/master/CHANGELOG.md" rel="nofollow">Angular Changelog</a>

To make it work, have your filter-comparator-object mirror the structure of the iterator object and bind the property you want to filter to some form element, for example:

<code>
	&lt;input type="text" ng-model="item.with.a.deep.object.property"&gt;
</code>]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>149</wp:comment_id>
			<wp:comment_author><![CDATA[Gio Zunino]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[giovanni.zunino@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[176.198.112.20]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-02-17 11:32:22]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-02-17 11:32:22]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Forgot the 2nd part :)
<code>
	&lt;ul&gt;
		&lt;li ng-repeat="myItem in items | filter: item"&gt;
			&lt;!-- Do some angular magic with deep nested properties --&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
</code>]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>150</wp:comment_id>
			<wp:comment_author><![CDATA[CruX]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[crux@project-insanity.org]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[193.158.197.38]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-07-11 10:55:53]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-07-11 10:55:53]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[For people wanting a version of this that doesn't require underscore.js.

<code>
var module = angular.module('app', []);

module.filter("property", ["$filter", function($filter){
    var parseString = function(input){
        return input.split(".");
    }
 
    function getValue(element, propertyArray) {
        var value = element;

        angular.forEach(propertyArray, function(property) {
            value = value[property];
        });
 
        return value;
    }
 
    return function (array, propertyString, target) {
        var properties = parseString(propertyString);
 
        return $filter('filter')(array, function(item){
            return getValue(item, properties) == target;
        });
    }
}]);
</code>]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>151</wp:comment_id>
			<wp:comment_author><![CDATA[Ryan S]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[gloves12@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[143.127.128.10]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-08-22 04:23:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-08-22 04:23:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Correct!  The syntax is this for item.address.county:

]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>148</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>152</wp:comment_id>
			<wp:comment_author><![CDATA[Ryan S]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[gloves12@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[143.127.128.10]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-08-22 04:23:37]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-08-22 04:23:37]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[div ng-repeat="it in items | filter:{ address: { country: 'Canuckistan'}}"]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>151</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>153</wp:comment_id>
			<wp:comment_author><![CDATA[laxika]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[laxika91@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[37.191.60.189]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-11-06 11:13:31]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-11-06 11:13:31]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[It's as easy as:

filter:{data: { name: search.name}}

object.data.name

since 1.2.13]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>154</wp:comment_id>
			<wp:comment_author><![CDATA[Anil Singh]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[anil.singh581@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://www.code-sample.com/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[122.177.158.72]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-12-23 06:01:34]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-12-23 06:01:34]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Nice Example!!!

It might help too
http://www.code-sample.com/2014/12/filter-nested-properties-in-angularjs.html]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>155</wp:comment_id>
			<wp:comment_author><![CDATA[Reena]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[rina.singh851@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[122.177.216.167]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-02-20 04:18:12]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-02-20 04:18:12]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[@Anil, Its very helpful for me!! Thank you!!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>154</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>156</wp:comment_id>
			<wp:comment_author><![CDATA[Dinesh]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[dineshaudichya@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[14.141.89.18]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-03-21 15:28:18]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-03-21 15:28:18]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[how would i search by subject if object is like 
<code>[{roll_no:123,subjects:[{id:1,name:"Maths"}, {id:2,name:"English"}]}, {roll_no:453,subjects:[{id:2,name:"Computer"}, {id:1,name:"Maths"}]}] </code>]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Building a prefix trie</title>
		<link>https://onoffswitch.net/2014/04/14/building-prefix-trie/</link>
		<pubDate>Mon, 14 Apr 2014 08:00:48 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4348</guid>
		<description></description>
		<content:encoded><![CDATA[<a href="http://en.wikipedia.org/wiki/Trie" target="_blank" rel="noopener noreferrer">Prefix trie</a>'s are cool data structures that let you compress a dictionary of words based on their shared prefix. If you think about it, this makes a lot of sense. Why store <code>abs</code>, <code>abbr</code>, and <code>abysmal</code> when you only need to store <code>a,b,b,r,s,y,s,m,a,l</code>. Only storing what you have to (based on prefix) in this example gives you a 70% compression ratio! Not too bad, and it would only get better the more words you added.  

The classical way of dealing with prefix tries is to store the suffixes using a map, but for fun I tried something different and used a list.  

<h2>Data</h2>

The main data structure I had was like this

[haskell]
type Key a = [a]

data Trie key = Node (Maybe key) [Trie key] Bool deriving (Show, Eq, Read)
[/haskell]

So a trie is really just a node that has a list of other tries (which would be its suffixes). You can see an example of the structure with the word "foo":

[haskell]
[Node (Just 'f') 
     [Node (Just 'o') 
        [Node (Just 'o') [] True] 
     False] 
 False]
[/haskell]

It's printable to the screen, equatable, and can be serialized from text. It also contains a boolean field representing if it is the end of a word or not. You can see the boolean at the last "o" of the word, indicating the word terminates.  All the other characters are non-terminating fields.

<h2>Searching</h2>

Let's add a helper method to find a key in a list of tries. Remember that each root can contain a list of other trie's, so it'll be nice to be able to say "<em>hey, we're at root X which has a list of possible suffix starts Y. Let's find the next root from the list of Y and return it</em>".

[haskell]
{-
    Finds a key in the trie list that matches the target key
-}
findKey :: (Eq t) =&gt; t -&gt; [Trie t] -&gt; Maybe (Trie t)
findKey key tries = L.find (\(Node next _ _) -&gt; next == Just key) tries
[/haskell]

It takes an equatable value, and a list of Trie's of those values, and finds the key returned as an option type.

We could even now write code to find a Trie

[haskell]
{-
    Takes a key list and finds the trie that fullfils that prefix
-}
findTrie :: (Eq t) =&gt; Key t -&gt; [Trie t] -&gt; Maybe (Trie t)
findTrie [] _ = Nothing
findTrie (x:[]) tries = findKey x tries 
findTrie (x:xs) tries = findKey x tries &gt;&gt;= nextTrie
    where nextTrie (Node _ next _) = findTrie xs next
[/haskell]

Since <code>findKey</code> is an option type, we can leverage the option monadic bind operator <code>>>=</code>. This operator will pass a result to the next function only if the result is a <code>Just</code> type. This is a way to safely process 'nullable' items.

<h2>Insertion</h2>

Now the insertion code, which honestly was the most complicated.

[haskell]
insert :: (Eq t) =&gt; Key t -&gt; [Trie t] -&gt; [Trie t]
insert [] t = t
insert (x:xs) tries = 
    case findKey x tries of 
        Nothing -&gt; [(Node (Just x) (insert xs [])) isEndWord]++tries
        Just value -&gt; 
            let (Node key next word) = value
            in [Node key (insert xs next) (toggleWordEnd word)]++(except value)
    where 
        except value = (L.filter ((/=) value) tries)
        isEndWord = if xs == [] then True else False
        toggleWordEnd old = if xs == [] then True else old
[/haskell]

The trick to pure functional insertion is that you need to <em>rebuild</em> the entire data structure WHILE you search for where you want to actually add things.  

In the first case where you try to find a key but didn't find it, you want to add the key as a neighbor to the other trees. So if you have a root word of "<em>abd</em>" but you add "<em>def</em>", the root character of "<em>d</em>" doesn't exist yet and needs to be added, hence creating a new list and appending it to the original tries list. Then you recursively go through the new element and add the remaining list. This is why the creation of the new element has a recursive call back to the insert function with the remaining list <code>xs</code>.  Also, since you created the new element as part of a recursive call, you know this element is already being properly appended to its root.  

But, if the key already exists then that means you found a suffix that matches. At this point you need to bubble through the inner suffix tree to add the remaining items you want (basically following the suffix trie down to a point where you haven't already found a suffix root). Once thing to remember is that you also need to recreate the key you found, excluding the original element (hence the <code>except value</code> since we want to exclude the old version of the key before). 

On top of that, if you've already processed a root, but your word now ends at a root, you need to toggle if that suffix is a word completion. For example, if I've already added "<em>abcdef</em>" then "<em>f</em>" is the end of the word. But now if I add "<em>abc</em>" as a word, then both "<em>c</em>" AND "<em>f</em>" are end words (so "<em>c</em>"'s end word boolean needs to be toggled to true).  

I found this concept to be a little complicated because its both finding, updating, and rebuilding the data structure all at the same time, but with practice it does get more natural to trace through.

<h2>Getting the words</h2>

Once you can build a trie though, we can leverage haskells list comprehensions to fold the words back out

[haskell]
{- 
    Gives you all the available words in the trie list
-}
allWords :: [Trie b] -&gt; [[b]]
allWords tries = 
    let raw = rawWords tries
    in map (flatMap id) raw
    where 
        flatMap f = Fold.concatMap (Fold.toList . f)
        rawWords tries = [key:next
                            | (Node key suffixes isWord) &lt;- tries
                            , next &lt;- 
                                if isWord then 
                                    []:(rawWords suffixes)
                                else 
                                    rawWords suffixes]
[/haskell]

Since we have a list of roots, we can go through each suffix and append the root character to it. If we encounter a word, we can pop in an empty list to create a delimiter.  The list comprehension will do every combination of every root with its suffixes, so we will get each word.

But, this makes a list of characters and so I wanted to flatmap the list to create strings.

<h2>Guessing a suffix</h2>

We can even do auto complete stuff now! Let's look at how to guess what the available suffixes would be:

[haskell]
{- 
    This function takes a key list and returns the 
    full words that match the key list. But, since we already 
    know about the source key that matches (the input) we can
    prepend that information to any suffix information that is left.
    If the node found that matches the original query is a word in itself
    we can prepend that too since its a special case
-}
guess :: (Eq a) =&gt; Key a -&gt; [Trie a] -&gt; Maybe [Key a]
guess word trie = 
    findTrie word trie &gt;&gt;= \(Node _ next isWord) -&gt; 
    return $ (source isWord) ++ (prependOriginal word $ allWords next)
    where 
        source isWord = if isWord then [word] else []
        prependOriginal word = map (\elem -&gt; word ++ elem)
[/haskell]

The idea is we have a string, and our original roots. We then look for the suffixes and when we find the target trie that ends at the word we have, we can find all the words after that and prepend our original word.  

<h2>Examples</h2>

Lets look at some basic examples of how to use the Trie:

[code]
*PrefixTree&gt; tries = build [&quot;word&quot;, &quot;wooordz&quot;, &quot;happy&quot;, &quot;hoppy&quot;]

*PrefixTree&gt; guess &quot;h&quot; tries
Just [&quot;hoppy&quot;,&quot;happy&quot;]

*PrefixTree&gt; allWords tries
[&quot;hoppy&quot;,&quot;happy&quot;,&quot;wooordz&quot;,&quot;word&quot;]

*PrefixTree&gt; tries
[Node (Just 'h') [Node (Just 'o') [Node (Just 'p') [Node (Just 'p') [Node (Just 'y') [] True] False] False] False,Node (Just 'a') [Node (Just 'p') [Node (Just 'p') [Node (Just 'y') [] True] False] False] False] False,Node (Just 'w') [Node (Just 'o') [Node (Just 'o') [Node (Just 'o') [Node (Just 'r') [Node (Just 'd') [Node (Just 'z') [] True] False] False] False] False,Node (Just 'r') [Node (Just 'd') [] True] False] False] False]

*PrefixTree&gt; guess &quot;ha&quot; tries
Just [&quot;happy&quot;]

*PrefixTree&gt; guess &quot;hi&quot; tries
Nothing
[/code]  

<h2>Conclusion</h2>

It would be a lot faster to have leveraged a hash map instead of a list for the suffixes, since each time I need to find the suffix root I have to do an O(n) traversal, instead of an O(1), but since this isn't a production trie I'm not worried about it.  It was still a fun exercise!

For a full example check my <a href="https://github.com/devshorts/Playground/tree/master/haskell_trie" target="_blank" rel="noopener noreferrer">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4348</wp:post_id>
		<wp:post_date><![CDATA[2014-04-14 08:00:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-04-14 08:00:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-prefix-trie]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="autocomplete"><![CDATA[autocomplete]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="post_tag" nicename="trie"><![CDATA[trie]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558781240;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4011;}i:1;a:1:{s:2:"id";i:3847;}i:2;a:1:{s:2:"id";i:3656;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Building LINQ in Java pt 2</title>
		<link>https://onoffswitch.net/2014/02/24/building-linq-java-pt-2/</link>
		<pubDate>Mon, 24 Feb 2014 08:00:16 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4355</guid>
		<description></description>
		<content:encoded><![CDATA[In my <a href="http://onoffswitch.net/building-linq-java/" title="Building LINQ in Java">last post</a> I discussed building a static class that worked as the fluent interface exposing different iterator sources that provide transformations. For 1:1 iterators, like take, skip, while, for, nth, first, last, windowed, etc, you just do whatever you need to do internally in the iterator by manipulating the output the stream.

But if you want to do a transformation like a map, you need to project the input source to something else. Now the base class each iterator inherits from isn't good enough. But thankfully we can just add another generic parameter and create a new mappable base class that can handle transformations.

The following iterator handles projecting from a source to a result type and yields an enumerable iterator of the result type.

[java]
package com.devshorts.enumerable.iterators;

import java.util.function.Function;

public class MapIterator&lt;TSource, TResult&gt; extends EnumerableIterator&lt;TResult&gt; {

    private Function&lt;TSource, TResult&gt; projection;

    /***
     * Need this constructor for flatMap
     * @param input
     */
    protected MapIterator(Iterable input){
        super(input);

        // by default the projection is the id function
        this.projection = i -&gt; (TResult)i;
    }

    public MapIterator(Iterable&lt;TSource&gt; source, Function&lt;TSource, TResult&gt; projection) {
        this(source);

        this.projection = projection;
    }

    @Override
    public boolean hasNext() {
        return source.hasNext();
    }

    @Override
    public TResult next() {
        return projection.apply((TSource)source.next());
    }
}
[/java]

It's exposed in the main base class by passing in a projection function

[java]
public &lt;TResult&gt; Enumerable&lt;TResult&gt; map(Function&lt;TSource, TResult&gt; mapFunc){
        return enumerableWithIterator(source -&gt;
                new MapIterator&lt;TSource, TResult&gt;(source, i -&gt; mapFunc.apply(i)));
}

private &lt;TResult&gt; Enumerable&lt;TResult&gt; enumerableWithIterator(Function&lt;Iterable&lt;TSource&gt;, Iterator&lt;TResult&gt;&gt; generator){
        return new Enumerable&lt;&gt;(_ig -&gt; generator.apply(this));
}
[/java]

So enumerableWithIterator takes a function that returns an iterable, and wraps a new Enumerable fluent type to wrap the current iterator source (this). The "_ig" parameter is ignored, hence the name.  

<h2>Flat Map</h2>

Now that we have a basic projection iterator, we can also implement a flat map. Flat map is the functional term for what some languages called "collect" or "selectMany". It takes a list of lists and yields each element in all the lists. Basically it flattens everything.

First lets check the flat map iterator. We need to consume each underlying list, buffer it, and then yield each element of each list when requested. If we've consumed the entire buffered list we need to get the next list.

Note how the flat map iterator inherits from the map iterator.  The reason for this is that if we want to project from one type to another we need to inherit from a class that exposes two generic types. The basic EnumerableIterator only exposes one generic.  We could have inherited from that as well and added the extra generics, but I think it was nicer to group maps in a map inheritance tree, and everything else under the basic class.

[java]
package com.devshorts.enumerable.iterators;

import java.util.List;
import java.util.function.Function;

/**
 * Created with IntelliJ IDEA.
 * User: anton.kropp
 * Date: 11/11/13
 * Time: 1:04 PM
 * To change this template use File | Settings | File Templates.
 */
public class FlatMapIterator&lt;TSource, TResult&gt; extends MapIterator&lt;TSource, TResult&gt; {
    private Function&lt;TSource, List&lt;TResult&gt;&gt; flatMapper;

    public FlatMapIterator(Iterable&lt;TSource&gt; source, Function&lt;TSource, List&lt;TResult&gt;&gt; flatMapper) {
        super(source);
        this.flatMapper = flatMapper;
    }

    private List&lt;TResult&gt; _bufferedResult;
    private Integer idx = 0;

    @Override
    public boolean hasNext() {
        if(_bufferedResult == null &amp;&amp; source.hasNext()){
            _bufferedResult = flatMapper.apply((TSource)source.next());

            idx = 0;

            return true;
        }

        if(_bufferedResult != null){
            if(idx &lt; _bufferedResult.size()){
                return true;
            }

            _bufferedResult = null;

            return hasNext();
        }

        return false;
    }


    public TResult next() {
        TResult item = _bufferedResult.get(idx);
        idx++;
        return item;
    }
}
[/java]

<h2>Other fun iterators</h2>

Since we have basic sequence manipulators and element transformations, we can do more complex LINQ type things.  I like intersperse and windowed, since they are common in functional programming.  Intersperse puts in an element inbetween each other element in a sequence. Imagine you have a list of characters ['a, 'b', 'c'] and you want to add in a comma in between them all. You can intersperse it with a comma and get ['a, ',','b',',','c'].

Technically intersperse is a subset of intercalate (terms taken from Haskells Data.List package). Intercalate will put in the elements of another list inbetween the elements of the first list.  To get intersperse you pass in an array of size one:

[java]
public class IntercalateIterator&lt;TSource&gt; extends EnumerableIterator&lt;TSource&gt; {
    private List&lt;TSource&gt; intercalator;

    public IntercalateIterator(Iterable&lt;TSource&gt; input, List&lt;TSource&gt; intercalator) {
        super(input);
        this.intercalator = intercalator;
    }

    private boolean intercalate = false;
    private int idx = 0;

    @Override
    public boolean hasNext(){
        if(intercalate &amp;&amp; source.hasNext()){
            return idx &lt; intercalator.size();
        }

        return source.hasNext();
    }

    @Override
    public TSource next(){
        TSource n;
        if(intercalate){
            n = intercalator.get(idx);

            idx++;

            if(idx == intercalator.size()){
                intercalate = false;
            }
        }
        else{
            idx = 0;

            n = source.next();

            intercalate = true;
        }

        return n;
    }


}
[/java]

And check windowed. This yields a list of lists where each list is a sliding window of size N across the source list. If you a have a list

[code]
1;2;3;4
[/code]

And you apply a window of size 2, you will get

[code]
[[1;2], [2;3], [3;4]]
[/code]

This can be pretty handy in some scenarios

[java]
package com.devshorts.enumerable.iterators;

import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Queue;

public class WindowedIterator&lt;TSource&gt; extends MapIterator&lt;TSource, List&lt;TSource&gt;&gt; {
    private final int windowSize;
    private boolean windowSeeded;
    List&lt;TSource&gt; queue = new LinkedList&lt;&gt;();
    List&lt;TSource&gt; next = new LinkedList&lt;&gt;();

    public WindowedIterator(Iterable&lt;TSource&gt; input, int windowSize) {
        super(input);
        this.windowSize = windowSize;
    }


    @Override
    public boolean hasNext(){
        if(!windowSeeded){
            seedWindow();

            windowSeeded = true;
        }

        return next.size() &gt; 0;
    }

    @Override
    public List&lt;TSource&gt; next(){
        queue = new LinkedList&lt;&gt;(next);

        nextWindow();

        if(next.size() != windowSize){
            next.clear();
        }

        return (List&lt;TSource&gt;)(queue);
    }

    private void nextWindow(){
        next.remove(0);

        if(it().hasNext()){
            next.add(it().next());
        }
    }

    private void seedWindow(){
        int window = windowSize;
        while(it().hasNext() &amp;&amp; window &gt; 0){
            next.add(it().next());
            window--;
        }
    }

    private Iterator&lt;TSource&gt; it(){
        return (Iterator&lt;TSource&gt;)(source);
    }

}
[/java]

<h2>Using it</h2>

Using these features is now really easy, here are some sample tests that demonstrate our new iterators

[java]
@Test
public void FlatMap(){
    assertEquals(asList(&quot;5&quot;, &quot;4&quot;, &quot;3&quot;, &quot;2&quot;, &quot;1&quot;),
                Enumerable.init(asList(asList(&quot;5&quot;), asList(&quot;4&quot;), asList(&quot;3&quot;), asList(&quot;2&quot;), asList(&quot;1&quot;)))
                        .flatMap(i -&gt; i)
                        .map(i -&gt; i.toString())
                        .toList());
}

@Test
public void Map(){
    assertEquals(asList(&quot;5&quot;, &quot;4&quot;, &quot;3&quot;, &quot;2&quot;, &quot;1&quot;),
                Enumerable.init(asList(5, 4, 3, 2, 1))
                        .map(i -&gt; i.toString())
                        .toList());
}

@Test
public void Windowed(){
    assertEquals(asList(asList(1, 2), asList(2, 3), asList(3, 4)),
                Enumerable.init(asList(1, 2, 3, 4))
                        .windowed(2)
                        .toList());
}


[/java]

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4355</wp:post_id>
		<wp:post_date><![CDATA[2014-02-24 08:00:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-02-24 08:00:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-linq-java-pt-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="iterator"><![CDATA[iterator]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="linq"><![CDATA[linq]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1557352026;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4316;}i:1;a:1:{s:2:"id";i:3367;}i:2;a:1:{s:2:"id";i:3497;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Logitech mx mouse zoom button middle click on Ubuntu</title>
		<link>https://onoffswitch.net/2014/02/03/logitech-mx-mouse-zoom-button-middle-click-ubuntu/</link>
		<pubDate>Mon, 03 Feb 2014 03:58:32 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4359</guid>
		<description></description>
		<content:encoded><![CDATA[Any good engineer has their own tools of their trade: keyboard, mouse, and licenses to their favorite editors (oh and a badass <a href="http://www.hermanmiller.com/products/seating/performance-work-chairs/embody-chairs.html" target="_blank" rel="noopener noreferrer">chair</a>).

I work now on an Ubuntu box and I wanted to get my logitech MX mouse's zoom button to act as middle click. I really like this functionality since its easy to copy, paste, close windows, and open new links with this button.

However, the button mapping in Ubuntu isn't trivial. On windows you used the setpoint program to do it and called it a day. But in linux land you need to put more work into it.

<a href="http://forums.logitech.com/t5/Mice-and-Pointing-Devices/Guide-for-setup-Performance-MX-mouse-on-Linux-with-KDE/td-p/517167" target="_blank" rel="noopener noreferrer">Here</a> is a great tutorial describing how to do it, but for the lazy, here is the mapping you need.  

[code]
&quot;xte 'mouseclick 2'&quot;
  b:13+Release
[/code]

What this says is "when button 13 is clicked, then released, issue a mouseclick 2 command". <code>xte</code> is a program that simulates mouse and keyboard events, and <code>xbindkeys</code> (whose config you edit to set the xte mapping) is a program that lets you bind one key or mouse event to another key or mouse event.

Once I did this and started up <code>xbindkeys</code> then my zoom button (button 13) now worked as middle click (mouseclick 2).  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4359</wp:post_id>
		<wp:post_date><![CDATA[2014-02-03 03:58:32]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-02-03 03:58:32]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[logitech-mx-mouse-zoom-button-middle-click-ubuntu]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560926272;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4244;}i:1;a:1:{s:2:"id";i:4905;}i:2;a:1:{s:2:"id";i:4306;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A simple templating engine</title>
		<link>https://onoffswitch.net/2014/03/10/simple-template-engine/</link>
		<pubDate>Mon, 10 Mar 2014 08:00:07 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4365</guid>
		<description></description>
		<content:encoded><![CDATA[I wanted to talk about templating, since templating is a common thing you run into. Often times you want to cleanly do a string replace on a bunch of text, and sometimes even need minimal language processing to do what you want. For example, Java has a templating engine called Velocity, but lots of languages have libraries that do this kind of work. I thought it'd be fun to create a small templating engine from scratch with F# as an after work exercise.

The goal is to give the templating processor a set of lookup bags that can be resolved by variables. For example, if I use a variable <code>$devshorts.isgreat</code> that should correspond to a bag that is keyed first off of <code>devshorts</code> which returns a new bag, and then a new bag that has a key <code>isgreat</code> which should return a value.
                                                              
<h2>Getting the AST</h2>

First, lets parse the language and get an abstract syntax tree.  Anything that is prefixed with dollar sign is a language construct, anything not is a literal.  As with most parsing tasks, I jump straight to fparsec.

[fsharp]
namespace FPropEngine

module Parser = 

    open FParsec

    type Ast = 
        | Bag of string list
        | Literals of string
        | ForLoop of string * Ast * Ast list

    let tokenPrefix = '$'

    let tagStart = pstring (string tokenPrefix)

    let token n = tagStart &gt;&gt;. pstring n |&gt;&gt; ignore 

    let tagDelim = eof &lt;|&gt; spaces1

    let endTag = token &quot;end&quot; 
    
    let forTag = token &quot;for&quot; 

    let languageSpecific = [attempt endTag; forTag] |&gt; List.map (fun i -&gt; i .&gt;&gt; tagDelim)

    let anyReservedToken = attempt (languageSpecific |&gt; List.reduce (&lt;|&gt;))

    let tokenable = many1Chars (satisfy isDigit &lt;|&gt; satisfy isLetter)

    let element = attempt (tokenable .&gt;&gt; pstring &quot;.&quot;) &lt;|&gt; tokenable

    let nonTokens = many1Chars (satisfy (isNoneOf [tokenPrefix])) |&gt;&gt; Literals

    let bag = tagStart &gt;&gt;. many1 element |&gt;&gt; Bag

    let innerElement = notFollowedBy anyReservedToken &gt;&gt;. (nonTokens &lt;|&gt; bag)

    let tagFwd, tagImpl = createParserForwardedToRef()

    let forLoop = parse {
        do! spaces
        do! forTag
        do! spaces
        do! skipAnyOf &quot;$&quot;
        let! alias = tokenable
        do! spaces
        let! _ = pstring &quot;in&quot;
        do! spaces
        let! elements = bag
        do! spaces
        let! body = many tagFwd
        do! spaces
        do! endTag
        do! spaces
        return ForLoop (alias, elements, body) 
    } 

    tagImpl := attempt forLoop &lt;|&gt; innerElement

    let get str = 
        match run (many tagFwd) str with
             | Success(r, _, _) -&gt; r 
             | Failure(r,_,_) -&gt; failwith &quot;nothing&quot;
[/fsharp]

I've exposed only one language construct (a for loop), and anything else is just a basic string replace bag (which will already be deconstructed into its individual components, i.e. <code>$foo.bar</code> will be <code>["foo";"bar"]</code>).

<h2>Contexts</h2>

The next thing we need is a way to store a context, and to resolve a requested path from the context.  Since I want to be able to add key value pairs to the context but have the values be different (sometimes they should be a string, other times they should be other context bags), we need to be able to handle that.

For example, lets say I make a context called "anton".  In this context I want to have key "isGreat" that resolves to "kropp".  That would end up being a leaf node in this context path. But how do I represent a path like "anton.shmanton.isGreat".  The key "shmanton" should resolve to a new context under the current context of "anton".  Also, in order to leverage for loops, we need some keys to resolve to multiple values. So now we have 3 types of results: a string, a string list, or another context.  Given that, lets create a context class that can handle creating these contexts, as well as resolving a context path.

[fsharp]
module Formatter = 
    open Parser
    open System.Collections.Generic

    type Context () =    
        let ctxs = new Dictionary&lt;string, ContextType&gt;()
        let runtime = new Dictionary&lt;string, string&gt;()

        member x.add (key, values) = ctxs.[key] &lt;- List values
        member x.add (key, value)  = ctxs.[key] &lt;- Value value
        member x.add (key, ctx)    = ctxs.[key] &lt;- More ctx

        member x.runtimeAdd (key, value) = runtime.[key] &lt;- value
        member x.runtimeRemove key = runtime.Remove key |&gt; ignore
    
        member x.add (dict:Dictionary&lt;string, string&gt;) = 
            for keys in dict do
                ctxs.[keys.Key] &lt;- Value keys.Value

        member x.resolve list = 
            match list with 
                | [] -&gt; None
                | h::t -&gt; 
                    if runtime.ContainsKey h then
                        Some [runtime.[h]]
                    else if ctxs.ContainsKey h then
                        ctxs.[h].resolve t
                    else 
                        None            

    and ContextType = 
        | Value of string
        | List of string list
        | More of Context
        member x.resolve list = 
            match x with 
                | Value str -&gt; Some [str]
                | List strs -&gt; Some strs
                | More ctx -&gt; ctx.resolve list
[/fsharp]

One thing that is tricky here: <code>ctxs.[h].resolve t</code> doesn't call the same <code>resolve</code> function on the Context class. It actually calls the resolve function on the ContextType.  This way each type can resolve itself.  If you call resolve on a string, it'll return itself (as a list).  If you resolve on a list, it'll return the list.  But, if you call resolve on a context, it'll proxy that request back to the Context class.  

You may also be wondering what "runTimeAdd" and "runtimeRemove" are.  Those will make sense when we actually create the language interpreter. It may be a little overkill to call this a "language" but it kind of is!

<h2>Applying the context to the AST</h2>

Now we need to interpret the syntax tree and apply the context bag to any context related tokens. If anybody read my previous posts about my language I wrote, this should all sound pretty similar (cause it is!)

[fsharp]
module Runner =  
    open Formatter   
    open Parser 

    let rec private eval (ctx : Context) = function 
        | Bag list -&gt; 
            match ctx.resolve list with
                | Some item -&gt; item
                | None -&gt; [List.fold (fun acc i -&gt; acc + &quot;.&quot; + i) &quot;$&quot; list]
        | Literals l -&gt; [l]
        | ForLoop (alias, bag, contents) -&gt; 
            [for value in (eval ctx bag) do
                ctx.runtimeAdd (alias, value)
                for elem in contents do
                    yield! eval ctx elem
                ctx.runtimeRemove alias]


    let run ctx text = 
        Parser.get text 
            |&gt; List.map (eval ctx)
            |&gt; List.reduce List.append
            |&gt; List.reduce (+)
[/fsharp]

What we have here is an eval function that acts as the main interpreter dispatch loop.  It's asked to evaluate the current token its given based on its current context.  

If we have a string literal, we just return it (as a list, since I am creating a list of evaluated results).  

If there is a bag (like <code>$anton.isgreat</code>) then try and resolve the bag path from the context.

If there is a for loop we want to evaluate the result of the for predicate and bind its value to the alias.  Then for each element we want to evaluate the contents of the for loop.  This is where we need to create a runtime storage of the alias, so we can do later lookups in the context.  You can see that each for loop adds its alias to the context and then removes it from the context afterwards.  This would mimic a regular language where inner loops can access outer declared variables, but not vice versa.  

<h2>Trying it out</h2>

Let's give our templating engine a whirl:

[fsharp]

let artists = new Context()
let root = new Context()

artists.add(&quot;nirvana&quot;, [&quot;come as you are&quot;;&quot;smells like teen spirit&quot;]);
root.add(&quot;artists&quot;, artists );

let templateText = &quot;$for $song in $artists.nirvana
		The current song is $song!
		$for $secondTime in $artists.nirvana
			Oh lets just loop again for fun. First value: $song, second: $secondTime
		$end
	 $end&quot;
	 
[/fsharp]

And the result is

[fsharp]
&gt; Runner.run root templateText;;
val it : string =
  &quot;The current song is come as you are!
		Oh lets just loop again for fun. First value: come as you are, second: come as you are
		Oh lets just loop again for fun. First value: come as you are, second: smells like teen spirit
   The current song is smells like teen spirit!
		Oh lets just loop again for fun. First value: smells like teen spirit, second: come as you are
		Oh lets just loop again for fun. First value: smells like teen spirit, second: smells like teen spirit
		&quot;
[/fsharp]
	
Not too bad!

Full source available at my <a href="https://github.com/devshorts/Playground/tree/master/FPropBag" target="_blank" rel="noopener noreferrer">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4365</wp:post_id>
		<wp:post_date><![CDATA[2014-03-10 08:00:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-03-10 08:00:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[simple-template-engine]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="fparsec"><![CDATA[fparsec]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="parsing"><![CDATA[parsing]]></category>
		<category domain="post_tag" nicename="tempating"><![CDATA[tempating]]></category>
		<category domain="post_tag" nicename="velocity"><![CDATA[velocity]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554621603;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3710;}i:1;a:1:{s:2:"id";i:4306;}i:2;a:1:{s:2:"id";i:4737;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>157</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #11, 2014 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2014/03/17/f-weekly-11-2014/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.135.48.142]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-03-17 07:44:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-03-17 07:44:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp wrote about &#8220;A simple templating engine&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Creating futures</title>
		<link>https://onoffswitch.net/2014/05/12/creating-futures/</link>
		<pubDate>Mon, 12 May 2014 08:00:22 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4394</guid>
		<description></description>
		<content:encoded><![CDATA[Futures (and promises) are a fun and useful <a href="http://en.wikipedia.org/wiki/Futures_and_promises" target="_blank" rel="noopener noreferrer">design pattern</a> in that they help encapsulate asynchronous work into composable objects. That and they help hide away the actual asynchronous execution implementation.  It doesn't matter if the future is finally resolved on the threadpool, in a new thread, or in an event loop (like nodejs).

Asynchronous work wrapped in futures has garnered a lot of attention in the javascript world since they alleviate the heavy use of nested callbacks to return a final result.  But futures aren't limited to just javascript, the C# async keyword is a form of a future, Java has a futures class, and a lot of other languages have the ability to use futures.  

In order to to demystify the concept of Futures lets build own version.  Futures aren't hard to implement, even when you have a language that doesn't have them built in (or if you are on the .NET micro without async or Tasks).  All we need to do is encapsulate a lambda and create an API that lets us chain deferred futures together.  Lets look at a final unit test to demonstrate what we're trying to accomplish. 

[csharp]
private void TestFutureImpl()
{
    int count = 0;

    Func&lt;int&gt; action = () =&gt;
    {
        Console.WriteLine(&quot;Running &quot; + count);
        Thread.Sleep(TimeSpan.FromMilliseconds(count * 1000));
        Console.WriteLine(&quot;Resolving &quot; + count);

        count++;
        return count;
    };

    var future = new NewThreadFuture&lt;int&gt;(action).Then(action).Then(action);

    Console.WriteLine(&quot;All setup, nonblock but now wait&quot;);

    Thread.Sleep(TimeSpan.FromSeconds(2));

    Console.WriteLine(&quot;Requesting result&quot;);

    var result = future.Resolve();

    Assert.AreEqual(3, result);
}
[/csharp]

With an output of 

[csharp]
All setup, nonblock but now wait
Running 0 
Resolving 0
Running 1
Resolving 
Running 
Requesting result 
Resolving 2 
[/csharp]

You can see the deferred action waits for a certain period of time, so it could take some time to complete.  But, with our future we can encapsulate this work, compose two other futures (that will evaluate after the first is complete), and finally when we <em>ask</em> for the result it will either block until its done, or immediately return the result if it was evaluated.

For my purposes, I wrote an eager evaluated futures class, this means that once the future is instantiated it immediately starts to try and execute.  The upside to this is that the result is available sooner, the downside is that if nobody requests the result you did work you didn't have to.

Lets take a look at what's really going on. Here is the basic skeleton of the future.  I wrap the passed in function in another function that is responsible for exception handling, as well as notifying whoever else is listening that the action completed (by setting a manual rest event mutex).  The other function is responsible for either waiting for the mutex to complete, or returning the completed result.  Subclasses can implement the <code>Execute</code> method which would just either run the wrapped method in a new thread, or run it in a thread pool. It honestly doesn't matter how its executed, it can even be run synchronously if you wanted to!

[csharp]
public abstract class Future&lt;T&gt;
{
    private bool _isComplete;

    private ManualResetEvent _mutex = new ManualResetEvent(false);

    private Exception _ex;

    private readonly object _lock = new object();

    private T _result;

    public Future(Func&lt;T&gt; function)
    {
        Execute(Wrapped(function));
    }

    protected abstract void Execute(Action wrapped);

    private Action Wrapped(Func&lt;T&gt; function)
    {
        return () =&gt;
        {
            try
            {
                _result = function();

                lock (_lock)
                {
                    _isComplete = true;
                }
            }
            catch (Exception ex)
            {
                _ex = ex;
            }
            finally
            {
                _mutex.Set();
            }
        };
    }

    public T Resolve()
    {
        lock (_lock)
        {
            if (_isComplete)
            {
                return _result;
            }
        }

        _mutex.WaitOne();

        if (_ex != null)
        {
            throw _ex;
        }

        return _result;        
    }

    // ....
[/csharp] 

Now lets look at how to compose futures.  The idea is you have one future, and the next future won't run until the first is complete.  You can either pass in the result of the first future, or just run another action (with no input).  From the users perspective you get one future that represents the result of all the composed actions.  All we need to do is to create a new lambda that first resolves the previous one (via the closure), then executes the next one, all wrapped in a new future!

[csharp]
public abstract Future&lt;T&gt; Then(Func&lt;T&gt; next);

protected Func&lt;T&gt; ThenWithoutResult(Func&lt;T&gt; next)
{
    return () =&gt;
    {
        Resolve();

        return next();
    };
}

protected Func&lt;Y&gt; ThenWithResult&lt;Y&gt;(Func&lt;T, Y&gt; next)
{
    return () =&gt;
    {
        var previousResult = Resolve();

        return next(previousResult);
    };
}

public abstract Future&lt;Y&gt; Then&lt;Y&gt;(Func&lt;T, Y&gt; next);
[/csharp]

Look at the implementation of one of the subclasses:

[csharp]
public class NewThreadFuture&lt;T&gt; : Future&lt;T&gt;
{
    public NewThreadFuture(Func&lt;T&gt; function) : base(function)
    {
    }

    protected override void Execute(Action wrapped)
    {
        var runner = new Thread(new ThreadStart(wrapped));

        runner.Start();
    }

    public override Future&lt;T&gt; Then(Func&lt;T&gt; next)
    {
        return new NewThreadFuture&lt;T&gt;(ThenWithoutResult(next));
    }

    public override Future&lt;Y&gt; Then&lt;Y&gt;(Func&lt;T, Y&gt; next)
    {
        return new NewThreadFuture&lt;Y&gt;(ThenWithResult(next));
    }
}
[/csharp]

Simple!  Now we can create asynchronous actions (remember that asynchronous just means nonblocking, but they are IN ORDER), and represent the entire workflow with a single future object.  From an implementors perspective we can now also control how we execute the actions, whether on the threadpool or on new threads (or we can add other mechanisms if we want).  This is because the future base class handles all the resolving synchronization making sure everything happens in order (just non-blocking).

For full source, check out my <a href="https://github.com/devshorts/Playground/tree/master/Futures/Futures" target="_blank" rel="noopener noreferrer">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4394</wp:post_id>
		<wp:post_date><![CDATA[2014-05-12 08:00:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-05-12 08:00:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[creating-futures]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="async"><![CDATA[async]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="design-patterns"><![CDATA[design patterns]]></category>
		<category domain="post_tag" nicename="futures"><![CDATA[futures]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561421346;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4296;}i:1;a:1:{s:2:"id";i:3435;}i:2;a:1:{s:2:"id";i:3565;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Constraint based sudoku solver</title>
		<link>https://onoffswitch.net/2014/06/02/sudoku-solver/</link>
		<pubDate>Mon, 02 Jun 2014 08:00:29 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4411</guid>
		<description></description>
		<content:encoded><![CDATA[A few weekends ago I decided to give solving <a href="http://en.wikipedia.org/wiki/Sudoku" target="_blank" rel="noopener noreferrer">Sudoku</a> a try. In case you aren't familiar with Sudoku, here is what an unsolved board looks like

[caption width="364" align="aligncenter"]<img src="http://onoffswitch.net/wp-content/uploads/2014/06/364px-Sudoku-by-L2G-20050714.svg_.png" width="200" height="200" class /> from wikipedia[/caption]

And here is a solved one

[caption width="364" align="aligncenter"]<img src="http://onoffswitch.net/wp-content/uploads/2014/06/364px-Sudoku-by-L2G-20050714_solution.svg_.png" width="200" height="200" class /> from wikipedia[/caption]

Sudoku, of size 3 is pretty easy.  Make a snapshot of the board, pick a random open cell, find out what its available possibilities are and set it to a value.   To figure out it's possibilities you need get the cells "group". This means all the values of the 3x3 cell it's in, as well as all the values of the row that it's in and the columns that it's in.  

Based on what is available, you can choose a number that isn't taken, plop it in down, and then recursively repeat. If nothing is available, and the board isn't empty, you messed up and the recursion will backtrack.  

Let's get solvin'

<h2>Some helper functions</h2>

Let's assume the board is a 2 dimensional nullable integer array, and that we have a class called <code>Location</code> that just encapsulates an (x,y) tuple:

[csharp]
public int? Get(int x, int y)
 {
     if (x &gt; _board.Length || y &gt; _board.Length)
     {
         throw new Exception(&quot;invalid position&quot;);
     }

     return _board[x, y];
 }

 public void Set(Location location, int value)
 {
     _board[location.X, location.Y] = value;

     for (int i = 0; i &lt; _emptySpaces.Count; i++)
     {
         if (_emptySpaces[i].X == location.X &amp;&amp; _emptySpaces[i].Y == location.Y)
         {
             _emptySpaces.RemoveAt(i);
             return;
         }
     }
 }
[/csharp]

Easy enough.  Let's also keep track of empty spaces as we set things since we'll want to be able to query for empty spaces later (rather than finding them), and have a wrapper to update values of the board.

Now lets make sure we can get all the information regarding a cell's group. This will be relevant for our calculations. It's a lot of boring boilerplate, but here it is:

[csharp]
public IEnumerable&lt;int&gt; UsedNumbersInSpace(Location location)
 {
     int x = location.X;
     int y = location.Y;

     foreach (var item in GetCol(x, y))
     {
         if (item.HasValue)
         {
             yield return item.Value;
         }
     }

     foreach (var item in GetRow(x, y))
     {
         if (item.HasValue)
         {
             yield return item.Value;
         }
     }

     foreach (var item in GetSquare(x, y))
     {
         if (item.HasValue)
         {
             yield return item.Value;
         }
     }
 }

 private IEnumerable&lt;int?&gt; GetRow(int x, int y)
 {
     for (int i = 0; i &lt; N * N; i++)
     {
         yield return Get(i, y);
     }
 }

 private IEnumerable&lt;int?&gt; GetCol(int x, int y)
 {
     for (int i = 0; i &lt; N * N; i++)
     {
         yield return Get(x, i);
     }
 }

 private IEnumerable&lt;int?&gt; GetSquare(int x, int y)
 {
     int xStart = x - (x % N);
     int yStart = y - (y % N);

     for (int i = xStart; i &lt; xStart + N; i++)
     {
         for (int j = yStart; j &lt; yStart + N; j++)
         {
             yield return Get(i, j);
         }
     }
 }
[/csharp]

<h2>Solving the board</h2>

Now for the fun part. Let's solve the board using a basic recursive backtracking brute force attempt:

[csharp]
public class Solver
{
    public static Board Solve(Board b)
    {
        var nextOpen = b.NextEmpty();

        if (nextOpen == null)
        {
            return b;
        }

        var taken = b.UsedNumbersInSpace(nextOpen);

        var available = b.TotalSpaceValues
                         .Except(taken)
                         .ToList();

        if (available.Count == 0)
        {
            return null;
        }

        foreach (var possible in available)
        {
            var newBoard = b.Snapshot();

            newBoard.Set(nextOpen, possible);

            var next = Solve(newBoard);

            if (next != null)
            {
                return next;
            }
        }

        return null;
    }
}
[/csharp]

Let's assume that <code>b.NextEmpty()</code> returns the first value from the <code>emptyList</code> backing collection in the board.  Basically giving you a random empty value on each iteration.  

That would totally work, but what happens when you move to a 4x4 board? Brute forcing it no longer really works.  The runtime of a board is n^n, where n is the number of characters.  So for a sudoku of size 3, thats a 9^9 runtime of 387420489. Shitty, but doable.  But for 4x4 now you're at 16^16 which is 18446744073709551616. Holy moly, our algorithm isn't gonna work anymore!

This is where a <a href="http://en.wikipedia.org/wiki/Constraint_programming" target="_blank" rel="noopener noreferrer">constraint based</a> approach would work.  Instead of just doing things as part of a single cycle (get empty spot, find available slots, put in valid piece, repeat), what if when we put in a number we also make some basic decisions and try to minimize the search space.  

<ol>
<li> If a cell group only has 1 open position, fill it.  Continue to iterate through the board until the rule comes up false.  </li>
<li> After it comes up false, return the next open position who has the least amount of available items to choose from. I.e. if cell (1,1) has the possibility of being [1,2,3,4,5] and cell (5,4) has the possibility of being [1,2], return cell (5,4) as the next empty cell.  This maximizes your failure rate and means you spend less time backtracking since your decision trees are more likely to fail sooner.
</li>
</ol>

It's constraint based because the moment we pin a cell that only has 1 possibility, we may have changed other parts of the board. Maybe now other spaces <em>also</em> only have one available space! We can keep going down this path until there is no more easy wins. So by choosing these values we've used some basic rules and logic to help us with our brute force search (which we still need to do when we are given too many options).

Given this, an easy way to tack this into the code above is to modify the <code>NextEmpty</code> function.

[csharp]
public Location NextEmpty()
{
    if (_emptySpaces.Count == 0)
    {
        return null;
    }

    var possibles = new Dictionary&lt;Location, List&lt;int&gt;&gt;();

    foreach (var emptySpace in _emptySpaces)
    {
        possibles[emptySpace] = TotalSpaceValues.Except(UsedNumbersInSpace(emptySpace)).ToList();

        if (possibles[emptySpace].Count == 1)
        {
            Set(emptySpace, possibles[emptySpace].First());

            return NextEmpty();
        }
    }

    return possibles.MinBy(kvp =&gt; kvp.Value.Count()).Key;
}
[/csharp]

So what this code does is as you call for the next empty, it tries to constrain the board when it finds a primo spot to pin.  Keeping in mind that at each solving iteration a full copy of the board is returned, its ok to mutate the board with this side effect.  As you work through sudoku on each iteration, the possible questionable space to work through minimizes and you can now reasonably solve 4x4 boards pretty quickly!

This is really just a C# implementation of <a href="http://norvig.com/sudoku.html">Peter Norvig's</a> sudoku solver, and if you'd like to see the full source (including the same tests that Peter Norvig used) check out my <a href="https://github.com/devshorts/Playground/tree/master/Sudoko">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4411</wp:post_id>
		<wp:post_date><![CDATA[2014-06-02 08:00:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-06-02 08:00:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[sudoku-solver]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="algorithms"><![CDATA[algorithms]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="sudoku"><![CDATA[sudoku]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554788393;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:4914;}i:2;a:1:{s:2:"id";i:3016;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Single producer many consumer</title>
		<link>https://onoffswitch.net/2014/02/26/single-producer-consumer/</link>
		<pubDate>Wed, 26 Feb 2014 22:21:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4435</guid>
		<description></description>
		<content:encoded><![CDATA[When I'm bored, I like to roll my own versions of things that already exist. That's not to say I use them in production, but I find that they are great learning tools. If you read the blog regularly you probably have realized I do this A LOT.  Anyways, today is no different. I was thinking about single producer, multiple consumer functions, like an <a href="http://aws.amazon.com/sns/" target="_blank" rel="noopener noreferrer">SNS</a> Topic, but for your local machine.  In reality, the best way to do this would be to publish your event through an Rx stream and consume it with multiple subscribers, but that's no fun. I want to roll my own!

BlockingCollection in .NET supports thread safe multiple consumers, but only 1 item will ever get dequeued from your collection. That means that if you have multiple threads waiting on a consuming enumerable, only one of them will get a result (not all of them). That's not that good if you want to have copies of your item dispatched to multiple subscribers.  But, if that is what you want, check out this other <a href="http://onoffswitch.net/async-producerconsumer-the-easy-way/" target="_blank" rel="noopener noreferrer">post of mine</a>.

What I need is a blocking consumer, a way to publish items, a threadsafe way to add and remove subscriptions, and a way to concurrently dispatch dequeued items to subscribed consumers. 

First let me show a subscriber instance. This is like a public token that the consumer of the topic will get when they subscribe.  All it has is a registered <code>OnNext</code> action and a way to unsubscribe itself from whatever its subscribed to. 

[csharp]
public class Subscriber&lt;T&gt;{
	private Action&lt;Subscriber&lt;T&gt;&gt; UnSubscribeAction { get; set; }

	public Action&lt;T&gt; OnNext{ get; private set; }

	public void UnSubscribe(){
		UnSubscribeAction (this);
	}

	public Subscriber(Action&lt;Subscriber&lt;T&gt;&gt; unsubscribe, Action&lt;T&gt; onNext){
		UnSubscribeAction = unsubscribe;

		OnNext = onNext;
	}
}
[/csharp]

And now the actual single producer many consumer (SPMC) implementation.  It's responsible for handling the listening on the consuming enumerable, the dispatching into the blocking collection, as well as parallelizing the re-distribution of the consumers. It's pretty simple!

[csharp]
public class SPMC&lt;T&gt; : IDisposable
{
	public SPMC (int boundedSize = int.MaxValue)
	{
		_blockingCollection = new BlockingCollection&lt;T&gt; (boundedSize);
	}

	private Object _lock = new object();

	private List&lt;Subscriber&lt;T&gt;&gt; _consumers = new List&lt;Subscriber&lt;T&gt;&gt;();

	private BlockingCollection&lt;T&gt; _blockingCollection;

	public Subscriber&lt;T&gt; Subscribe(Action&lt;T&gt; onNext){
		lock (_lock) {
			
			Action&lt;Subscriber&lt;T&gt;&gt; removalAction = instance =&gt; {
				lock (_lock) {
					_consumers.Remove (instance);
				}
			};

			var subscriber = new Subscriber&lt;T&gt; (removalAction, onNext);

			_consumers.Add (subscriber);

			return subscriber;
		}		
	}

	public void Start(){
		new Thread (() =&gt; {
			foreach (var item in _blockingCollection.GetConsumingEnumerable()) {
				lock (_lock) {
					Parallel.ForEach (_consumers, consumer =&gt; consumer.OnNext (item));					
				}
			}
		}).Start();
	}

	public void Stop(){
		_blockingCollection.CompleteAdding ();
	}

	public void Publish(T item){
		_blockingCollection.Add (item);
	}

	#region IDisposable implementation

	public void Dispose ()
	{
		Stop ();
	}

	#endregion
}
[/csharp]

And of course, a unit test to demonstrate its usage

[csharp]
[Test]
public void TestCase ()
{
	var subscriber1Collect = new List&lt;string&gt; ();
	var subscriber2Collect = new List&lt;string&gt; ();
	var subscriber3Collect = new List&lt;string&gt; ();

	var spmc = new SPMC&lt;String&gt; ();

	var subscriber1 = spmc.Subscribe(subscriber1Collect.Add);

	spmc.Subscribe(subscriber2Collect.Add);

	spmc.Start ();

	var t = new Thread (() =&gt; {
		while (true) { 
			Thread.Sleep (TimeSpan.FromMilliseconds (1000));

			spmc.Publish (DateTime.Now.ToString ());
		}
	});

	t.IsBackground = true;

	t.Start ();

	Thread.Sleep (TimeSpan.FromSeconds (5));

	subscriber1.UnSubscribe ();

	spmc.Subscribe(subscriber3Collect.Add);

	Thread.Sleep (TimeSpan.FromSeconds (5));

	Assert.GreaterOrEqual (subscriber3Collect.Count, 4);
	Assert.GreaterOrEqual (subscriber2Collect.Count, 9);
	Assert.GreaterOrEqual (subscriber1Collect.Count, 4);
}
[/csharp]

The asserts are greater than or equal just to give a 1 second wiggle room for the time dispatch variance.  
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4435</wp:post_id>
		<wp:post_date><![CDATA[2014-02-26 22:21:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-02-26 22:21:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[single-producer-consumer]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="async"><![CDATA[async]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="queue"><![CDATA[queue]]></category>
		<category domain="post_tag" nicename="rx"><![CDATA[Rx]]></category>
		<category domain="post_tag" nicename="topic"><![CDATA[topic]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561845469;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:532;}i:1;a:1:{s:2:"id";i:2985;}i:2;a:1:{s:2:"id";i:7777;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Debugging F# NUnit equals for mixed type tuples</title>
		<link>https://onoffswitch.net/2014/02/27/debugging-f-nunit-equals-mixed-type-tuples/</link>
		<pubDate>Thu, 27 Feb 2014 00:49:18 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4440</guid>
		<description></description>
		<content:encoded><![CDATA[Twitter user <a href="https://twitter.com/richardadalton" target="_blank" rel="noopener noreferrer">Richard Dalton</a> asked a great question recently:

<img src="http://onoffswitch.net/wp-content/uploads/2014/02/Screen-Shot-2014-02-26-at-4.08.38-PM.png" alt="Twitter question" width="590" height="292" class="aligncenter size-full wp-image-4441" />

And after a bit more digging he then mentioned 

<img src="http://onoffswitch.net/wp-content/uploads/2014/02/Screen-Shot-2014-02-26-at-4.10.17-PM.png" alt="Twitter question" width="586" height="293" class="aligncenter size-full wp-image-4442" />

Interesting. I downloaded the NUnit source and saw this:

[csharp highlight="61"]
public bool AreEqual(object x, object y, ref Tolerance tolerance)
{
    this.failurePoints = new List&lt;FailurePoint&gt;();

    if (x == null &amp;&amp; y == null)
        return true;

    if (x == null || y == null)
        return false;

    if (object.ReferenceEquals(x, y))
        return true;

    Type xType = x.GetType();
    Type yType = y.GetType();

    EqualityAdapter externalComparer = GetExternalComparer(x, y);
    if (externalComparer != null)
        return externalComparer.AreEqual(x, y);

    if (xType.IsArray &amp;&amp; yType.IsArray &amp;&amp; !compareAsCollection)
        return ArraysEqual((Array)x, (Array)y, ref tolerance);

    if (x is IDictionary &amp;&amp; y is IDictionary)
        return DictionariesEqual((IDictionary)x, (IDictionary)y, ref tolerance);

    //if (x is ICollection &amp;&amp; y is ICollection)
    //    return CollectionsEqual((ICollection)x, (ICollection)y, ref tolerance);

    if (x is IEnumerable &amp;&amp; y is IEnumerable &amp;&amp; !(x is string &amp;&amp; y is string))
        return EnumerablesEqual((IEnumerable)x, (IEnumerable)y, ref tolerance);

    if (x is string &amp;&amp; y is string)
        return StringsEqual((string)x, (string)y);

    if (x is Stream &amp;&amp; y is Stream)
        return StreamsEqual((Stream)x, (Stream)y);

    if (x is DirectoryInfo &amp;&amp; y is DirectoryInfo)
        return DirectoriesEqual((DirectoryInfo)x, (DirectoryInfo)y);

    if (Numerics.IsNumericType(x) &amp;&amp; Numerics.IsNumericType(y))
        return Numerics.AreEqual(x, y, ref tolerance);

    if (tolerance != null &amp;&amp; tolerance.Value is TimeSpan)
    {
        TimeSpan amount = (TimeSpan)tolerance.Value;

        if (x is DateTime &amp;&amp; y is DateTime)
            return ((DateTime)x - (DateTime)y).Duration() &lt;= amount;

        if (x is TimeSpan &amp;&amp; y is TimeSpan)
            return ((TimeSpan)x - (TimeSpan)y).Duration() &lt;= amount;
    }

    if (FirstImplementsIEquatableOfSecond(xType, yType))
        return InvokeFirstIEquatableEqualsSecond(x, y);
    else if (xType != yType &amp;&amp; FirstImplementsIEquatableOfSecond(yType, xType))
        return InvokeFirstIEquatableEqualsSecond(y, x);
    
    return x.Equals(y);
}
[/csharp]

That last line tipped me off. So lets look at some tests now:

[fsharp]
&gt; z;;
 
val it : int [] * int = ([|1|], 1)
 
&gt; p;;
 
val it : int [] * int = ([|1|], 1)
 
&gt; z.Equals(p);;
 
val it : bool = false
 
&gt; z = p;;
 
val it : bool = true
[/fsharp]

So that makes sense, there's no custom equality comparer for tuples, and since the references are different the obj equals fails.  But why do the other things Richard said hold true then?

Well arrays have their own custom comparer that compares contents, that much is visible in the NUnit source.  And tuples look to generate the same hash code IF they have value types in them, which you can test in fsi.

[fsharp]
&gt; let ref1 = ([], 2);;

val ref1 : 'a list * int

&gt; let ref2 = ([], 2);;

val ref2 : 'a list * int

&gt; ref1.GetHashCode();;

val it : int = 2

&gt; ref2.GetHashCode();;

val it : int = 2
[/fsharp]

But if a tuple contains a reference type

[fsharp]
&gt; let ref3 = ([||], 1);;

val ref3 : 'a [] * int

&gt; let ref4 = ([||], 1);;

val ref4 : 'a [] * int

&gt; ref3.GetHashCode();;

val it : int = -1869554978

&gt; ref4.GetHashCode();;

val it : int = -259699334
[/fsharp]

Suddenly they aren't equal!  Arrays, being a built in .NET primitive, follow <a href="http://stackoverflow.com/questions/720177/default-implementation-for-object-gethashcode" target="_blank" rel="noopener noreferrer">these semantics</a> for generating a hash code.  Basically, they return a different value per each instance of the object in the app domain.  

Now why does the f# <code>=</code> operator work? From the <a href="https://github.com/fsharp/fsharp/blob/master/src/fsharp/FSharp.Core/prim-types.fs#L1975" target="_blank" rel="noopener noreferrer">source</a>, it looks like they have created custom comparators for f# types which does structural equality:

[fsharp]
// Note: because these FastEqualsTupleN functions are devirtualized by (=), they have PER semantics
let inline FastEqualsTuple2 (comparer:System.Collections.IEqualityComparer) (x1,x2) (y1,y2) = 
	GenericEqualityWithComparerFast comparer x1 y1 &amp;&amp;
	GenericEqualityWithComparerFast comparer x2 y2

let inline FastEqualsTuple3 (comparer:System.Collections.IEqualityComparer) (x1,x2,x3) (y1,y2,y3) = 
	GenericEqualityWithComparerFast comparer x1 y1 &amp;&amp;
	GenericEqualityWithComparerFast comparer x2 y2 &amp;&amp;
	GenericEqualityWithComparerFast comparer x3 y3
            
// .... etc ...

let inline (=) x y = GenericEquality x y
[/fsharp]

Neat! I love it when things make sense.




]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4440</wp:post_id>
		<wp:post_date><![CDATA[2014-02-27 00:49:18]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-02-27 00:49:18]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[debugging-f-nunit-equals-mixed-type-tuples]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="equality"><![CDATA[equality]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554308616;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3735;}i:1;a:1:{s:2:"id";i:4411;}i:2;a:1:{s:2:"id";i:2735;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>158</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #9, 2014 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2014/03/03/f-weekly-9-2014/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.8.139]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-03-02 21:04:54]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-03-02 21:04:54]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp posted &#8220;Debugging F# NUnit equals for mixed type tuples&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>RxJava Observables and Akka actors</title>
		<link>https://onoffswitch.net/2014/03/07/rxjava-observables-akka-actors/</link>
		<pubDate>Fri, 07 Mar 2014 23:41:36 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4456</guid>
		<description></description>
		<content:encoded><![CDATA[I was playing with both <a href="http://akka.io/" target="_blank" rel="noopener noreferrer">akka</a> and <a href="https://github.com/Netflix/RxJava" target="_blank" rel="noopener noreferrer">rxjava</a> and came across the <a href="java.dzone.com/articles/creating-rxjava-observable" target="_blank" rel="noopener noreferrer">following post</a> that described how to map rxjava observables from messages posted to akka actors. 

Since my team works in java, I decided to try mapping the concept to java directly, but found that there was an issue. When I tried to have multiple subscribers listen on the stream I'd get an exception since more than one subscriber would send the "subscribe" message and try to modify the akka receive context.

I also wanted to make it easier to extend the actors to be able to process a piece of work, and then resubmit it for consumption by the observable.

<h2>The subscribe command messages</h2>

First, let me show the commands we can send to the actors. This is just mapping the scala union type that the original blog post had.  The <code>@Data</code> attribute is part of <a href="projectlombok.org/features/index.html" target="_blank" rel="noopener noreferrer">project lombok</a> and auto creates an immutable class with a constructor and getters for your private final fields:

[java]
package com.devshorts.rx;

import java.io.Serializable;

public class UnSubscribe implements Serializable {}
[/java]

And

[java]
package com.devshorts.rx;

import lombok.Data;
import rx.functions.Action1;

import java.io.Serializable;

@Data
public class Subscribe implements Serializable {
    private final Action1 subscription;
}
[/java]

<h2>Observable actor</h2>

Second, let me show the mapped observable actor. This is the abstract class that all observable actors should inherit from and it manages changing the default akka context to invoke receive messages on the supplied procedure. All this means is that when the actor gets a <code>Subscribe</code> it'll change what function it uses to receive messages to the one that is supplied by the subscribe object: <code>onRecieve</code> won't ever get called anymore.  <code>unbecome</code> would undo that and set it back to the default handler.

[java]
package com.devshorts.rx;

import akka.actor.UntypedActor;
import akka.japi.Procedure;

public abstract class ObservableActor extends UntypedActor {

    @Override
    public void onReceive(final Object o) throws Exception {
        if(o instanceof Subscribe){
            System.out.println(&quot;Subscribed!&quot;);

            // change the default 'onReceive' behavior to now be the anonymous class
            // implementation.  This means that all new requests will go to
            // processMessage and returned to the observable as a transformation
            getContext().become(new Procedure&lt;Object&gt;() {
                @Override
                public void apply(Object message) throws Exception {
                    if(message instanceof UnSubscribe){
                        getContext().unbecome();

                        System.out.println(&quot;Unsubscribed&quot;);
                    }
                    else{
                        Subscribe subscriber = (Subscribe)(o);

                        subscriber.getSubscription().call(processMessage(message));
                    }
                }
            });
        }
        else{
            System.out.println(&quot;Default behavior &quot; + o);
        }

    }

    protected abstract Object processMessage(Object message);
}
[/java]

Notice the abstract method though. This is what I want all subsequent actors to implement and acts as the "do work" method.

Here's an actor that just re-dispatches its input:

[java]
package com.devshorts.rx;

public class AkkEcho extends ObservableActor {
    @Override
    protected Object processMessage(Object message) {
        return message;
    }
}
[/java]

And here is one that modifies the input a little

[java]
package com.devshorts.rx;

public class AkkaMapEcho extends ObservableActor {
    @Override
    protected Object processMessage(Object message) {
        String m = (String)(message);

        return m + &quot; mapped!&quot;;
    }
}
[/java]

<h2>Creating the observable wrapper</h2>

Below is the observable wrapper. It creates a publish subject that handles incoming and outgoing messages, as well as taking care of instantiating only <em>one</em> observable bound the actor.  What is returned is now a safe consumable stream that multiple subscribers can read off of:

[java]
package com.devshorts.rx;

import akka.actor.ActorRef;
import rx.Observable;
import rx.Subscriber;
import rx.functions.Action1;
import rx.subjects.PublishSubject;

public class ObservableUtil {

    public static &lt;T&gt; Observable&lt;T&gt; fromActor(final ActorRef actor){
        final PublishSubject&lt;T&gt; subj = PublishSubject.create();

        Observable&lt;T&gt; observable = Observable.create(new Observable.OnSubscribe&lt;T&gt;() {
            @Override
            public void call(final Subscriber&lt;? super T&gt; subscriber) {

                /**
                 * Create an initial subscribe method that modifies
                 * the actors default behavior to proxy the request to the
                 * subscribers 'onNext' function.  This way
                 * when someone posts to the actor, we intercept the actors RESPONSE
                 * and pipe it into the subscribers work queue.
                 */
                Subscribe msg = new Subscribe(new Action1&lt;T&gt;() {
                    @Override
                    public void call(T o) {
                        subscriber.onNext(o);
                    }
                });

                actor.tell(msg, ActorRef.noSender());
            }
        });

        /**
         * Create one subscriber to this actor observable and re-proxy the result
         * to the subject (this lets other people subscribe to the subject, and keeps
         * the akka observable from having to worry about managing who is substring to what
         * and de-muddles up the behavior modification code. this call also invokes the 
         * subscribe command pattern above.
         */
        observable.subscribe(new Action1&lt;T&gt;() {
            @Override
            public void call(T o) {
                subj.onNext(o);
            }
        });

        /**
         * Return the subject's observable stream for others to subscribe on
         */
        return subj.asObservable();
    }
}
[/java]

<h2>Using it</h2>

You can imagine a distributed system where you have actors and they are receiving messages, but you want to work on their output transformations or listen to them via the observable API. 

This makes it really nice to have uniform time based event behaviors that you can leverage in your code. It no longer matters that the events are sourced from an akka actor, or if they are sourced from futures, or iterables, or whatever. They just exist, and you have subscribed to them.


Now let's check out a unit test that uses the actor. We'll have two observables that listen and capture events, and the test will also be responsible for posting values to the actor.

[java]
/**
 * Wrap an akka actor's behavior into an observable stream.
 *
 * Now your producer api is the actor, but your consumers can
 * manipulate the underlying event stream to create behaviors
 * @throws InterruptedException
 */
@Test
public void AkkaObservable() throws InterruptedException {
    final Object mutex = new Object();

    final ActorRef actor = createActorOfType(AkkEcho.class);

    final List&lt;String&gt; results = new ArrayList&lt;&gt;();
    final List&lt;String&gt; distinctResults = new ArrayList&lt;&gt;();

    final Observable&lt;String&gt; observable = ObservableUtil.fromActor(actor);

    observable.subscribe(new Action1&lt;String&gt;() {
        @Override
        public void call(String o) {
            System.out.println(o);

            if(o.equals(&quot;done&quot;)){
                synchronized (mutex){ mutex.notify(); }
            }
            else{
                results.add(o);
            }
        }
    });

    observable.distinct().subscribe(new Action1&lt;String&gt;() {
        @Override
        public void call(String o) {
            distinctResults.add(o);
        }
    });

    actor.tell(&quot;foo&quot;,  ActorRef.noSender());
    actor.tell(&quot;foo&quot;,  ActorRef.noSender());
    actor.tell(&quot;foo&quot;,  ActorRef.noSender());
    actor.tell(&quot;bar&quot;,  ActorRef.noSender());
    actor.tell(&quot;done&quot;, ActorRef.noSender());

    synchronized (mutex){ mutex.wait(); }

    Assert.assertEquals(results, Arrays.asList(&quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;bar&quot;));
    Assert.assertEquals(distinctResults, Arrays.asList(&quot;foo&quot;, &quot;bar&quot;, &quot;done&quot;));
}

private ActorRef createActorOfType(Class&lt;? extends Actor&gt; clazz) {
    ActorSystem system = ActorSystem.create(&quot;client&quot;);

    return system.actorOf(Props.create(clazz), &quot;rcv&quot;);
}
[/java]

Oh, and this is onoffswitch.net's 100th post! wooo!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4456</wp:post_id>
		<wp:post_date><![CDATA[2014-03-07 23:41:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-03-07 23:41:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rxjava-observables-akka-actors]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="actors"><![CDATA[actors]]></category>
		<category domain="post_tag" nicename="akka"><![CDATA[akka]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="functional-reactive"><![CDATA[functional reactive]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="rx"><![CDATA[Rx]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561910971;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4627;}i:1;a:1:{s:2:"id";i:4593;}i:2;a:1:{s:2:"id";i:4629;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>159</wp:comment_id>
			<wp:comment_author><![CDATA[Thomas Kreyling]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[thomas.kreyling@googlemail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[85.181.18.134]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-18 12:25:11]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-18 12:25:11]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hi Anton,

I was wondering whether this will work with an actor that is remote from the code that wants to observe the actor.
Is Action1 subscription serialized and send to the remote actor?
Does subscriber.getSubscription().call back to the local machine?

Best regards,
Thomas]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>160</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.143.125.150]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-30 01:12:31]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-30 01:12:31]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Probably won't work across machines, but I think you can refactor this a bit to wrap the actor reference to make it do that.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>159</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Strongly typed powershell csv parser</title>
		<link>https://onoffswitch.net/2014/03/22/strongly-typed-powershell-csv-parser/</link>
		<pubDate>Sat, 22 Mar 2014 20:25:42 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4463</guid>
		<description></description>
		<content:encoded><![CDATA[Somehow I missed the powershell boat. I've been a .NET developer for years and I trudged through using the boring old cmd terminal, frequently mumbling about how much I missed zsh. But something snapped and I decided to really dive into powershell and learn why those who use it really love the hell out of it.  After realizing that the reason everyone loves it is because everything is strongly typed and you can use .NET in your shell I was totally sold.  

My first forays into powershell included customizing the shell environment. First I got <a href="https://code.google.com/p/conemu-maximus5/" target="_blank" rel="noopener noreferrer">conemu</a> and made it look nice and pretty. Next was to get an ls highlighting module, since I love that about unix shells.

I set up a few fun aliases in <a href="https://github.com/devshorts/powershell_scripts" target="_blank" rel="noopener noreferrer">my profile</a> and felt ready to conquer the world! My next experiment was to try and create an actual binary cmdlet.  I figured, what better way than to create a csv reader.  Now, I realize there is already an <code>Import-Csv</code> cmdlet that types your code, but I figured I'd write one from scratch, since apparently that's what I tend to do (instead of inventing anything new).

My hope was to make it so that it would emit strongly typed objects (which it does), but forwarning, you don't get intellisense on it in the shell.  This is due to the fact that types are generated at runtime and not compile time.  

For the lazy, here is a link to the <a href="https://github.com/devshorts/Playground/tree/master/PowerShellCmdlets/Csv" target="_blank" rel="noopener noreferrer">github</a>.

<h2>The Plan</h2>

At first I thought I'd just wrap the F# csv type provider, but I realized that the type provider needs a template to generate its internal data classes. That won't do here because the cmdlet needs to accept any arbitrary csv file and strongly type at runtime.

To solve that, I figured I could leverage the <a href="http://fsharp.github.io/FSharp.Data/library/CsvFile.html" target="_blank" rel="noopener noreferrer">F# data csv library</a> which would do the actual csv parsing, and then emit runtime bytecode to create data classes representing the header values.

As emitting bytecode is a pain in the ass, I wanted to keep my data classes simple.  If I had a csv like:

[code]
Name,Age,Title
Anton,30,Sr Engineer
Faisal,30,Sr Engineer
[/code]

Then I wanted to emit a class like

[csharp]
public class Whatever{
    public String Name;
    public String Age;
    public String Title;

    public Whatever(String name, String age, String title){ 
       Name = name;
       Age = age;
       Title = title;
    }
}
[/csharp]

Since that would be the bare minimum that powershell would need to display the type.  

<h2>Emitting bytecode</h2>

First, lets look at the final result of what we need.  The best way to do this is to create a sample type in an assembly and then to use <code>Ildasm</code> (an IL disassembler) to view the bytecode. For example, the following class

[csharp]
using System;

namespace Sample
{
    public class Class1
    {
        public String foo;
        public String bar;

        public Class1(String f, String b)
        {
            foo = f;
            bar = b;
        }
    }
}
[/csharp]

Decompiles into this:

[code]
.method public hidebysig specialname rtspecialname 
        instance void  .ctor(string f,
                             string b) cil managed
{
  // Code size       24 (0x18)
  .maxstack  8
  IL_0000:  ldarg.0
  IL_0001:  call       instance void [mscorlib]System.Object::.ctor()
  IL_0006:  nop
  IL_0007:  nop
  IL_0008:  ldarg.0
  IL_0009:  ldarg.1
  IL_000a:  stfld      string Sample.Class1::foo
  IL_000f:  ldarg.0
  IL_0010:  ldarg.2
  IL_0011:  stfld      string Sample.Class1::bar
  IL_0016:  nop
  IL_0017:  ret
} // end of method Class1::.ctor
[/code]

While I didn't just divine how to write bytecode by looking at the IL (I followed some other blog posts), when I got an "invalid bytecode" CLR runtime error, it was nice to be able to compare what I was emitting which what I expected to emit. This way simple errors (like forgetting to load something on the stack) became pretty apparent.

To emit the proper bytecode, we need a few boilerplate items: an assembly, a type builder, an assembly builder, a module builder, and a field builder.  These are responsible for the metadata you need to finally emit your built type.

[fsharp]
let private assemblyName = new AssemblyName(&quot;Dynamics&quot;)

let private assemblyBuilder = AppDomain.CurrentDomain.DefineDynamicAssembly(assemblyName, AssemblyBuilderAccess.RunAndSave)

let private moduleBuilder = assemblyBuilder.DefineDynamicModule(assemblyName.Name, assemblyName.Name + &quot;.dll&quot;)

let private typeBuilder typeName = moduleBuilder.DefineType(typeName, TypeAttributes.Public)

let private fieldBuilder (typeBuilder:TypeBuilder) name fieldType : FieldBuilder = 
    typeBuilder.DefineField(name, fieldType, FieldAttributes.Public)

let private createConstructor (typeBuilder:TypeBuilder) typeList =
    typeBuilder.DefineConstructor(MethodAttributes.Public, CallingConventions.Standard, typeList |&gt; List.toArray)

[/fsharp]

None of this is really all that interesting and hopefully is self explanatory. 

The <code>fieldBuilder</code> is important since that will let us declare our local fields.  In fact, once we've declared our local fields using the builder, the only bytecode we have to emit is the constructor (which accepts arguments and instantiates fields in them).

Here is the necessary code to build such a constructor.  

[fsharp]
let private callDefaultConstructor (gen: ILGenerator) = 
    let objType = typeof&lt;obj&gt;
    gen.Emit(OpCodes.Call, objType.GetConstructor(Type.EmptyTypes))
    gen.Emit(OpCodes.Ldarg_0)

let private loadThis (gen: ILGenerator) = 
    gen.Emit(OpCodes.Ldarg_0)
    gen

let private emitNewInstanceRef (gen : ILGenerator) =
    gen |&gt; loadThis |&gt; callDefaultConstructor

let private assignField (argIndex : int) (field : FieldBuilder) (gen : ILGenerator) =     
    gen.Emit(OpCodes.Ldarg, argIndex)
    gen.Emit(OpCodes.Stfld, field)
    gen

let private loadConstructorArg (gen : ILGenerator) ((num, field) : int * FieldBuilder) = 
    gen |&gt; loadThis |&gt; assignField num field

let private completeConsructor (gen : ILGenerator) = gen.Emit(OpCodes.Ret)

let private build (fields : FieldBuilder list) (cons : ConstructorBuilder) = 
    let generator = cons.GetILGenerator()

    generator |&gt; emitNewInstanceRef

    let fieldsWithIndexes = fields |&gt; List.zip [1..(List.length fields)]

    fieldsWithIndexes
        |&gt; List.map (loadConstructorArg generator)
        |&gt; ignore

    generator |&gt; completeConsructor
[/fsharp]

A few points of interest. 

<ul>
<li>Calls that make reference to OpCodes.Ldarp_0 are loading the "this" object to work on. </li>
<li>OpCodes.Stdfld sets the passed in field to the value previously pushed on the stack.</li>
<li>Opcodes.Ldarg with the index passed to it is a dynamic way of saying "load argument X onto the stack"</li>
</ul>

The final piece of the block is to tie it all together. Create field instances, take the target types and create a constructor, then return the type.

[fsharp]
type FieldName = string
type TypeName = string

let make (name : TypeName) (types : (FieldName * Type) list)= 
    let typeBuilder = typeBuilder name
    let fieldBuilder = fieldBuilder typeBuilder
    let createConstructor = createConstructor typeBuilder        
    let fields = types |&gt; List.map (fun (name, ``type``) -&gt; fieldBuilder name ``type``)
    let definedConstructor = types |&gt; List.map snd |&gt; createConstructor

    
    definedConstructor |&gt; build fields

    typeBuilder.CreateType()
[/fsharp]

<h2>Instantiating your type</h2>

Lets say we have a record that describes a field, its type, and a target value

[fsharp]
type DynamicField = {
    Name : String;
    Type : Type;
    Value: obj;
}
[/fsharp]

Then we can easily instantiate a target type with

[fsharp]
let instantiate (typeName : TypeName) (objInfo : DynamicField list) =
    let values = objInfo |&gt; List.map (fun i -&gt; i.Value) |&gt; List.toArray
    let types  = objInfo |&gt; List.map (fun i -&gt; (i.Name, i.Type))

    let t = make typeName types

    Activator.CreateInstance(t, values)
[/fsharp]

It's important to note that <code>values</code> is an <code>obj []</code>.  Because its an object array we can pass it to the activates overloaded function that wants a <code>params obj[]</code> and so it'll treat each object in the object array as another argument to the constructor.

<h2>Dynamic static typing of CSV's</h2>

Since there is a way to dynamically create classes at runtime, it should be easy for us to leverage this to do the csv strong typing. In fact, the entire reader is this and emits to you a list of strongly typed entries:

[fsharp]
open System
open System.Reflection
open System.IO
open DataEmitter
open FSharp.Data.Csv

module CsvReader = 
    let rand = System.Random()

    let randomName() = rand.Next (0, 999999) |&gt; string

    let defaultHeaders size = [0..size] |&gt; List.map (fun i -&gt; &quot;Unknown Header &quot; + (string i))

    let load (stream : Stream) = 
        let csv = CsvFile.Load(stream).Cache()

        let headers = match csv.Headers with 
                        | Some(h) -&gt; h |&gt; Array.toList
                        | None -&gt; csv.NumberOfColumns |&gt; defaultHeaders

        let fields = headers |&gt; List.map (fun fieldName -&gt; (fieldName, typeof&lt;string&gt;))

        let typeData = make (randomName()) fields

        [
            for item in csv.Data do       
                let paramsArr = item.Columns |&gt; Array.map (fun i -&gt; i :&gt; obj)
                yield Activator.CreateInstance(typeData, paramsArr)         
        ]
[/fsharp]

The <code>randomName()</code> is a silly workaround to make sure I don't create the same <code>Type</code> in an assembly.  Each time you run the csv reader it'll create a new random type representing that csv's data. I could maybe have optimized this that if someone calls in for a type with the same list of headers that another type had then to re-use that type instead of creating a duplicate, oh well.

<h2>Using the reader from the cmdlet</h2>

Like I mentioned in the beginning, there is a major flaw here.  The issue is that since my types are generated at runtime (which was really fun to do), it doesn't help me at all.  Cmdlet's need to expose their output types via an <code>OutputType</code> attribute, and since its an attribute I can't expose the type dynamically.

Either way, here is the entire csv cmdlet

[fsharp]
namespace CsvHandler

open DataEmitter
open System.Management.Automation
open System.Reflection
open System
open System.IO

[&lt;Cmdlet(&quot;Read&quot;, &quot;Csv&quot;)&gt;]
type CsvParser() =
    inherit PSCmdlet()
    
    [&lt;Parameter(Position = 0)&gt;]
    member val File : string = null with get, set

    override this.ProcessRecord() = 
        let (fileNames, _) = this.GetResolvedProviderPathFromPSPath this.File
             
        for file in fileNames do   
            use fileStream = File.OpenRead file

            fileStream 
                |&gt; CsvReader.load 
                |&gt; List.toArray 
                |&gt; this.WriteObject
[/fsharp]

This reads an implicit file name (or file with wildcards) and leverages the inherited <code>PsCmdlet</code> class to resolve the path from the passed in file (or expand any splat'd files like <code>some*</code>).  All we do now is pass each file stream to the reader, convert to an array, and pass it to the next item in the powershell pipe.

<h2>See it in action</h2>

Maybe this whole exercise was overkill, but let's finish it out anyways. Let's say we have a csv like this:

[code]
Year,Make,Model,Description,Price
1997,Ford,E350,&quot;ac, abs, moon&quot;,3000.00
1999,Chevy,&quot;Venture &quot;&quot;Extended Edition&quot;&quot;&quot;,&quot;&quot;,4900.00
1999,Chevy,&quot;Venture &quot;&quot;Extended Edition, Very Large&quot;&quot;&quot;,,5000.00
1996,Jeep,Grand Cherokee,&quot;MUST SELL!
air, moon roof, loaded&quot;,4799.00
[/code]

We can do the following

<img src="http://onoffswitch.net/wp-content/uploads/2014/03/output1.png" alt="output1" width="996" height="577" class="aligncenter size-full wp-image-4466" />

And filter on items

<img src="http://onoffswitch.net/wp-content/uploads/2014/03/filter.png" alt="filter" width="1045" height="138" class="aligncenter size-full wp-image-4467" />

<h2>Cleanup</h2>

After getting draft one done, I thought about the handling of the IL generator in the Data Emitter. There are two things I wanted to accomplish:

1. Clean up having to seed the generator reference to all the functions
2. Clean up passing an auto incremented index to the field initializer

After some mulling I realized that implementing a computation expression to handle the seeded state would be perfect for both scenarios.  We can create an IlBuilder computation expression that will hold onto the reference of the generator and pass it to any function that uses <code>do!</code> syntax.  We can do the same for the auto incremented index with a different builder. Let me show you the final result and then the builders:

[fsharp]
let private build (fields : FieldBuilder list) (cons : ConstructorBuilder) =
    let generator = cons.GetILGenerator()

    let ilBuilder = new ILGenBuilder(generator)

    let forNextIndex = new IncrementingCounterBuilder()

    ilBuilder {   
        do! loadThis
        do! callDefaultConstructor
        do! loadThis

        for field in fields do
            do! loadThis
            do! forNextIndex { return loadArgToStack }
            do! field |&gt; setFieldFromStack  
            
        do! emitReturn
    }
[/fsharp]

And both builders:

[fsharp]
(* encapsulates an incrementable index *)
type IncrementingCounterBuilder () = 
    let mutable start = 0
    member this.Return(expr) = 
        start &lt;- start + 1            
        expr start 
          
(* Handles automatically passing the il generator through the requested calls *)
type ILGenBuilder (gen: ILGenerator) = 
    member this.Bind(expr, func)= 
        expr gen
        func () |&gt; ignore

    member this.Return(v) = ()
    member this.Zero () = ()
    member this.For(col, func) = for item in col do func item
    member this.Combine expr1 expr2 = ()
    member this.Delay expr = expr()
[/fsharp]

Now all mutability and state is contained in the expression. I think this is a much cleaner implementation and the functions I used in the builder workflow didn't have to have their function signatures changed!

<h2>Conclusion</h2>

Sometimes you just jump in and don't realize the end goal won't work, but I did learn a whole lot figuring this out so the time wasn't wasted.

Check out full source at <a href="https://github.com/devshorts/Playground/tree/master/PowerShellCmdlets/Csv" target="_blank" rel="noopener noreferrer">my github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4463</wp:post_id>
		<wp:post_date><![CDATA[2014-03-22 20:25:42]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-03-22 20:25:42]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[strongly-typed-powershell-csv-parser]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="bytecode"><![CDATA[bytecode]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="csv"><![CDATA[csv]]></category>
		<category domain="post_tag" nicename="f"><![CDATA[F#]]></category>
		<category domain="post_tag" nicename="powershell"><![CDATA[powershell]]></category>
		<category domain="post_tag" nicename="reflection"><![CDATA[reflection]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560310780;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4598;}i:1;a:1:{s:2:"id";i:4529;}i:2;a:1:{s:2:"id";i:4213;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>161</wp:comment_id>
			<wp:comment_author><![CDATA[F# Weekly #12, 2014 | Sergey Tihon&#039;s Blog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://sergeytihon.wordpress.com/2014/03/24/f-weekly-12-2014/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[66.155.8.88]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-03-24 07:49:56]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-03-24 07:49:56]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] Anton Kropp presented &#8220;Strongly typed powershell csv parser&#8220;. [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Avoiding nulls with expression trees</title>
		<link>https://onoffswitch.net/2014/04/04/avoiding-nulls-expression-trees/</link>
		<pubDate>Fri, 04 Apr 2014 04:23:28 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4493</guid>
		<description></description>
		<content:encoded><![CDATA[I've blogged about this subject <a href="http://onoffswitch.net/minimizing-null-ref/" target="_blank" rel="noopener noreferrer">before</a>, but I REALLY hate null refs.  This is one of the reasons I love F# and other functional languages, null ref's almost never happen. But, in the real world I work as a C# dev and have to live with C#'s... nuisances.  

In the other post, a big problem with the dynamic proxy was that it only worked with virtual methods, so it wasn't really all that practical. This time around I decided to try a different route and leverage expression tree's to actually build out the if checks automatically.

For the impatient, full source available at my <a href="https://github.com/devshorts/MonadicNull" target="_blank" rel="noopener noreferrer">github </a>and the library is available on <a href="https://www.nuget.org/packages/Devshorts.MonadicNull/0.2.1" target="_blank" rel="noopener noreferrer">nuget</a>

<h2>Demonstration</h2>
Let me demonstrate the final usage first.  If all of users properties and methods return null, executing this whole chain would fail starting at the null result of <code>GetSchool()</code>. But, by using the Option static class we can safely deconstruct and inspect the expression, returning if the chain is valid, what failed in it, and what was the final value (if one existed).

[csharp]
public void TestGetSafe()
{
    var user = new User();

    MethodValue&lt;string&gt; name = Option.Safe(() =&gt; user.GetSchool().District.Street.Name);

    Assert.IsFalse(name.ValidChain());
}
[/csharp]

The lambda is converted to an expression tree and rebuilt into this glorious mess:

[code]
.Lambda #Lambda1&lt;System.Func`2[NoNulls.Tests.SampleData.User,Devshorts.MonadicNull.MethodValue`1[System.String]]&gt;(NoNulls.Tests.SampleData.User $u)
{
    .Block() {
        .Block(NoNulls.Tests.SampleData.User $var1) {
            $var1 = $u;
            .If ($var1 == null) {
                .New Devshorts.MonadicNull.MethodValue`1[System.String](
                    null,
                    &quot;u&quot;,
                    False)
            } .Else {
                .Block(NoNulls.Tests.SampleData.School $var2) {
                    $var2 = .Call $var1.GetSchool();
                    .If ($var2 == null) {
                        .New Devshorts.MonadicNull.MethodValue`1[System.String](
                            null,
                            &quot;u.GetSchool()&quot;,
                            False)
                    } .Else {
                        .Block(NoNulls.Tests.SampleData.District $var3) {
                            $var3 = $var2.District;
                            .If ($var3 == null) {
                                .New Devshorts.MonadicNull.MethodValue`1[System.String](
                                    null,
                                    &quot;u.GetSchool().District&quot;,
                                    False)
                            } .Else {
                                .Block(NoNulls.Tests.SampleData.Street $var4) {
                                    $var4 = $var3.Street;
                                    .If ($var4 == null) {
                                        .New Devshorts.MonadicNull.MethodValue`1[System.String](
                                            null,
                                            &quot;u.GetSchool().District.Street&quot;,
                                            False)
                                    } .Else {
                                        .Block(System.String $var5) {
                                            $var5 = $var4.Name;
                                            .New Devshorts.MonadicNull.MethodValue`1[System.String](
                                                $var5,
                                                &quot;u.GetSchool().District.Street.Name&quot;,
                                                True)
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}
[/code]

The return value of the <code>Safe</code> method is a new object called <code>MethodValue<T></code> which tells you if the expression was successful (i.e. no nulls), or if it did fail, <em>what</em> failed (i.e. where in the expression it failed).  And of course if the chain is valid you can get the value safely.

<h2>The magic</h2>

The magic is actually pretty easy.  Because we have access to the expression tree we can rewrite it however we want.  If you aren't familiar with expression trees, the idea is that the compiler can convert a lambda into it's abstract syntax tree. That means you can traverse it (with a visitor) and know information about the objects.

Since we need to know the information about the whole call chain we need to iterate over it all first.  As we iterate we should track of what happened in the call chain so we can re-iterate over it later and manipulate it at the end.

[csharp]
private readonly Stack&lt;Expression&gt; _expressions = new Stack&lt;Expression&gt;();
    
private Expression _finalExpression;

private void CaptureFinalExpression(Expression node)
{
    if (_finalExpression == null)
    {
        _finalExpression = node;
    }
}

protected override Expression VisitLambda&lt;Y&gt;(Expression&lt;Y&gt; node)
{
    base.Visit(node.Body);

    CaptureFinalExpression(node.Body);

    if (node.Parameters.Count &gt; 0)
    {
        _expressions.Push(node.Parameters.First());

        var final = BuildFinalStatement();

        return Expression.Lambda(final, node.Parameters);
    }
    
    return Expression.Lambda(BuildFinalStatement());
}

protected override Expression VisitMethodCall(MethodCallExpression node)
{            
    _expressions.Push(node);

    return Visit(node.Object);
}

protected override Expression VisitMember(MemberExpression node)
{            
    _expressions.Push(node);

    return Visit(node.Expression);
}
[/csharp]

In the process of visiting the nodes, we pushed each piece of the call chain into a stack.  Now we can build it all back out. The bulk of the work is in this recursive function. It iterates back through the stack, maps the previous call to a variable, and builds out if not null checks.


[csharp]
private Expression BuildIfs(Expression current, Expression prev = null)
{
    var stringRepresentation = Expression.Constant(current.ToString(), typeof(string));

    var variable = Expression.Parameter(current.Type, NextVarName);

    Expression evaluatedExpression = EvaluateExpression(current, prev);

    var assignment = Expression.Assign(variable, evaluatedExpression);

    var end = _expressions.Count == 0;

    var nextExpression =
         !end
            ? BuildIfs(_expressions.Pop(), variable)
            : LastExpression(variable, stringRepresentation);

    Expression blockBody;

    if (!end)
    {
        var whenNull = OnNull(stringRepresentation);

        blockBody = CheckForNull(variable, whenNull, nextExpression);
    }
    else
    {
        blockBody = nextExpression;
    }

    return Expression.Block(new [] { variable }, new[] { assignment, blockBody });           
}
[/csharp]


When I say "map the previous call" I mean building out something like this:

[csharp]
var var1 = user;
if(var1 != null){
   var var2 = var1.school
   
   if(var2 != null) 
   ....
}
[/csharp]

The initial statement "user" needs to get assigned to a variable. Then this variable needs to be transformed into a new call where we call ".school" on it. We know what to do with ".school" because the call for ".school" was captured as part of the stack iteration. During the iteration of the expression tree in the visitor we were able to capture each portion of the tree.

Look:

<img src="http://onoffswitch.net/wp-content/uploads/2014/04/2014-04-03-21_14_26-.png" alt="2014-04-03 21_14_26-" width="370" height="133" class="aligncenter size-full wp-image-4500" />

Given that we have each piece we can now inspect it and manipulate other pieces with it

<img src="http://onoffswitch.net/wp-content/uploads/2014/04/2014-04-03-20_58_12-NoNulls-Debugging-Microsoft-Visual-Studio-Administrator.png" alt="2014-04-03 20_58_12-NoNulls (Debugging) - Microsoft Visual Studio (Administrator)" width="725" height="187" class="aligncenter size-full wp-image-4495" />

Now lets assign it to a variable

<img src="http://onoffswitch.net/wp-content/uploads/2014/04/2014-04-03-20_59_07-NoNulls-Debugging-Microsoft-Visual-Studio-Administrator.png" alt="2014-04-03 20_59_07-NoNulls (Debugging) - Microsoft Visual Studio (Administrator)" width="551" height="69" class="aligncenter size-full wp-image-4496" />

From here on out we've cached the value.

The evaluate expression function is pretty simple:

[csharp]
private Expression EvaluateExpression(Expression current, Expression prev)
{
    if (prev == null)
    {
        return current;
    }

    if (current is MethodCallExpression)
    {
        var method = current as MethodCallExpression;

        return Expression.Call(prev, method.Method, method.Arguments);
    }

    if (current is MemberExpression)
    {
        var member = current as MemberExpression;

        return Expression.MakeMemberAccess(prev, member.Member);
    }

    return current;
}
[/csharp]

The rest of the work is boilerplate of creating if checks and returning the final result when necessary

[csharp]
private Expression OnNull(ConstantExpression stringRepresentation)
{
    var falseVal = Expression.Constant(false);

    var nullValue = Expression.Constant(default(T), _finalExpression.Type);

    return Expression.New(MethodValueConstructor, new Expression[] { nullValue, stringRepresentation, falseVal });
}

private Expression CheckForNull(ParameterExpression variable, Expression whenNull, Expression nextExpression)
{
    var ifNull = Expression.ReferenceEqual(variable, Expression.Constant(null));

    return Expression.Condition(ifNull, whenNull, nextExpression);
}

private Expression LastExpression(ParameterExpression variable, ConstantExpression stringRepresentation)
{
    var trueVal = Expression.Constant(true);
 
    return Expression.New(MethodValueConstructor, new Expression[] { variable, stringRepresentation, trueVal });
}
[/csharp]

<h2>Conclusion</h2>

Building expression tree's was a little complicated. There aren't any return statements, and I ran into a lot of weird errors assigning and accessing variables (you need to use certain overloads of the block expression, which I only figured out after reading a <a href="http://stackoverflow.com/a/3370894/310196" target="_blank" rel="noopener noreferrer">jon skeet answer</a> on stack overflow). Still, the resulting code is concise and clean, and until the .? operator shows up this isn't a bad alternative!  

Full source is available at my <a href="https://github.com/devshorts/MonadicNull" target="_blank" rel="noopener noreferrer">github</a>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4493</wp:post_id>
		<wp:post_date><![CDATA[2014-04-04 04:23:28]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-04-04 04:23:28]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[avoiding-nulls-expression-trees]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ast"><![CDATA[ast]]></category>
		<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="expression-tree"><![CDATA[expression tree]]></category>
		<category domain="post_tag" nicename="null"><![CDATA[null]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561472234;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:3779;}i:2;a:1:{s:2:"id";i:4862;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Instagram viewer with node and angular</title>
		<link>https://onoffswitch.net/2014/04/28/instagram-viewer-node-angular/</link>
		<pubDate>Mon, 28 Apr 2014 08:00:51 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4515</guid>
		<description></description>
		<content:encoded><![CDATA[I have an artist <a href="http://www.jeremyjams.com/" target="_blank" rel="noopener noreferrer">buddy</a> who is working on an art installation and asked me if there was a way to display a realtime view of an instagram hashtag feed on a projector. 

Unfortunately there isn't anything right out of the box available, but I offered to write him a quick app that he could fire up that would give him what he wanted.

<em>For the impatient, full source is available at my <a href="https://github.com/devshorts/Playground/tree/master/Gecker" target="_blank" rel="noopener noreferrer">github</a>.
</em>

One way to do this would be to hook into the instagram <a href="http://instagram.com/developer/realtime/" target="_blank" rel="noopener noreferrer">realtime API</a>. Using the realtime API you subscribe to tags or users via their API and supply a url callback. Instagram will then callback to your endpoint with an HTTP GET, validate that you actually did request (via a handshake response), and then post to your endpoint whenever something on that subscription has changed.  What it won't do is actually give you the information you need to display, it's only a notification that something changed (and gives you information to pull back what changed).

On the one hand this would work great, but the downside is that it requires a publicly exposed endpoint to work.  Given that I'm distributing this to a buddy who may not have access to the router at the art installation to set up port forwarding, I went with a more low-tech solution: rss tag polling.

My first idea was to just have this be a UI only page that pulled from the instagram RSS by tag feed periodically, but I ran into cross site origin failures.  In order to get over that I wrote a small node app that does the rss tag query on a timer, xml parsing, xml data translation (into a more useful DTO), and uses socket.io to push the new information to a simple angular app.

<h2>The server</h2>

First I abstracted the concept of a server into its own class where we could inject callbacks if we wanted to externally.  I exposed the routing as an injection function so any other consumers can add routes to the expression app (so if I wanted to build the realtime API I could) without the server caring at all what's going on.

[js]
var express = require('express');
var http = require('http');
var app = express();
exports.Server = function(){
    var hostRoot = __dirname + '/../ui';

    console.log(hostRoot);

    app.use(express.bodyParser());
    app.use(express.methodOverride());
    app.use(app.router);
    app.use(express.static(hostRoot));
    app.use(express.errorHandler());

    this.start = function(){
        var server = http.createServer(app);

        var port = process.env.PORT || 3000;
        server.listen(port);

        console.log(&quot;listneing on &quot; + port);

        return server;
    };

    this.addRoutes = function(callback){
        callback(app);
    };
};
[/js]

I also added a class that abstracts socket.IO and lets us issue an action on connect, as well as external invoking an action. The idea here is that the moment someone connects we want to make sure to send the most to date data. Given that the client load will realistically be only 1, maybe 2 people, there isn't an issue of server spam here.

[js]
var io = require('socket.io');

exports.RealTime = function(server){

    var socketIO = io.listen(server);

    socketIO.set('log level', 1);

    var root = this;

    this.onLogin = function(pushTo){
        root.loginFunction = pushTo;

        return root;
    };

    this.run = function(){
        socketIO.sockets.on('connection', function(socket){
            console.log(&quot;connected&quot;);

            socket.on(&quot;disconnect&quot;, function(){
                console.log(&quot;disconnect&quot;);
            });

            root.loginFunction(root.push);
        });

        return this;
    };

    this.push = function(data) {
        socketIO.sockets.json.emit(&quot;data&quot;, data);
    }
};
[/js]

I also have a class that encapsulates pulling data from the instagram RSS feed by tag and transforms the result into a simpler object

[js]
var request = require('request');
var _ = require('underscore')._;
var xml2js = require(&quot;xml2js&quot;);
var Instagram = require(&quot;instagram-node-lib&quot;);

exports.InstagramRss = function(tag, takeAmount){
    var options = {
        host: &quot;http://instagram.com/tags/&quot; + tag + &quot;/feed/recent.rss&quot;,
        method: 'GET'
    };

    this.query = function(callback){
        function extractor(body){
            var parser = xml2js.Parser();

            return parser.parseString(body, function(err, r){
                var items =
                    _.chain(r.rss.channel[0].item)
                        .map(function(element){
                            return {
                                link : element.link[0],
                                title: element.title[0]
                            }
                        })
                        .take(takeAmount)
                        .value();

                callback(items);
            });
        }

        request(options.host, function (error, response, body) {
            if (!error &amp;&amp; response.statusCode == 200) {
                extractor(body);
            }
        })
    };
};
[/js]

Now finally the node entrypoint

[js]
var openurl = require(&quot;openurl&quot;);

var Server = require(&quot;./src/server&quot;).Server,
    RealTime = require(&quot;./src/realtime&quot;).RealTime,
    InstagramRss = require(&quot;./src/instagramRss&quot;).InstagramRss;

var App = function(){

    var config = require('./config.json');

    var rss = new InstagramRss(config.tag, config.take);

    var server = new Server();

    var realtime = {};

    this.run = function(){
        runOnTimer(config.interval);

        realtime = new RealTime(server.start()).onLogin(rss.query).run();
    };

    function runOnTimer(interval){
        setInterval(function(){
            rss.query(realtime.push)
        }, interval * 1000);
    }
};


new App().run();

openurl.open('http://localhost:3000');
[/js]

The idea now is that anyone who connects to the websocket will immediately get an rss query pushed to them.  From then on at an interval configured by config.json we'll just send any new stuff to them.

<h2>The UI</h2>

The ui is dirt simple. It's just a single angularJS page that registers a service and callback representing the realtime push, as well as a controller and a directive to manage displaying new instagram elements.  

The main angular app, below, takes care of registering services and directives, as well as the initial routing (using ui-router)

[js]
function App(){
    this.run = function(app){
        new ServiceInitializer().initServices(app);
        new Directives().initDirectives(app);

        applyConfigs(app);
    };

    function applyConfigs(app){
        app.config(function($stateProvider, $urlRouterProvider){

            $urlRouterProvider.otherwise(&quot;/&quot;);

            $stateProvider.state('main', {
                url:&quot;/&quot;,
                templateUrl: &quot;partials/feed.html&quot;,
                controller: feedController
            })
        });
    }
}
[/js]

The service initializer and services are just wrappers on the realtime subscription

[js]
function ServiceInitializer(){
    this.initServices = function (app){
        app.service('realtime', realtime);
    };

    function realtime(){
        function basePath(){
            var pathArray = window.location.href.split( '/' );
            var protocol = pathArray[0];
            var host = pathArray[2];
            return protocol + '//' + host;
        }

        var socket = io.connect(basePath());

        var rssQueryClients = [];

        socket.on('data', function(data){
            _.forEach(rssQueryClients, function(client){
                client(data);
            });
        });

        this.registerRssPush = function (client){
            rssQueryClients.push(client);
        };
    }
}
[/js]

And the directive that drives the single image display. I set it up so that the image and tagged text fade in together when the image has completed loading.

[js]
function Directives(){
    this.initDirectives = function(app){
        app.directive('instagram', instagram)
    };

    function instagram(){
        return {
            restrict: 'E',
            scope: {
                data:&quot;=&quot;
            },
            templateUrl: 'partials/directives/instagram-directive.html',
            link: function (scope, element, attrs){
                var img = $(element).find(&quot;img&quot;)[0];
                var txt = $(element).find(&quot;.image-text&quot;)[0];

                $(img).bind(&quot;load&quot;, function(event){
                    $(img).css(&quot;opacity&quot;, 1);
                    $(txt).css(&quot;opacity&quot;, 1);
                });
            }
        };
    }
}
[/js]

The only controller we need is to handle the realtime socket and filtering of the new input data

[js]
function feedController($scope, realtime, $http){
    $scope.feed = [];

    realtime.registerRssPush(function (data) {
        console.log(&quot;got data&quot;);

        var map = {};

        _.forEach($scope.feed, function (item){
            map[item.link] = true;
        });

        _.forEach(data, function (item) {
            if(!map.hasOwnProperty(item.link)){
                $scope.feed.unshift(item);
            }
        });

        $scope.$apply();
    });
}
[/js]

Since the data comes in via a websocket and not in the scope of a wrapped angular service we need to do a $scope.apply to make sure the page redraws.

The view directive represents one single instagram image

[html]
&lt;img src=&quot;{{data.link}}&quot;/&gt;

&lt;div class=&quot;image-text&quot;&gt;
    {{ data.title }}
&lt;/div&gt;
[/html]

And the main view is just a repeater of the directives (controlled by the main controller)

[html]
&lt;div class=&quot;repeat-body&quot;&gt;
    &lt;div class=&quot;instagram-element&quot; ng-repeat=&quot;item in feed&quot;&gt;
        &lt;instagram data=&quot;item&quot;&gt;&lt;/instagram&gt;
    &lt;/div&gt;
&lt;/div&gt;
[/html]

To drive the whole thing the relevant index page blocks look like

[html]
///.. js includes etc... 

         &lt;script&gt;
            var app = angular.module(&quot;app&quot;,  [&quot;ui.router&quot;]);

            new App().run(app);
        &lt;/script&gt;
    &lt;/head&gt;
    &lt;body &gt;
        &lt;!-- Add your site or application content here --&gt;
        &lt;div ui-view&gt;

        &lt;/div&gt;
    &lt;/body&gt;
[/html]

<h2>Conclusion</h2>

Apps like this I think node.js really excels at.  It's os agnostic, and I can package the whole thing up and send it to my buddy to run. With libraries like underscore js I can even leverage higher order functions and keep things nice and clean.

If I was going to make this a production thing, instead of just a weekend project, I would've used typescript to give myself strong typing.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4515</wp:post_id>
		<wp:post_date><![CDATA[2014-04-28 08:00:51]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-04-28 08:00:51]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[instagram-viewer-node-angular]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="angular"><![CDATA[angular]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="instagram"><![CDATA[instagram]]></category>
		<category domain="post_tag" nicename="node"><![CDATA[node]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560156459;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:1587;}i:1;a:1:{s:2:"id";i:4783;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>162</wp:comment_id>
			<wp:comment_author><![CDATA[Leandro]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[lelado@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[200.130.2.5]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-05-02 13:00:51]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-05-02 13:00:51]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Congratulations on your initiative enjoyed.
I am trying to do similar and found its implementation, but was unable to test it. This config.json missing the file, which would content him.
Thank you very much.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>163</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.56.224.158]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-05-03 03:05:58]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-05-03 03:05:58]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Leandro, add a config.json that looks like this

<code>
{
    "tag": "your_hash_tag",    
    "take": 10,
    "interval": 15
}
</code>

Tag = hashtag you want
Take = number of elements to pull from the rss feed (the idea is that if you have a tag that has a million element syou can't really display that much anyways)
Interval = time in seconds to poll the rss feed]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>162</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>164</wp:comment_id>
			<wp:comment_author><![CDATA[Rodrigo]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[rodrigo@rayo-laser.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[200.104.227.107]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-29 16:21:48]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-29 16:21:48]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hello master.

There is a way I can test this on my localhost? I'm a newbie. I downloaded all github files and placed them in www folder (using MAMP), but it seems like is not that easy.

Thank you!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>165</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.56.225.186]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-06-29 17:07:38]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-06-29 17:07:38]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Just do an npm install to pull back the dependencies, create a config file (I posted it in another comment here) and do node app.js to start it up. You don't need to host it anywhere, it should open a browser for you]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>164</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Short and sweet powershell prompt with posh-git</title>
		<link>https://onoffswitch.net/2014/07/09/short-sweet-powershell-prompt-posh-git/</link>
		<pubDate>Wed, 09 Jul 2014 18:58:33 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4529</guid>
		<description></description>
		<content:encoded><![CDATA[My company has fully switched to git and it's been great.  Most people at work use SourceTree as a gui to manage their git workflow, some use only command line, and I use a mixture of posh-git in powershell with tortoise git when I need to visualize things.

Posh-git, if you load the <a href="https://github.com/dahlbyk/posh-git/blob/master/profile.example.ps1" target="_blank" rel="noopener noreferrer">example</a> from your profile, will set the default prompt to be the current path. If you go into a git directory it'll also add the git status.  Awesome. But if you are frequently in directories that are 10+ levels deep, suddenly your prompt is just obscenely long.

For example, this is pretty useless right?

<img src="http://onoffswitch.net/wp-content/uploads/2014/07/2014-07-09-11_53_20-.png" alt="2014-07-09 11_53_20-" width="947" height="174" class="aligncenter size-full wp-image-4530" />

Obviously it's a fictitious path, but sometimes you run into them, and it'd be nice to optionally shorten that up.  

It's easy to define a shortPwd function and expose a global "MAX_PATH" variable that can be reset.

[code]
$MAX_PATH = 5

function ShortPwd
{
    $finalPath = $pwd
    $paths = $finalPath.Path.Split('\')

    if($paths.Length -gt $MAX_PATH){
        $start = $paths.Length - $MAX_PATH
        $finalPath = &quot;..&quot;
        for($i = $start; $i -le $paths.Length; $i++){
            $finalPath = $finalPath + &quot;\&quot; + $paths[$i]
        }
    }

    return $finalPath
}
[/code]

In the posh-git example, make sure to load your custom function first, then change 

[code]
Write-Host($pwd.ProviderPath) -nonewline
[/code]

To

[code]
Write-Host (ShortPwd) -nonewline -foregroundcolor green
[/code]

(I like my prompt green)

Now you can dynamically toggle the max length. I've set it to 5, but if you change it the prompt will immediately update:

<img src="http://onoffswitch.net/wp-content/uploads/2014/07/2014-07-09-11_57_40-poshgit-powershell_scripts-master-Admin.png" alt="2014-07-09 11_57_40-posh~git ~ powershell_scripts [master] (Admin)" width="526" height="167" class="aligncenter size-full wp-image-4531" />

For this and other powershell scripts check out my <a href="https://github.com/devshorts/powershell_scripts" target="_blank" rel="noopener noreferrer">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4529</wp:post_id>
		<wp:post_date><![CDATA[2014-07-09 18:58:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-07-09 18:58:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[short-sweet-powershell-prompt-posh-git]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="git"><![CDATA[git]]></category>
		<category domain="post_tag" nicename="powershell"><![CDATA[powershell]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561303870;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4892;}i:1;a:1:{s:2:"id";i:4631;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>wcf Request Entity Too Large</title>
		<link>https://onoffswitch.net/2014/07/16/wcf-request-entity-large/</link>
		<pubDate>Wed, 16 Jul 2014 20:17:06 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4535</guid>
		<description></description>
		<content:encoded><![CDATA[I ran into a stupid issue today with WCF request entity too large errors. If you're sure your bindings are set properly on both the server and client, make sure to double check that the service name and contract's are set properly in the server.  

My issue was that I had at some point refactored the namespaces where my service implementations were, and didn't update the web.config. For the longest time things continued to work, but once I reached the default max limit (even though I had a binding that set the limits much higher), I got the 413 errors.  

So where I had this:

[code]
&lt;service name=&quot;Foo.Bar.Service&quot;&gt;
	&lt;endpoint address=&quot;&quot; binding=&quot;basicHttpBinding&quot; bindingConfiguration=&quot;LargeHttpBinding&quot; contract=&quot;Foo.Bar.v1.Service.IService&quot;/&gt;
&lt;/service&gt;
[/code]

I needed

[code]
&lt;service name=&quot;Foo.Bar.Implementation.Service&quot;&gt;
	&lt;endpoint address=&quot;&quot; binding=&quot;basicHttpBinding&quot; bindingConfiguration=&quot;LargeHttpBinding&quot; contract=&quot;Foo.Bar.v1.Service.IService&quot;/&gt;
&lt;/service&gt;
[/code]

How WCF managed to work when the service name was pointing to a non-existent class, I have no idea. But it did.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4535</wp:post_id>
		<wp:post_date><![CDATA[2014-07-16 20:17:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-07-16 20:17:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[wcf-request-entity-large]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="wcf"><![CDATA[wcf]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558691229;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:4515;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Leveraging message passing to do currying in ruby</title>
		<link>https://onoffswitch.net/2014/08/24/leveraging-message-passing-currying-ruby/</link>
		<pubDate>Sun, 24 Aug 2014 02:43:58 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4539</guid>
		<description></description>
		<content:encoded><![CDATA[I'm not much of a ruby guy, but I had the inkling to play with it this weekend.  The first thing I do when I'm in a new language is try to map constructs that I'm familiar with, from basic stuff like object instantiation, singletons, inheritance, to more complicated paradigms like lambdas and currying.

I came across this <a href="http://www.sitepoint.com/functional-programming-techniques-with-ruby-part-ii/" target="_blank" rel="noopener noreferrer">blog post</a> that shows that ruby has a way to auto curry lambdas, which is actually pretty awesome.  However, I was a little confused by the syntax

[ruby]
a.send(fn, b)
[/ruby]

I'm more used to ML style where you would do

[code]
fn a b 
[/code]

So what is <code>a.send</code> doing?  

<h2>Message passing</h2>

Ruby exposes its dynamic dispatch as a message passing mechanism (like objective c), so you can send "messages" to objects. It's like being able to say "hey, execute this function (represented by a string) on this context".  

If you think of it that way, then <code>a.send(fn, b)</code> translates to "execute function 'fn' on the context a, with the argument of b".  This means that fn better exist on the context of 'a'.  

As an example, this curries the multiplication function:

[ruby]
apply_onContext = lambda do |fn, a, b|
  a.send(fn, b)
end

mult = apply_onContext.curry.(:*, 5)

puts mult.(2)
[/ruby]

This prints out <code>10</code>. First a lambda is created that sends a message to the object 'a' asking it to execute the the function <code>*</code> (represented as an <a href="http://en.wikipedia.org/wiki/String_interning" target="_blank" rel="noopener noreferrer">interned string</a>).

Then we can leverage the curry function to auto curry the lambda for us creating almost F# style curried functions.  The syntax of ".(" is a shorthand of .call syntax which executes a lambda.

If we understand message passing we can construct other lambdas now too:

[ruby]
class Test
  def add(x, y)
    x + y
  end

  def addOne
    apply_onClass = lambda do |fn, x, y|
      send(fn, x, y)
    end

    apply_onClass.curry.(:add, 1)
  end
end

puts Test.new.addOne.(4)
[/ruby]

This returns a curried lambda that invokes a message :add on the source object.  

<h2>Getting rid of the dot</h2>

Ruby 1.9 doesn't let you define what <code>()</code> does so you are forced to call lambdas with the dot syntax. However, ruby has other interesting features that let you alias a method to another name. It's like moving the original method to a new name.  

You can do this to any  method you have access to so you can get <a href="http://www.leonardoborges.com/writings/2008/08/07/why-i-like-ruby-1-alias_method/" target="_blank" rel="noopener noreferrer">the benefits</a> of method overriding without needing to actually do inheritance.

Taking advantage of this you can actually hook into the default missing message exception on object (which is invoked when a "message" isn't caught). Catching the missing method exception and then executing a .call on the object (if it accepts that message) lets us fake the parenthesis.

Here is a <a href="https://github.com/coderrr/parenthesis_hacks/blob/master/lib/lambda.rb" target="_blank" rel="noopener noreferrer">blog post</a> that shows how to do it.

Obviously it sucks to leverage exception handling, but hey, still neat.

<h2>Conclusion</h2>

While nowhere near as succinct as f#

[fsharp]
let addOne = (+) 1
[/fsharp]

But learning new things about other languages is interesting :)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4539</wp:post_id>
		<wp:post_date><![CDATA[2014-08-24 02:43:58]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-08-24 02:43:58]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[leveraging-message-passing-currying-ruby]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="lambdas"><![CDATA[lambdas]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560523888;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4306;}i:1;a:1:{s:2:"id";i:2020;}i:2;a:1:{s:2:"id";i:3565;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>AngularJS for .Net developers</title>
		<link>https://onoffswitch.net/2014/08/24/angularjs-net-developers/</link>
		<pubDate>Sun, 24 Aug 2014 03:15:29 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4550</guid>
		<description></description>
		<content:encoded><![CDATA[A few months ago I was asked to be a technical reviewer on a new packt pub book called <a href="https://www.packtpub.com/web-development/learning-angularjs-net-developers" target="_blank" rel="noopener noreferrer">AngularJS for .Net developers</a>.  It mostly revolves around ServiceStack (not web API) and building a full stack application with angular. I actually really enjoyed reading it and thought it touched on a lot of great points that a developer who is serious needs to know about.

Unfortunately I think the book doesn't do a very good job at explaining angular in general.  It's certainly geared to the experienced developer who has worked with angular and servicestack/c# REST before.  

Still, if you are interested in using angular as a .net developer its an informative and quick read!  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4550</wp:post_id>
		<wp:post_date><![CDATA[2014-08-24 03:15:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2014-08-24 03:15:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[angularjs-net-developers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[angularjs-net-developres]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559926341;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3710;}i:1;a:1:{s:2:"id";i:4028;}i:2;a:1:{s:2:"id";i:4515;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>166</wp:comment_id>
			<wp:comment_author><![CDATA[Aharon Koss]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[aharonkoss@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[209.180.147.94]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2014-11-13 20:25:59]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2014-11-13 20:25:59]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I felt the same way reading angularjs for .NET developers. In reading that book I was really disappointed. I wanted a clearer explanation on how you could integrate SQL server into the service api. Do you have any references that show you how to do this?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Creating stronger value type contracts</title>
		<link>https://onoffswitch.net/2015/01/05/creating-stronger-type-contracts/</link>
		<pubDate>Mon, 05 Jan 2015 19:35:56 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4556</guid>
		<description></description>
		<content:encoded><![CDATA[I've long been annoyed that value types don't have strong semantic information attached to them such that the compiler would barf if I try and pass an value type that isn't semantically the same as what the function wanted.  For example, what does the following signature mean other than than taking in 2 ints and returning a bool?

[code]
IsLoggedIn :: int -&gt; int -&gt; bool
[/code]

What I'd really like the signature to look like is

[code]
IsLoggedIn :: UserId -&gt; SessionId -&gt; bool
[/code]

In F# you can do this sort of with type aliases and augmenting the signature with the type information. However, its just editor magic, it doesn't actually compile to anything that would stop you from accidentally calling a function with the arguments reversed. An int is an int is an int, right? 

[code]
var userId = 1
var sessionId = 2

IsLoggedIn(sessionId, userId)
[/code]

This is perfectly valid to the compiler and means you won't catch it until its unit tested, peer reviewed (if everyone is paying attention), or found during runtime. I'd like to have this caught at compile time.

<h2>A solution</h2>

Instead, what if we wrapped important value types into their own structs?  Something like this:

[csharp]
public struct UserId
{
    private readonly Int32 _int;

    private UserId(int @int)
    {
        _int = @int;
    }

    public static explicit operator UserId(int value)
    {
        return new UserId(value);
    }

    public static implicit operator int(UserId value)
    {
        return value._int;
    }
}

public struct SessionId
{
    private readonly Int32 _int;

    private SessionId(int @int)
    {
        _int = @int;
    }

    public static explicit operator SessionId(int value)
    {
        return new SessionId(value);
    }

    public static implicit operator int(SessionId value)
    {
        return value._int;
    }
}
[/csharp]

Structurally its exactly the same as an int, and it doesn't cost you anything to use it. But, now the compiler will fail if you try and pass this 

<img src="http://onoffswitch.net/wp-content/uploads/2015/01/strongtypes.png" alt="strongtypes" width="554" height="204" class="aligncenter size-full wp-image-4557" />

<h2>Adding some meta data</h2>

Now there is a problem though of project boundaries. What I mean is when your data types go over the wire, either through webapi or wcf or to a DB or some other form.  To make this actually useful we have to introduce a few more things to make it easy to be transparent through these edge cases.

First, lets augment the strong inheritance heirarchy. I have interfaces that look like:

[csharp]
public interface IStrongType&lt;out T&gt; : IAcceptStrongTypeVisitor
{
    T UnderlyingValue();        
}
[/csharp]

[csharp]
public interface IInt32 : IStrongType&lt;int&gt;
{
    string ToString(string format);

    string ToString(string format, IFormatProvider provider);

    string ToString(IFormatProvider provider);
}
[/csharp]

[csharp]
public interface IStrongTypeVistor&lt;out TResult&gt;
{        
    TResult Visit(IGuid data);
    TResult Visit(IInt32 data);
    TResult Visit(IInt64 data);
    TResult Visit(IFloat data);
    TResult Visit(IDouble data);
}
[/csharp]

Now I can have strong types implement their corresponding value type interfaces (I want a strong int to have the same methods as a regular int) as well as visit on top of them. 

<h2>Serializing JSON</h2>

The first order of business is serializing to and from JSON.  To do that I've created a json converter that knows how to cast and get the underlying value of a strong type

[csharp]
public class PrimitiveJsonConverter&lt;T&gt; : JsonConverter
    where T : struct
{
    public override void WriteJson(JsonWriter writer, object value, JsonSerializer serializer)
    {
        var underlying = (IStrongType&lt;T&gt;)(value);

        writer.WriteValue(underlying.UnderlyingValue());
    }

    public override object ReadJson(JsonReader reader, Type objectType, object existingValue, JsonSerializer serializer)
    {
        var data = serializer.Deserialize&lt;T&gt;(reader);

        return data.ExplicitCastTo(objectType);
    }

    public override bool CanConvert(Type objectType)
    {
        return typeof(T).IsAssignableFrom(objectType);
    }
}
[/csharp]

And a utility extension method to reflectively invoke the explicit cast operator on a type
[csharp]
public static object ExplicitCastTo(this object obj, Type type)
{
    var castOperator = type.GetMethod(&quot;op_Explicit&quot;, new[] { obj.GetType() });

    if (castOperator == null)
    {
        throw new InvalidCastException(&quot;Can't cast to &quot; + type.Name);
    }

    return castOperator.Invoke(null, new[] { obj });
}
[/csharp]

Now a strong type will actually look like this:

[csharp]
[Serializable]
[JsonConverter(typeof(IntCaster))]
public struct SessionId : IInt32
{
    private readonly Int32 _int;

    private SessionId(int @int)
    {
        _int = @int;
    }

    public static explicit operator SessionId(int value)
    {
        return new SessionId(value);
    }

    public static implicit operator int(SessionId value)
    {
        return value._int;
    }

    public int UnderlyingValue()
    {
        return _int;
    }

    public TResult Accept&lt;TResult&gt;(IStrongTypeVistor&lt;TResult&gt; visitor)
    {
        return visitor.Visit(this);
    }

    public override string ToString()
    {
        return _int.ToString();
    }

    public string ToString(string format)
    {
        return _int.ToString(format);
    }

    public string ToString(string format, IFormatProvider provider)
    {
        return _int.ToString(format, provider);
    }

    public string ToString(IFormatProvider provider)
    {
        return _int.ToString(provider);
    }
}
[/csharp]

<h2>Handling WCF</h2>

For WCF I took a lazier approach. For my use case we can distribute an assembly that contains all the strong types and ask consumers to re-use the types when generating proxies.  If they don't, WCF will auto generate a type that boxes the result and send it to you anyways. It makes it more annoying for a consumer, but not impossible to use.  

<h2>WebAPI parameter parsing</h2>

Web Api exposes a way to hook into the parameter/object binding as things come in over the wire.  You just need to implement the <code>HttpParameterBinding</code> abstract class.  Then you register the binder either as an attribute on your data object, or as an attribute on the parameter in the web api method, or via registration at startup.

First I'll show the parameter binder base class. 

[csharp]
public class PrimitiveBinder&lt;T&gt; : HttpParameterBinding where T : struct
{
    public PrimitiveBinder(HttpParameterDescriptor descriptor)
        : base(descriptor)
    {
    }

    public override Task ExecuteBindingAsync(ModelMetadataProvider metadataProvider, HttpActionContext actionContext, CancellationToken cancellationToken)
    {
        var value = actionContext.RequestContext.RouteData.Values[Descriptor.ParameterName];

        var @struct = TypeDescriptor.GetConverter(typeof(T)).ConvertFromString(value.ToString());

        actionContext.ActionArguments[Descriptor.ParameterName] = @struct.ExplicitCastTo(Descriptor.ParameterType);

        var tsc = new TaskCompletionSource&lt;object&gt;();
        tsc.SetResult(null);
        return tsc.Task;
    }
}
[/csharp]

You can see that it converts the string representation into the raw value type (which is T), then it casts the raw value type into the type of the parameter

Each strong type wrapper will have its own implementation of the binder where it passes in the underlying value type of T. As an example, here is int:

[csharp]
public class IntBinder : PrimitiveBinder&lt;int&gt;
{
    public IntBinder(HttpParameterDescriptor descriptor)
        : base(descriptor)
    {
    }
}
[/csharp]

When the application boots up it calls the registration on the http config

[csharp]
public static void RegisterStrongTypes(HttpConfiguration config)
{
    config.ParameterBindingRules.Add(FindDescriptor);
}

private static HttpParameterBinding FindDescriptor(HttpParameterDescriptor descriptor)
{
    if (typeof(IGuid).IsAssignableFrom(descriptor.ParameterType))
    {
        return new GuidBinder(descriptor);
    }

    if (typeof(IInt32).IsAssignableFrom(descriptor.ParameterType))
    {
        return new IntBinder(descriptor);
    }

    if (typeof(IFloat).IsAssignableFrom(descriptor.ParameterType))
    {
        return new FloatBinder(descriptor);
    }

    return null;
}        
[/csharp]

Now web api knows what to do based on the tagged interface of the strong type (in the int example, they are all of IInt32 interfaces).

As an example, if this is our method on the controller

[csharp]
[Route(&quot;int/{test}&quot;), HttpGet]
public IntExample Test(IntExample test)
{
    return test;
}
[/csharp]

We can pass in an int 

[code]
localhost/api/int/1
[/code]

And get the same value echoed back out to us without having to add any annotations to either the controller, the method, or the parameters.

<h2>Dapper</h2>

For the project I work on, we also use dapper as our micro ORM.  This is great since it lets us pass in anonymous objects representing stored procedure parameters and it can auto map into objects and primitives for us.  However, dapper has no idea what to do with our strong types so we have to augment its type serializer.

This is where the visitor interface comes into play. On application load, we can leverage a registration function that tells dapper what to do for the specific types.  The one thorn here is that dapper can't map an interface type to a type handler, it needs to be for each type explicity. Thats OK since we can reflectively find all types that implement the root IStrongType<T> interface and register them automatically

[csharp]
internal class StrongTypeMapper : SqlMapper.ITypeHandler
{
    public void SetValue(IDbDataParameter parameter, object value)
    {
        parameter.DbType = new DbTypeStrongVisitor().Visit(value as IAcceptStrongTypeVisitor);

        parameter.Value = new UnderlyingStrongTypeVisitor().Visit(value as IAcceptStrongTypeVisitor);
    }
    
    public object Parse(Type destinationType, object value)
    {
        return value.ExplicitCastTo(destinationType);
    }
}

internal class DbTypeStrongVisitor : IStrongTypeVistor&lt;DbType&gt;
{
       
    public DbType Visit(IGuid guid)
    {
        return DbType.Guid;
    }

    public DbType Visit(IInt32 data)
    {
        return DbType.Int32;
    }

    public DbType Visit(IInt64 data)
    {
        return DbType.Int64;
    }

    public DbType Visit(IFloat data)
    {
        return DbType.Double;
    }

    public DbType Visit(IDouble data)
    {
        return DbType.Decimal;
    }

    public DbType Visit(IAcceptStrongTypeVisitor visitor)
    {
        return visitor.Accept(this);
    }        
}
[/csharp]

And the initializer call:

[csharp]
public static void InitTypeMappings()
{
    Assembly.GetExecutingAssembly()
            .FindStrongTypes()
            .ForEach(i =&gt; SqlMapper.AddTypeHandler(i, new StrongTypeMapper()));
}
[/csharp]

Where <code>FindStrongTypes</code> finds all the types that implement the generic interface of IStrongType<T>.

<h2>But the boilerplate!</h2>

This is great and all, but kind of annoying to manage by hand. It's a lot of boilerplate to write for what used to just be an int or float or guid.  To combat this, my team is using a custom code generator that auto generates strong types along with the visitor information, and a bunch of other auto-gend data for our codebase.  It'd be easy to write your own, since the template is pretty much exactly the same.  
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4556</wp:post_id>
		<wp:post_date><![CDATA[2015-01-05 19:35:56]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-01-05 19:35:56]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[creating-stronger-type-contracts]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="c"><![CDATA[c#]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dapper"><![CDATA[dapper]]></category>
		<category domain="post_tag" nicename="types"><![CDATA[types]]></category>
		<category domain="post_tag" nicename="wcf"><![CDATA[wcf]]></category>
		<category domain="post_tag" nicename="webapi"><![CDATA[webapi]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561600152;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:4411;}i:2;a:1:{s:2:"id";i:3899;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>167</wp:comment_id>
			<wp:comment_author><![CDATA[Jake]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[jakes@outlook.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.143.125.150]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-06 20:18:27]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-06 20:18:27]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I WANT THIS RIGHT NOW. TODAY. Where can i download this code? do you have a github repo for this?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>168</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.143.125.150]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-06 20:19:15]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-06 20:19:15]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[No github repo yet unfortunately, but I'll work on extracting this out and publishing it along with a sample command line tool to auto gen strong types. In the meantime all the core pieces are in the blog post.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>167</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>169</wp:comment_id>
			<wp:comment_author><![CDATA[Eamon Nerbonne]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[eamon@nerbonne.org]]></wp:comment_author_email>
			<wp:comment_author_url>http://eamon.nerbonne.org/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[87.208.17.253]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-13 19:39:47]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-13 19:39:47]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Neat!  Are you going to release something like this to github?  I've been wrestling with similar problems, and I'm really curious where this is going :-).  It's possible roslyn will mean you don't need the command line auto-gen by the way - nuget packages will be able to tie into the compiler.

From my own experience doing this kind of stuff, I've got a tip - hopefully a useful one to you, but that depends on your usage:

The continual use of reflection means this code will likely be slow - if you e.g. use this in Dapper and have large selects, that's going to be the bottleneck.  Fortunately, that's entirely preventable - if you make your StrongTypeMapper generic, then Parse can use a static readonly delegate that's constructed with the op_Explicit method - reflection will be limited to the static constructor; and runtime overhead will be a single delegate call.

This still is far from optimal, because for no particularly good reason Dapper's ITypeHandler deals with objects which means you're going to be making lots of allocations, but it's likely to make a significant difference nevertheless - a reflection call is much more expensive than an allocation.

SetValue could be similarly optimized, but that's not likely to matter as much since database calls tend to have few parameters but many results.  

If you don't have any large selects, this won't matter (i.e. reporting stuff), but if you do - I've noticed that the bottleneck in large but straightforward selects seems to be client-side (i.e. .NET side) data deserialization, specifically this kind of thing, so there's a good chance this is on your hot path.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>170</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.143.125.150]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-14 18:38:54]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-14 18:38:54]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Great recommendations! Internally we have some utiltieis to convert reflective parameter getting/setting into delegates so we can wrap it with that. 

We talked about hooking into rosyln plugin to auto gen the stuff  resharper style, but never got around to doing it

As for github, I'll try and put together a demo at least of what we're using internally, without exposing our internals :)
]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>169</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>171</wp:comment_id>
			<wp:comment_author><![CDATA[david]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[david_hehir@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[137.92.29.35]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-20 04:05:13]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-20 04:05:13]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[You seem to be missing the definition for IntCaster, unless I'm missing something here?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>172</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.143.125.150]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-01-30 01:10:10]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-01-30 01:10:10]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I was, sorry. The int cast is simple:

[csharp]
public class IntCaster : PrimitiveJsonConverter&lt;int&gt;
{             
}
[/csharp]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>171</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Quickly associate file types with a default program</title>
		<link>https://onoffswitch.net/2015/01/05/quickly-associate-file-types-default-program/</link>
		<pubDate>Mon, 05 Jan 2015 22:45:19 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4568</guid>
		<description></description>
		<content:encoded><![CDATA[I use JuJuEdit to open all my log files since it starts up fast, is pretty bare bones, but better than notepad.  The way my log4net appender is set up is that log files are kept for 10 days and get a <code>.N</code> appended to them for each backup. I.e.

[code]
FooLog.log
FooLog.log.1
FooLog.log.2
[/code]

Etc. 

I hate having to go through each one and set the default program to open since its slow and annoying.  A faster way is to use cmd (not powershell!) and use the assoc and ftype commands.

You can associate an extension (like <code>.2</code>) with a "file type" (which doesn't really mean anything) and then map the file type to a program to open.

For example:

[code]
&gt;ftype logfile=&quot;C:\Program Files (x86)\Jujusoft\JujuEdit\JujuEdit.exe&quot; %1
&gt;assoc .3=logfile
&gt;assoc .4=logfile
&gt;assoc .5=logfile
&gt;assoc .6=logfile
...
[/code]

And now they all open with juju edit. If i ever want to change it I just re-run ftype and all my log files will now open with another program]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4568</wp:post_id>
		<wp:post_date><![CDATA[2015-01-05 22:45:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-01-05 22:45:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[quickly-associate-file-types-default-program]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560873800;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4737;}i:1;a:1:{s:2:"id";i:3608;}i:2;a:1:{s:2:"id";i:2985;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Conditional injection with scala play and guice</title>
		<link>https://onoffswitch.net/2015/01/30/conditional-injection-scala-play-guice/</link>
		<pubDate>Fri, 30 Jan 2015 01:08:21 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4575</guid>
		<description></description>
		<content:encoded><![CDATA[It's been a crazy year for me. For those who don't know I moved from the east coast to the west coast to work for a rather <em>large</em> softare company in seattle (I'll let you figure which one out) and after a few short weeks realized I made a horrible mistake and left the team. I then found a cool job at a smaller .net startup that was based in SF and met some awesome people and learned a lot. But, I've been poached by an old coworker and am now going to go work at a place that uses more open source things so I decided to kick into gear and investigate scala and play.

For the most part I'm doing a mental mapping of .NET's web api framework to the scala play framework, but the more I play in play (pun intended) the more I like it.

On one of my past projects a coworker of mine set up a really interesting framework leveraging ninject and web api where you can conditionally inject a data source for test data by supplying a query parameter to a rest API of "test".  So the end result looks something like:

[csharp]
[GET(&quot;foo/{name}&quot;)]
public void GetExample(string name, [IDataSource] dataProvider){
   // act on data provider
}
[/csharp]

The way it chose the correct data provider is by leveraging a custom parameter binder that will resolve the source from the ninject kernel based on the query parameters.  I've found that this worked out really well in practice. It lets the team set up some sample data while testers/qa/ui devs can start building out consuming code before the db layers are even complete.

I really liked working with this pattern so I wanted to see how we can map this to the scala play framework. Forgive me if what I post isn't idiomatic scala, I've only been at it for a day :)

First I want to define some data sources

[scala]
trait DataSource{
  def get : String
}

class ProdSource extends DataSource{
  override def get: String = &quot;prod&quot;
}

class TestSource extends DataSource {
  override def get : String = &quot;test&quot;
}
[/scala]

It should be pretty clear whats going on here. I've defined two classes that implement the data source trait. Which one that gets injected should be defined by a query parameter.

Guice lets you define bindings for the same trait (interface) to a target class based on "keys".  What this means is you can say "<em>give me class A, and use the default binding</em>", or you can say "<em>give me class A, but the one that is tagged with interface Test</em>".  When you register the classes you can provider this extra tagging mechanism.  This is going to be useful because you can now request different versions of the interface from the binding kernel.

Lets just walk through the remaining example. First we need the interface, but Guice wants it to be an annotation. Since scala has weird support for annotations and the JVM has shitty type erasure, I had to write the annotation in java

[java]
import com.google.inject.BindingAnnotation;

import java.lang.annotation.ElementType;
import java.lang.annotation.Retention;
import java.lang.annotation.RetentionPolicy;
import java.lang.annotation.Target;

@Target(ElementType.TYPE)
@Retention(RetentionPolicy.RUNTIME)
@BindingAnnotation
public @interface TestAnnotation {}
[/java]

I'm honestly not even sure I need the @Target, but whatever.

Next we're gonna create some binding modules for Guice to use where we can specify the conditional binding:

[scala]
package Modules

import annotations.TestAnnotation
import com.google.inject.AbstractModule
import controllers.{DataSource, ProdSource, TestSource}

class SourceModule extends AbstractModule {

  override def configure(): Unit = {
    testable(classOf[DataSource], classOf[ProdSource], classOf[TestSource])
  }

  def testable[TInterface, TMain &lt;: TInterface, TDev &lt;: TInterface](
               interface: Class[TInterface],
               main:      Class[TMain],
               test:      Class[TDev]) = {
    val markerClass = classOf[TestAnnotation]

    bind(interface).to(main)

    bind(interface) annotatedWith markerClass to test
  }
}
[/scala]

What this is saying is that given the 3 types (the main interface, the implementation of the main item, and the implementation of the dev item) to conditionally bind the dev item to the marker class of "TestAnnotation".  This will make sense when you see how its used.

As normal, guice is used to set up the controller instantation with the source module registered.

[scala]
import Modules.{DbModule, SourceModule}
import com.google.inject.Guice
import play.api.GlobalSettings

object Global extends GlobalSettings {

  val kernel = Guice.createInjector(new SourceModule())

  override def getControllerInstance[A](controllerClass: Class[A]): A = {
    kernel.getInstance(controllerClass)
  }
}
[/scala]

Now comes the fun part of actually resolving the query parameter.  I'm going to wrap an action and create a new action so we can get a nodejs style <code>(datasource, request) =></code> lambda.  

[scala]
trait Sourceable{
  val kernelSource : Injector

  def WithSource[T] (clazz : Class[T]) (f: ((T, Request[AnyContent]) =&gt; Result)) : Action[AnyContent] = {
    Action { request =&gt; {
      val binder =
        request.getQueryString(sourceableQueryParamToggle) match {
          case Some(_) =&gt; kernelSource.getInstance(Key.get(clazz, classOf[TestAnnotation]))
          case None =&gt; kernelSource.getInstance(clazz)
        }

      f(binder, request)
    }}
  }

  def sourceableQueryParamToggle = &quot;test&quot;
}
[/scala]

The kernel never has to be registered since Guice will auto inject it when its asked for (its implicity available). Whats happening here is that we set up the kernel and the target interface type we want to get (i.e. DataSource).  If the query string matches the sourceable query param toggle (i.e. the word "test") then it'll pick up the registered data source using the "test annotation" marker.  Otherwise it uses the default.

Finally the controller now looks like this:

[scala]
@Singleton
class Application @Inject() (db : DbAccess, kernel : Injector) extends Controller with Sourceable {
  override val kernelSource: Injector = kernel

  def binding(name : String) = WithSource(classOf[DataSource]){ (provider, request) =&gt;
  {
    val result = name + &quot;: &quot; + provider.get

    Ok(result)
  }}
}
[/scala]

And the route

[scala]
GET /foo/:name @controllers.Application.binding(name: String)
[/scala]

The kernel value is provided to the trait and any other methods can now ask for a data provider of a particular type and get it.  

Full source available at my <a href="https://github.com/devshorts/scala-injector">github</a>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4575</wp:post_id>
		<wp:post_date><![CDATA[2015-01-30 01:08:21]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-01-30 01:08:21]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[conditional-injection-scala-play-guice]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dependency-injection"><![CDATA[dependency injection]]></category>
		<category domain="post_tag" nicename="guice"><![CDATA[guice]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1557884187;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4939;}i:1;a:1:{s:2:"id";i:4961;}i:2;a:1:{s:2:"id";i:4919;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Simple log context wrapper</title>
		<link>https://onoffswitch.net/2015/02/03/simple-log-context-wrapper/</link>
		<pubDate>Tue, 03 Feb 2015 03:09:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4586</guid>
		<description></description>
		<content:encoded><![CDATA[I'm still toying around with the scala play! framework and I wanted to check out how I can make logging contextual information easy.  In the past with .NET I've used and written libraries that wrap the current log provider and give you extra niceties with logging. One of my favorites was being able to do stuff like

[csharp]
var foo = &quot;1&quot;;
var bar = &quot;2&quot;;
logger.With(new { foo, bar }).Info(&quot;data&quot;)
[/csharp]

Which would output a log line like

[code]
data, foo=1; bar=2
[/code]

The logger's "with" was even chainable so you could capture a previously built "with" context and re-use it. It was really nice when you want to create a baseline logging context for complex functions.

Java doesn't have the concept of anonymous classes and I'm not sure you can do optimizations with reflection like you can with .NET (creating reflective based property invokers into delegates).

Either way, this makes for a good experiment.

First off, the final product

[scala]
val logInfo = With(
  &quot;request-uri&quot; -&gt; rh.uri,
  &quot;request-time&quot; -&gt; (System.currentTimeMillis() - start)
)
val compound = logInfo and With(&quot;date&quot; -&gt; &quot;bar&quot;)

logger.info(&quot;handled&quot;, compound)
[/scala]

Which outputs

[code]
[info] LoggingFilter$ - handled request-uri=/; request-time=164; date=bar
[info] LoggingFilter$ - handled request-uri=/assets/javascripts/jquery-1.9.0.min.js; request-time=347; date=bar
[info] LoggingFilter$ - handled request-uri=/assets/stylesheets/main.css; request-time=362; date=bar
[info] LoggingFilter$ - handled request-uri=/assets/images/favicon.png; request-time=478; date=bar
[/code]

This is a snippet I'm playing with in a root level timing filter for a scala play app. The "date -> bar" association is just for demonstration of combining contexts

The full filter looks like

[scala]
object LoggingFilter extends Filter{
  val logger = Log(getClass)

  val start = System.currentTimeMillis()

  override def apply(f: (RequestHeader) =&gt; Future[Result])(rh: RequestHeader): Future[Result] = {

    val result = f(rh)

    val logInfo = With(
      &quot;request-uri&quot; -&gt; rh.uri,
      &quot;request-time&quot; -&gt; (System.currentTimeMillis() - start)
    )

    val compound = logInfo and With(&quot;date&quot; -&gt; &quot;bar&quot;)

    logger.info(&quot;handled&quot;, compound)

    result
  }
}
[/scala]

Basically this creates a "With" object that is composable with other "with" objects which takes in a variable list of tuples and internally stores them as a map.

The factory function "Log" just instantiates the initial context object for tracking of state and captures the scala Play! logger to pass in

[scala]
object Log {
  def apply(src : Class[_]) = new Log(Logger(src))
}

class LogData(data: Map[String, String]) {

  protected val logMap = data

  def asLog = logMap.foldLeft(&quot;&quot;)((acc, kv) =&gt; acc + kv._1 + &quot;=&quot; + kv._2 + &quot;; &quot;).trim.stripSuffix(&quot;;&quot;)

  def and(tup: (String, Any)) = {
    val (x, y) = tup
    new LogData(logMap.updated(x, y.toString))
  }

  def and(other: LogData) = {
    new LogData(logMap ++ other.logMap)
  }
}
[/scala]

Now you should see the log wrapper. It just wraps the scala play logger and takes in an extra log data if its passed in:

[scala]
class Log(logger: LoggerLike) {

  def getMessage(s: String, data: LogData): String = {
    if (data != null) {
      return s + &quot; &quot; + data.asLog
    }

    s
  }

  def info(s: String, m: LogData = null, t: Throwable = null) = logger.info(getMessage(s, m), t)

  def debug(s: String, m: LogData = null, t: Throwable = null) = logger.debug(getMessage(s, m), t)

  def warn(s: String, m: LogData = null, t: Throwable = null) = logger.warn(getMessage(s, m), t)

  def error(s: String, m: LogData = null, t: Throwable = null) = logger.error(getMessage(s, m))

}
[/scala]

At this point we just need to create the with context class, which again is just a factory function for the LogData class

[scala]
object With {
  def apply(tup: (String, Any)*) = {
    new LogData(tup.map(i =&gt; (i._1, i._2.toString)).toMap)
  }
}
[/scala]

Since the main logger takes LogData instances this kind of works out well

Now we can get nicely uniform formatted messages for easy parsing in utilities like Splunk]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4586</wp:post_id>
		<wp:post_date><![CDATA[2015-02-03 03:09:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-02-03 03:09:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[simple-log-context-wrapper]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="logging"><![CDATA[logging]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"f251743520edff2facd5c40ee081a536";a:2:{s:7:"expires";i:1558534014;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4593;}i:1;a:1:{s:2:"id";i:4862;}i:2;a:1:{s:2:"id";i:4945;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tiny types scala edition</title>
		<link>https://onoffswitch.net/2015/02/04/tiny-types-scala-edition/</link>
		<pubDate>Wed, 04 Feb 2015 21:30:15 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4589</guid>
		<description></description>
		<content:encoded><![CDATA[Previously I wrote about generating value type wrappers on top of C# primitives for better handling of domain level knowledge.  This time I decided to try it out in scala as I'm jumping into the JVM world.

With scala we don't have the value type capability that c# has, but we can sort of get there with implicits and case classes.

The simple gist is to generate stuff like

[scala]
package com.devshorts.data

case class foo(data : String)
case class bar(data : String)
case class bizBaz(data : Int)
case class Data(data : java.util.UUID)
[/scala]

And the implicit conversions

[scala]
package com.devshorts.data

object Conversions{
    implicit def convertfoo(i : foo) : String = i.data
    implicit def convertbar(i : bar) : String = i.data
    implicit def convertbizBaz(i : bizBaz) : Int = i.data
    implicit def convertData(i : Data) : java.util.UUID = i.data
}
[/scala]

Now you get a similar feel of primitive wrapping with function level unboxing and you can pass your primitive case class wrappers to more generic functions.

For this case I wrote a simple console generator and played around with zsh auto completion for it too. Full source located at my <a href="https://github.com/devshorts/scala-tiny-types">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4589</wp:post_id>
		<wp:post_date><![CDATA[2015-02-04 21:30:15]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-02-04 21:30:15]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tiny-types-scala-edition]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561511191;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:4905;}i:2;a:1:{s:2:"id";i:4862;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Auto scaling akka routers</title>
		<link>https://onoffswitch.net/2015/03/11/auto-scaling-akka-routers/</link>
		<pubDate>Wed, 11 Mar 2015 23:26:35 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4593</guid>
		<description></description>
		<content:encoded><![CDATA[I'm working on a project where I need to multiplex many requests through a finite set of open sockets. For example, I have 200 messages, but I can only have at max 10 sockets open.  To accomplish this I've wrapped the sockets in akka actors and am using an akka routing mechanism to "share" the 10 open sockets through a roundrobin queue.  

This works out great, since now the consumers (who are rabbit mq listeners) just post messages to a facacde on the resource, and akka will route the request and do the appropriate work for me.

However, I wanted to know of a clean way to be able to add more resources (or remove them). Say at runtime I am asked to add 10 more open connections, or that suddenly we need to scale down to 5 connections.  I'd like the router to be able to manage that for me.  

It took a little poking around, but its not that complicated to do. The router manages a list of routees and you can pick a random one you want to remove (or add new ones). To remove one, send it a poison pill, and have the context unwatch it so the supervisor stops caring if it fails or not.  Then tell the router to stop routing messages to it. When the poison pill reaches the actor (it'll finish processing its messages first) then it'll stop itself and you can do cleanup. In my case this is where I'd close the open socket.

A full scala example is here:

[scala]

import akka.actor._
import akka.routing._

case class Add()

case class Remove()

class Worker(id: Integer) extends UntypedActor {
  println(s&quot;Made worker $id&quot;)

  @throws[Exception](classOf[Exception]) override
  def preStart(): Unit = {
    println(s&quot;Starting $id&quot;)
  }

  @throws[Exception](classOf[Exception]) override
  def postStop(): Unit = {
    println(s&quot;Stopping $id&quot;)
  }

  @throws[Exception](classOf[Exception])
  override def onReceive(message: Any): Unit = message match {
    case _ =&gt; println(s&quot;Message received on actor $id&quot;)
  }
}

class Master extends Actor {

  var count = 0

  def makeWorker() = {
    val id = count

    count = count + 1

    context.actorOf(Props(new Worker(id)))
  }

  var router = {
    val startingRouteeNumber = 2

    val initialRoutees = Seq.fill(startingRouteeNumber) {
      val worker = makeWorker()
      context watch worker
      ActorRefRoutee(worker)
    }

    Router(RoundRobinRoutingLogic(), initialRoutees.toIndexedSeq)
  }

  def receive = {
    case Remove =&gt;
      println(&quot;Removing route&quot;)

      val head = router.routees.head.asInstanceOf[ActorRefRoutee].ref

      head ! PoisonPill

      context unwatch head

      router = router.removeRoutee(head)

      printRoutes()


    case Add =&gt;
      println(&quot;Adding route&quot;)

      val worker = makeWorker()

      context watch worker

      router = router.addRoutee(worker)

      printRoutes()


    case w: AnyRef =&gt;

      printRoutes()

      router.route(w, sender())
  }

  def printRoutes(): Unit = {
    val size = router.routees.size

    println(s&quot;Total routes $size&quot;)
  }
}

object Main extends App {
  var system = ActorSystem.create(&quot;foo&quot;)

  var master = system.actorOf(Props[Master])

  master ! &quot;do work&quot;

  master ! Remove

  master ! &quot;do more work&quot;

  master ! &quot;do even more work&quot;

  master ! Add

  master ! &quot;do work again&quot;
}
[/scala]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4593</wp:post_id>
		<wp:post_date><![CDATA[2015-03-11 23:26:35]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-03-11 23:26:35]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[auto-scaling-akka-routers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="akka"><![CDATA[akka]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1555264548;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4627;}i:1;a:1:{s:2:"id";i:4456;}i:2;a:1:{s:2:"id";i:4596;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Simplifying class matching with java 8</title>
		<link>https://onoffswitch.net/2015/03/13/simplifying-class-matching-java-8/</link>
		<pubDate>Fri, 13 Mar 2015 23:51:30 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4596</guid>
		<description></description>
		<content:encoded><![CDATA[I'm knee deep in akka these days and its a great queueing framework, but unfortunately I'm stuck using java and not able to use scala (business decisions, not mine!) so pattern matching on incoming untyped events can be kind of nasty.

You frequently see stuff like this in receive methods:

[java]
public void onReceive(Object message){
 if(message instanceof Something){

 }
 else if (message instanceof SomethingElse){

 }
 .. etc
}
[/java]

And while that technically works, I really hate it because it promotes a monolothic function doing too much work. It also encourages less disciplined devs to put logic into the if block. While this is fine for a few checks, what happens when you need to dispatch 10, or 20 different types? It's not uncommon in actor based systems to have lots of small message types.  

Also, because akka gives you your object as a type erased Object you can't use normal dispatching mechanisms like overloaded functions.  And to complicate things even more, you can't really use a visitor pattern without conflating business logic into your data access objects. 

In reality, all I want is that my receive function should act as dispatcher, doing the right type checking on your object and executing a function that does the specific work. Now I can write small functions for each particular message type and keep that logic encapsulated and composable.

Thankfully with java 8 and lambadas we can create some simple combinator style executors that let us do stuff like this:

[java]
@Override public void onReceive(final Object message) throws Exception {
    match().with(Payload.class, this::handlePayload)
           .with(ResizeWorkLoadMessage.class, this::processResize)
           .with(HeartBeat.class, this::heartBeat)
           .fallthrough(i -&gt; logger.with(i).warn(&quot;Unknown type called to actor, cannot route&quot;))
           .exec(message);
}

private void handlePayload(Payload payload){
 // ...
}

private void processResize(ResizeWorkLoadMessage resizeWorkload){
 // ...
}
[/java]

Now we have a simple cast matcher that checks a raw object type, does a monadic check to see which matcher succeeds (going from top down with priority) and if nothing matches executes the fallthrough.  

Building something like this is pretty trivial. It's just a combinator that captures the current state and delegates to the next state if the current cast doesn't succeed:

[java]
import java.util.function.BiFunction;
import java.util.function.Consumer;

public class ClassMatcher {

    private final BiFunction&lt;Object, Consumer&lt;Object&gt;, Boolean&gt; binder;

    private ClassMatcher(BiFunction&lt;Object, Consumer&lt;Object&gt;, Boolean&gt; next) {
        this.binder = next;
    }

    public void exec(Object o) {
        binder.apply(o, null);
    }

    public &lt;Y&gt; ClassMatcher with(final Class&lt;Y&gt; targetClass, final Consumer&lt;Y&gt; consumer) {
        return new ClassMatcher((obj, next) -&gt; {

            if (binder.apply(obj, next)) {
                return true;
            }

            if (targetClass.isAssignableFrom(obj.getClass())) {
                final Y as = (Y) obj;

                consumer.accept(as);

                return true;
            }

            return false;
        });
    }

    public ClassMatcher fallthrough(final Consumer&lt;Object&gt; consumer) {
        return new ClassMatcher((obj, next) -&gt; {

            if (binder.apply(obj, next)) {
                return true;
            }

            consumer.accept(obj);

            return true;

        });
    }

    public static ClassMatcher match() {
        return new ClassMatcher((a, b) -&gt; false);
    }
}
[/java]

<h2>Performance</h2>

Reddit seemed to be obsessed about the perf costs with this implementation. I spun up JMH and gave it a whirl to compare the cost of this vs if statements.

You do pay a small performance penalty for this higher level abstraction but I think its a small price to pay. Almost any higher abstraction pays a penalty of some sort. In JMH benchmarking methods that used an if tree took a pretty constant 20-50 nanoseconds to complete, and ones using a matcher took about 2-4 times longer (around 90 nanoseconds for a matcher of 4 cases).  

Caching an instance of your dispatcher cut the perf time in half, and when you build out lots of match statements it makes a more noticable difference (20 match statements recreated each time was 500 nanoseconds and cached it was 150 nanoseconds).  Downside to caching is that you can't close over anything and create adhoc functions inline, but if you use it as a pure dispatcher then caching is fine.  

Just as an example a simple cacher:

[java]
import java.util.function.Supplier;

public class ClassMatchCache {
    private ClassMatcher matcher;

    public ClassMatcher cache(Supplier&lt;ClassMatcher&gt; matchFactory) {
        if(matcher == null){
            matcher = matchFactory.get();
        }

        return matcher;
    }
}
[/java]

And you can now use it

[java]
public class EventHandler {
  private ClassMatchCache mainDispatcher = new ClassMatchCache();
  
  public void dispatch(Object o){
    mainDispatcher.cache(
        () -&gt; match().with(Bar.class, this::bar)
                     .with(Biz.class, this::biz)
                     .with(Baz.class, this::baz)
                     .with(Foo1.class, this::foo)
                     .with(Foo2.class, this::foo)
                     .with(Foo3.class, this::foo)
                     .with(Foo4.class, this::foo)
                     .with(Foo5.class, this::foo)
                     .with(Foo13.class, this::foo)
                     .fallthrough(this::fallthrough))
              .exec(o);
  }
}
[/java]

Or use whatever caching mechanism works for you. 

But in the end, do 100 nanoseconds matter to you to jump through all these hoops? To put it in perspective the following statement

[java]
IntStream.range(0, 1000).map(i -&gt; i + 1).sum();
[/java]

Takes 400 times longer, averaging 4000 nanoseconds. And even then, what's 4000 nanoseconds? Thats 4 microseconds, and .004 milliseconds.  In the large scope of real projects this is insignificant. 

The rule of thumb with all optimizations is don't prematurely optimize. If you really had a perf hit, switch your code to if statements.  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4596</wp:post_id>
		<wp:post_date><![CDATA[2015-03-13 23:51:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-03-13 23:51:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[simplifying-class-matching-java-8]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="akka"><![CDATA[akka]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="combinators"><![CDATA[combinators]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560443277;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4627;}i:1;a:1:{s:2:"id";i:4456;}i:2;a:1:{s:2:"id";i:4629;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>173</wp:comment_id>
			<wp:comment_author><![CDATA[the_felis_leo]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[the_felis_leo@hotmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[217.167.130.4]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-06-14 16:48:32]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-06-14 16:48:32]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Suggestions : 

Replace <code>targetClass.isAssignableFrom(obj.getClass())</code>
With <code>targetClass.isInstance(obj)</code>

Replace <code>final Y as = (Y) obj;</code>
With <code>final Y as = targetClass.cast(obj);</code>

Instead of chaining Matcher,
Prefers using Map.
(A HashMap as resolvedStrategies, and a Linked/SortedMap as configuration)

When no resolvedStrategies,
then iterate configuration
   until targetClass.isInstance(obj),
   else use fallback.
Finally register this new resolvedStrategy.


Option :
 - Auto-Sort configuration to match most specific classes before parents.
 - Provide a configuration factory based on reflection.

This last point is a little tedious, and pollute stacktrace
(but we could expected debugger to filter that)
Synthetic Example:
<code>
Object visitor;
Class visitorType;
// For visitor's methods
MethodHandle staticHandler = MethodHandles.lookup().find/unreflect(...); // on visitorType
MethodHandle handler = staticConsumer.bind(visitor);
Consumer consumer = MethodHandleProxies.asInterfaceInstance(Consumer.class, handler);
</code>
Or simply define consumer as a lamba invoking reflected method.
(So that you implicitly define a unique proxy with minimum overhead)
(But prefers MethodHandler over Method because of underlying jvm optimisation)

You could cache unbound configuration by reflected class,
and return configuration bound to visitor instance.

Better: weak cache unbound configuration,
and do strong reference from  bound configuration to cached unbound config.
Manage  cached unbound config so that on finally you remove weak cache entry
(in order to release Class reference)]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>174</wp:comment_id>
			<wp:comment_author><![CDATA[Gideon Liem]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[gideonliem@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[176.117.57.240]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2018-11-13 08:21:22]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2018-11-13 08:21:22]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Actually your solution doesn't need a bi function. If you change it into a Function it also works.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Installing leinigen on windows</title>
		<link>https://onoffswitch.net/2015/03/14/installing-leinigen-windows/</link>
		<pubDate>Sat, 14 Mar 2015 22:20:02 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4598</guid>
		<description></description>
		<content:encoded><![CDATA[Figured I'd spend part of the afternoon and play with clojure but was immediately thwarted trying to install <a href="http://leiningen.org/">leiningen</a> on windows via powershell. I tried the msi installer but it didn't seem to do anything, so I went to my <code>~/.lein/bin</code> folder and ran 

[code]
.lein\bin&gt; .\lein.bat self-install
Downloading Leiningen now...
SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc
syswgetrc = C:\Program Files (x86)\Gow/etc/wgetrc
--2015-03-14 15:08:48--  https://github.com/technomancy/leiningen/releases/download/2.5.1/leiningen-2.5.1-standalone.zip
Resolving github.com... 192.30.252.131
Connecting to github.com|192.30.252.131|:443... connected.
ERROR: cannot verify github.com's certificate, issued by `/C=US/O=DigiCert Inc/OU=www.digicert.com/CN=DigiCert SHA2 Extended Validation Server CA':
  Unable to locally verify the issuer's authority.
To connect to github.com insecurely, use `--no-check-certificate'.
Unable to establish SSL connection.

Failed to download https://github.com/technomancy/leiningen/releases/download/2.5.1/leiningen-2.5.1-standalone.zip
[/code]

Hmm, thats weird.  For some reason the cert isn't validating with wget (that I have installed via Gow).  

A quick google showed that this is a common problem using the gow wget, and I wasn't about to use the unsecured certificate check.  I opened up the leinigen installer bat file and saw that it does a check trying to see what kind of download function your shell has. It checks if you have wget, curl, or if you are in powershell (in which case it creates a .net webclient and downloads the target file).

Since I have gow in my path wget comes up first, so I just switched around the order and things now work happy!

The relevant section in the lein.bat file is

[code]
:DownloadFile
rem parameters: TargetFileName Address
if NOT &quot;x%HTTP_CLIENT%&quot; == &quot;x&quot; (
    %HTTP_CLIENT% %1 %2 
    goto EOF
)
call powershell -? &gt;nul 2&gt;&amp;1
if NOT ERRORLEVEL 1 (
    powershell -Command &quot;&amp; {param($a,$f) (new-object System.Net.WebClient).DownloadFile($a, $f)}&quot; &quot;&quot;%2&quot;&quot; &quot;&quot;%1&quot;&quot;
    goto EOF
)
call curl --help &gt;nul 2&gt;&amp;1
if NOT ERRORLEVEL 1 (
    rem We set CURL_PROXY to a space character below to pose as a no-op argument
    set CURL_PROXY= 
    if NOT &quot;x%HTTPS_PROXY%&quot; == &quot;x&quot; set CURL_PROXY=&quot;-x %HTTPS_PROXY%&quot;
    call curl %CURL_PROXY% -f -L -o  %1 %2
    goto EOF
)
call Wget --help &gt;nul 2&gt;&amp;1
if NOT ERRORLEVEL 1 (
    call wget -O %1 %2
    goto EOF
)
[/code]

Once the self install completes now lein is available.  

On a side note, I think you probably could have just downloaded the release file and plopped it into the <code>~/.lein/self-installs</code> folder and it would work too]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4598</wp:post_id>
		<wp:post_date><![CDATA[2015-03-14 22:20:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-03-14 22:20:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[installing-leinigen-windows]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="clojure"><![CDATA[clojure]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560799432;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:8009;}i:1;a:1:{s:2:"id";i:4327;}i:2;a:1:{s:2:"id";i:1268;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>175</wp:comment_id>
			<wp:comment_author><![CDATA[DCWhatthe]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[galois@nycap.rr.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[72.231.187.162]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-03-29 20:29:29]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-03-29 20:29:29]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Lein has been installed, since more than a year ago, no problem.  But I just received the same problem as yours, when trying to upgrade, thus:


     lein upgrade


The solution you mentioned seems to start to work.  But the problem is, that the upgrade process replaces the lein.bat.  So the re-ordering of the calls to wget/curl/powershell are backed out.

So at this point, I'm not even sure how to upgrade lein.  Might just have to leave it, as is.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>176</wp:comment_id>
			<wp:comment_author><![CDATA[Leiningen is missing its dependencies. (On Windows) - leiningen]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://leiningen.javacss.space/2016/01/17/leiningen-is-missing-its-dependencies-on-windows/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[108.61.218.12]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-01-26 07:51:23]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-01-26 07:51:23]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] http://onoffswitch.net/installing-leinigen-windows/ [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Handling subclassed constraints with a DSL in java 8</title>
		<link>https://onoffswitch.net/2015/03/25/building-dsls-java-8/</link>
		<pubDate>Wed, 25 Mar 2015 01:42:01 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4613</guid>
		<description></description>
		<content:encoded><![CDATA[I really like doing all of my domain modeling with clean DSL's (domain specific languages). Basically I want my code to read like a sentence, and to hide all the magic behind things. When things read clearly even a non professional can determine if something is wrong.  The ideal scenario is to have your code read like pseudocode since nobody really cares what the internals are, what matters is your general solution.

I found myself recently in a scenario where I have some methods that do work, and they all return a subtype of a response root. Something like:

[java]
class Response extends ResponseRoot {}

class Worker{
   Response doWork(Request request) { // }
}
[/java]

And I need to create a dynamic callback given the context of the request. So basically I want 

[java]
Consumer&lt;ResponseRoot&gt; callback = generateCallback(request);

Response response = doWork(request);

callback.accept(response);
[/java]

However I am going to have a lot of this same boilerplate for many different kinds of requests.  In a previous post I mentioned the <code>match</code> on runtime objects and this is the next phase of that scenario: take an untyped object, cast it to see what kind of object it is, depending on the object do a strongly typed method and then execute the callback.

I could create a bunch of methods that just create a new client, do the work, then issue the callback but thats no fun.  Why not something like

[java]
@Override public void processAsyncable(final Object input) throws Exception {
    final Consumer&lt;ResponseRoot&gt; complete = getCompletionCallback(input);
    
    match().with(SubRequest.class, afterDoing(this::subRequest, then(complete)))
           .exec(input);
}

private SubRequestResponse subRequest(final SubRequest availabilityCheckEvent) throws Throwable {
    // ...
}
[/java]

What is the <code>afterDoing</code> and the <code>then(complete)</code>?  Its reminiscent of junit matchers and their DSL.


[java]
import org.jooq.lambda.fi.util.function.CheckedFunction;

import java.util.function.Consumer;

public class CompletionCallback {

    public static &lt;TInput, TRoot, TResponse extends TRoot&gt; Consumer&lt;TInput&gt; afterDoing(CheckedFunction&lt;TInput, TResponse&gt; function, ThenVerb&lt;TRoot&gt; verb) {
        return input -&gt;
        {
            TRoot result = null;
            try {
                result = function.apply(input)
            }
            catch (Throwable throwable) {
                throw new RuntimeException(throwable);
            }

            verb.accept(result);
        };
    }

    public static &lt;T&gt; ThenVerb&lt;T&gt; then(Consumer&lt;T&gt; consumer) {
        return new ThenVerb&lt;&gt;(consumer);
    }
}

class ThenVerb&lt;Y&gt; {
    private final Consumer&lt;Y&gt; consumer;

    public ThenVerb(Consumer&lt;Y&gt; consumer) {
        this.consumer = consumer;
    }

    public void accept(Y item){
        consumer.accept(item);
    }
}
[/java]

The idea here is to try and model the "<em>Response subclasses RootResponse and I have a function that can take that subclassed item but I only know about the root</em>" statement.  This is why there are 3 generics.  By defining the verb and making it generic we can now give the verb the generic constraint necessary to model this <code>Response extends ResponseRoot</code> constraint. The first parameter gets the input function that generates the value to pass to the completor. The verb just wraps the consumer which is the final completion object. 

While it looks like it's a lot of extra work, what I like about this pattern is that your code reads now like a sentence.  
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4613</wp:post_id>
		<wp:post_date><![CDATA[2015-03-25 01:42:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-03-25 01:42:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[building-dsls-java-8]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dsl"><![CDATA[dsl]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554060671;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:4629;}i:2;a:1:{s:2:"id";i:1587;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Getting battery percentage in zsh</title>
		<link>https://onoffswitch.net/2015/03/30/battery-percentage-zsh-osx-maverick/</link>
		<pubDate>Mon, 30 Mar 2015 01:15:14 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4622</guid>
		<description></description>
		<content:encoded><![CDATA[I'm on osx maverick still at home on my laptop and I spent part of today dicking around customizing my zsh shell.  I wanted to be able to show my battery percentage in the shell and it's really pretty easy.

First, the main shell function 

[bash]
function get_battery()
{
    current_battery=`system_profiler SPPowerDataType | grep -i &quot;charge remaining&quot; | awk '{print $4}'`

    max_battery=`system_profiler SPPowerDataType | grep -i &quot;full charge capacity&quot; | awk '{print $5}'`

    percent=`bc &lt;&lt;&lt; &quot;scale=4; ${current_battery}/${max_battery} * 100&quot;`

    printf '%.0f' $percent 
}
[/bash]

This queries from the profiler the charge current and remaining, uses bc to get a floating point division, and then just shows the integer value of that (we could round it too but I was lazy).

Now, we just need to tie it into the prompt. I'm using the <a href="https://github.com/robbyrussell/oh-my-zsh/blob/master/themes/steeef.zsh-theme" target="_blank" rel="noopener noreferrer">steef</a> prompt by default and just tweaked it a bit:

[bash]
battery_percentage=&quot;$(get_battery)%%&quot;

directory_info=&quot;${_prompt_steeef_colors[5]}%~%f&quot; 

datetime=&quot;%F{yellow}[%D{%a %I:%M:%S %p}]%f&quot; # yellow [Day hh:mm:ss am/pm] followed by reset color %f

PROMPT=&quot;${battery_percentage} ${datetime} ${directory_info} ${vcs} 
&quot;&quot;$ &quot;

RPROMPT=&quot;&quot;
[/bash]

The two percent symbols just escapes the percent symbol.

Make sure the <code>battery_percentage</code> function is defined before you load your prompt in your .zshrc file. 

Now here's what I got:

<img src="http://onoffswitch.net/wp-content/uploads/2015/03/Screen-Shot-2015-03-29-at-6.13.50-PM.png" alt="Screen Shot 2015-03-29 at 6.13.50 PM" width="356" height="63" class="aligncenter size-full wp-image-4623" />
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4622</wp:post_id>
		<wp:post_date><![CDATA[2015-03-30 01:15:14]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-03-30 01:15:14]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[battery-percentage-zsh-osx-maverick]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="osx"><![CDATA[osx]]></category>
		<category domain="post_tag" nicename="shell"><![CDATA[shell]]></category>
		<category domain="post_tag" nicename="zsh"><![CDATA[zsh]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560766546;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4631;}i:1;a:1:{s:2:"id";i:4589;}i:2;a:1:{s:2:"id";i:4463;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Adding MDC logging to akka</title>
		<link>https://onoffswitch.net/2015/04/07/adding-mdc-logging-akka/</link>
		<pubDate>Tue, 07 Apr 2015 01:05:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4627</guid>
		<description></description>
		<content:encoded><![CDATA[I've mentioned before, but I'm working heavily in a project that is leveraging akka. I am really enjoying the message passing model and so far things are great, but tying in an MDC for the SLFJ logging context proved complicated.  I had played with the custom executor model described <a href="http://yanns.github.io/blog/2014/05/04/slf4j-mapped-diagnostic-context-mdc-with-play-framework/">here</a> but hadn't attempted the akka custom dispatcher.

I was thinking that a custom dispatcher would work great to pass along the MDC since then you'd never have to think about it, but unfortunately I couldn't get it to work. Akka kept failing to instantiate the dispatcher. I was also worried about configuration data and possible tuning that you might lose giving akka your own dispatcher configurator. 

So, given that I wasn't quite sure what to do. What I ended up with however was a little extra work but turned out well.  I went with an augmented dispatcher/subscriber model. Basically for every event that I send out I wrap it in a <code>PersistentableMessage</code> which traps the fields of the MDC that I care about, and then on any actor I have them subclass a custom logging base class that pops out the persistent message container, sets the MDC, and gives the actor the underlying message.

For my project we're tracking everything with what we call a <code>CorrelationId</code> which is just a UUID.

<h2>The message wrapper</h2>

[java]
@Data
public class PersistableMessageContext implements CorrelationIdGetter, CorrelationIdSetter {
    private final Object source;

    private UUID correlationId;

    public PersistableMessageContext(Object source){
        this.source = source;

        try {
            final String s = MDC.get(FilterAttributes.CORR_ID);

            setCorrelationId(UUID.fromString(s));
        }
        catch(Throwable ex){}
}
[/java]

This is the message that I want to pass around.  By containing its source correlation ID it can later be used to set the context when its being consumed

<h2>The actor base</h2>

I now have all my actors subclass this class

[java]
public abstract class LoggableActor extends UntypedActor {
    @Override public void onReceive(final Object message) throws Exception {
        Boolean wasSet = false;

        if (CorrelationIdGetter.class.isAssignableFrom(message.getClass())) {
            final UUID correlationId = ((CorrelationIdGetter) message).getCorrelationId();

            if (correlationId != null) {
                MDC.put(FilterAttributes.CORR_ID, correlationId.toString());

                wasSet = true;
            }
        }

        if (message instanceof PersistableMessageContext) {
            onReceiveImpl(((PersistableMessageContext) message).getSource());
        }
        else {
            onReceiveImpl(message);
        }

        if(wasSet) {
            MDC.remove(FilterAttributes.CORR_ID);
        }
    }

    public abstract void onReceiveImpl(final Object message) throws Exception;
}
[/java]

This lets me pass in anything that implements a <code>CorrelationIdGetter</code> and if it happens to also be a persisted message, pop out the inner message.

<h2>Sending out messages</h2>

Now the big issue here is to make sure that we are consistent in publishing messages. This means using routers, broadcasts, etc, all  have to make sure to push out a message wrapped in a persistent container. To help make that easier I created a few augmented akka publisher classes. Below is a class with static methods (to make it easy to import) that wrap an actor ref or a router.

[java]
import akka.actor.ActorRef;
import akka.routing.Broadcast;
import akka.routing.Router;

/**
 * Utilitiy to provide context propagation on akka messages
 */
public class AkkaAugmenter {

    public static AkkaAugmentedActor wrap(ActorRef src) {
        return new AkkaAugmentedActor(){
            @Override public void tell(final Object msg, final ActorRef sender) {
                final PersistableMessageContext persistableMessageContext = new PersistableMessageContext(msg);

                src.tell(persistableMessageContext, sender);
            }

            @Override public ActorRef getActor() {
                return src;
            }
        };
    }

    public static AkkaAugmentedRouter wrap(Router src) {
       return new AkkaAugmentedRouter() {
           @Override public void route(final Object msg, final ActorRef sender) {
               final PersistableMessageContext persistableMessageContext = new PersistableMessageContext(msg);

               src.route(persistableMessageContext, sender);
           }

           @Override public void broadcast(final Object msg, final ActorRef sender) {
               final PersistableMessageContext persistableMessageContext = new PersistableMessageContext(msg);

               src.route(new Broadcast(persistableMessageContext), sender);
           }

           @Override public Router getRouter() {
               return src;
           }
       };
    }
}
[/java]

The augmented actor:

[java]
public interface AkkaAugmentedActor {
    void tell(Object msg, ActorRef sender);

    ActorRef getActor();
}
[/java]

And the augmented router:

[java]
public interface AkkaAugmentedRouter {
    void route(Object msg, ActorRef sender);

    void broadcast(Object msg, ActorRef sender);

    Router getRouter();
}
[/java]

<h2>Conclusion</h2>

And now all I need to do is to wrap a default actor or router give to me by akka. From here on out all messages are auto wrapped and my MDC is properly propagated.  While I would have liked to not rely on convention this way, at least I made it simple once you've made the right types.  

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4627</wp:post_id>
		<wp:post_date><![CDATA[2015-04-07 01:05:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-04-07 01:05:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[adding-mdc-logging-akka]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="akka"><![CDATA[akka]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="mdc"><![CDATA[MDC]]></category>
		<category domain="post_tag" nicename="slf4j"><![CDATA[slf4j]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560404400;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4596;}i:1;a:1:{s:2:"id";i:4629;}i:2;a:1:{s:2:"id";i:4593;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Converting akka scala futures to java futures</title>
		<link>https://onoffswitch.net/2015/05/07/converting-akka-scala-futures-java-futures/</link>
		<pubDate>Thu, 07 May 2015 01:13:48 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4629</guid>
		<description></description>
		<content:encoded><![CDATA[Back in akka land!  I'm using the ask pattern to get results back from actors since I have a requirement to block and get a result (I can't wait for an actor to push at a later date).  Thats fine, but converting from scala futures to java completable futures is a pain.  I also, (like mentioned in another post) want to make sure that my async responses capture and set the MDC for proper logging.

My final usage should look something like:

[java]
private &lt;Response, Request&gt; Future&lt;Response&gt; askActorForResponseAsync(Request source) {
    final FiniteDuration askTimeout = new FiniteDuration(config.getAskForResultTimeout().toMillis(), TimeUnit.MILLISECONDS);

    final Timeout timeout = new Timeout(askTimeout);

    final scala.concurrent.Future&lt;Object&gt; ask = Patterns.ask(master.getActor(), new PersistableMessageContext(source), timeout);

    return FutureConverter.fromScalaFuture(ask)
                          .executeOn(actorSystem.dispatcher())
                          .thenApply(i -&gt; (Response) i);
}
[/java]

The idea is that I'm going to translate a scala future with a callback into a completable future java promise.  

Next up, the future converter:

[java]
public class FutureConverter {
    public static &lt;T&gt; FromScalaFuture&lt;T&gt; fromScalaFuture(scala.concurrent.Future&lt;T&gt; future) {
        return new FromScalaFuture&lt;&gt;(future);
    }
}
[/java]

This is just an entrypoint into a new class that can give you a nice fluent interface to provide the execution context.

Next, a class whose job is to create an akka callback and convert it into a completable future.

[java]
import scala.concurrent.ExecutionContext;
import scala.concurrent.Future;

import java.util.concurrent.CompletableFuture;

public class FromScalaFuture&lt;T&gt; {

    private final Future&lt;T&gt; future;

    public FromScalaFuture(Future&lt;T&gt; future) {
        this.future = future;
    }

    public CompletableFuture&lt;T&gt; executeOn(ExecutionContext context) {
        final CompletableFuture&lt;T&gt; completableFuture = new CompletableFuture&lt;&gt;();

        final AkkaOnCompleteCallback&lt;T&gt; completer = AkkaCompletionConverter.&lt;T&gt;createCompleter((failure, success) -&gt; {
            if (failure != null) {
                completableFuture.completeExceptionally(failure);
            }
            else {
                completableFuture.complete(success);
            }
        });

        future.onComplete(completer.toScalaCallback(), context);

        return completableFuture;
    }
}
[/java]

And finally another guy whose job it is to translate java functions into akka callbacks:

[java]
import akka.dispatch.OnComplete;

@FunctionalInterface 
public interface AkkaOnCompleteCallback&lt;T&gt; {
    OnComplete&lt;T&gt; toScalaCallback();
}
[/java]

[java]
import akka.dispatch.OnComplete;
import org.slf4j.MDC;

import java.util.Map;
import java.util.function.BiConsumer;


public class AkkaCompletionConverter {
    /**
     * Handles closing over the mdc context map and setting the responding future thread with the
     * previous context
     *
     * @param callback
     * @return
     */
    public static &lt;T&gt; AkkaOnCompleteCallback&lt;T&gt; createCompleter(BiConsumer&lt;Throwable, T&gt; callback) {
        return () -&gt; {

            final Map&lt;String, String&gt; oldContextMap = MDC.getCopyOfContextMap();

            return new OnComplete&lt;T&gt;() {
                @Override public void onComplete(final Throwable failure, final T success) throws Throwable {
                    // capture the current threads context map
                    final Map&lt;String, String&gt; currentThreadsContext = MDC.getCopyOfContextMap();

                    // set the closed over context map
                    if(oldContextMap != null) {
                        MDC.setContextMap(oldContextMap);
                    }

                    callback.accept(failure, success);

                    // return the current threads previous context map
                    if(currentThreadsContext != null) {
                        MDC.setContextMap(currentThreadsContext);
                    }
                }
            };
        };
    }
}
[/java]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4629</wp:post_id>
		<wp:post_date><![CDATA[2015-05-07 01:13:48]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-05-07 01:13:48]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[converting-akka-scala-futures-java-futures]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="akka"><![CDATA[akka]]></category>
		<category domain="post_tag" nicename="async"><![CDATA[async]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="futures"><![CDATA[futures]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559708751;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4456;}i:1;a:1:{s:2:"id";i:4627;}i:2;a:1:{s:2:"id";i:4394;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>177</wp:comment_id>
			<wp:comment_author><![CDATA[Alex]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[alex@piggottfamily.co.uk]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[108.40.126.160]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-05-23 21:14:43]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-05-23 21:14:43]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Hey - this looks really cool and exactly what I'm after ... my brain is too addled (And possibly not good enough in the first place) to follow the missing steps - ie what AkkaOnCompleteCallback and what .toScalaCallback() look like, any clues you can give me? Thanks!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>178</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[107.77.97.119]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-06-05 18:50:08]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-06-05 18:50:08]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Alex, my bad. I forgot to post an interface. The toscalacallback is just executing the lambda that the akkaoncompletecallback returns. 

I'm on travel now but I'll post the missing pieces next week.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>177</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Shareable zsh environment: EnvZ</title>
		<link>https://onoffswitch.net/2015/04/13/shareable-zsh-environment-envz/</link>
		<pubDate>Mon, 13 Apr 2015 01:44:26 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4631</guid>
		<description></description>
		<content:encoded><![CDATA[Introducing <a href="https://github.com/devshorts/EnvZ" target="_blank" rel="noopener noreferrer">EnvZ</a>.

<h2>What is Envz?</h2>

During the course of normal production development we all tend to write a bunch of shell scripts and other useful command line utilities that help us out.  Usually the end up being a bunch of one offs or stored in one mega .zshrc file.  However, there's something to be said about having a small framework to share environment utilities and to use as a jump off to "version" a shared set of utilities with team mates.

With that in mind I've been building out a small zsh bootstrapper that builds on tools that I like and use (like yadr) and gives a way to add pluggable team modules into it.  All a pluggable module is is a sym link to another directory that auto loads .sh files on shell start.  And while it sounds like a small thing, it's actually really nice to be able to have different teams version different sets of shell scripts and be able to easily link and share them with environments.

<h2>Example</h2>

For example, let me make a quick folder called <code>team1</code>, put in a dummy shell script and link it to my environment:

<img src="http://onoffswitch.net/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-7.36.45-PM.png" alt="Screen Shot 2015-04-12 at 7.36.45 PM" width="1474" height="1192" class="aligncenter size-full wp-image-4638" />

Notice how our function exists immediately after linking the env!

To unload it:

<img src="http://onoffswitch.net/wp-content/uploads/2015/04/Screen-Shot-2015-04-12-at-7.38.18-PM.png" alt="Screen Shot 2015-04-12 at 7.38.18 PM" width="628" height="204" class="aligncenter size-full wp-image-4639" />

You can imagine now having multiple git repos that you want to share as team specific utilities or bootstrapping. All someone has to do is check out your folder and add it to their environment.

<h2>Features</h2>

Things EnvZ gives you 

<ul>
<li>Opinionated bootstrap (installs python, pip, checks for yadr, gives you a default gui editor [atom])</li>
<li>Simplifies reloading your .zsh environment and autocompletes</li>
<li>Lets you break up your environment into multiple folders or git repos and link them to yours with simple commands with autocomplete</li>
<li>Defines a clean place to add zsh completion files</li>
<li>Auto sets up github enterprise to work with <code>hub</code> and provides json command line parsing with <code>jq</code>
<li>Auto installs cask if its not there</li>
<li>A hook into your teammates source directory. If everyone uses EnvZ then you can be assured that $SRC_DIR is set</li>
<li>Easier pull request autocomplete (via the <code>pr</code> command, the current hub one is broken) that lets you pick the target branch and a message</li>
</ul>

I like yadr and other opinionated setups because they get you up and running fast, but I always found that once you are up and running those bootstrappers didn't thin about how to share your configurations and other utilities.  With a team of 3 or 4 people you may want to have some scripts for you, some for the team, and maybe some for side projects that you can have people bootload up.  

If thats the case, EnvZ is a good option. It's lightweight, easy to change, and easy to load up new envs with.

Enjoy!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4631</wp:post_id>
		<wp:post_date><![CDATA[2015-04-13 01:44:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-04-13 01:44:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[shareable-zsh-environment-envz]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="discussion"><![CDATA[Discussion]]></category>
		<category domain="post_tag" nicename="envz"><![CDATA[envz]]></category>
		<category domain="post_tag" nicename="shell"><![CDATA[shell]]></category>
		<category domain="post_tag" nicename="zsh"><![CDATA[zsh]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561704446;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4673;}i:2;a:1:{s:2:"id";i:4463;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Testing puppet with docker and python</title>
		<link>https://onoffswitch.net/2015/07/08/testing-puppet-docker-python/</link>
		<pubDate>Wed, 08 Jul 2015 00:50:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4673</guid>
		<description></description>
		<content:encoded><![CDATA[In all the past positions I've been in I've been lucky enough to have a dedicated ops team to handle service deployment, cluster health, and machine managmenent. However, at my new company there is much more of a "self serve" mentality such that each team needs to handle things themselves. On the one hand this is a huge pain in my ass, since really the last thing I want to do is deal with clusters and machines. On the other hand though, because we have the ability to spin up openstack boxes in our data centers at the click of a button, each team has the flexibility to host their own infrastructrure and stack.  

For the most part my team and I are deploying our java services using dockerized containers. Our container is a centos7 base image with a logstash forwarder in it and some other minor tooling, and we run our java service in the foreground.  All we need to have on our host boxes is a bootloader script that we execute to shut down old docker containers and spin up new docker containers, and of course docker.  To get docker and our bootloader (and of course manage things like our jenkins instances, RMQ clusters, cassandra nodes, etc) we are using puppet.

After deep diving into puppet my first question was "how do I test this?". Most suggestions indicate testing is two fold

<ol>
<li>Syntax checking</li>
<li>Integration testing on isolated machines</li>
</ol>

The first element is a no brainer. You run the puppet syntax checker and you get some output. That's not that helpful though, other than making sure I didn't fat finger something.  And the second point really sucks. You have to manually check if everything worked.  As an engineer I shudder at the word "manual", so I set out to create an isolated test framework that my team can use to simulate and automatically test puppet scripts both local and on jenkins.

To do that, I wrote <a href="https://github.com/devshorts/Puppety">puppety</a>.  It's really stupidly simple.  The gist is you have a puppet master in a docker container who auto signs anyone who connects, and you have a puppet agent in a docker container who connects, syncs, and then runs tests validating the sync was complete.

<h2>Puppety structure</h2>

If you look at the git repo, you'll see there are two main folders:

[code]
/data
/test
[/code]

The <code>/data</code> folder is going to map to the <code>/etc/puppet</code> folder on our puppet master. It should contain all the stuff we want to deploy as if we plopped that whole folder onto the puppet root.

The test folder contains the python test runners, as well as the dockerized containers for both the master and the agent.  

<h2>Testing a node</h2>

If you have a node configuration in an environment you can test a node by annotating it like so:

[code]
# node-test: jenkins/test-server
node &quot;test.foo.com&quot; {
  file {'/tmp/example-ip':                                            # resource type file and filename
    ensure  =&gt; present,                                               # make sure it exists
    mode    =&gt; 0644,                                                  # file permissions
    content =&gt;  &quot;Here is my Public IP Address: ${ipaddress_eth0}.\n&quot;,  # note the ipaddress_eth0 fact
  }
}
[/code]

Lets say this node sits in a definition file in <code>/etc/puppet/environments/develop/manifests/nodes/jenkins.pp</code>

Our test runner can pick up that we asked to test the jenkins node, and template our manifests such that during run time the actual node definition looks like

[code]
# node-test: jenkins/test-server
node /docker_host.*/ {
  file {'/tmp/example-ip':                                            # resource type file and filename
    ensure  =&gt; present,                                               # make sure it exists
    mode    =&gt; 0644,                                                  # file permissions
    content =&gt;  &quot;Here is my Public IP Address: ${ipaddress_eth0}.\n&quot;,  # note the ipaddress_eth0 fact
  }
}
[/code]

Now, when the dockerized puppet container connects, it assumes the role of the jenkins node!  

The tests sit in a folder called tests/runners and the test name is the path to the test to run. It's that simple.

We are also structuring our puppet scripts in terms of roles. Roles using a custom facter who reads from <code>/etc/.role/role</code> to find out the role name of a machine. So this way, when a machine connects to puppet it'll say "I'm this role" and puppet can switch on the role to know what configurations to apply. 

To support this, we can annotate role tests like so

[code]
node default {
  case $node_role{
    # role-test: roles/slave-test
    'slave': {
      file {'/tmp/node-role': # resource type file and file
        ensure  =&gt; present,   # make sure it exists
        mode    =&gt; 0644,      # file permissions
        content =&gt;    &quot;Here is my Role ${$node_Role}.\n&quot;,  # note the node role
       }
    }
    # role-test: roles/listener-test
    'listener': {
      file { '/tmp/listener': # resource type file and file
        ensure  =&gt; present,   # make sure it exists
        mode    =&gt; 0644,      # file permissions
        content =&gt;    &quot;I am a listener&quot;,  # note the node role
      }
    }
   }
}
[/code]

When the <code>roles/slave-test</code> gets run the test runner will add the role <code>slave</code> to the right file, such that when the container connects it'll assume that role.

The tests themselves are trivial. They use <code>pytest</code> syntax and look like this:

[python]
from util.puppet_utils import *

@agent
def test_file_exists():
    assert file_exists(&quot;/tmp/example-ip&quot;)

@agent
def test_ip_contents_set():
    assert contents_contains('/tmp/example-ip', 'Here is my Public IP Address')

@master
def test_setup():
    print &quot;foo&quot;
[/python]

Tests are annotated by where they'll run at. Agent tests run after a sync, but master tests will run BEFORE the master runs. This is so you can do any setup on the master you need. Need to drop in some custom data before the agent starts? Perfect place to do it.  

<h2>Getting test results on jenkins</h2>

The fun part about this is that we can output the result of each test into a linked docker volume. Our jenkins test runner just looks like:

[shell]
cd test

PATH=$WORKSPACE/venv/bin:/usr/local/bin:$PATH

virtualenv venv

. venv/bin/activate

pip install -r requirements.txt

python test-runner.py -e develop --all

python test-runner.py -e production --all
[/shell]

And we can collect our results to get a nice test graph

<img src="http://onoffswitch.net/wp-content/uploads/2015/07/Screen-Shot-2015-07-07-at-5.38.23-PM.png" alt="Screen Shot 2015-07-07 at 5.38.23 PM" width="901" height="359" class="aligncenter size-full wp-image-4674" />

To deploy we have cron job on the puppet master to pull back our puppet scripts git repo and merge the <code>data</code> folder into its /etc/puppet folder. 

<h2>Debugging the containers</h2>

Sometimes using puppety goes wrong and it's nice to see whats going on. Because each container exposes an entrypoint script we can pass in a debug flag to get access to a shell so we can run the tests manually:

[code]
$ docker run -it -h docker_host -v ~/tmp/:/opt/local/tmp puppet-tests/puppet-agent --debug /bin/bash
[/code]

Now we can execute the entrypoint by hand, or run puppet by hand and play around.  

<h2>Conclusion</h2>

All in all this has worked really well for our team. It's made it easy for us to prototype and play with our infrastructure scripts in a controlled environment locally. And since we are able to now actually write tests against our infrastructure we can feel more comfortable about pushing changes out to prod.  
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4673</wp:post_id>
		<wp:post_date><![CDATA[2015-07-08 00:50:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-07-08 00:50:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[testing-puppet-docker-python]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="docker"><![CDATA[docker]]></category>
		<category domain="post_tag" nicename="jenkins"><![CDATA[jenkins]]></category>
		<category domain="post_tag" nicename="puppet"><![CDATA[puppet]]></category>
		<category domain="post_tag" nicename="python"><![CDATA[python]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1556506239;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:4978;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Automating deployments with salt, puppet, jenkins and docker</title>
		<link>https://onoffswitch.net/2015/08/13/automating-deployments-salt-puppet-jenkins-docker/</link>
		<pubDate>Thu, 13 Aug 2015 22:32:46 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4699</guid>
		<description></description>
		<content:encoded><![CDATA[I know, its a buzzword mouthful.  My team has had good first success leveraging jenkins, salt, sensu, puppet, and docker to package and monitor distributed java services with a one click deployment story so I wanted to share how we've set things up.

First and foremost, I've never been an ops guy. I've spent time on build systems like msbuild and fake, but never a full pipeline solution that also had to manage infrastructure, but there's a first for everything. Most companies I've worked at have had all this stuff set up already, and out of the minds of developers, but the place I am at now does not.  I actually think it's been a great opportunity to dive into the full flow of how do you get your damn code out the door. I think if you are developing an application and don't know how it gets from git to your box, you should do some digging, because there is A LOT that happens. 

Given that my team doesn't have a one click solution to just so "<a href="https://www.youtube.com/watch?v=gs0w98-ekFk" target="_blank" rel="noopener noreferrer">magic</a>!" I set out to build an initial workflow for our team that would let us package, deploy, monitor, and version our infrastructure. I wanted the entire setup to be jenkins driven so everything we have is versioned in github enterprise with jenkins hooks to build repositories. Full disclaimer, this isn't meant to work for enormous teams or companies, but just for what mine and another team are doing. 

None of this is really all that new or interesting to anyone whose done it, but I wanted to document how and why we did it the way we have. 

<h1>Puppet</h1>

The first thing we did was create a puppet repo. You may have remembered in a past post I blogged before how that led to testing puppet scripts with docker (<em>which sort of fizzled due to puppet <a href="http://developerblog.redhat.com/2014/05/05/running-systemd-within-docker-container/">not having access to systemd</a> for service starting. It would work on a pure linux box, but given boot2docker doesn't have cgroups to virtually mount it wouldn't work on a mac</em>).  Our puppet scripts are set up by environment:

<img src="http://onoffswitch.net/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.25.37-PM.png" alt="Puppet directory structure" width="523" height="645" class="aligncenter size-full wp-image-4700" />

Where our "/data" folder will get mapped to "/etc/puppet".  

We use a custom <a href="https://puppetlabs.com/facter">facter</a>  to identify nodes in our environments.  Instead of setting up our site.pp manifest with actual node matching, everyone has a custom role and our site delegates what to do based on that role:

[code]
$rmq_master = &quot;...&quot;
$salt_master = &quot;...&quot;
$sensu_host = &quot;...&quot;

node default {
  case $node_role{
   
    'rmq_master': {
      class { 'domains::monitoring::salt::client':
        salt_master =&gt; $salt_master
      }

      class { &quot;domains::rmq::master&quot; :
        sensu_host =&gt; $sensu_host
      }
    }

    'rmq_slave': {
      class { 'domains::monitoring::salt::client':
        salt_master =&gt; $salt_master
      }

      class { &quot;domains::rmq::slave&quot; :
        master_nodename =&gt; $rmq_master,
        sensu_host      =&gt; $sensu_host
      }
    }

    'sensu_server': {
      class { 'domains::monitoring::salt::client':
        salt_master =&gt; $salt_master
      }

      class { &quot;domains::monitoring::sensu::server&quot;: }
    }

    'jenkins-master' : {
      class { 'domains::monitoring::salt::client':
        salt_master =&gt; $salt_master
      }

      class { &quot;domains::cicd::jenkins-master&quot;:
        sensu_host =&gt; $sensu_host
      }
    }

    'jenkins-slave' : {
      class { 'domains::monitoring::salt::client':
        salt_master =&gt; $salt_master
      }

      class { &quot;domains::cicd::jenkins-slave&quot;:
        sensu_host =&gt; $sensu_host
      }
    }

    'elk-server': {
      class { 'domains::monitoring::salt::client':
        salt_master =&gt; $salt_master
      }

      class { &quot;domains::monitoring::elk::server&quot; :
        sensu_host =&gt; $sensu_host
      }
    }
  }
}
[/code]

With the exceptions of the master hostnames that we need to know about we can now spin up new machines, link them to puppet, and they become who they are. We store the role files in /etc/.config/role on the agents and our facter looks like this

[ruby]
# node_role.rb
require 'facter'
Facter.add(:node_role) do
   confine :kernel =&gt; 'Linux'
      setcode do
        Facter::Core::Execution.exec('cat /etc/.config/role')
      end
end
[/ruby]

Linking them up to puppet is easy too, we <a href="https://gist.github.com/devshorts/e3d89b4e5dce9f145439">wrote some scripts</a> to help bootstrap machines since we don't want to have manually install puppet on each box, then configure its puppet.conf file to point to a master, etc. We want to just from our shell spin up new VM's in our openstack instance, and create new roles quickly.  And by adding on zsh autocompletion we can get a really nice experience:

<img src="http://onoffswitch.net/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.33.05-PM.png" alt="Bootstrapping puppet agent" width="505" height="138" class="aligncenter size-full wp-image-4702" />

<h1>Salt</h1>

<a href="http://saltstack.com/">Saltstack</a> is an orchestration tool built on zeromq that we use to delegate tasks and commands. Basically anytime we need to execute a command on a machine, or role, or group of machines, we'll use salt.  For example, let me ping all the machines in our salt group:

<img src="http://onoffswitch.net/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-3.24.12-PM.png" alt="Screen Shot 2015-08-13 at 3.24.12 PM" width="439" height="262" class="aligncenter size-full wp-image-4720" />

You can run arbitrary commands on machines too, or you can write your own custom python modules to execute (test.ping is a module called test with a method called ping that will get executed on all machines).

Leveraging salt, we can deploy our puppet scripts continuously trigged from a git repo change. When you commit into github a jenkins job is trigged. The jenkins job runs the puppet syntax validator on each puppet file:

[code]
#!/usr/bin/env bash

pushd data

for file in `find . -name &quot;*.pp&quot;`; do
   echo &quot;Validating $file&quot;

   puppet parser validate $file

   if [ $? -ne 0 ]; then
     popd
   	 exit 1;
   fi
done;

popd
[/code]

If that succeeds it dispatches a command to salt with a nodegroup of the puppet master.  All this does is <a href="https://wiki.jenkins-ci.org/display/JENKINS/saltstack-plugin">execute a remote command</a> (via the salt REST api) on the puppet master machine (or group of machines, we really dont care) and it checks out the git repo at a particular commit into a temp folder, blows out the environment and custom modules folders in /etc/puppet and then copies over the new files.  

The nice thing here, is that even if we screwed up with our puppet scripts, we can still execute salt commands and roll things back with our jenkins job.  

We take this salt deployment one step further for deploying applications in docker containers.  

<h1>Docker</h1>

My team is developing java services that deploy into openstack VMs running centos7.  While we could push RPM's into spacewalk and orchestrate uninstalling and reinstalling rpms via puppet, I've had way too many instances in the past of config files getting left behind, or some other garbage mucking up my otherwise pristine environment. We also wanted a way to simulate the exact prod environment on our local machines. Building distributed services is easy, but debugging them is really hard, especially when you have lots of dependencies (rmq, redis, cassandra, decoupled listeners, ingest apis, health monitors, etc).  For that reason we chose to package up our java service as a <a href="https://maven.apache.org/plugins/maven-shade-plugin/">shaded jar</a> (we use <a href="http://www.dropwizard.io/">dropwizard</a>) and package it into a docker image.

We built a base docker image for java services and then bundle our config, jar, and bootstrap scripts, into the base image. After our jenkins build of our service is complete, our package process takes the final artifact (which actually is an RPM, which is created using the maven RPM plugin), templates out a dockerfile, and creates a tar file with both files (RPM and Dockerfile) to artifact. This way we can use the <a href="https://wiki.jenkins-ci.org/display/JENKINS/Promoted+Builds+Plugin">jenkins promotion plugin</a> to do the actual docker build. The tar file is there because jenkins promotions are asynchronous and can run on different slaves, so we want to include the raw artifacts as part of the artifacting for access later.

Here is an example of the templated dockerfile which will generates our docker image. The <code>--RPM_SRC--</code> and <code>--RPM_NAME--</code> placeholders get replaced during build

[code]
FROM artifactory/java-base

# add the rpm
RUN mkdir /rpms

ADD --RPM_SRC-- /rpms/

# install the rpm
RUN yum -y install /rpms/--RPM_NAME--

# set the service to run
ENV SERVICE_RUN /data/bin/service
[/code]

And below you can see the different jenkins promotion steps here. The first star is artifacting the docker image, and the second is deploying it to puppet via salt:

<img src="http://onoffswitch.net/wp-content/uploads/2015/08/Screen-Shot-2015-08-13-at-2.51.58-PM.png" alt="Deploy services" width="826" height="190" class="aligncenter size-full wp-image-4705" />



Once we artifact the docker image, it's pushed out to artifactory.  All our images are tagged with <code>app_name:git_sha</code> in artifactory. To deploy the docker image out into the wild we use salt to push the expected git sha for that application type onto the puppet master.  I mentioned one custom facter that we wrote that nodes use to report their information back, but we also have a second.  

Nodes store a json payload of their docker app arguments, the container version, and some other metadata, so that they know what they are currently running.  They also report this json payload back to puppet, and puppet can determine if their currently running git_sha is the same as the expected one.  This way we know all their runtime arguments that were passed, etc. If anything is different then puppet stops the current container, and loads on the new one.  All our containers are named, since it makes managing this easier (vs using anonymous container names).  When we do update an application to the newest version we write that json back to disk in a known location. Here is the factor that collects all the docker version information for us:

[ruby]
# Build a hash of all the different app jsons, located in /etc/.config/versions
# there could be many docker applications running on this machine, which is why we bundle them all up together
require 'facter'
require 'json'
Facter.add(:node_info) do
  confine :kernel =&gt; 'Linux'
  setcode do
    node_information = {}
    path = '/etc/.config/versions'

    if File.exist?(path)
      Dir.foreach(path) do |app_type|
        next if File.directory?(File.join(path, app_type))
        content = File.read(File.join(path, app_type))
        data_hash = JSON.parse(content)
        node_information[app_type] = data_hash
      end
    end

    node_information
  end
end
[/ruby]

To ensure reboot tolerance and application restart tolerance our base docker image runs an instance of monit which is the foreground process for our application. We also make sure all docker containers are started with <code>--restart always</code> and that the docker service is set to start on reboot.

<h1>Cluster swinging</h1>

We are still working on doing cluster swinging, but we'll leverage salt as well as a/b node roles in puppet to do that. For example, we could tag half of our RMQ listeners as A roles, and the other half B roles. When we go to publish we can publish by application type, so <code>RMQ_A_ROLE</code> will be an application type. Those can all switch over while the B roles stay the same.  We can also leverage salt to do HAProxy swings. If we send a message to puppet to use a different haproxy config, then tell the haproxy box to update we've done part of a swing. Same in the reverse. All of this can be managed with jenkins promotion steps and can be undone by re-promoting an older step.

<h1>Conclusion</h1>

This is just the beginning of our ops story on my team, as we have a lot of other work to do, but now that the basics are there we can ramp much faster. Whats interesting is that there are a lot of different varieties of production level tooling available, but no real consensus on how to do any of it. In my research I found endless conflicting resources and suggestions, and it seemed like every team (including mine!) was just sort if reinventing the process. I'm hoping that how we have things set up will work well, but our application is still in the major development phases so we have yet to really see how this will pan out in production (though our development cluster has 30 machines that we manage this way now).

And of course there are still issues. For example, we are using a puppet sensu plugin that keeps restarting the sensu process every 60 seconds. It's not the end of the world, but it is annoying.  We had to write our own docker puppet module since the one out there didn't do what we wanted.  The RMQ puppet module doesn't support FQDN mode for auto adding slaves, so we had to write that. We had to write tooling to help bootstrap boxes and deploy the puppet scripts, etc.  While none of this is technologically challenging from a purely code perspective, it wasn't an easy process as most of your time spent debugging is "why the hell isn't this working, it SHOULD!" only to find you missed a comma somewhere (why can't we just do everything in strongly typed languages...).  And of course, headaches are always there when you try something totally new, but in the end it was an interesting journey and gives me so much more appreciate for ops people.  They never get enough credit.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4699</wp:post_id>
		<wp:post_date><![CDATA[2015-08-13 22:32:46]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-08-13 22:32:46]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[automating-deployments-salt-puppet-jenkins-docker]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="automation"><![CDATA[automation]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="docker"><![CDATA[docker]]></category>
		<category domain="post_tag" nicename="jenkins"><![CDATA[jenkins]]></category>
		<category domain="post_tag" nicename="puppet"><![CDATA[puppet]]></category>
		<category domain="post_tag" nicename="salt"><![CDATA[salt]]></category>
		<category domain="post_tag" nicename="sensu"><![CDATA[sensu]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560966243;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4673;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:5000;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Adventures in pretty printing JSON in haskell</title>
		<link>https://onoffswitch.net/2015/08/15/adventures-pretty-printing-json-haskell/</link>
		<pubDate>Sat, 15 Aug 2015 22:13:11 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4725</guid>
		<description></description>
		<content:encoded><![CDATA[Today I gave atom haskell-ide a whirl and wanted to play with haskell a bit more. I've played with haskell in the past and always been put off by the tooling. To be fair, I still kind of am. I love the idea of the language but the tooling is just not there to make it an enjoyable exploratory experience. I spend half my time in the repl inspecting types, the other half on hoogle, and the 3rd half (yes I know) being frustrated that I can't just type in package names and explore API's in sublime or atom or wherever I am. Now that I'm on a mac, maybe I'll give leksah another try. I tried it a while ago it didn't work well.

Anyways, I digress. Playing with haskell and I thought I'd try poking with Aeson, the JSON library.  Like Scala, you have to define your objects as json parseable/serializable (unlike java/c# which use runtime reflection).  Thankfully if you enable some language extensions its just a matter of adding <code>Generic</code> to your derives list and making sure the data type is of the <code>ToJSON</code> and <code>FromJSON</code> typeclasses. Mostly I was just copying the examples from <a href="https://www.fpcomplete.com/school/starting-with-haskell/libraries-and-frameworks/text-manipulation/json">here</a>.

My sample class is 

[haskell]
{-# LANGUAGE DeriveGeneric #-}

module Types where

import Data.Aeson
import GHC.Generics

data Person =
  Person  { firstName :: String
           ,lastName :: String
          } deriving(Show, Generic)

instance ToJSON Person
instance FromJSON Person
[/haskell]

And I just wanted to make a simple hello world where I'd read in some data, make my object, and print pretty json to the screen.  

On my first try:

[haskell]
import Data.Aeson
import Types

process :: IO String
process = getLine

main = do
  putStrLn &quot;First name&quot;
  firstName &lt;- process

  putStrLn &quot;Last name&quot;
  lastName &lt;- process

  let person = Person firstName lastName

  print $ (encode person)

  return ()
[/haskell]

When I run <code>cabal build;cabal run</code> I now get

[code]
First name
anton
Last name
kropp
&quot;{\&quot;lastName\&quot;:\&quot;kropp\&quot;,\&quot;firstName\&quot;:\&quot;anton\&quot;}&quot;
[/code]

Certainly JSON, but I want it <em>pretty</em>.  I found <a href="https://hackage.haskell.org/package/aeson-pretty">aeson-pretty</a> and gave that a shot. Now I'm doing:

[haskell]
import Data.Aeson
import Data.Aeson.Encode.Pretty
import Types

process :: IO String
process = getLine

main = do
  putStrLn &quot;First name&quot;
  firstName &lt;- process

  putStrLn &quot;Last name&quot;
  lastName &lt;- process

  let person = Person firstName lastName

  print $ (encodePretty person)

  return ()
[/haskell]

And I got:

[code]
First name
anton
Last name
kropp
&quot;{\n    \&quot;lastName\&quot;: \&quot;kropp\&quot;,\n    \&quot;firstName\&quot;: \&quot;anton\&quot;\n}&quot;
[/code]

Hmm. I can see that it <em>should</em> be pretty, but it isn't.  How come?  Lets check out the types:

[haskell]
Prelude &gt; import Data.Aeson
Prelude Data.Aeson &gt; :t encode
encode :: ToJSON a =&gt; a -&gt; Data.ByteString.Lazy.Internal.ByteString
[/haskell]

Whats a lazy bytestring? 

Well, from <a href="https://www.fpcomplete.com/school/to-infinity-and-beyond/pick-of-the-week/bytestring-bits-and-pieces">fpcomplete</a>

<blockquote>ByteString provides a more efficient alternative to Haskell's built-in String which can be used to store 8-bit character strings and also to handle binary data</blockquote>

And the lazy one is the, well, lazy version.  Through some googling I find that the right way to print the bytestring is by using the "putStr" functions in the Data.ByteString package. But as a good functional programmer, I want to encapsulate that and basically make a useful function that given the json object I can get a plain ol happy string and decide how to print it later.

I need to somehow make a lazy bytestring into a regular string. This leads me to this:

[haskell]
getJson :: ToJSON a =&gt; a -&gt; String 
getJson d = unpack $ decodeUtf8 $ BSL.toStrict (encodePretty d)
[/haskell]

So I first evaluate the bytestring into a strict version (instead of lazy), then decode it to utf8, then unpack the text class into strings (apparenlty text is more efficient but more API's use String).

[haskell]
Prelude &gt; :t toStrict
toStrict :: ByteString -&gt; Data.ByteString.Internal.ByteString

Prelude &gt; :t decodeUtf8
decodeUtf8 :: Data.ByteString.Internal.ByteString -&gt; Text

Prelude &gt; :t Data.Text.unpack
Data.Text.unpack :: Text -&gt; String
[/haskell]

And now, finally:

[haskell]
import Data.Aeson
import Data.Aeson.Encode.Pretty
import qualified Data.ByteString.Lazy as BSL
import Data.Text
import Data.Text.Encoding
import Types

process :: IO String
process = getLine

getJson :: ToJSON a =&gt; a -&gt; String 
getJson d = unpack $ decodeUtf8 $ BSL.toStrict (encodePretty d)

main = do
  putStrLn &quot;First name&quot;
  firstName &lt;- process

  putStrLn &quot;Last name&quot;
  lastName &lt;- process

  let person = Person firstName lastName

  print $ getJson person

  return ()
[/haskell]

Which gives me

[haskell]
First name
anton
Last name
kropp
&quot;{\n    \&quot;lastName\&quot;: \&quot;kropp\&quot;,\n    \&quot;firstName\&quot;: \&quot;anton\&quot;\n}&quot;
[/haskell]

AGHH! Still!  Ok, more googling.  Google google google.  

Last piece of the puzzle is that print is really <code>putStrLn . show</code>

[haskell]
Prelude &gt; let x = putStrLn . show
Prelude &gt; x &quot;foo\nbar&quot;
&quot;foo\nbar&quot;
[/haskell]

And if we just do

[haskell]
Prelude &gt; putStrLn &quot;foo\nbar&quot;
foo
bar
[/haskell]

The missing ticket.  Finally all put together:

[haskell]
import Data.Aeson
import Data.Aeson.Encode.Pretty
import qualified Data.ByteString.Lazy as BSL
import Data.Text
import Data.Text.Encoding
import Types

process :: IO String
process = getLine

getJson :: ToJSON a =&gt; a -&gt; String
getJson d = unpack $ decodeUtf8 $ BSL.toStrict (encodePretty d)

main = do
  putStrLn &quot;First name&quot;
  firstName &lt;- process

  putStrLn &quot;Last name&quot;
  lastName &lt;- process

  let person = Person firstName lastName

  putStrLn $ getJson person

  return ()
[/haskell]

Which gives me:

[code]
$ cabal build; cabal run
Building sample-0.1.0.0...
Preprocessing executable 'sample' for sample-0.1.0.0...
[3 of 3] Compiling Main             ( src/Main.hs, dist/build/sample/sample-tmp/Main.o )
Linking dist/build/sample/sample ...
Preprocessing executable 'sample' for sample-0.1.0.0...
Running sample...
First name
anton
Last name
kropp
{
    &quot;lastName&quot;: &quot;kropp&quot;,
    &quot;firstName&quot;: &quot;anton&quot;
}
[/code]

Source available at my <a href="https://github.com/devshorts/Playground/tree/master/haskell/aeson-tests">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4725</wp:post_id>
		<wp:post_date><![CDATA[2015-08-15 22:13:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-08-15 22:13:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[adventures-pretty-printing-json-haskell]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="aeson"><![CDATA[aeson]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="haskell"><![CDATA[haskell]]></category>
		<category domain="post_tag" nicename="json"><![CDATA[json]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558720432;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4244;}i:1;a:1:{s:2:"id";i:4327;}i:2;a:1:{s:2:"id";i:4348;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>179</wp:comment_id>
			<wp:comment_author><![CDATA[Robb Shecter]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[robb@weblaws.org]]></wp:comment_author_email>
			<wp:comment_author_url>http://robb.weblaws.org</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[71.193.199.110]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-08-07 08:57:33]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-08-07 08:57:33]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I discovered Data.String.Conversions and it's a tremendous help: 

https://gist.github.com/dogweather/a5176a48eca597fa31064e146ef85ad9#file-analyze-amendment-hs-L10]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Dynamic HAProxy configs with puppet</title>
		<link>https://onoffswitch.net/2015/10/18/dynamic-ha-reload-puppet/</link>
		<pubDate>Sun, 18 Oct 2015 20:48:30 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4737</guid>
		<description></description>
		<content:encoded><![CDATA[I've posted a little about puppet and our teams ops in the past since my team has pretty heavily invested in the dev portion of the ops role.  Our initial foray into ops included us building a pretty basic puppet role based system which we use to coordinate docker deployments of our java services. 

We use HAProxy as our software load balancer and the v1 of our infrastructure managment had us versioning a hardcoded haproxy.cfg for each environment and pushing out that config when we want to add or remove machines from the load balancer. It works, but it has a few issues

<ol>
<li>Cluster swings involve checking into github. This pollutes our version history with a bunch of unnecessary toggling</li>
<li>Difficult to automate swings since its flat file config driven and requires the config to be pushed out from puppet</li>
</ol>

Our team did a little brainstorming and came up with a nice solution which is to data drive it from some sort of json blob.  By abstracting who provides the json blob and just building out our ha proxy config from structured data we can move to an API to serve this up for us.  Step one was to replace our haproxy.conf with some sort of flat file json. The workflow we have isn't changing, but its setting us up for success. Step two is to tie in something like consul to provide the json for us. 

The first thing we need to do to support this is get puppet to know how to load up json from either a file or from an api. To do that we built an extra puppet custom function which we put into our <code>/etc/puppet/modules/custom/lib/puppet/functions</code> folder:

[ruby]
require 'json'
require 'rest-client'

module Puppet::Parser::Functions
  newfunction(:json_provider, :type =&gt; :rvalue) do |args|

    begin
      url=args[0]

      info(&quot;Getting json from url #{url}&quot;)

      if File.exists?(url)
        raw_json = File.read(url)
      else
        raw_json = RestClient.get(url)
      end

      data = JSON.parse(raw_json)

      info(&quot;Got json #{data}&quot;)

      data
    rescue Exception =&gt; e
      warning(&quot;Error accessing url #{url} from args '#{args}' with exception #{e}&quot;)

      raise Puppet::ParseError, &quot;Error getting value from url #{url} exception #{e}&quot;
    end
  end
end
[/ruby]

And we need to make sure the puppetmaster knows where all its gems are so we we've added

[code]
 if ! defined(Package['json']) {
    package { 'json':
      ensure   =&gt; installed,
      provider =&gt; 'gem'
    }
  }

  if ! defined(Package['rest-client']) {
    package { 'rest-client':
      ensure   =&gt; installed,
      provider =&gt; 'gem'
    }
  }
[/code]

To our puppet master role .pp.

At this point we can define what our ha proxy json file would look like. A sample structure that we've settled on looks like this:

[code]
{
  &quot;frontends&quot;: [
    {
      &quot;name&quot;: &quot;main&quot;,
      &quot;bind&quot;: &quot;*&quot;,
      &quot;port&quot;: 80,
      &quot;default_backend&quot;: &quot;app&quot;
    },
    {
      &quot;name&quot;: &quot;legacy&quot;,
      &quot;bind&quot;: &quot;*&quot;,
      &quot;port&quot;: 8080,
      &quot;default_backend&quot;: &quot;app&quot;
    }
  ],
  &quot;backends&quot;: [
    {
      &quot;name&quot;: &quot;app&quot;,
      &quot;options&quot;: [
        &quot;balance roundrobin&quot;
      ],
      &quot;servers&quot;: [
        {
          &quot;name&quot;: &quot;api1&quot;,
          &quot;host&quot;: &quot;api1.cloud.dev:8080&quot;,
          &quot;option&quot;: &quot;check&quot;
        },
        {
          &quot;name&quot;: &quot;api2&quot;,
          &quot;host&quot;: &quot;api1.cloud.dev:8080&quot;,
          &quot;option&quot;: &quot;check&quot;
        }
      ]
    }
  ]
}
[/code]

Using this structure we can dynamically build out our haproxy.conf using ruby's erb templating that puppet hooks into. Below is our ha proxy erb template. It assumes that <code>@config</code> is in the current scope which should be a json object in the puppet file.  While the config is pretty basic, we don't use any ACLs or too many custom options, we can always tweak the base haproxy config or add more metadata to our json structure to support more options.

[ruby]
#---------------------------------------------------------------------
# Example configuration for a possible web application.  See the
# full configuration options online.
#
#   http://haproxy.1wt.eu/download/1.4/doc/configuration.txt
#
#---------------------------------------------------------------------

#---------------------------------------------------------------------
# Global settings
#---------------------------------------------------------------------
global
    # to have these messages end up in /var/log/haproxy.log you will
    # need to:
    #
    # 1) configure syslog to accept network log events.  This is done
    #    by adding the '-r' option to the SYSLOGD_OPTIONS in
    #    /etc/sysconfig/syslog
    #
    # 2) configure local2 events to go to the /var/log/haproxy.log
    #   file. A line like the following can be added to
    #   /etc/sysconfig/syslog
    #
    #    local2.*                       /var/log/haproxy.log
    #
    log         127.0.0.1 local2

    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     4000
    user        haproxy
    group       haproxy
    daemon

    # turn on stats unix socket
    stats socket /var/lib/haproxy/stats level admin

#---------------------------------------------------------------------
# common defaults that all the 'listen' and 'backend' sections will
# use if not designated in their block
#---------------------------------------------------------------------
defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option http-server-close
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          1m
    timeout server          1m
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 3000

listen stats :1936
    mode http
    stats enable
    stats hide-version
    stats realm Haproxy\ Statistics
    stats uri /
    stats auth admin:password
#---------------------------------------------------------------------
# main frontend which proxys to the backends
#---------------------------------------------------------------------
&lt;% @config[&quot;frontends&quot;].each do |frontend| %&gt;
frontend  &lt;%= frontend[&quot;name&quot;] %&gt; &lt;%= frontend[&quot;bind&quot;] %&gt;:&lt;%= frontend[&quot;port&quot;] %&gt;
    default_backend             &lt;%= frontend[&quot;default_backend&quot;] %&gt;
&lt;% end %&gt;
#---------------------------------------------------------------------
# backends
#---------------------------------------------------------------------

&lt;% @config[&quot;backends&quot;].each do |backend| %&gt;
backend &lt;%= backend[&quot;name&quot;] %&gt;
    &lt;%- if backend[&quot;options&quot;] != nil -%&gt;
        &lt;%- backend[&quot;options&quot;].each do |option| -%&gt;
    &lt;%= option %&gt;
        &lt;%- end -%&gt;
    &lt;%- end -%&gt;
    &lt;%- backend[&quot;servers&quot;].each do |server| -%&gt;
    server  &lt;%= server[&quot;name&quot;] %&gt; &lt;%= server[&quot;host&quot;] %&gt; &lt;%= server[&quot;option&quot;] %&gt;
    &lt;%- end -%&gt;
&lt;% end %&gt;
[/ruby]

This builds out a simple set of named frontends that point to a set of backends. We can populate backends for the different swing configurations (A cluster, B cluster, etc) and then toggle the default frontend to swing.

But, we still have to provide for a graceful reload. There is <a href="http://engineeringblog.yelp.com/2015/04/true-zero-downtime-haproxy-reloads.html">a lot</a> of <a href="http://serverfault.com/questions/580595/haproxy-graceful-reload-with-zero-packet-loss">documentation</a> out there on this, but the gist is that you want to cause clients to retry under the hood while you restart, so that the actual requester of the connection doesn't notice a blip in service.  To do that we can leverage the codified structure as well with another template

[ruby]
#!/bin/bash

# hold/pause new requests
&lt;% @config[&quot;frontends&quot;].each do |frontend| %&gt;
/usr/sbin/iptables -I INPUT -p tcp --dport &lt;%= frontend[&quot;port&quot;] %&gt; --syn -j DROP
&lt;% end %&gt;

sleep 1

# gracefully restart haproxy
/usr/sbin/haproxy -f /etc/haproxy/haproxy.cfg -p /var/run/haproxy.pid -sf $(cat /var/run/haproxy.pid)

# allow new requests to come in again
&lt;% @config[&quot;frontends&quot;].each do |frontend| %&gt;
/usr/sbin/iptables -D INPUT -p tcp --dport  &lt;%= frontend[&quot;port&quot;] %&gt; --syn -j DROP
&lt;% end %&gt;
[/ruby]

This inserts a rule for each frontend port to drop SYN packets silenty. SYN is the first packet type used in the <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol#Connection_establishment">tcp 3 way handshake</a> and by dropping it the client will retry a few times <a href="http://www.streppone.it/cosimo/blog/2011/07/how-to-detect-tcp-retransmit-timeouts-in-your-network/">after some interval</a> to reconnect.  This does mean the initial client will experience a slight delay, but their request will go through vs getting completely dropped.

Now our final haproxy.pp file looks like

[code]
class custom::loadbalancers::dynamic_ha(
  $load_balance_path = undef,
  $identity = undef # a unique seed to make sure the haproxy reloads dont stomp
)
{

  if $load_balance_path  == undef {
    fail 'Pass in a load balance source path. Can be either a file on disk or a GET json url'
  }

  if $identity == undef {
    fail &quot;Identity for ha should be unique and set. This creates a temp file for reloading the haproxy gracefully&quot;
  }

  package { 'haproxy':
    ensure =&gt; installed
  } -&gt;

  service { 'haproxy':
    enable =&gt; true,
    ensure =&gt; running,
  }  -&gt;

  package { 'haproxyctl':
    ensure    =&gt; installed,
    provider  =&gt; &quot;gem&quot;
  }

  $config = json_provider($load_balance_path)

  $rand = fqdn_rand(1000, $identity)

  $file = &quot;/tmp/$identity-ha-reload.sh&quot;

  file { '/etc/haproxy/haproxy.cfg':
    ensure   =&gt; present,
    mode     =&gt; 644,
    notify   =&gt; Exec['hot-reload'],
    content  =&gt; template(&quot;custom/app/ha.conf.erb&quot;)
  }

  file { $file:
    content  =&gt; template(&quot;custom/app/ha_reload.conf.erb&quot;),
    mode     =&gt; 0755
  } -&gt;
  exec { 'hot-reload' :
    require     =&gt; File[$file],
    command     =&gt; $file,
    path        =&gt; &quot;/usr/bin:/usr/sbin&quot;,
    refreshonly =&gt; true
  }
}
[/code]

With this, we can now drive everything from either a json file, or from a GET rest endpoint that provides JSON.  We're planning on using consul as a simple key value store with an api to be able to drive the json payload.  At that point our swings get the current json configuration, change the default endpoint for the frontned, post it back, and issue a puppet command to the ha proxies via salt nodegroups and we're all good!
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4737</wp:post_id>
		<wp:post_date><![CDATA[2015-10-18 20:48:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-10-18 20:48:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dynamic-ha-reload-puppet]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="haproxy"><![CDATA[haproxy]]></category>
		<category domain="post_tag" nicename="ops"><![CDATA[ops]]></category>
		<category domain="post_tag" nicename="puppet"><![CDATA[puppet]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="post_tag" nicename="salt"><![CDATA[salt]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560772678;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4673;}i:2;a:1:{s:2:"id";i:2985;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Project angelhair: Building a queue on cassandra</title>
		<link>https://onoffswitch.net/2015/11/27/project-angelhair-building-queue-cassandra/</link>
		<pubDate>Fri, 27 Nov 2015 22:35:44 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4750</guid>
		<description></description>
		<content:encoded><![CDATA[
<em>Edit: this project has since been moved to CassieQ: <a href="https://github.com/paradoxical-io/cassieq" title="https://github.com/paradoxical-io/cassieq">https://github.com/paradoxical-io/cassieq</a></em>

A few weeks ago my work had a hack day and I got together with some of my coworker friends and we decided to build a queue on top of Cassandra.  

For the impatient, give it a try (<a href="https://hub.docker.com/r/onoffswitch/angelhair/" target="_blank" rel="noopener noreferrer">docker hub</a>): 

[code]
docker run -it \
    -e CLUSTER_NAME=&quot;&quot; \
    -e KEYSPACE=&quot;&quot; \
    -e CONTACT_POINTS=&quot;&quot; \
    -e USERNAME=&quot;&quot; \
    -e PASSWORD=&quot;&quot; \
    -e USE_SSL=&quot;&quot; \
    -e DATA_CENTER=&quot;&quot; \
    -e METRICS_GRAPHITE &quot;true&quot; \
    -e GRAPHITE_PREFIX=&quot; \
    -e GRAPHITE_URL=&quot;&quot;  \
    onoffswitch/angelhair
[/code]

The core features for what we called <a href="https://github.com/devshorts/angelhair" target="_blank" rel="noopener noreferrer">Project Angelhair</a> was to handle:

- long term events (so many events that AMQ or RMQ might run out of storage space)
- connectionless - wanted to use http 
- invisibility - need messages to disappear when they are processing but be able to come back 
- highly scaleable - wanted to distribute a docker container that just did all the work

Building a queue on cassandra isn't a trivial task and is rife with problems.  In fact, this is pretty well known and in general the consensus is <em>don't build a queue on Cassandra</em>.  

But why not?  There are a few reasons.  In general, the question you want to answer with a queue is "<em>what haven't I seen</em>". A simple way to do this is when a message is consumed to delete it.  However, with cassandra, deletes aren't immediate. They are tombstoned, so they will exist for the compaction period. This means even if you have only 1 message in your queue, cassandra has to scan all the old deleted messages before it finds it. With high load this can be a LOT of extra work. But thats not the only problem.  You have problems of how to distribute your messages across the ring. If you put all your messages for a queue into one partition key now you haven't evenly distributed your messages and have a skewed distribution of work. This is going to manifest in really poor performance.  

On top of all of that, cassandra has poor support for atomic transactions, so you can't easily say "let me get, process, and consume" in one atomic action.  Backing stores that are owned by a master (like sqlserver) let you do atomic actions much better since they have either have an elected leader who can manage this or are a single box. Cassandra isn't so lucky.

Given all the problems described, it may seem insane to build a queue on Cassandra. But cassandra is a great datastore that is massively horizontally scaleable. It also exists at a lot of organizations already. Being able to use a horizontally scaleable data store means you can ingest incredible amounts of messages.

<h1>How does angelhair work?</h1>

Angelhair works with 3 pointers into a queue.

A reader bucket pointer
A repair bucket pointer
An invisibility pointer

In order to scale and efficiently act as a queue we need to leverage cassandra partitioning capabilities. Queues are actually messages bucketized into a fixed size group called a bucket. Each message is assigned a monotonically increasing id that maps itself into a bucket.  For example, if the bucket is size 20 and you have id 21, that maps into bucket 1 (21/20). This is done using a table in cassandra whose only job is to provide monotonic values for a queue:

[code]
CREATE TABLE monoton (
  queuename text PRIMARY KEY,
  value bigint
);
[/code]

By bucketizing messages we can distribute messages across the cassandra clusters. 

Messages are always put into the bucket they correlate to, regardless if previous buckets are full.  This means that messages just keep getting put into the end, as fast as possible.

Given that messages are put into their corresponding bucket, the reader has a pointer to its active bucket (the reader bucket pointer) and scans the bucket for unacked visible messages. If the bucket is full it tombstones the bucket indicating that the bucket is closed for processing. If the bucket is NOT full, but all messages in the bucket are consumed (or being processed) AND the monotonic pointer has already advanced to the next bucket, the current bucket is also tombstoned. This means no more messages will ever show up in the current bucket... sort of

<h1>Repairing delayed writes</h1>

Without synchronizing reads and writes you can run into a situation  where you can have a delayed write. For example, assume you generate monotonic ids in this sequence:

[code]
Id 19
Id 20

Write 20 &lt;-- bucket advances to bucket 1 
             (assuming bucket size of 20) and 
             bucket 0 is tombstoned (closed)

Write 19 &lt;-- but message 19 writes into 
             bucket 0, even though 0 
             was tombstoned!
[/code]

In this scenario id 20 advances the monotonic bucket to bucket 1 (given buckets are size 20). That means the reader tombstones bucket 0. But what happens to message 19? We don't want to lose it, but as far as the reader is concerned it's moved onto bucket 1 and off of bucket 0.

This is where the concept of a repair worker comes into play. The repair worker's job is to slowly follow the reader and wait for tombstoned buckets. It has its own pointer (the repair bucket pointer) and polls to find when a bucket is tombstoned.  When a bucket is tombstoned the repair worker will wait for a configured timeout for out of order missing messages to appear. This means if a slightly delayed write occurs then the repair worker will actually pick it up and then republish it to the last active bucket.  We're gambling on probability here, the assumption is that if a message is going to be successfully written then it will be written within time T. That time is configurable when you create the queue.  

But there is also a scenario like this:

[code]
Id 19
Id 20

!!Write 19 ---&gt; This actually dies and fails to write!
Write 20

[/code]

In this scenario we claimed Id's 19 and 20, but 19 failed to write. Once 20 is consumed the reader tombstones the bucket and the repair worker kicks in.  But 19 isn't ever going to show up! In this case, the repair worker waits for the configured time and if after that time the message <em>isn't</em> written then we assume that that message is dead and will never be processed. Then the repair worker advances its pointer and moves on.

This means we don't necessarily guarantee FIFO, however we do (reasonably) guarantee messages will appear.  The repair worker never moves past a non completed bucket, though since its just a pointer we can always repair the repair worker by moving the pointer back.

<h1>Invisibility</h1>

Now the question comes up as how to deal with invisibility of messages. Invisible messages are important since with a conncectionless protocol (like http) we need to know if a message worker is dead and its message has to go back for processing. In queues like RMQ this is detected when a channel is disconnected (i.e. the connection is lost).  With http not so lucky.  

To track invisibility there is a separate pointer tracking the last invisible pointer. When a read comes in, we first check the invisibility pointer to see if that message is now visible.

If it is, we can return it. If not, get the next available message.  

If the current invisible pointer is already acked then we need to find the next invisible pointer. This next invisible pointer is the first non-acked non-visible message. If there isn't one in the current bucket, the invisibility pointer moves to the next bucket until it finds one or no messages exist, but never move past a message that hasn't been delivered before.  This way it won't accidentally skip a message that hasn't been sent out yet.

If however, there are two messages that get picked up at the same time the invis pointer is scanning through the invis pointer could choose the wrong id. In order to prevent this, we update the invis pointer to the destination if it's less than the current (i.e. we need to move back), or if its not then only update if the current reader owns the current invis pointer (doing an atomic update).

<h1>API</h1>

Angelhair has a simple API.  

- Put a message into a queue (and optionally specify an initial invisiblity)
- Get a message from a queue
- Ack the message using the message pop reciept (which is an encoded version and id metadata). The pop reciept is unique for each message dequeue. If a message comes back alive and is available for processing again it gets a new pop recipet. This also lets us identify a unique consumer of a message since the current atomic version of the message is encoded in the pop reciept. 

Doesn't get much easier than that!

<h1>Conclusion</h1>

There are a couple implementations of queues on cassandra out there that we found while researching this. One is from <a href="https://github.com/Netflix/astyanax/wiki/Message-Queue" target="_blank" rel="noopener noreferrer">netflix</a> but their implementation builds a lock system on top of cassandra and coordinates reads/writes using locking.  Some other implementations used wide rows (or CQL lists in a single row) to get around the tombstoning, but that limits the number of messages in your "queue" to 64k messages.

While we haven't tested angelhair in a stressed environment, we've decided to give it a go in some non critical areas in our internal tooling.  But so far we've had great success with it!  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4750</wp:post_id>
		<wp:post_date><![CDATA[2015-11-27 22:35:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-11-27 22:35:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[project-angelhair-building-queue-cassandra]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cassandra"><![CDATA[cassandra]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="docker"><![CDATA[docker]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="queue"><![CDATA[queue]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559648543;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4839;}i:1;a:1:{s:2:"id";i:4800;}i:2;a:1:{s:2:"id";i:4783;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>180</wp:comment_id>
			<wp:comment_author><![CDATA[Andrew Mills]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[ammills01@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[23.121.214.173]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-06-02 21:23:50]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-06-02 21:23:50]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Good post.  I'm looking at building a system that can ingest and store ~1 billion records daily and Cassandra seems to be the best answer.  Part of this system would work as a queue to process those records as they come in and send them to other systems.  With that, I've considered building a queue system with the Cassandra data similar to what you have outlined here.  Have you all moved out of the non-critical areas and started to stress test this?  In general, have you come across any roadblocks in the last several months?]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>181</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[23.23.174.237]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-06-02 22:41:48]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-06-02 22:41:48]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Andrew, CassieQ has moved out of the non-critical areas and we did some stress tests but not that many. Overall we found it to be really stable, and the biggest tuning to be done would be at the compaction layer in your cassandra cluster itself.  If you are interested in helping stress it let me know and I'd love to help with the results.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>180</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Plugin class loaders are hard</title>
		<link>https://onoffswitch.net/2015/12/12/class-loaders-hard/</link>
		<pubDate>Sat, 12 Dec 2015 01:31:34 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4764</guid>
		<description></description>
		<content:encoded><![CDATA[Plugin based systems are really common. Jenkins, Jira, wordpress, whatever.  Recently I built a plugin workflow for a system at work and have been mired in the joys of the class loader. For the uninitiated, a class in Java is identified uniquely by the class loader instance it is created from as well as its fully qualified class name. This means that <code>foo.bar</code> class loaded by class loader A is <em>not the same</em> as <code>foo.bar</code> class loaded by class loader B.  

There are actually some cool things you can do with this, especially in terms of code isolation. Imagine your plugins are bundled as shaded jars that contain all the internal dependencies. By leveraging class loaders you can isolate potentially conflicting versions of libraries from the host application and the plugin.  But, in order to communicate to the host layer, you need a strict set of shared interfaces that the host layer always owns.  When building the uber jar you exclude the host interfaces from being bundled (and all its transitive dependencies which in maven can be done by using scope <code>provided</code>). This means that they will always be loaded by the host.

In general, class loaders are heirarchical. They ask their parent if a class has been loaded, and if so returns that. In order to do plugins you need to invert that process. First look inside the uber-jar, and then if you can't find a class then look up. 

An example can be found <a href="http://tech.puredanger.com/2006/11/09/classloader/">here</a> and copied for the sake of internet completeness:

[java]
import java.net.URL;
import java.net.URLClassLoader;
import java.net.URLStreamHandlerFactory;
import java.util.UUID;

public class PostDelegationClassLoader extends URLClassLoader {

    private final UUID id = UUID.randomUUID();

    public PostDelegationClassLoader(URL[] urls, ClassLoader parent, URLStreamHandlerFactory factory) {
        super(urls, parent, factory);
    }

    public PostDelegationClassLoader(URL[] urls, ClassLoader parent) {
        super(urls, parent);
    }

    public PostDelegationClassLoader(URL[] urls) {
        super(urls);
    }

    public PostDelegationClassLoader() {
        super(new URL[0]);
    }

    @Override
    public Class&lt;?&gt; loadClass(String name) throws ClassNotFoundException {
        try (ThreadCurrentClassLoaderCapture capture = new ThreadCurrentClassLoaderCapture(this)) {
            Class loadedClass = findLoadedClass(name);

            // Nope, try to load it
            if (loadedClass == null) {
                try {
                    // Ignore parent delegation and just try to load locally
                    loadedClass = findClass(name);
                }
                catch (ClassNotFoundException e) {
                    // Swallow - does not exist locally
                }

                // If not found, just use the standard URLClassLoader (which follows normal parent delegation)
                if (loadedClass == null) {
                    // throws ClassNotFoundException if not found in delegation hierarchy at all
                    loadedClass = super.loadClass(name);
                }
            }
            return loadedClass;
        }
    }

    @Override
    public URL getResource(final String name) {
        final URL resource = findResource(name);

        if (resource != null) {
            return resource;
        }

        return super.getResource(name);
    }
}
[/java]

But this is just the tip of the fun iceberg. If all your libraries play nice then you may not notice anything. But I recently noticed using the apache xml-rpc library that I would get a SAXParserFactory class def not found exception, specifically bitching about instantiating the sax parser factory. I'm not the only one apparenlty, <a href="https://answers.atlassian.com/questions/104121/im-blocked-help-cannot-be-cast-to-javax.xml.parsers.saxparserfactory">here is a discussion</a> about a JIRA plugin that wasn't happy. After much code digging I found that the classloader being used was the one bound to the threads current context.

Why in the world is there a classloader bound to thread local? JavaWorld has a <a href="http://www.javaworld.com/article/2077344/core-java/find-a-way-out-of-the-classloader-maze.html:">nice blurb</a> about this 

<blockquote>Why do thread context classloaders exist in the first place? They were introduced in J2SE without much fanfare. A certain lack of proper guidance and documentation from Sun Microsystems likely explains why many developers find them confusing.

In truth, context classloaders provide a back door around the classloading delegation scheme also introduced in J2SE. Normally, all classloaders in a JVM are organized in a hierarchy such that every classloader (except for the primordial classloader that bootstraps the entire JVM) has a single parent. When asked to load a class, every compliant classloader is expected to delegate loading to its parent first and attempt to define the class only if the parent fails.

Sometimes this orderly arrangement does not work, usually when some JVM core code must dynamically load resources provided by application developers. Take JNDI for instance: its guts are implemented by bootstrap classes in rt.jar (starting with J2SE 1.3), but these core JNDI classes may load JNDI providers implemented by independent vendors and potentially deployed in the application's -classpath. This scenario calls for a parent classloader (the primordial one in this case) to load a class visible to one of its child classloaders (the system one, for example). Normal J2SE delegation does not work, and the workaround is to make the core JNDI classes use thread context loaders, thus effectively "tunneling" through the classloader hierarchy in the direction opposite to the proper delegation. </blockquote>

This means that whenever I'm delegating work to my plugins I need to be smart about capturing my custom plugin class loader and putting it on the current thread before execution. Otherwise if a misbehaving library accesses the thread classloader, it can now have access to the ambient root class loader and IFF the same class name exists in the host application it will load it. This could potentially conflict with other classes from the same package that aren't loaded this way and in general cause mayhem.

The solution here was a simple class modeled after .NET's disposable pattern using Java's try/finally auto closeable.

[java]
public class ThreadCurrentClassLoaderCapture implements AutoCloseable {
    final ClassLoader originalClassLoader;

    public ThreadCurrentClassLoaderCapture(final ClassLoader newClassLoader) {
        originalClassLoader = Thread.currentThread().getContextClassLoader();

        Thread.currentThread().setContextClassLoader(newClassLoader);
    }

    @Override
    public void close() {
        Thread.currentThread().setContextClassLoader(originalClassLoader);
    }
}
[/java]

Which is used before each and every invocation into the interface of the plugin (where <code>connection</code> is the plugin reference)

[java]
@Override
public void start() throws Exception {
    captureClassLoader(connection::start);
}

@Override
public void stop() throws Exception {
    captureClassLoader(connection::stop);
}

@Override
public void heartbeat() throws Exception {
    captureClassLoader(connection::heartbeat);
}

private void captureClassLoader(ExceptionRunnable runner) throws Exception {
    try (ThreadCurrentClassLoaderCapture capture = new ThreadCurrentClassLoaderCapture(connection.getClass().getClassLoader())) {
        runner.run();
    }
}
[/java]

However, this isn't the only issue.  Imagine a scenario where you support both class path loaded plugins AND remote loaded plugins (via shaded uber-jar). And lets pretend that on the classpath is a jar with the same namespaces and classes as that in an uberjar. To be more succinct, you have a delay loaded shared library on the class path, and a version of that library that is shaded loaded via the plugin mechanism.

Technically there shouldn't be any issues here.  The class path plugin gets all its classes resolved from the root scope. The plugin gets its classes (of the same name) from the delegated provider.  Both use the same shared set of interfaces of the host. The issue arrises if you have a library like reflectasm, which dynamically emits bytecode at runtime.  

Look at this code:

[java]
AccessClassLoader loader = AccessClassLoader.get(type);
synchronized (loader) {
	try {
		accessClass = loader.loadClass(accessClassName);
	} catch (ClassNotFoundException ignored) {
		String accessClassNameInternal = accessClassName.replace('.', '/');
		String classNameInternal = className.replace('.', '/');
		ClassWriter cw = new ClassWriter(ClassWriter.COMPUTE_MAXS);
[/java]

Which is a snippet from reflectasm as its generating a runtime byte code emitter that can access fields for you.  It creates a class name like <code>your.class.nameMethodAccess</code>. If the class name isn't found, it generates the bytecode and then writes it into the owning classes class loader.

In the scenario of a plugin using this library, it will check the loader and see that the plugin classloader AND rootscope loader do not have the emitted class name, and so a class not found exception is thrown. It will then write the class into the <em>target types class loader</em>. This would be the delegated loader, and provides the isolation we want.

However, if the class path plugin (what I call an embedded plugin) runs this code, the dynamic runtime class is written into the <em>root scope</em> loader.  This means that all delegating class loaders will eventually find this type since they always do a delegated pass to the root!

The important thing to note here is that using a delegated loader does not mean every class that comes out of it is tied to the delegated loader. Only classes that are found inside of the delegated loader are bound to it. If a class is resolved by the parent, the class is linked to the parent. 

In this scenario with the root class loader being polluted with the same class name, I don't think there is much you can do other than avoid it.

Anyways, maybe I should have used OSGi...?]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4764</wp:post_id>
		<wp:post_date><![CDATA[2015-12-12 01:31:34]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2015-12-12 01:31:34]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[class-loaders-hard]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="asm"><![CDATA[asm]]></category>
		<category domain="post_tag" nicename="classloader"><![CDATA[classloader]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="osgi"><![CDATA[osgi]]></category>
		<category domain="post_tag" nicename="plugin"><![CDATA[plugin]]></category>
		<category domain="post_tag" nicename="runtime"><![CDATA[runtime]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561588999;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4844;}i:1;a:1:{s:2:"id";i:4306;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>182</wp:comment_id>
			<wp:comment_author><![CDATA[Nick]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[heitzsub@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[77.96.75.41]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2015-12-14 07:55:04]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2015-12-14 07:55:04]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[I too have had this tremendously painful experience in the past. Very good job writing this up. 
In my own experience, I found that I could *only* take the approach of treating plugins as fully composed shaded jars...delegating upwards for libraries such as spring and ibatis never seemed to work, despite the thread context trick...this those libraries had to new included below the plugin. Oddly, libraries like log4j and jndi had the opposite problem,and always had to be delegated up. And never, ever try to write your own hash code implementation for your custom classloader,or risk screwing up annotation caching. ..

In the end, it was almost more trouble than it was worth, and osgi seemed like it may have been the better choice.  Good luck!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Dalloc - coordinating resource distribution using hazelcast</title>
		<link>https://onoffswitch.net/2016/01/22/coordinating-resource-distribution-hazelcast/</link>
		<pubDate>Fri, 22 Jan 2016 01:54:44 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4783</guid>
		<description></description>
		<content:encoded><![CDATA[A fun problem that has come up during the implementation of <a href="https://github.com/paradoxical-io/cassieq">cassieq</a> (a distributed queue based on cassandra) is how to evenly distribute resources across a group of machines.  There is a scenario in cassieq where writes can be delayed, and as such there is a custom worker in the app (by queue) who watches a queue to see if a delayed write comes in and republishes the message to a bucket later on. It's transparent to the user, but if we have multiple workers on the same queue we could potentially republish the message twice.  While technically that falls within the SLA we've set for cassieq (at least once delivery) it'd be nice to avoid this particular race condition.

To solve this, I've clustered the cassieq instances together using <a href="http://hazelcast.org/">hazelcast</a>.  Hazelcast is a pretty cool library since it abstracts away member discovery/connection and gives you events on membership changes to make it easy for you to build distributed data grids. It also has a lot of great primitives that are useful in building distributed workflows. Using hazelcast, I've built a simple resource distributor that uses shared distributed locks and a master set of allocations across cluster members to coordinate who can "grab" which resource.

For the impatient you can get <a href="https://github.com/paradoxical-io/dalloc">dalloc</a> from 

[java]
&lt;dependency&gt;
    &lt;groupId&gt;io.paradoxical&lt;/groupId&gt;
    &lt;artifactId&gt;dalloc&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;
[/java]

The general idea in dalloc is that each node creates a resource allocator who is bound to a resource group name (like "Queues").  Each node supplies a function to the allocator that generates the master set of resources to use, and a callback for when resources are allocated.  The callback is so you can wire in async events and when allocations need to be rebalanced outside of a manual invocation (like cluster member/join).

The entire resource allocation library API deals with abstractions on what a resource is, and lets the client map their internal resource into a <code>ResourceIdentity</code>. For cassieq, it's a queue id.

When an allocation is triggered (either manually or via a member join/leave event) the following occurs:

<ul>
<li>Try and acquire a shared lock for a finite period of time</li>
<li>If you acquired the lock, acquire a map of what has been allocated to everyone else and compare what is available from your master set to what is available</li>
<li>Given the size of the current cluster, determine how many resources you are allowed to claim (by even distribution). If you don't have your entire set claimed, take as many as you can to fill up. If you have too many claimed, give some resources up</li>
<li>Persist your changes to the master state map</li>
<li>Dispatch to your callback what the new set of resources should be</li>
</ul>

Hazelcast supports distributed maps, where part of the map is sharded by its map key on different nodes. However, I'm actually explicitly NOT distributing the map across the cluster. I've put ownership of the resource set on "one" node (but the map is replicated so if that node goes down the map still exists).  This is because each node is going to have to try and do a claim. If each node claims, and then calls to every other node, thats n^2 IO operations. Compare that to every node making N operations.

The library also supports bypassing this mechanism and instead supports a much more "low-tech" solution of manual allocation. All this means is that you pre-define how many nodes there should be, and which node number a node is. Then each node sorts the input data and grabs a specific slice out of the input set based on its id. It doesn't give any guarantees to non-overlap, but it does give you an 80% solution to a hard problem.

<a href="https://twitter.com/jakeswenson">Jake</a>, the other <a href="http://paradoxical.io/">paradoxical</a> member suggested that there could be a nice alternative solution using a similar broadcast style of quorum using paxos. Each node broadcasts what it's claiming and the nodes agree on who is allowed to do what.  I probably wouldn't use hazelcast for that, though the primitives of paxos (talking to all members of a cluster) are there and it'd be interesting to build paxos on top of hazelcast now that I think about it...

Anyways, abstracting distributed resource allocation is nice, because as we make improvements to how we want to tune the allocation algorithms all dependent services get it for free. And free stuff is my favorite.

]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4783</wp:post_id>
		<wp:post_date><![CDATA[2016-01-22 01:54:44]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-22 01:54:44]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[coordinating-resource-distribution-hazelcast]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dalloc"><![CDATA[dalloc]]></category>
		<category domain="post_tag" nicename="distributed"><![CDATA[distributed]]></category>
		<category domain="post_tag" nicename="hazelcast"><![CDATA[hazelcast]]></category>
		<category domain="post_tag" nicename="paradoxical"><![CDATA[paradoxical]]></category>
		<category domain="post_tag" nicename="paxos"><![CDATA[paxos]]></category>
		<category domain="post_tag" nicename="quorum"><![CDATA[quorum]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554534280;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4839;}i:1;a:1:{s:2:"id";i:4750;}i:2;a:1:{s:2:"id";i:4800;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Leadership election with cassandra</title>
		<link>https://onoffswitch.net/2016/01/21/leadership-election-cassandra/</link>
		<pubDate>Thu, 21 Jan 2016 22:56:55 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4784</guid>
		<description></description>
		<content:encoded><![CDATA[Cassandra has a neat feature that lets you expire data in a column. Using this handy little feature, you can create simple leadership election using cassandra.  The whole process is <a href="http://www.datastax.com/dev/blog/consensus-on-cassandra">described here</a> which talks about leveraging Cassandras consensus and the column expiration to create leadership electors.

The idea is that a user will try and claim a slot for a period of time in a leadership table. If a slot is full, someone else has leadership.  While the leader is still active they needs to heartbeat the table faster than the columns TTL to act as a keepalive.  If it fails to heartbeat (i.e. it died) then its leadership claim can be relinquished and someone else can claim it.  Unlike most leadership algorithms that claim a single "host" as a leader, I needed a way to create leaders sharded by some "group". I call this a  "LeadershipGroup" and we can leverage the expiring columns in cassandra to do this!

To make this easier, I've wrapped this algorithm in a java library available <a href="https://github.com/paradoxical-io/cassandra.leadership">from paradoxical</a>. For the impatient

[java]
&lt;dependency&gt;
    &lt;groupId&gt;io.paradoxical&lt;/groupId&gt;
    &lt;artifactId&gt;cassandra-leadership&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/dependency&gt;
[/java]

The gist here is that you need to provide a schema similar to 

[code]
CREATE TABLE leadership_election (
    group text PRIMARY KEY,
    leader_id text
);
[/code]

Though the actual column names can be custom defined.  You can define a leadership election factory using Guice like so

[java]
public class LeadershipModule extends AbstractModule {
    @Override
    protected void configure() {
        bind(LeadershipSchema.class).toInstance(LeadershipSchema.Default);

        bind(LeadershipStatus.class).to(LeadershipStatusImpl.class);

        bind(LeadershipElectionFactory.class).to(CassandraLeadershipElectionFactory.class);
    }
}
[/java]

<ul>
<li><code>LeadershipStatus</code> is a class that lets you query who is leader for what "group". For example, you can have multiple workers competing for leadership of a certain resource.  </li>
<li><code>LeadershipSchema</code> is a class that defines what the column names in your schema are named. By default if you use the sample table above, the Default schema maps to that</li>
<li><code>LeadershipElectionFactory</code> is a class that gives you instances of LeadershipElection classes, and I've provided a cassandra leadership factory</li>
</ul>

Once we have a leader election we can try and claim leadership:

[java]
final LeadershipElectionFactory factory = new CassandraLeadershipElectionFactory(session);

// create an election processor for a group id
final LeadershipElection leadership = factory.create(LeadershipGroup.random());

final LeaderIdentity user1 = LeaderIdentity.valueOf(&quot;user1&quot;);

final LeaderIdentity user2 = LeaderIdentity.valueOf(&quot;user2&quot;);

assertThat(leadership.tryClaimLeader(user1, Duration.ofSeconds(2))).isPresent();

Thread.sleep(Duration.ofSeconds(3).toMillis());

assertThat(leadership.tryClaimLeader(user2, Duration.ofSeconds(3))).isPresent();
[/java]

When you claim leadership you claim it for a period of time and if you get it you get a leadership token that you can heartbeat on. And now you have leadership!

As usual, full source available at my <a href="https://github.com/paradoxical-io/cassandra.leadership">github</a>

 ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4784</wp:post_id>
		<wp:post_date><![CDATA[2016-01-21 22:56:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-21 22:56:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[leadership-election-cassandra]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cassandra"><![CDATA[cassandra]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="leadership"><![CDATA[leadership]]></category>
		<category domain="post_tag" nicename="paradoxical"><![CDATA[paradoxical]]></category>
		<category domain="post_tag" nicename="quorum"><![CDATA[quorum]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560857090;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4800;}i:1;a:1:{s:2:"id";i:4750;}i:2;a:1:{s:2:"id";i:4783;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Cassandra DB migrations</title>
		<link>https://onoffswitch.net/2016/01/23/cassandra-db-migrations/</link>
		<pubDate>Sat, 23 Jan 2016 23:03:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4800</guid>
		<description></description>
		<content:encoded><![CDATA[When doing any application that involves a persistent data storage you usually need a way to upgrade and change your database using a set of scripts.  Working with patterns like ActiveRecord you get easy up/down by version migrations. But with cassandra, which traditionally was schemaless, there aren't that many tools out there to do this.

One thing we have been using at my work and at paradoxical is a simple java based <a href="https://github.com/paradoxical-io/cassandra-loader">cassandra loader tool</a> that does "up" migrations based on db version scripts.  

Assuming you have a folder in your application that stores db scripts like

[code]
db/scripts/01_init.cql
db/scripts/02_add_thing.cql
..
db/sripts/10_migrate_users.cql
..
[/code]

Then each script corresponds to a particular db version state. It's current state depends on all previous states. Our cassandra loader tracks db versions in a <code>db_version</code> table and lets you apply runners against a keyspace to move your schema (and data) to the target version. If your db is already at a version it does nothing, or if your db is a few versions back the runner will only run the required versions to get you to latest (or to the version number you want).

Taking this one step further, when working at least in Java we have the luxury of using <a href="https://github.com/jsevellec/cassandra-unit">cassandra-unit</a> to actually run an embedded cassandra instance available for unit or integration tests.  This way you don't need to mock out your database, you actually run all your db calls through the embedded cassandra. We use this heavily in <a href="https://github.com/paradoxical-io/cassieq">cassieq</a> (a distributed queue based on cassandra).

One thing our cassandra loader can do is be run in library mode, where you give it the same set of db scripts and you can build a fresh db for your integration tests:

[java]
public static Session create() throws Exception {
    return CqlUnitDb.create(&quot;../db/scripts&quot;);
}
[/java]

Running the loader in standalone mode (by downloading the <code>runner</code> <a href="https://repo1.maven.org/maven2/io/paradoxical/cassandra.loader/1.1" target="_blank" rel="noopener noreferrer">maven classifier</a>) lets you run the migration runner in your console:

[code]
&gt; java -jar cassandra.loader-runner.jar

Unexpected exception:Missing required options: ip, u, pw, k
usage: Main
 -f,--file-path &lt;arg&gt;         CQL File Path (default =
                              ../db/src/main/resources)
 -ip &lt;arg&gt;                    Cassandra IP Address
 -k,--keyspace &lt;arg&gt;          Cassandra Keyspace
 -p,--port &lt;arg&gt;              Cassandra Port (default = 9042)
 -pw,--password &lt;arg&gt;         Cassandra Password
 -recreateDatabase            Deletes all tables. WARNING all
                              data will be deleted! 
 -u,--username &lt;arg&gt;          Cassandra Username
 -v,--upgrade-version &lt;arg&gt;   Upgrade to Version
[/code]

The advantage to unifying all of this is that you can test your db scripts in isolation and be confident that they work!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4800</wp:post_id>
		<wp:post_date><![CDATA[2016-01-23 23:03:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-23 23:03:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[cassandra-db-migrations]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cassandra"><![CDATA[cassandra]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="migration"><![CDATA[migration]]></category>
		<category domain="post_tag" nicename="paradoxical"><![CDATA[paradoxical]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554980380;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4750;}i:1;a:1:{s:2:"id";i:4784;}i:2;a:1:{s:2:"id";i:3452;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Serialization of lombok value types with jackson</title>
		<link>https://onoffswitch.net/2016/01/26/serialization-lombok-types-jackson/</link>
		<pubDate>Tue, 26 Jan 2016 23:09:33 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4803</guid>
		<description></description>
		<content:encoded><![CDATA[For anyone who uses lombok with jackson, you should checkout <a href="https://github.com/paradoxical-io/jackson-lombok">jackson-lombok</a> which is a fork from <a href="https://github.com/xebia/jackson-lombok">xebia</a> that allows lombok value types (and lombok generated constructors) to be json creators.

The original authors compiled their version against jackson-core 2.4.* but the new version uses 2.6.*. Props needs to go to github <a href="https://github.com/kazuki-ma">user kazuki-ma</a> for submitting a PR that actually addresses this. Paradoxical just took those fixes and published.

Anyways, now you get the niceties of being able to do:

[java]
@Value
public class ValueType{
    @JsonProperty
    private String name;
    
    @JsonProperty
    private String description;
}
[/java]

And instantiate your mapper:

[java]
new ObjectMapper().setAnnotationIntrospector(new JacksonLombokAnnotationIntrospector());
[/java]

Enjoy!
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4803</wp:post_id>
		<wp:post_date><![CDATA[2016-01-26 23:09:33]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-26 23:09:33]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[serialization-lombok-types-jackson]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="jackson"><![CDATA[jackson]]></category>
		<category domain="post_tag" nicename="lombok"><![CDATA[lombok]]></category>
		<category domain="post_tag" nicename="value"><![CDATA[value]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561483126;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4456;}i:1;a:1:{s:2:"id";i:4919;}i:2;a:1:{s:2:"id";i:4862;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Logging the easy way</title>
		<link>https://onoffswitch.net/2016/01/28/logging-easy/</link>
		<pubDate>Thu, 28 Jan 2016 21:23:02 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4805</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>This is a cross post from <a href="http://engineering.godaddy.com/logging-the-easy-way/">the original posting at godaddy's engineering blog</a>. This is a project I have spent considerable time working on and leverage a lot.</blockquote>

Logging is a funny thing. Everyone knows what logs are and everyone knows you should log, but there are no hard and fast rules on how to log or what to log. Your logs are your first line of defense against figuring out issues live. Sometimes logs are the only line of defense (especially in time sensitive systems).

That said, in any application good logging is critical. Debugging an issue can be made ten times easier with simple, consistent logging. Inconsistent or poor logging can actually make it impossible to figure out what went wrong in certain situations. Here at GoDaddy we want to make sure that we encourage logging that is consistent, informative, and easy to search.

Enter the GoDaddy Logger. This is a SLF4J wrapper library that encourages us to fall into the pit of success when dealing with our logging formats and styles in a few ways:

<ul>
<li>Frees you from having to think about what context fields need to be logged and removes any worries about forgetting to log a value,</li>
<li>Provides the ability to skip personal identifiable information from being logged,</li>
<li>Abstracts out the actual format of the logs from the production of them. By decoupling the output of the framework from the log statements themselves, you can easily swap out the formatter when you want to change the structure and all of your logging statements will be consistently logged using the new format.</li>
</ul>

A lot of teams at GoDaddy use ELK (Elasticsearch, Logstash, Kibana) to search logs in a distributed system. By combining consistent logging with ELK (or Splunk or some other solution), it becomes relatively straight forward for developers to correlate and locate related events in their distributed systems.

<h1>THE GODADDY LOGGER</h1>

In an effort to make doing the right thing the easy thing, our team set out to build an extra layer on top of SLF4J – The GoDaddy Logger. While SLF4J is meant to abstract logging libraries and gives you a basic logging interface, our goal was to extend that interface to provide for consistent logging formats. One of the most important things for us was that we wanted to provide an easy way to log objects rather than having to use string formatting everywhere.

<h1>CAPTURING THE CONTEXT</h1>

One of the first things we did was expose what we call the ‘with’ syntax. The ‘with’ syntax builds a formatted key value pair, which by default is “key=value;”, and allows logging statements to be more human readable. For example:

[java]
logger.with(“first-name”, “GoDaddy”)
     .with(“last-name”, “Developers!”)
     .info(“Logging is fun”);
[/java]

Using the default logging formatter this log statement outputs:

Logging is fun; first-name=“GoDaddy”; last-name=”Developers!”.
We can build on this to support deep object logging as well. A good example is to log the entire object from an incoming request. Instead of relying on the .toString() of the object to be its loggable representation, we can crawl the object using reflectasm and format it globally and consistently. Let’s look at an example of how a full object is logged.

[java]
Logger logger = LoggerFactory.getLogger(LoggerTest.class);
Car car = new Car(“911”, 2015, “Porsche”, 70000.00, Country.GERMANY, new Engine(“V12”));
logger.with(car).info(“Logging Car”);
[/java]

Like the initial string ‘with’ example, the above log line produces:

[java]
14:31:03.943 [main] INFO com.godaddy.logger.LoggerTest – Logging Car; cost=70000.0; country=GERMANY; engine.name=”V12”; make=”Porsche”; model=”911”; year=2015
[/java]

All of the car objects info is cleanly logged in a consistent way. We can easily search for a model property in our logs and we won’t be at the whim of spelling errors of forgetful developers. You can also see that our logger nests object properties in dot object notation like “engine.name=”V12””. To accomplish the same behavior using SLF4J, we would need to do something akin to the following:

Use the Car’s toString functionality:

Implement the Car object’s toString function:

[java]
String toString() {
     Return “cost=” + cost + “; country=” + country + “; engine.name=” + (engine == null ? “null” : engine.getName()) … etc.
}
[/java]

Log the car via it’s toString() function:

[java]
logger.info(“Logging Car; {}”, car.toString());
[/java]

Use String formatting

[java]
logger.info(&quot;Logging Car; cost={}; country={};e.name=\&quot;{}\&quot;; make=\&quot;{}\&quot;; model=\&quot;{}\&quot;; &quot; + &quot;year={}; test=\&quot;{}\&quot;&quot;, car.getCost(), car.getCountry(), car.getEngine() == null ? null : car.getEngine().getName(), car.getMake(), car.getModel(), car.getYear());
[/java]

Our logger combats these unfortunate scenarios and many others by allowing you to set the recursive logging level, which defines the amount of levels deep into a nested object you want to have logged and takes into account object cycles so there isn’t infinite recursion.

<h1>SKIPPING SENSITIVE INFORMATION</h1>

The GoDaddy Logger provides annotation based logging scope support giving you the ability to prevent fields/methods from being logged with the use of annotations. If you don’t want to skip the entity completely, but would rather provide a hashed value, you can use an injectable hash processor to hash the values that are to be logged. Hashing a value can be useful since you may want to log a piece of data consistently but you may not want to log the actual data value. For example:

[java]
import lombok.Data;
  @Data
 public class AnnotatedObject {
     private String notAnnotated;
       @LoggingScope(scope = Scope.SKIP)
     private String annotatedLogSkip;
     public String getNotAnnotatedMethod() {
          return &quot;Not Annotated&quot;;
     }  
     @LoggingScope(scope = Scope.SKIP)
     public String getAnnotatedLogSkipMethod() {
          return &quot;Annotated&quot;;
     }
       @LoggingScope(scope = Scope.HASH)
     public String getCreditCardNumber() {
          return &quot;1234-5678-9123-4567&quot;;
     }
}
[/java]

If we were to log this object:

[java]
AnnotatedObject annotatedObject = new AnnotatedObject();
annotatedObject.setAnnotatedLogSkip(“SKIP ME”);
annotatedObject.setNotAnnotated(“NOT ANNOTATED”); 
logger.with(annotatedObject).info(“Annotation Logging”);
[/java]

The following would be output to the logs:

[java]
09:43:13.306 [main] INFO com.godaddy.logging.LoggerTest – Annotating Logging; creditCardNumber=”5d4e923fe014cb34f4c7ed17b82d6c58; notAnnotated=”NOT ANNOTATED”; notAnnotatedMethod=”Not Annotated”
[/java]

Notice that the annotatedLogSkip value of “SKIP ME” is not logged. You can also see that the credit card number has been hashed. The GoDaddy Logger uses Guava’s MD5 hashing algorithm by default which is not cryptographically secure, but definitely fast. And you’re able to provide your own hashing algorithm when configuring the logger.

<h1>LOGGING CONTEXT</h1>

One of the more powerful things of the logger is that the ‘with’ syntax returns a new immutable captured logger. This means you can do something like this:

[java]
Logger contextLogger = logger.with(“request-id”, 123);
contextLogger.info(“enter”); 
// .. Do Work 
contextLogger.info(“exist”);
[/java]

All logs generated off the captured logger will include the captured with statements. This lets you factor out common logging statements and cleans up your logs so you see what you really care about (and make less mistakes).

<h1>CONCLUSION</h1>

With consistent logging we can easily search through our logs and debug complicated issues with confidence. As an added bonus, since our log formatting is centralized and abstracted, we can also make team-wide or company-wide formatting shifts without impacting developers or existing code bases.

Logging is hard. There is a fine line between logging too much and too little. Logging is also best done while you write code vs. as an afterthought. We’ve really enjoyed using the GoDaddy Logger and it’s really made logging into a simple and unobtrusive task. We hope you take a look and if you find it useful for yourself or your team let us know!

For more information about the GoDaddy Logger, check out the <a href="https://github.com/godaddy/godaddy-logger">GitHub project</a>, or if you’re interested in working on these and other fun problems with us, check out our jobs page.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4805</wp:post_id>
		<wp:post_date><![CDATA[2016-01-28 21:23:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-01-28 21:23:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[logging-easy]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="godaddy"><![CDATA[godaddy]]></category>
		<category domain="post_tag" nicename="logging"><![CDATA[logging]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560268662;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4586;}i:1;a:1:{s:2:"id";i:4945;}i:2;a:1:{s:2:"id";i:4627;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>RMQ failures from suspended VMs</title>
		<link>https://onoffswitch.net/2016/02/04/rmq-failures-suspended-vms/</link>
		<pubDate>Thu, 04 Feb 2016 21:18:29 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4810</guid>
		<description></description>
		<content:encoded><![CDATA[My team recently ran into a bizarre RMQ partition failure in a production cluster. RMQ doesn't handle partition failures well, and while you can set up auto recovery (such as suspension of minority groups) you need to manually recover from it.  The one time I've encountered this I got a very useful message in the admin managment page indicating that parts of the cluster were in partition failure, but this time things went weird.

Symptoms:

<ul>
<li>Could not gracefully restart rmq using <code>rabbitmqctl stop_app/start_app</code>.  The commands would stall</li>
<li>Could not list queues for any vhost. <code>rabbitmqctl list_queues -p [vhost]</code> would stall</li> 
<li>Logs showed partition failure</li>
<li>People could not consistently log into the admin api without stalls, or other strange issues even when clearing browsing data/local storage/incognito/different browsers</li>
<li>Rebooting the master did not help</li>
</ul>

In the end the solution was to do an NTP time sync, turn off all clustered slaves (shut down their VM's, not go into suspension). Once that occurred, the master was rebooted and then it stabilized.  After that we brought up each slave one by one until it went green.

Anyways, figured I'd share the symptoms and the solution in case anyone else runs into it.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4810</wp:post_id>
		<wp:post_date><![CDATA[2016-02-04 21:18:29]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-02-04 21:18:29]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[rmq-failures-suspended-vms]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="failure"><![CDATA[failure]]></category>
		<category domain="post_tag" nicename="partition"><![CDATA[partition]]></category>
		<category domain="post_tag" nicename="rmq"><![CDATA[rmq]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1546394009;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2635;}i:1;a:1:{s:2:"id";i:4945;}i:2;a:1:{s:2:"id";i:4737;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>A toy generational garbage collector</title>
		<link>https://onoffswitch.net/2016/02/20/toy-generational-garbage-collector/</link>
		<pubDate>Sat, 20 Feb 2016 20:55:03 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4816</guid>
		<description></description>
		<content:encoded><![CDATA[Had a little downtime today and figured I'd make a toy generational garbage collector, for funsies.  A friend of mine was once asked this as an interview question so I thought it might make for some good weekend practice.

For those not familiar, a common way of doing garbage collection in managed languages is to have the concept of multiple generations.  All newly created objects go in gen0. New objects are also the most probably to be destroyed, as there is a lot of transient stuff that goes in an application.  If an element survives a gc round it gets promoted to gen1.  Gen1 doesn't get GC'd as often. Same with gen2.  

A GC cycle usually consists of iterating through application root nodes (so starting at main and traversing down) and checking to see where a reference lays in which generation. If we're doing a gen1 collection, we'll also do gen0 and gen1. However, if you're doing gen0 only and a node is already laying in gen1, you can just bail early and say "<em>meh, this node and all its references are probably ok for now, we'll try this later</em>".

For a really great visualization checkout this <a href="http://blogs.msdn.com/b/abhinaba/archive/2009/03/02/back-to-basics-generational-garbage-collection.aspx">msdn article</a> on generation garabage collection.

And now on to the code!  First lets start with what is an object

[java]
@Data
@EqualsAndHashCode(of = &quot;id&quot;)
public class Node {
    private final String id;

    public Node(String id) {
        this.id = id;
    }

    private final List&lt;Node&gt; references = new ArrayList&lt;&gt;();

    public void addReference(Node node) {
        references.add(node);
    }

    public void removeReference(Node node) {
        references.removeIf(i -&gt; i.getId().equals(node.getId()));
    }
}
[/java]

For the purposes of the toy, its just some node with some unique id.

Lets also define an enum of the different generations we'll support and their ordinal values 

[java]
public enum Mode {
    Gen0,
    Gen1
}
[/java]

Next, lets make an allocator who can allocate new nodes. This would be like a <code>new</code> syntax behind the scenes

[java]
public class Allocator {
    @Getter
    private Set&lt;Node&gt; gen0 = new HashSet&lt;&gt;();

    @Getter
    private Set&lt;Node&gt; gen1 = new HashSet&lt;&gt;();

    public Node newNode() {
        return newNode(&quot;&quot;);
    }

    public Node newNode(String tag) {
        final Node node = new Node(tag + UUID.randomUUID());

        getGen0().add(node);

        return node;
    }

    public Mode locateNode(Node tag) {
        if (gen1.contains(tag)) {
            return Mode.Gen1;
        }

        return Mode.Gen0;
    }
    ....
[/java]

At this point we can allocate a new node, and assign nodes references.

[java]
final Allocator allocator = new Allocator();

final Node root = allocator.newNode();

root.addReference(allocator.newNode());
root.addReference(allocator.newNode());
root.addReference(allocator.newNode());
[/java]

Still haven't actually collected anything though yet. So lets write a garbage collector

[java]
public class Gc {
    private final Allocator allocator;

    public Gc(Allocator allocator) {
        this.allocator = allocator;
    }

    public void collect(Node root, Mode mode) {
        final Allocator.Marker marker = allocator.markBuilder(mode);

        mark(root, marker, mode);

        marker.sweep();
    }

    private void mark(Node root, Allocator.Marker marker, Mode mode) {
        final Mode found = allocator.locateNode(root);

        if (found.ordinal() &gt; mode.ordinal()) {
            return;
        }

        marker.mark(root);

        root.getReferences().forEach(ref -&gt; mark(ref, marker, mode));
    }
}
[/java]

The GC dos a DFS on the root object reference and marks all visible nodes with some marker builder (yet to be shown).  If the generational heap that the node lives in is less than or equal to the mode we are on, we'll mark it, otherwise just skip it. This works because later we'll only prune from generation heaps according to the mode

Now comes the fun part, and its the marker

[java]
public static class Marker {

    private final Set&lt;String&gt; marks;
    private final Allocator allocator;

    private final Mode mode;

    public Marker(Allocator allocator, final Mode mode) {
        this.allocator = allocator;
        this.mode = mode;
        marks = new HashSet&lt;&gt;();
    }

    public void mark(Node node) {
        marks.add(node.getId());
    }

    public void sweep() {
        Predicate&lt;Node&gt; remove = node -&gt; !marks.contains(node.getId());

        allocator.getGen0().removeIf(remove);

        allocator.promote(Mode.Gen0);

        switch (mode) {
            case Gen0:
                break;
            case Gen1:
                allocator.getGen1().removeIf(remove);
                allocator.promote(Mode.Gen1);
                break;
        }
    }
}
[/java]

All we do here is when we mark, tag the node in a set. When we go to sweep, go through the generations less than or equal to the current and remove unmarked nodes, as well as promote the surviving nodes to the next heap!

Still missing two last functions in the allocator which are promote and the marker builder

[java]
public Marker markBuilder(final Mode mode) {
    return new Marker(this, mode);
}

private void promote(final Mode mode) {
    switch (mode) {
        case Gen0:
            gen1.addAll(gen0);
            gen0.clear();
            break;
        case Gen1:
            break;
    }
}
[/java]

Now we can put it all together and write some tests:

Below you can see the promotion in action.  
[java]
final Allocator allocator = new Allocator();

final Gc gc = new Gc(allocator);

final Node root = allocator.newNode();

root.addReference(allocator.newNode());
root.addReference(allocator.newNode());
root.addReference(allocator.newNode());

final Node removable = allocator.newNode(&quot;remove&quot;);

removable.addReference(allocator.newNode(&quot;dangle1&quot;));
removable.addReference(allocator.newNode(&quot;dangle2&quot;));

root.addReference(removable);

assertThat(allocator.getGen0().size()).isEqualTo(7);

gc.collect(root, Mode.Gen0);

assertThat(allocator.getGen0().size()).isEqualTo(0);
assertThat(allocator.getGen1().size()).isEqualTo(7);
[/java]

Nothing can be collected since all nodes have references, but we've cleared the gen0 and moved all nodes to gen1

[java]
root.removeReference(removable);

gc.collect(root, Mode.Gen1);

assertThat(allocator.getGen0().size()).isEqualTo(0);
assertThat(allocator.getGen1().size()).isEqualTo(4);
[/java]

Now we can actually remove the reference and do a gen1 collection. You can see now that the gen1 heap size went down by 3 (so the removable node, plus its two children) since those nodes are no longer reachable

And just for fun, lets show that gen1 collection works as well

[java]
final Node gen1Remove = allocator.newNode();

root.addReference(gen1Remove);

gc.collect(root, Mode.Gen1);

assertThat(allocator.getGen0().size()).isEqualTo(0);
assertThat(allocator.getGen1().size()).isEqualTo(5);

root.removeReference(gen1Remove);

gc.collect(root, Mode.Gen1);

assertThat(allocator.getGen0().size()).isEqualTo(0);
assertThat(allocator.getGen1().size()).isEqualTo(4);
[/java]

And there you have it, a toy generational garbage collector :)

For the full code, check out this <a href="https://gist.github.com/devshorts/1d8d422790c6fc509760">gist</a>
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4816</wp:post_id>
		<wp:post_date><![CDATA[2016-02-20 20:55:03]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-02-20 20:55:03]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[toy-generational-garbage-collector]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="gc"><![CDATA[gc]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="toy"><![CDATA[toy]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561521171;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3435;}i:1;a:1:{s:2:"id";i:3452;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>183</wp:comment_id>
			<wp:comment_author><![CDATA[Shai Almog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[shai@codenameone.com]]></wp:comment_author_email>
			<wp:comment_author_url>https://www.codenameone.com/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[37.142.245.53]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-21 04:29:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-21 04:29:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[That's a very interesting way to explain GC's.

We built a concurrent GC for Codename One but it isn't generational as that would be quite challenging. In the past we integrated ARC like behavior into the GC but that didn't perform as nicely as we'd hoped.

We went thru a lot of challenges to get concurrency to work correctly. The devil is deeply within the details and synchronization is probably the biggest real world challenge when building a GC. 

The ParparVM code is written in Java as generated C source code, you can actually see the generated "mark" methods in the C code that ParparVM generates: https://github.com/codenameone/CodenameOne/tree/master/vm]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>184</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.143.97.46]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-21 04:48:35]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-21 04:48:35]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Yeah it's funny you mention it. I was certainly thinking about the concurrency problem when playing with this as doing an entire traversal of all nodes requires a full stop the world.  I can only imagine the difficulties doing a concurrent GC where you need to be able to guarantee throughput of allocation as well as consistent cleanup.  Thanks for sharing!]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>183</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>185</wp:comment_id>
			<wp:comment_author><![CDATA[Shai Almog]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[shai@codenameone.com]]></wp:comment_author_email>
			<wp:comment_author_url>https://www.codenameone.com/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[37.142.133.100]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-21 06:42:47]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-21 06:42:47]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[The problem with representing this in Java is that you don't have the stacks.

E.g. when allocating every thread does its own allocation and stack work so individually every thread is "single threaded".  So you just need the one thread to stop so you can mark its stack. 

There are some complexities though e.g. object being passed from one thread to another and static context which is accessed by multiple threads.

Getting into this really helped me understand the benefits/complexities of GC a lot, we avoided a lot of the locking logic and were able to make a very efficient implementation.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>184</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>186</wp:comment_id>
			<wp:comment_author><![CDATA[hahaman]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[hahaman@gmailmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[95.34.214.42]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2016-02-22 10:10:36]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2016-02-22 10:10:36]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Uh, so a GC in a GC'ed language... that's confusing as shits.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Consistent hashing for fun</title>
		<link>https://onoffswitch.net/2016/03/23/consistent-hashing-fun/</link>
		<pubDate>Wed, 23 Mar 2016 23:01:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4824</guid>
		<description></description>
		<content:encoded><![CDATA[I think <a href="https://en.wikipedia.org/wiki/Consistent_hashing">consistent hashing</a> is pretty fascinating. It lets you define a ring of machines that shard out data by a hash value. Imagine that your hash space is 0 -> Int.Max, and you have 2 machines. Well one machine gets all values hashed from 0 -> Int.Max/2 and the other from Int.Max/2 -> Int.Max.  Clever.  This is one of the major algorithms of distributed systems like cassandra and dynamoDB.

For a good visualization, check out this <a href="http://www.paperplanes.de/2011/12/9/the-magic-of-consistent-hashing.html/">blog post</a>.

The fun stuff happens when you want to add replication and fault tolerance to your hashing. Now you need to have replicants and manage when machines join and add.  When someone joins, you need to re-partition the space evenly and re-distribute the values that were previously held. 

Something similar when you have a node leave, you need to make sure that whatever it was responsible for in its primray space AND the things it was responsible for as a secondary replicant, are re-redistributed amongst the remaining nodes.  

But the beauty of consistent hashing is that the replication basically happens for free! And so does redistribution!

Since my new feature is in all in Scala, I figured I'd write something up to see how this might play out in scala.

For the impatient, the full source is<a href="https://github.com/devshorts/consistent-hasher/blob/better/src/main/scala/com/example"> here</a>.

First I started with some data types

[scala]
case class HashValue(value: String) extends AnyRef

case class HashKey(key: Int) extends AnyRef with Ordered[HashKey] {
  override def compare(that: HashKey): Int = key.compare(that.key)
}

object HashKey {
  def safe(key: Int) = new HashKey(Math.abs(key))
}

case class HashRange(minHash: HashKey, maxHash: HashKey) extends Ordered[HashRange] {
  override def compare(that: HashRange): Int = minHash.compare(that.minHash)
}
[/scala]

I chose to wrap the key in a positive space since it made things slightly easier. In reality you want to use md5 or some actual hashing function, but I relied on the hash code here.

And then a machine to hold values:

[scala]
import scala.collection.immutable.TreeMap

class Machine[TValue](val id: String) {
  private var map: TreeMap[HashKey, TValue] = new TreeMap[HashKey, TValue]()

  def add(key: HashKey, value: TValue): Unit = {
    map = map + (key -&gt; value)
  }

  def get(hashKey: HashKey): Option[TValue] = {
    map.get(hashKey)
  }

  def getValuesInHashRange(hashRange: HashRange): Seq[(HashKey, TValue)] ={
    map.range(hashRange.minHash, hashRange.maxHash).toSeq
  }

  def keepOnly(hashRanges: Seq[HashRange]): Seq[(HashKey, TValue)] = {
    val keepOnly: TreeMap[HashKey, TValue] =
      hashRanges
      .map(range =&gt; map.range(range.minHash, range.maxHash))
      .fold(map.empty) { (tree1, tree2) =&gt; tree1 ++ tree2 }

    val dropped = map.filter { case (k, v) =&gt; !keepOnly.contains(k) }

    map = keepOnly
    
    dropped.toSeq
  }
}
[/scala]

A machine keeps a sorted tree map of hash values. This lets me really quickly get things within ranges.  For example, when we re-partition a machine, it's no longer responsible for the entire range set that it was before. But it may still be responsible for parts of it.  So we want to be able to tell a machine <em>hey, keep ranges 0-5, 12-20, but drop everything else</em>. The tree map lets me do this really nicely.

Now for the fun part, the actual consistent hashing stuff. 

Given a set of machines, we need to define how the circular partitions is defined

[scala]
private def getPartitions(machines: Seq[Machine[TValue]]): Seq[(HashRange, Machine[TValue])] = {
  val replicatedRanges: Seq[HashRange] = Stream.continually(defineRanges(machines.size)).flatten

  val infiteMachines: Stream[Machine[TValue]] = 
       Stream.continually(machines.flatMap(List.fill(replicas)(_))).flatten

  replicatedRanges
  .zip(infiteMachines)
  .take(machines.size * replicas)
  .toList
}
[/scala]

What we want to make sure is that each node sits on multiple ranges, this gives us the replication factor. To do that I've duplicated the machines in the list by the replication factor, and made sure all the lists cycle around indefinteily, so while they are not evenly distributed around the ring (they are clustered) they do provide fault tolerance

Lets look at what it takes to put a value into the ring:

[scala]
private def put(hashkey: HashKey, value: TValue): Unit = {
  getReplicas(hashkey).foreach(_.add(hashkey, value))
}

private def getReplicas(hashKey: HashKey): Seq[Machine[TValue]] = {
  partitions
  .filter { case (range, machine) =&gt; hashKey &gt;= range.minHash &amp;&amp; hashKey &lt; range.maxHash }
  .map(_._2)
}
[/scala]

We need to make sure that for each replica in the ring that sits on a hash range, that we insert it into that machine.  Thats pretty easy, though we can improve this later with better lookups

Lets look at a get

[scala]
def get(hashKey: TKey): Option[TValue] = {
  val key = HashKey.safe(hashKey.hashCode())

  getReplicas(key)
  .map(_.get(key))
  .collectFirst { case Some(x) =&gt; x }
}
[/scala]

Also similar.  Go through all the replicas, and find the first one to return a value

Now lets look how to add a machine into the ring

[scala]
def addMachine(): Machine[TValue] = {
  id += 1

  val newMachine = new Machine[TValue](&quot;machine-&quot; + id)

  val oldMachines = partitions.map(_._2).distinct

  partitions = getPartitions(Seq(newMachine) ++ oldMachines)

  redistribute(partitions)

  newMachine
}
[/scala]

So we first create a new list of machines, and then ask how to re-partition the ring. Then the keys in the ring need to redistribute themselves so that only the nodes who are responsible for certain ranges contain those keys

[scala]
def redistribute(newPartitions: Seq[(HashRange, Machine[TValue])]) = {
  newPartitions.groupBy { case (range, machine) =&gt; machine }
  .flatMap { case (machine, ranges) =&gt; machine.keepOnly(ranges.map(_._1)) }
  .foreach { case (k, v) =&gt; put(k, v) }
}
[/scala]

Redistributing isn't that complicated either.  We group all the nodes in the ring by the machine they are on, then for each machine we tell it to only keep values that are in its replicas.  The machine <code>keepOnly</code> function takes a list of ranges and will remove and <em>return</em> anything not in those ranges.  We can now aggregate all the things that are "emitted" by the machines and re insert them into the right location

Removing a machine is really similiar

[scala]
def removeMachine(machine: Machine[TValue]): Unit = {
  val remainingMachines = partitions.filter { case (r, m) =&gt; !m.eq(machine) }.map(_._2)

  partitions = getPartitions(remainingMachines.distinct)

  redistribute(partitions)
}
[/scala]

And thats all there is to it! Now we have a fast, simple consistent hasher.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4824</wp:post_id>
		<wp:post_date><![CDATA[2016-03-23 23:01:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-03-23 23:01:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[consistent-hashing-fun]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="distributed"><![CDATA[distributed]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="post_tag" nicename="toy"><![CDATA[toy]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554617742;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4805;}i:1;a:1:{s:2:"id";i:4783;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>CassieQ at the Seattle Cassandra Users Meetup</title>
		<link>https://onoffswitch.net/2016/04/19/cassieq-seattle-cassandra-users-meetup/</link>
		<pubDate>Tue, 19 Apr 2016 16:21:30 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4839</guid>
		<description></description>
		<content:encoded><![CDATA[Last night Jake and I presented <a href="https://github.com/paradoxical-io/cassieq" target="_blank" rel="noopener noreferrer">CassieQ</a> (the distributed message queue on cassandra) at the seattle cassandra users meetup at the Expedia building in Bellevue.  Thanks for everyone who came out and chatted with us, we certainly learned a lot and had some great conversations regarding potential optimizations to include in CassieQ.  

A couple good points that came up where how to minimize the use of compare and set with the monoton provider, whether we can move to time UUID's for "auto" incrementing monotons.  Another interesting tidbit was the discussion of using potential time based compaction strategies that are being discussed that could give a big boost given the workflow cassieq has.  

But my favorite was the suggestion that we create "kafka" mode and move the logic of storing pointer offsets out of cassieq and onto the client, in which case we could get enormous gains since we no longer need to do compare and sets for multiple consumers.  If we do see that pull request come in I think both Jake and I would be pretty stoked.

Anyways, the slides of our presentation are available here: <a href="paradoxical.io/slides/cassieq" target="_blank" rel="noopener noreferrer">paradoxical.io/slides/cassieq</a> (<a href="https://github.com/paradoxical-io/paradoxical.io/blob/gh-pages/slides/cassieq/cq.key" target="_blank" rel="noopener noreferrer">keynote</a>)]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4839</wp:post_id>
		<wp:post_date><![CDATA[2016-04-19 16:21:30]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-04-19 16:21:30]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[cassieq-seattle-cassandra-users-meetup]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cassandra"><![CDATA[cassandra]]></category>
		<category domain="post_tag" nicename="cassieq"><![CDATA[cassieq]]></category>
		<category domain="post_tag" nicename="meetup"><![CDATA[meetup]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1559832373;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4783;}i:1;a:1:{s:2:"id";i:4750;}i:2;a:1:{s:2:"id";i:4800;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Unit testing DNS failovers</title>
		<link>https://onoffswitch.net/2016/06/02/unit-testing-dns-failovers/</link>
		<pubDate>Thu, 02 Jun 2016 23:17:06 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4844</guid>
		<description></description>
		<content:encoded><![CDATA[Something that's come up a few times in my career is the difficulty of validating if and when your code can handle actual DNS changes. A lot of times testing that you have the right JVM settings and that your 3rd party clients can handle it involves mucking with hosts files, nameservers, or stuff like Route53 and waiting around. Then its hard to automate and deterministically reproduce. However, you can hook into the DNS resolution in the JVM to control what gets resolved to what.  And this way you can tweak the resolution in a test and see what breaks! I found some info at this <a href="rkuzmik.blogspot.com/2006/08/local-managed-dns-java_11.html">blog post</a> and cleaned it up a bit for usage in scala.  

The magic sauce to pull this off is to make sure you override the default <code>sun.net.spi.nameservice.NameServiceDescriptor</code>. Internally in the <code>InetAddress</code> class it tries to load an instance of the interface <code>NameServiceDescriptor</code> using the Service loader mechanism. The service loader looks for resources in <code>META-INF/services/fully.qualified.classname.to.override</code> and instantiates whatever fully qualified class name is that class name override file.

For example, if we have

[code]
cat META-INF/services/sun.net.spi.nameservice.NameServiceDescriptor
io.paradoxical.test.dns.LocalNameServerDescriptor
[/code]

Then the <code>io.paradoxical.test.dns.LocalNameServerDescriptor</code> will get created. Nice.

What does that class actually look like?

[scala]
class LocalNameServerDescriptor extends NameServiceDescriptor {
  override def getType: String = &quot;dns&quot;

  override def createNameService(): NameService = {
    new LocalNameServer()
  }

  override def getProviderName: String = LocalNameServer.dnsName
}
[/scala]

The type is of <code>dns</code> and the name service implementation is our own class.  The provider name is something we have custom defined as well below:

[scala]
object LocalNameServer {
  Security.setProperty(&quot;networkaddress.cache.ttl&quot;, &quot;0&quot;)

  protected val cache = new ConcurrentHashMap[String, String]()

  val dnsName = &quot;local-dns&quot;

  def use(): Unit = {
    System.setProperty(&quot;sun.net.spi.nameservice.provider.1&quot;, s&quot;dns,${dnsName}&quot;)
  }

  def put(hostName: String, ip: String) = {
    cache.put(hostName, ip)
  }

  def remove(hostName: String) = {
    cache.remove(hostName)
  }
}

class LocalNameServer extends NameService {

  import LocalNameServer._

  val default = new DNSNameService()

  override def lookupAllHostAddr(name: String): Array[InetAddress] = {
    val ip = cache.get(name)
    if (ip != null &amp;&amp; !ip.isEmpty) {
      InetAddress.getAllByName(ip)
    } else {
      default.lookupAllHostAddr(name)
    }
  }

  override def getHostByAddr(bytes: Array[Byte]): String = {
    default.getHostByAddr(bytes)
  }
}
[/scala]

Pretty simple. We have a cache that is stored in a singleton companion object with some helper methods on it, and all we do is delegate looking into the cache. If we can resolve the data in the cache we return it, otherwise just proxy it to the default resolver.

The <code>use</code> method sets a system property that says to use the dns resolver of name <code>local-dns</code> as the highest priority <code>nameservice.provider.1</code> (lower numbers are higher priority)

Now we can write some tests and see if this works!

[scala]
@RunWith(classOf[JUnitRunner])
class DnsTests extends FlatSpec with Matchers {
  LocalNameServer.use()

  &quot;DNS&quot; should &quot;resolve&quot; in {
    val google = resolve(&quot;www.google.com&quot;)

    google.getHostAddress shouldNot be(&quot;127.0.0.1&quot;)
  }

  it should &quot;be overridable&quot; in {
    LocalNameServer.put(&quot;www.google.com&quot;, &quot;127.0.0.1&quot;)

    val google = resolve(&quot;www.google.com&quot;)

    google.getHostAddress should be(&quot;127.0.0.1&quot;)

    LocalNameServer.remove(&quot;www.google.com&quot;)
  }

  it should &quot;be undoable&quot; in {
    LocalNameServer.put(&quot;www.google.com&quot;, &quot;127.0.0.1&quot;)

    val google = resolve(&quot;www.google.com&quot;)

    google.getHostAddress should be(&quot;127.0.0.1&quot;)

    LocalNameServer.remove(&quot;www.google.com&quot;)

    resolve(&quot;www.google.com&quot;).getHostAddress shouldNot be(&quot;127.0.0.1&quot;)
  }

  def resolve(name: String) = InetAddress.getByName(name)
}
[/scala]

Happy dns resolving!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4844</wp:post_id>
		<wp:post_date><![CDATA[2016-06-02 23:17:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-06-02 23:17:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[unit-testing-dns-failovers]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dns"><![CDATA[dns]]></category>
		<category domain="post_tag" nicename="java"><![CDATA[java]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558791887;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:7777;}i:1;a:1:{s:2:"id";i:4589;}i:2;a:1:{s:2:"id";i:4991;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Scripting  deployment of clusters in asgard</title>
		<link>https://onoffswitch.net/2016/06/08/scripting-deployment-clusters-asgard/</link>
		<pubDate>Wed, 08 Jun 2016 00:38:19 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4851</guid>
		<description></description>
		<content:encoded><![CDATA[We use asgard at work to do deployments in both qa and production. Our general flow is to check in, have jenkins build, an AMI is created, and then ... we have to manually go to asgard and deploy it. That sucks.

However, its actually not super hard to write some scripts to find the latest AMI for a cluster and prepare an automated deployment pipeline from a template.  Here you go:

[code]
function asgard(){
  verb=$1
  url=&quot;https://my.asgard.com/us-east-1/$2&quot;
  shift
  http ${VERB} --verify=no &quot;$url&quot; -b
}

function next-ami(){
  cluster=$1

  prepare-ami $cluster true | \
    jq &quot;.environment.images | reverse | .[0]&quot;
}

function prepare-ami(){
  cluster=$1

  includeEnv=$2

  asgard GET &quot;deployment/prepare/${cluster}?deploymentTemplateName=CreateAndCleanUpPreviousAsg&amp;includeEnvironment=${includeEnv}&quot;
}

function get-next-ami(){
  cluster=$1

  next=`next-ami ${cluster} | jq &quot;.id&quot;`

  prepare-ami ${cluster} &quot;false&quot; | jq &quot;.lcOptions.imageId |= ${next}&quot;
}

function start-deployment(){
  cluster=$1
  payload=$2

  echo $payload | asgard POST &quot;deployment/start/${cluster}&quot;
}
[/code]

The gist here is to 

<ul>
<li>Find the next AMI image of a cluster</li>
<li>Get the prepared JSON for the next deployment</li>
<li>Update the prepared json with the new ami image</li>
</ul>

To use it you'd do

[code]
&gt; clusterName=&quot;foo&quot;
&gt; next=`get-next-ami $clusterName`
&gt; start-deployment $clusterName $next
{
    &quot;deploymentId&quot;: &quot;1773&quot;
}
[/code]

And thats it!  ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4851</wp:post_id>
		<wp:post_date><![CDATA[2016-06-08 00:38:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-06-08 00:38:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[scripting-deployment-clusters-asgard]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="asgard"><![CDATA[asgard]]></category>
		<category domain="post_tag" nicename="aws"><![CDATA[aws]]></category>
		<category domain="post_tag" nicename="cicd"><![CDATA[cicd]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561497021;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:5000;}i:1;a:1:{s:2:"id";i:2274;}i:2;a:1:{s:2:"id";i:4737;}}}}]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Dealing with a bad symbolic reference in scala</title>
		<link>https://onoffswitch.net/2016/07/25/dealing-bad-symbolic-reference-scala/</link>
		<pubDate>Mon, 25 Jul 2016 22:55:06 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4858</guid>
		<description></description>
		<content:encoded><![CDATA[Every time this hits me I have to think about it.  The compiler barfs at you with something ambiguous like



<blockquote>[error] error: bad symbolic reference. the classpath might be incompatible with the version used when compiling Foo.class.</blockquote>



What this really is saying is that <code>Foo.class</code> references some import or class whose namespace isn't on the classpath or has fields missing.  I usually get this when I have a project level circular dependency via transitive includes. I.e.

[code]
Repo 1/
  /project A
  /project B -&gt; depends on C and A
Repo 2
  /project C -&gt; depends on A
[/code]

So here the dependency <code>C</code> pulls in a version of <code>A</code> but that version may not be the same that project <code>B</code> pulls in. If I do namespace refactoring in project A, then project B won't compile if those namespaces are used by project C.  It's a mess.

Thankfully scala lets you maintain folder structure outside of package namespace, unlike java.  So I can fake it till I make it by refactoring and keeping the old namespace, until I get a working build and then updating the secondary repo. It's like a two phase commit.
]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4858</wp:post_id>
		<wp:post_date><![CDATA[2016-07-25 22:55:06]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-07-25 22:55:06]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dealing-bad-symbolic-reference-scala]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dependencies"><![CDATA[dependencies]]></category>
		<category domain="post_tag" nicename="maven"><![CDATA[maven]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554608701;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4939;}i:1;a:1:{s:2:"id";i:4919;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Extracting scala method names from objects with macros</title>
		<link>https://onoffswitch.net/2016/08/03/extracting-scala-method-names-objects-macros/</link>
		<pubDate>Wed, 03 Aug 2016 00:33:53 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4862</guid>
		<description></description>
		<content:encoded><![CDATA[I have a soft spot in me for AST's ever since I went through the exercise of <a href="http://onoffswitch.net/building-a-custom-lexer/">building</a> my own <a href="https://github.com/devshorts/LanguageCreator" target="_blank" rel="noopener noreferrer">language</a>.  Working in Java I missed the dynamic ability to get compile time information, though I knew it was available as part of the annotation processing pipleine during compilation (which is how lombok works).  Scala has something similiar in the concept of macros: a way to hook into the compiler, manipulate or inspect the syntax tree, and rewrite or inject whatever you want.  It's a wonderfully elegant system that reminds me of Lisp/Clojure macros. 

I ran into a situation (as always) where I really wanted to get the name of a function dynamically. i.e.

[scala]
class Foo {
   val field: String = &quot;&quot; 
   def method(): Unit = {}
}

val name: String = ??.field // = &quot;field&quot;
[/scala]

In .NET this is pretty easy since at runtime you can create an expression tree which gives you the AST.  But I haven't been in .NET in a while, so off to macros I went!

First off, I found the documentation regarding macros to be lackluster. It's either rudimentary with trivial examples, or the learning curve was steep and I was too lazy to read through all of it.  Usually when I encounter scenarios like this I turn to exploratory programming, where I have a unit test that sets up a basic example and I leverage the debugger and intellij live REPL to poke through what I can and can't do.  Time to get set up. 

First, I needed to create a new submodule in my multi module maven project that would contain my macro. The reason is that you can't use macros in the same compilation unit that they are defined in.  You can however, use macros in a macros test since the compiler compiles test sources different from regular sources.

That said, debugging macros is harder than normal because you aren't debugging your running program, you are debugging the actual compiler.  I found this <a href="http://www.cakesolutions.net/teamblogs/2013/09/30/debugging-scala-macros" target="_blank" rel="noopener noreferrer">blog post</a> which was a life saver, even though it was missing a few minor pieces.

1. Set the main class to <code>scala.tools.nsc.Main</code>
2. Set the VM args to <code>-Dscala.usejavacp=true</code>
3. Set the program arguments to first point to the file containing the macro, then the file to compile that uses the macro:

<blockquote>-cp types.Types macros/src/main/scala/com/devshorts/common/macros/MethodNames.scala config/src/test/scala/config/ConfigProxySpec.scala</blockquote>


Now you can actually debug your macro! 

First let me show the test

[scala]
case class MethodNameTest(field1: Object) {
  def getFoo(arg: Object): Unit = {}
  def getFoo2(arg: Object, arg2: Object): Unit = {}
}

class MethodNamesMacroSpec extends FlatSpec with Matchers {
  &quot;Names macro&quot; should &quot;extract from an function&quot; in {
    methodName[MethodNameTest](_.field1) shouldEqual MethodName(&quot;field1&quot;)
  }

  it should &quot;extract when the function contains an argument&quot; in {
    methodName[MethodNameTest](_.getFoo(null)) shouldEqual MethodName(&quot;getFoo&quot;)
  }

  it should &quot;extract when the function contains multiple argument&quot; in {
    methodName[MethodNameTest](_.getFoo2(null, null)) shouldEqual MethodName(&quot;getFoo2&quot;)
  }

  it should &quot;extract when the method is curried&quot; in {
    methodName[MethodNameTest](m =&gt; m.getFoo2 _) shouldEqual MethodName(&quot;getFoo2&quot;)
  }
}
[/scala]

<img src="http://onoffswitch.net/wp-content/uploads/2016/08/macro.png" alt="macro" width="979" height="499" class="aligncenter size-full wp-image-4867" />

<code>methodName</code> here is a macro that extracts the method name from a lambda passed in of the parameterized generic type. What's nice about how scala set up their macros is you provide an alias for your macro such that you can re-use the macro but type it however you want.

[scala]
object MethodNames {
  implicit def methodName[A](extractor: (A) =&gt; Any): MethodName = macro methodNamesMacro[A]

  def methodNamesMacro[A: c.WeakTypeTag](c: Context)(extractor: c.Expr[(A) =&gt; Any]): c.Expr[MethodName] = {
     ...
   }
}
[/scala]

I've made the methodName function take a generic and a function that uses that generic (even though no actual instance is ever passed in).  The nice thing about this is I can re-use the macro typed as another function elsewhere.  Imagine I want to pin <code>[A]</code> so people don't have to type it. I can do exactly that!

[scala]
case class Configuration (foo: String)

implicit def config(extractor: Configuration =&gt; Any): MethodName = macro MethodNames.methodNamesMacro[Configuration]

config(_.foo) == &quot;foo&quot;
[/scala]

At this point its time to build the bulk of the macro. The idea is to inspect parts of the AST and potentially walk it to find the pieces we want.  Here's what I ended up with:

[scala]
def methodNamesMacro[A: c.WeakTypeTag](c: Context)(extractor: c.Expr[(A) =&gt; Any]): c.Expr[MethodName] = {
  import c.universe._

  @tailrec
  def resolveFunctionName(f: Function): String = {
    f.body match {
      // the function name
      case t: Select =&gt; t.name.decoded

      case t: Function =&gt;
        resolveFunctionName(t)

      // an application of a function and extracting the name
      case t: Apply if t.fun.isInstanceOf[Select] =&gt;
        t.fun.asInstanceOf[Select].name.decoded

      // curried lambda
      case t: Block if t.expr.isInstanceOf[Function] =&gt;
        val func = t.expr.asInstanceOf[Function]

        resolveFunctionName(func)

      case _ =&gt; {
        throw new RuntimeException(&quot;Unable to resolve function name for expression: &quot; + f.body)
      }
    }
  }

  val name = resolveFunctionName(extractor.tree.asInstanceOf[Function])

  val literal = c.Expr[String](Literal(Constant(name)))

  reify {
    MethodName(literal.splice)
  }
}
[/scala]

For more details on parts of the AST <a href="https://github.com/wolfe-pack/wolfe/wiki/Scala-AST-reference" target="_blank" rel="noopener noreferrer">here is a great resource</a>

In the first case, when we pass in <code>methodName[Config](_.method)</code> it gets mangled into a function with a body that is of <code>x$1.method</code>. The select indicates the x$1 instance and selects the <code>method</code> expression of it. This is an easy case.

In the block case that maps to when we call <code>methodName[Config](c => c.thing _)</code>. In this case we have a function but its curried.  In this scenario the function body is a block who's inner expression is a function.  But, the functions body of that inner function is an Apply.  



<blockquote>Apply takes two arguments -- a Select or an Ident for the function and a list of arguments</blockquote>



So that makes sense. 

The rest is just helper methods to recurse.

The last piece of the puzzle is to create an instance of a string literal and splice it into a new expression returning the <code>MethodName</code> case class that contains the string literal.

All in all a fun afternoons worth of code and now I get semantic string safety. A great use case here can be to type configuration values or other string semantics with a trait. You can get compile time refactoring + type safety.  Other use cases are things like database configurations and drivers (the .NET mongo driver uses expression trees to type an object to its underlying mongo collection).



 ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4862</wp:post_id>
		<wp:post_date><![CDATA[2016-08-03 00:33:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-08-03 00:33:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[extracting-scala-method-names-objects-macros]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="ast"><![CDATA[ast]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="macro"><![CDATA[macro]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560513278;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4991;}i:1;a:1:{s:2:"id";i:4575;}i:2;a:1:{s:2:"id";i:4905;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Mocking nested objects with mockito</title>
		<link>https://onoffswitch.net/2016/09/21/mocking-nested-objects-mockito/</link>
		<pubDate>Wed, 21 Sep 2016 22:55:16 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4871</guid>
		<description></description>
		<content:encoded><![CDATA[Yes, I know its a code smell. But I live in the real world, and sometimes you need to mock nested objects.  This is a scenario like: 

[code]
when(a.b.c.d).thenReturn(e)
[/code]

The usual pattern here is to create a mock for each object and return the previous mock:

[code]
val a = mock[A]
val b = mock[B]
val c = mock[C]
val d = mock[D]

when(a.b).thenReturn(b)
when(b.c).thenReturn(c)
when(c.d).thenReturn(d)
[/code]

But again, in the real world the signatures are longer, the types are nastier, and its never quite so clean.  I figured I'd sit down and solve this for myself once and for all and came up with:

[scala]
import org.junit.runner.RunWith
import org.mockito.Mockito
import org.scalatest.junit.JUnitRunner
import org.scalatest.{FlatSpec, Matchers}

@RunWith(classOf[JUnitRunner])
class Tests extends FlatSpec with Matchers {
  &quot;Mockito&quot; should &quot;proxy nested objects&quot; in {
    val parent = Mocks.mock[Parent]

    Mockito.when(
      parent.
        mock(_.getChild1).
        mock(_.getChild2).
        mock(_.getChild3).
        value.doWork()
    ).thenReturn(3)

    parent.value.getChild1.getChild2.getChild3.doWork() shouldEqual 3
  }
}

class Child3 {
  def doWork(): Int = 0
}

class Child2 {
  def getChild3: Child3 = new Child3
}

class Child1 {
  def getChild2: Child2 = new Child2
}

class Parent {
  def getChild1: Child1 = new Child1
}
[/scala]

As you can see in the full test we can create some mocks object, and reference the call chain via extractor methods.

The actual mocker is really pretty simple, it just looks nasty cause of all the lambdas/manifests. All thats going on here is a way to pass the next object to a chain and extract it with a method. Then we can create a mock using the manifest and assign that mock to the source object via the lambda.

[scala]
import org.mockito.Mockito

object Mocks {
  implicit def mock[T](implicit manifest: Manifest[T]) = new RichMockRoot[T]

  class RichMockRoot[T](implicit manifest: Manifest[T]) {
    val value = Mockito.mock[T](manifest.runtimeClass.asInstanceOf[Class[T]])

    def mock[Y](extractor: T =&gt; Y)(implicit manifest: Manifest[Y]): RichMock[Y] = {
      new RichMock[T](value, List(value)).mock(extractor)
    }
  }

  class RichMock[T](c: T, prevMocks: List[_]) {
    def mock[Y](extractor: T =&gt; Y)(implicit manifest: Manifest[Y]): RichMock[Y] = {
      val m = Mockito.mock[Y](manifest.runtimeClass.asInstanceOf[Class[Y]])

      Mockito.when(extractor(c)).thenReturn(m)

      new RichMock(m, prevMocks ++ List(m))
    }

    def value: T = c

    def mockChain[Y](idx: Int) = prevMocks(idx).asInstanceOf[Y]

    def head[Y] = mockChain[Y](0)
  }
}
[/scala]

The main idea here is just to hide away the whole "make b and have it return c" for you.  You can even capture all the intermediate mocks in a list (I called it a mock chain), and expose the first element of the list with <code>head</code>. With a little bit of scala manifest magic you can even get around needing to pass class files around and can leverage the generic parameter (boy, feels almost like .NET!). ]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4871</wp:post_id>
		<wp:post_date><![CDATA[2016-09-21 22:55:16]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-09-21 22:55:16]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[mocking-nested-objects-mockito]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="mockito"><![CDATA[mockito]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="post_tag" nicename="testing"><![CDATA[testing]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560218483;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4844;}i:1;a:1:{s:2:"id";i:4961;}i:2;a:1:{s:2:"id";i:4862;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>CassieQ @ Cassandra Summit</title>
		<link>https://onoffswitch.net/2016/10/15/cassieq-cassandra-summit/</link>
		<pubDate>Sat, 15 Oct 2016 22:13:53 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4879</guid>
		<description></description>
		<content:encoded><![CDATA[I had the great chance to talk at Cassandra summit 2016 this year about <a href="https://github.com/paradoxical-io/cassieq">cassieq</a>, the project I worked on with Jake Swenson at Paradoxical. For anyone interested, here's the video!

[embed]https://www.youtube.com/watch?v=LwOq6x-KUAE[/embed]]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4879</wp:post_id>
		<wp:post_date><![CDATA[2016-10-15 22:13:53]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-10-15 22:13:53]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[cassieq-cassandra-summit]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cassandra"><![CDATA[cassandra]]></category>
		<category domain="post_tag" nicename="cassieq"><![CDATA[cassieq]]></category>
		<category domain="category" nicename="tech-talks"><![CDATA[Tech talks]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554507519;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4839;}i:1;a:1:{s:2:"id";i:4783;}i:2;a:1:{s:2:"id";i:4750;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_9c78a38f7b50d4a028d33d2eb643cc53]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe title="CassieQ: The Distributed Message Queue Built on Cassandra (Anton Kropp, Curalate) | C* Summit 2016" width="610" height="343" src="https://www.youtube.com/embed/LwOq6x-KUAE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_9c78a38f7b50d4a028d33d2eb643cc53]]></wp:meta_key>
		<wp:meta_value><![CDATA[1561945419]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_7532e3fedf830a9c4e190a996ad38222]]></wp:meta_key>
		<wp:meta_value><![CDATA[<iframe title="CassieQ: The Distributed Message Queue Built on Cassandra (Anton Kropp, Curalate) | C* Summit 2016" width="640" height="360" src="https://www.youtube.com/embed/LwOq6x-KUAE?feature=oembed" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_oembed_time_7532e3fedf830a9c4e190a996ad38222]]></wp:meta_key>
		<wp:meta_value><![CDATA[1562281635]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Coproducts and polymorphic functions for safety</title>
		<link>https://onoffswitch.net/2016/10/15/coproducts-polymorphic-functions-safety/</link>
		<pubDate>Sat, 15 Oct 2016 22:58:45 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4881</guid>
		<description></description>
		<content:encoded><![CDATA[I was recently exploring <a href="https://github.com/milessabin/shapeless">shapeless</a> and a coworker turned me onto the interesting features of coproducts and how they can be used with polymorphic functions.

Frequently when using pattern matching you want to make sure that all cases are exhaustively checked. A non exhaustive pattern match is a runtime exception waiting to happen. As a scala user, I'm all about compile time checking.  For classes that I own I can enforce exhaustiveness by creating a sealed trait heirarchy:

[code lang=scala]
sealed trait Base
case class Sub1() extends Base
case class Sub2() extends Base
[/code]

And if I ever try and match on an <code>Base</code> type I'll get a compiler warning (that I can fail on) if all the types aren't matched.  This is nice because if I ever add another type, I'll get a (hopefully) failed build.

But what about the scenario where you <em>don't</em> own the types?

[code lang=scala]
case class Type1()
case class Type2()
case class Type3()
[/code]

They're all completely unrelated.  Even worse is how do you create a generic function that accepts an instance of those 3 types but no others?  You could always create overloaded methods:

[code lang=scala]
def takesType(type: Type1) = ???
def takesType(type: Type2) = ???
def takesType1(type: Type3) = ???
[/code]

Which works just fine, but what if that type needs to be passed through a few layers of function calls before its actually acted on?

[code lang=scala]
def doStuff(type: Type1) = ... takesType(type1)
def doStuff(type: Type2) = ... takesType(type2)
def doStuff(type: Type3) = ... takesType(type3)
[/code]

Oh boy, this is a mess.  We can't get around with just using generics with type bounds since there is no unified type for these 3 types.  And even worse is if we add another type. We could use an either like <code>Either[Type1, Either[Type2, Either[Type3, Nothing]]]</code>

Which lets us write just one function and then we have to match on the subsets.  This is kind of gross too since its polluted with a bunch of eithers.  Turns out though, that a coproduct is exactly this... a souped up either!

Defining

[code lang=scala]
type Items = Type1 :+: Type2 :+: Type3 :+: CNil
[/code]

(where CNil is the terminator for a coproduct)  we now have a unified type for our collection.  We can write functions like :

[code lang=scala]
def doStuff(item: Items) = {
  // whatever
  takesType(item)
}
[/code]

At some point, you need to lift an instance of <code>Type1</code> etc into a type of <code>Item</code> and this can be done by calling <code>Coproduct[Item](instance)</code>.  This call will fail to compile if the type of the instance is not a type of <code>Item</code>.  You also are probably going to want to actually do work with the thing, so you need to unbox this souped up either and do stuff with it

This is where the shapeless <code>PolyN</code> methods come into play.

[code lang=scala]
object Worker {
  type Items = Type1 :+: Type2 :+: Type3 :+: CNil

  object thisIsAMethod extends Poly1 {
    // corresponding def for the data type of the coproduct instance
    implicit def invokedOnType1 = at[Type1](data =&gt; data.toString)
    implicit def invokedOnType2 = at[Type2](data =&gt; data.toString)
    implicit def invokedOnType3 = at[Type3](data =&gt; data.toString)
  }

  def takesItem(item: Item): String = {
    thisIsAMethod(item)
  }
}

class Provider {
  Worker.takesItem(Coproduct[Item](Type1()) // ok
  Worker.takesItem(Coproduct[Item](WrongType()) // fails
}

[/code]

The object <code>thisIsAMethod</code> creates a bunch of implicit type dependent functions that are defined at all the elements in the coproduct.  If we add another option to our coproduct list, we'll get a compiler error when we try and use the coproduct against the polymorphic function.   This accomplishes the same thing as giving us the exhaustiveness check but its an even stronger guarantee as the build will fail.

While it is a lot of hoops to jump through, and can be a little mind bending, I've found that coproducts and polymorphic functions are a really nice addition to my scala toolbox. Being able to strongly enforce these kinds of contracts is pretty neat!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4881</wp:post_id>
		<wp:post_date><![CDATA[2016-10-15 22:58:45]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-10-15 22:58:45]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[coproducts-polymorphic-functions-safety]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="coproduct"><![CDATA[coproduct]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="post_tag" nicename="shapeless"><![CDATA[shapeless]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1555115004;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4862;}i:1;a:1:{s:2:"id";i:4961;}i:2;a:1:{s:2:"id";i:4905;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Deployment the paradoxical way</title>
		<link>https://onoffswitch.net/2016/11/08/deployment-paradoxical/</link>
		<pubDate>Tue, 08 Nov 2016 08:00:47 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4892</guid>
		<description></description>
		<content:encoded><![CDATA[First and foremost, this is all Jake Swensons brain child. But it's just too cool to not share and write about.  Thanks Jake for doing all the hard work :)

At paradoxical, we really like being able to crank out libraries and projects as fast as possible. We <em>hate</em> boilerplate and we <em>hate</em> repetition.  Everything should be automated.  For a long time we used maven archetypes to crank out services from a template and libraries from a template, and that worked reasonably well.  However, deployment was always kind of a manual process.  We had scripts in each repo to use the maven release plugin but our build system (Travis) wasn't wired into it. This meant that deploys of libraries/services required a manual (but simple) step to run.  We also had some kinks with our gpg keys and we weren't totally sure a clean way of having Travis be able to sign our artifacts in a secure way without our keys being checked into a bunch of different repos

Jake and I had talked a while ago about how nice it would be if we could

<ul>
<li>Have all builds to master auto deployed as snapshots</li>
<li>PR's built but not deployed</li>
<li>Creating a github release kicked off an actual release</li>
</ul>

The first two were reasonably easy with the travis scripts we already had, but it was the last one that was fun.

<a href="https://axelfontaine.com/blog/dead-burried.html">This article</a> was posted not long ago about simplifying your maven release process by chucking the maven release plugin and instead using the maven deploy directly.  If you could parameterize your maven artifact version number and have your build pass that in from the git tag, then we could really easily achieve git tag driven development!

To that end, Jake created a <a href="https://github.com/paradoxical-io/deployment">git project</a> that facilitated setting up all our repo's for tag driven deployment.  Each deployable of ours would check out this project as a submodule under <code>.deployment</code> which contains the tooling to make git tag releases happen.

<h2>To onboard</h2>

First things first, is that we need a way to delegate deployment after our travis build is complete. So you'd add the following to your projects travis file:

[code]
git:
  submodules: false
before_install:
  # https://git-scm.com/docs/git-submodule#_options:
  # --remote
  # Instead of using the superproject’s recorded SHA-1 to update the submodule,
  # use the status of the submodule’s remote-tracking (branch.&lt;name&gt;.remote) branch (submodule.&lt;name&gt;.branch).
  # --recursive
  # https://github.com/travis-ci/travis-ci/issues/4099
  - git submodule update --init --remote --recursive
after_success:
- ./.deployment/deploy.sh
[/code]

Which would pull the git deployment submodule, and delegate the after step to its deploy script.

You also need to add the deployment project as a parent of your pom:

[code]
&lt;parent&gt;
    &lt;groupId&gt;io.paradoxical&lt;/groupId&gt;
    &lt;artifactId&gt;deployment-base-pom&lt;/artifactId&gt;
    &lt;version&gt;1.0&lt;/version&gt;
&lt;/parent&gt;
[/code]

This sets up nice things for us like making sure we sign our GPG artifacts, include sources as part of our deployment, and attaches javadocs.

The last thing you need to do is parametarize your artifact version field:

<code>&lt;version&gt;1.0${revision}&lt;/version&gt;</code>

The parent pom defines <code>revision</code> and will set it to be either the git tag or <code>-SNAPSHOT</code> depending on context.

But, for those of you with strong maven experience, an alarm may fire that you can't parameterize the version field.  To solve that problem, Jake wrote a wonderful <a href="https://github.com/paradoxical-io/resolved-pom-maven-plugin">maven parameter resolver</a> which lets you white-list which fields need to be pre-processed before they are processed. This solves an issue where a deployed maven pom that has parameterized values that are set at build time only are captured for deployment. Without that, maven has issues resolving transitive dependencies.

Anyways, the base pom handles a lot of nice things :)

<h2>The deploy script</h2>

Now lets break down the after build <a href="https://github.com/paradoxical-io/deployment/blob/master/deploy.sh">deploy script</a>. It's job is to take the travis encrypted gpg keys (which are also password secured) and decrypt them, and run the right maven release given the git tags.

[code lang=text]
if [ -n &quot;$TRAVIS_TAG&quot; ]; then
    echo &quot;Deploying release version for tag &#039;${TRAVIS_TAG}&#039;&quot;
    mvn clean deploy --settings &quot;${SCRIPT_DIR}/settings.xml&quot; -DskipTests -P release -Drevision=&#039;&#039; $@
    exit $?
elif [ &quot;$TRAVIS_BRANCH&quot; = &quot;master&quot; ]; then
    echo &quot;Deploying snapshot version on branch &#039;${TRAVIS_BRANCH}&#039;&quot;
    mvn clean deploy --settings &quot;${SCRIPT_DIR}/settings.xml&quot; -DskipTests -P snapshot $@
    exit $?
else
    echo &quot;No deployment running for current settings&quot;
    exit 0
fi
[/code]

It's worth noting here a few magic things.

<ol>
<li>The <a href="https://github.com/paradoxical-io/deployment/blob/master/settings.xml">settings.xml</a> file is provided by submodule and contains a field for the gpg username and the parametrized password the  in every repo and contains the gpg user</li>
<li>Because the deploy script is invoked from the root of the project, even though the deploy script is in the deployment submodule it resolves paths from the script execution point (not where it the script lives at). This is why the script captures its own path and stores it as the <code>$SCRIPT_DIR</code> variable.</li>
</ol>

<h2>Release time!</h2>

Now that it's all set up we can safely merge to master whenever we want to publish a snapshot, and if we want to mark a release as public we just create a git tag for it.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4892</wp:post_id>
		<wp:post_date><![CDATA[2016-11-08 08:00:47]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-08 08:00:47]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[deployment-paradoxical]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="cicd"><![CDATA[cicd]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="deploy"><![CDATA[deploy]]></category>
		<category domain="post_tag" nicename="maven"><![CDATA[maven]]></category>
		<category domain="post_tag" nicename="travis"><![CDATA[travis]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561836196;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4673;}i:2;a:1:{s:2:"id";i:4800;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Dont be afraid of dependency updates</title>
		<link>https://onoffswitch.net/2016/11/25/dont-afraid-dependency-updates/</link>
		<pubDate>Fri, 25 Nov 2016 22:03:20 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4899</guid>
		<description></description>
		<content:encoded><![CDATA[Lots of place I've worked at have had an irrational fear of upgrading their dependencies. I understand why, when you have something that <code>works</code> you don't want to rock the boat.  You want to focus on building your product, not dealing with potential runtime errors.  Your ops team is happy, things are stable. Life is great.

However, just like running from your problems, freezing your dependencies is a recipe for disaster. Just like normal software maintenance, your dependencies MUST be upgraded on a regular basis. It sucks, nobody likes dealing with weird transitive issues, but without a regular upgrade schedule (every 6 months to a year at minimum) you run the risk of realizing that you can't upgrade at all!

This is a crappy place to be in, and you know when you're there because you try and pull in some updated library that has the features you want and/or need and everything either fails to compile, blows up at runtime, and you end up with a giant mishash of dependency exclusions and staring fruitless at dependency graphs trying to figure out "if I pick one minor version down of this and one minor version up of that maaaaybe it'll work"

What managers and sometimes even leads don't understand is that without staying on top of this, your cadence will slow down.  The first few years you won't notice, but if you let it stagnate, come 3, 4, or 5 years later it will be very hard to update.  Without updates you're missing on security fixes, industry standard changes, performance boosts, bug fixes, etc.

I'm not advocating for staying bleeding edge, but it is worth staying up to date. There is a difference.  Bleeding edge is usually alphas and betas of libraries/products, ones that haven't been battle tested or settled down (maybe the API is constantly in flux, looking at you angular 2.0).  But stable releases should be moved onto. Your team needs to have a plan for upgrading, and isolating dependencies and changes. You need to be able to silo projects so that upgrades in one place don't require major cascading upgrades somewhere else. If you run into that, unfortunately you have a poorly factored ecosystem that needs to be trimmed and decoupled.

And if you do find yourself in this situation, especially on something you inherited. I feel you. Trust me.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4899</wp:post_id>
		<wp:post_date><![CDATA[2016-11-25 22:03:20]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2016-11-25 22:03:20]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[dont-afraid-dependency-updates]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="dependencies"><![CDATA[dependencies]]></category>
		<category domain="category" nicename="rants"><![CDATA[Rants]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561858553;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:4764;}i:2;a:1:{s:2:"id";i:4028;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Strongly typed http headers in finatra</title>
		<link>https://onoffswitch.net/2017/02/12/stronlgy-typed-headers-finatra/</link>
		<pubDate>Sun, 12 Feb 2017 00:31:26 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4905</guid>
		<description></description>
		<content:encoded><![CDATA[When building service architectures one thing you need to solve is how to pass context between services. This is usually stuff like request id's and other tracing information (maybe you use <a href="http://zipkin.io/">zipkin</a>) between service calls. This means that if you set request id FooBar123 on an entrypoint to service A, if service A calls service B it should know that the request id is still FooBar123.  The bigger challenge is usually making sure that all thread locals keep this around (and across futures/execution contexts), but before you attempt that you need to get it into the system in the first place.

I'm working in <a href="https://twitter.github.io/finatra/">finatra</a> these days, and I love this framework. It's got all the things I loved from <a href="https://github.com/dropwizard/dropwizard">dropwizard</a> but in a scala first way. Todays challenge was that I wanted to be able to pass request http headers around between services in a typesafe way that could be used in thread local request contexts.  Basically I want to send

[code]
X-Magic-Header someValue
[/code]

And be able to resolve that into a <code>MagicHeader(value: T)</code> class.

The first attempt is easy, just parse header values into case classes:

[scala]
case class MagicHeader(value: String)
[/scala]

But the question I have is how do I enforce that the header string <code>X-Magic-Value</code> is directly correlated to the case class <code>MagicHeader</code>?

[scala]
object MagicHeader { 
   val key = &quot;X-Magic-Header&quot;
}

case class MagicHeader(value: String)
[/scala]

Maybe, but still, when someone sends the value out, they can make a mistake:

[scala]
setRequestHeader(&quot;X-mag1c-whatevzer&quot; -&gt; magicHeader.value)
[/scala]

That sucks, I don't want that. I want it strictly paired. I'm looking for what is in essence a case class that has 2 fields: key, value, but where the key is <em>fixed</em>. How do I do that?

I like to start with how I want to use something, and then work backwards to how to make that happen. Given that, lets say we want an api kind of like:

[scala]
object Experimental {
  val key = &quot;Experimental&quot;

  override type Value = String
}
[/scala]

And I'd like to be able to do something like

[scala]
val experimentKey = Experimental(&quot;experiment abc&quot;)
(experimentKey.key -&gt; experimentKey.value) shouldEqual
         (&quot;Experimental&quot; -&gt; &quot;experiment abc&quot;)
[/scala]

I know this means I need an apply method somewhere, and I know that I want a tuple of (key, value). I also know that because I have a path dependent type of the second value, that I can do something with that

Maybe I can fake an apply  method to be like

[scala]
trait ContextKey {
  val key: String

  /**
   * The custom type of this key
   */
  type Value

  /**
   * A tupel of (String, Value)
   */
  type Key = Product2[String, Value]

  def apply(data: Value): Key = new Key {
    override def _1: String = key

    override def _2: Value = data
  }
}
[/scala]

And update my object to be

[scala]
object Experimental extends ContextKey {
  val key = &quot;Experimental&quot;

  override type Value = String
}
[/scala]

Now my object has a mixin of an apply method that creates an anonmyous tuple of type <code>String, Value</code>.  You can create instances of <code>Experimental</code> but you can't ever set the key name itself! However, I can still <em>access</em> the pinned key because the anonymous tuple has it!

But in the case that I wanted, I wanted to use these as http header values. Which means I need to be able to parse a string into a type of <code>ContextKey#Value</code> which is path dependent on the object type.

We can do that by adding now a few extra methods on the ContextKey trait:

[scala]
trait ContextKeyType[T] extends Product2[String, T] {
  def unparse: String
}

trait ContextKey {
  self =&gt;
  val key: String

  /**
   * The custom type of this key
   */
  type Value

  /**
   * A tupel of (String, Value)
   */
  type Key = ContextKeyType[Value]

  /**
   * Utility to allow the container to provide a mapping from Value =&gt; String
   *
   * @param r
   * @return
   */
  def parse(r: String): Value

  def unparse(v: Value): String

  def apply(data: Value): Key = new Key {
    override def _1: String = key

    override def _2: Value = data

    /**
     * Allow a mapping of Value =&gt; String
     *
     * @return
     */
    override def unparse: String = self.unparse(data)

    override def equals(obj: scala.Any): Boolean = {
      canEqual(obj)
    }

    override def canEqual(that: Any): Boolean = {
      that != null &amp;&amp;
      that.isInstanceOf[ContextKeyType[_]] &amp;&amp;
      that.asInstanceOf[ContextKeyType[_]]._1 == key &amp;&amp;
      that.asInstanceOf[ContextKeyType[_]]._2 == data
    }
  }
}
[/scala]

This introduces a parse and unparse method which converts things to and from strings.  A http header object can now define how to convert it:

[scala]
object Experimental extends ContextKey {
  val key = &quot;Experimental&quot;
  override type Value = String

  override def parse(value: String): String = value

  override def unparse(value: String): String = value
}
[/scala]

So, if we want to maybe send JSON in a header, or a long/int/uuid we can now parse and unparse that value pre and post wire.

Now lets add a utility to convert a <code>Map[String, String]</code> which could represent an http header map, into a set of strongly typed context values:

[scala]
object ContextValue {
  def find[T &lt;: ContextKey](search: T, map: Map[String, String]): Option[T#Value] = {
    map.collectFirst {
      case (key, value) if search.key == key =&gt; search.parse(value)
    }
  }
}
[/scala]

Back in finatra land, lets add a http filter

[scala]
case class CurrentRequestContext(
  experimentId: Option[Experimental.Value],
)

object RequestContext {
  private val requestType = Request.Schema.newField[CurrentRequestContext]

  implicit class RequestContextSyntax(request: Request) {
    def context: CurrentRequestContext = request.ctx(requestType)
  }

  private[filters] def set(request: Request): Unit = {
    val data = CurrentRequestContext(
      experimentId = ContextValue.find(Experimental, request.headerMap)
    )

    request.ctx.update(requestType, data)
  }
}

/**
 * Set the remote context from requests 
 */
class RemoteContextFilter extends SimpleFilter[Request, Response] {
  override def apply(request: Request, service: Service[Request, Response]): Future[Response] = {
    RequestContext.set(request)

    service(request)
  }
}
[/scala]

From here on out, we can provide a set of strongly typed values that are basically case classes with hidden keys]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4905</wp:post_id>
		<wp:post_date><![CDATA[2017-02-12 00:31:26]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-02-12 00:31:26]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[stronlgy-typed-headers-finatra]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="finatra"><![CDATA[finatra]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558278890;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4945;}i:1;a:1:{s:2:"id";i:4919;}i:2;a:1:{s:2:"id";i:4939;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[passing-request-context-finatra]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>187</wp:comment_id>
			<wp:comment_author><![CDATA[Strongly typed headers in finatra | Ace Infoway]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://zerozone.com/mmp/aceinfoway/strongly-typed-headers-in-finatra-/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[52.2.32.161]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2017-02-12 09:15:58]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2017-02-12 09:15:58]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] typed headers in finatra&#013; {$excerpt:n}&#013; submitted by  /u/monumentshorts  [link] [comments]&#013; Source: [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Bit packing Pacman</title>
		<link>https://onoffswitch.net/2017/04/08/bit-packing-pacman/</link>
		<pubDate>Sat, 08 Apr 2017 23:28:11 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4914</guid>
		<description></description>
		<content:encoded><![CDATA[Haven't posted in a while, since I've been heads down in building a lot of cool tooling at work (blog posts coming), but had a chance to mess around a bit with something that came up in an interview question this week.

I frequently ask candidates a high level design question to build PacMan.  Games like pacman are fun because on the surface they are very simple, but if you don't structure your entities and their interactions correctly the design falls apart.

At some point during the interview we had scaled the question up such that there was now a problem of knowing at a particular point in the game what was nearby it.  For example, if the board is 100000 x 100000 (10 billion elements) how efficiently can we determine if there is a nugget/wall next to us? One option is to store all of these entities in a 2d array and just access the neighbors. However, if the entity is any non trivial object, then we now have at <a href="http://stackoverflow.com/a/258150/310196" target="_blank" rel="noopener noreferrer">minumum 16 bytes</a>. That means we're storing 160 gigs to access the board. Probably not something we can realistically do on commodity hardware.

Given we're answering only a "is something there or not" question, one option is to bit pack the answer.  In this sense you can leverage that each bit represents a coordinate in your grid.  For example in a 2D grid

[code]
0 1
2 3
[/code]

These positions could be represented by the binary value at that bit:

[code]
0 = 0b0001
1 = 0b0010
2 = 0b0100
3 = 0b1000
[/code]

If we do that, and we store a list of longs (64 bits, 8 bytes) then to store 10 billion elements we need:

[code]
private val maxBits = maxX * maxY
private val requiredLongs = (maxBits / 64) + 1
[/code]

Which ends up being 22,032,273 longs, which in turn is 176.2 MB.  Thats... a big savings.  Considering that the trivial form we stored 10,000,000,000 objects, this is a compression ratio of 450%.

Now, one thing the candidate brought up (which is a great point) is that this makes working with the values much more difficult.  The answer here is to provide a higher level API that hides away the hard bits.

I figured today I'd set down and do just that. We need to be able to do a few things

<ol>
<li>Find out how many longs to store</li>
<li>Find out given a coordinate which long it belongs to</li>
<li>In that long toggle the bit representing the coordinate if we want to set/unset it</li>
</ol>

[scala]
class TwoDBinPacker(maxX: Int, maxY: Int) {
  private val maxBits = maxX * maxY
  private val requiredLongs = (maxBits / 64) + 1
  private val longArray = new Array[Long](requiredLongs)

  def get(x: Int, y: Int): Boolean = {
    longAtPosition(x, y).value == 1
  }

  def set(x: Int, y: Int, value: Boolean) = {
    val p = longAtPosition(x, y)

    longArray(p.index) = p.set(value)
  }

  private def longAtPosition(x: Int, y: Int): BitValue = {
    val flattenedPosition = y * maxX + x

    val longAtPosition = flattenedPosition / 64

    val bitAtPosition = flattenedPosition % 64

    BitValue(longAtPosition, longArray(longAtPosition), bitAtPosition)
  }
}
[/scala]

With the helper class of a BitValue looking like:

[scala]
case class BitValue(index: Int, container: Long, bitNumber: Int) {
  val value = (container &gt;&gt; bitNumber) &amp; 1

  def set(boolean: Boolean): Long = {
    if (boolean) {
      val maskAt = 1 &lt;&lt; bitNumber

      container | maskAt
    } else {
      val maskAt = ~(1 &lt;&lt; bitNumber)

      container &amp; maskAt
    }
  }
}
[/scala]

At this point we can drive a scalatest:

[scala]
&quot;Bit packer&quot; should &quot;pack large sets (10 billion!)&quot; in {
  val packer = new TwoDBinPacker(100000, 100000)

  packer.set(0, 0, true)
  packer.set(200, 400, true)

  assert(packer.get(0, 0))
  assert(packer.get(200, 400))
  assert(!packer.get(99999, 88888))
}
[/scala]

And this test runs in 80ms.

Now, this is a pretty naive way of doing things, since we are potentially storing tons of unused longs.  A smarter way would be use a sparse set with skip lists, such that as you use a long you create it and mark it used, but things before it and after it (up to the next long) are marker blocks that can span many ranges. I.e.

[code]
{EmtpyBlock}[long, long, long]{EmptyBlock}[long]
[/code]

This way you don't have to store things you don't actually set.

Anyways, a fun little set of code to write. Full source available on my <a href="https://github.com/devshorts/lru/blob/master/src/main/scala/com/devhorts/binpack/BinPacker.scala" target="_blank" rel="noopener noreferrer">github</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4914</wp:post_id>
		<wp:post_date><![CDATA[2017-04-08 23:28:11]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-04-08 23:28:11]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[bit-packing-pacman]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="low-level"><![CDATA[low-level]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561898908;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3735;}i:1;a:1:{s:2:"id";i:3477;}i:2;a:1:{s:2:"id";i:4011;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_old_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[bit-packing]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>188</wp:comment_id>
			<wp:comment_author><![CDATA[Tracking batch queue fanouts | Onoffswitch.net]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://onoffswitch.net/tracking-batch-queue-fanouts/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[50.62.68.1]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2018-02-15 00:17:49]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2018-02-15 00:17:49]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] transferring N batch item ID&#8217;s though. What if instead of storing each item you leveraged a bitfield representing a set of items? Now instead of N items you only need N bits to logically track every [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>The HTTP driver pattern</title>
		<link>https://onoffswitch.net/2017/07/24/http-driver-pattern/</link>
		<pubDate>Mon, 24 Jul 2017 23:30:07 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4919</guid>
		<description></description>
		<content:encoded><![CDATA[Yet another SOA blog post, this time about calling services. I've seen a lot of posts, articles, even books, on how to <em>write</em> services but not a good way about <em>calling</em> services. It may seem trivial, isn't calling a service a matter of making a web request to one? Yes, it is, but in a larger organization it's not always so trivial.

<h1>Distributing fat clients</h1>

The problem I ran into was the service stack in use at my organization provided a feature rich client as an artifact of a services build. It had retries, metrics, tracing with zipkin, etc.  But, it also pulled in things like finagle, netty, jackson, and each service may be distributing slightly different versions of all of these dependencies.  When you start to consume 3, 4, 5 or more clients in your own service, suddenly you've gotten into an intractable mess of dependencies. Sometimes there's no actual way to resolve them all without forcing upgrades in other services!  That... sucks. It violates the idea of services in that my service is now coupled to your service.

You don't want to force service owners to have to write clients for each service they want to call. That'd be a big waste of time and duplicated effort. If your organization is mono-lingual (i.e. all java/scala/whatever) then its still worth providing a feature rich client that has the sane things built in: retries, metrics, tracing, fast fail, serialization, etc.  But you don't want services leaking all the nuts and bolts to each other.

One solution is to auto generate clients server side. This is akin to what <a href="https://docs.microsoft.com/en-us/dotnet/framework/wcf/whats-wcf">WCF</a> does, or projects like <a href="https://swagger.io/">swagger</a>, <a href="https://thrift.apache.org/">thrift</a> for RPC, etc.  The downside here is that the generated code is usually pretty nasty and sometimes its hard to plug in to augment the clients with custom tracing, correlation tracking, etc. Other times the API itself might need a few nicety helper methods that you don't want to expose in the raw API itself. But in the auto generated world, you can't do this.

There are other projects like <a href="http://square.github.io/retrofit/">Retrofit</a> that look like they solve the problem since your client is just an interface and its only dependency is <a href="http://square.github.io/okhttp/">OkHttp</a>.  But retrofit isn't scala friendly (None's need custom support, default arguments in methods are not properly intercepted, etc).  You're also bound to the back-compat story of retrofit/okhttp, assuming that they can do things like make sure older versions live side by side together.

In practice, I found that retrofit (even with scala's issues) didn't work well in a distributed services environment where everyone was at wildly different versions of things.

<h1>Abstracting HTTP</h1>

However, taking the idea from retrofit we can abstract away http calls with an http driver. Http really isn't that complicated, especially for how its used in conjuction with service to service calls:

[scala]
import scala.concurrent.{ExecutionContext, Future}

case class ApiRequest(
  path: String,
  queryParams: Seq[(String, Option[String])] = Nil,
  headers: Seq[(String, Option[String])] = Nil,
  options: Option[RequestOptions] = None
) 

case class RequestOptions(
  contentType: Option[String],
  characterSet: String = &quot;utf-8&quot;
)

/**
 * A response with a body
 *
 * @param data     The deserialized data
 * @param response The raw http response
 * @tparam T The type to deserialize
 */
case class BodyResponse[T](data: T, response: RawResponse)

/**
 * A raw response that contains code, the body and headers
 *
 * @param code
 * @param body
 * @param headers
 */
case class RawResponse(code: Int, body: String, headers: Map[String, List[String]])

/**
 * An http error that all drivers should throw on non 2xx
 *
 * @param code  The code
 * @param body  An optional body
 * @param error The inner exception (may be driver specific)
 */
case class HttpError(code: Int, body: Option[String], error: Exception)
  extends Exception(s&quot;Error ${code}, body: ${body}&quot;, error)

/**
 * Marker trait indicating an http client
 */
trait HttpClient

/**
 * The simplest HTTP Driver. This is used to abstract libraries that call out over the wire.
 *
 * Anyone can create a driver as long as it implements this interface
 */
trait HttpDriver {
  val serializer: HttpSerializer

  def get[TRes: Manifest](
    request: ApiRequest
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]

  def post[TReq: Manifest, TRes: Manifest](
    request: ApiRequest,
    body: Option[TReq]
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]

  def put[TReq: Manifest, TRes: Manifest](
    request: ApiRequest,
    body: Option[TReq]
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]

  def patch[TReq: Manifest, TRes: Manifest](
    request: ApiRequest,
    body: Option[TReq]
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]

  def custom[TReq: Manifest, TRes: Manifest](
    method: Methods,
    request: ApiRequest,
    body: Option[TReq]
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]

  def delete[TRes: Manifest](
    request: ApiRequest
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]

  def bytesRaw[TRes: Manifest](
    method: Methods,
    request: ApiRequest,
    body: Option[Array[Byte]]
  )(implicit executionContext: ExecutionContext): Future[BodyResponse[TRes]]
}
[/scala]

Service owners who want to distribute a client can create clients that have no dependencies (other than the driver definition. Platform maintainers, like myself, can be dilligent about making sure the driver interface <em>never breaks</em>, or if it does is broken in a new namespace such that different versions can peacefully co-exist in the same process.

An example client can now look like

[scala]
class ServiceClient(driver: HttpDriver) {
  def ping()(implicit executionContext: ExecutionContext): Future[Unit] = {
    driver.get[Unit](&quot;/health&quot;).map(_.data)
  }
}
[/scala]

But we still need to provide an implementation of a driver. This is where we can decouple things and provide drivers that are properly tooled with all the fatness we want (netty/finagle/zipkin tracing/monitoring/etc) and service owners can bind their clients to whatever driver they want.  Those provided implementations can be in their own shared library that only service's bind to (not service clients! i.e. terminal endpoints in the dependency graph)

There are few advantages here:

<ul>
<li>Clients can be distributed at multiple scala versions without dependency conflicts</li>
<li>It's much simpler to version manage and back-compat an interface/trait than it is an entire lib</li>
<li>Default drivers that <em>do the right thing</em> can be provided by the service framework, and back compat doesn't need to be taken into account there since the only consumer is the service (it never leaks).  </li>
<li>Drivers are simple to use, so if someone needs to roll their own client its really simple to do it</li>
</ul>

<h1>Custom errors</h1>

We can do some other cool stuff now too, given we've abstracted away how to call http code.  Another common issue with clients is dealing with meaningful errors that aren't just the basic http 5xx/4xx codes.  For example, if you throw a 409 conflict you may want the client to actually receive a <code>WidgetInIncorrectState</code> exception for some calls, and in other calls maybe a <code>FooBarInUse</code> error that contains more semantic information. Basically overloading what a 409 means for a particular call/query.  One way of doing this is with a discriminator in the error body:

[code lang=text]
HTTP 409 response:
{
   &quot;code&quot;: &quot;WidgetInIncorrectState&quot;,
   &quot;widgetName: &quot;foo&quot;,
   &quot;widgetSize&quot;: 1234
}
[/code]

Given we don't want client code pulling in a json library to do json parsing, the driver needs to support <a href="https://github.com/FasterXML/jackson-docs/wiki/JacksonPolymorphicDeserialization">context aware deserialization</a>.

To do that, I've exposed a <code>MultiType</code> object that defines

<ul>
<li>Given a path into the json object, which field defines the discriminator</li>
<li>Given a discriminator, which type to deserialize to</li>
<li>Which http error code to apply all this too</li>
</ul>

And it looks like:

[code lang=scala]
/**
 * A type representing deserialization of multiple types.
 *
 * @param discriminatorField The field that represents the textual &quot;key&quot; of what the subtype is. Nested fields can be located using
 *                           json path format of / delimited. I.e /foo/bar
 * @param pathTypes          The lookup of the result of the discriminatorField to the subtype mapper
 * @tparam T The supertype of all the subtypes
 */
case class MultiType[T](
  discriminatorField: String,
  pathTypes: Map[String, SubType[_ &lt;: T]]
)

/**
 * Represents a subtype as part of a multitype mapping
 *
 * @param path The optional json sub path (slash delimited) to deserialize the type as.
 * @tparam T The type to deserialize
 */
case class SubType[T: Manifest](path: Option[String] = None) {
  val clazz = manifest[T].runtimeClass.asInstanceOf[Class[T]]
}
[/code]

Using this in a client looks like:

[code lang=scala]
class ServiceClient(driver: HttpDriver) {
  val errorMappers = MultiType[ApiException](discriminatorField = &quot;code&quot;, Map(
    &quot;invalidData&quot; -&gt; SubType[InvalidDataException]()
  ))

  def ping()(implicit executionContext: ExecutionContext): Future[Unit] = {
    driver.get[Unit](&quot;/health&quot;).map(_.data).failWithOnCode(500, errorMappers)
  }
}
[/code]

This is saying that when I get the value <code>invalidData</code> in the json response of field <code>code</code> on an http 500 error, to actually throw an <code>InvalidDataException</code> in the client.

How does this work? Well just like the http driver, we've abstracted the serializer and that's all plugged in by the service consumer

[code lang=scala]
case class DiscriminatorDoesntExistException(msg: String) extends Exception(msg)

object JacksonHttpSerializer {
  implicit def jacksonToHttpSerializer(jacksonSerializer: JacksonSerializer): HttpSerializer = {
    new JacksonHttpSerializer(jacksonSerializer)
  }
}

class JacksonHttpSerializer(jackson: JacksonSerializer = new JacksonSerializer()) extends HttpSerializer {
  override def fromDiscriminator[SuperType](multiType: MultiType[SuperType])(str: String): SuperType = {
    val tree = jackson.objectMapper.readTree(str)

    val node = tree.at(addPrefix(multiType.discriminatorField, &quot;/&quot;))

    val subType = multiType.pathTypes.get(node.textValue()).orElse(multiType.defaultType).getOrElse {
      throw new RuntimeException(s&quot;Discriminator ${multiType.discriminatorField} does not exist&quot;)
    }

    val treeToDeserialize = subType.path.map(m =&gt; tree.at(addPrefix(m, &quot;/&quot;))).getOrElse(tree)

    jackson.objectMapper.treeToValue(treeToDeserialize, subType.clazz)
  }

  override def toString[T](data: T): String = {
    jackson.toJson(data)
  }

  override def fromString[T: Manifest](str: String): T = {
    jackson.fromJson(str)
  }

  private def addPrefix(s: String, p: String) = {
    p + s.stripPrefix(p)
  }
}
[/code]

<h1>Inherent issues</h1>

While there are a lot of goodies in abstracting serialization and http calling into a library API provided with implementations (drivers), it does handicap the clients a little bit. Things like doing custom manipulation of the raw response, any sort of business logic, adding other libraries, etc is really frowned upon.  I'd argue this is a good thing and that this should all be handled at the service level since a client is always a <em>nice to have</em> and not a <em>requirement</em>.

<h1>Conclusion</h1>

The ultimate goal in SOA is separation. But 100% separation should not mean copy-pasting things, reinventing the wheel, or not sharing any code.  It just means you need to build the proper lightweight abstractions to help keep strong barriers between services without creating a distributed monolith.

With the http drive abstraction pattern it's now easy to provide drives that use finagle-http under the hood, or okhttp, or apache http, etc.  Client writers can share their model and client code with helpful utilities without leaking dependencies.  And most importantly, service owners can update dependencies and move to new scala versions without fearing that their dependencies are going to cause runtime or compile time issues against pulled in clients, all while still iterating quickly and safely.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4919</wp:post_id>
		<wp:post_date><![CDATA[2017-07-24 23:30:07]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-24 23:30:07]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[http-driver-pattern]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="library"><![CDATA[library]]></category>
		<category domain="post_tag" nicename="patterns"><![CDATA[patterns]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1557689075;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4939;}i:1;a:1:{s:2:"id";i:4945;}i:2;a:1:{s:2:"id";i:289;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>From Thrift to Finatra</title>
		<link>https://onoffswitch.net/2017/07/24/thrift-finatra/</link>
		<pubDate>Mon, 24 Jul 2017 23:36:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4939</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>Originally posted on the <a href="http://engineering.curalate.com/2017/07/05/from-thrift-to-finatra.html">curalate engineering blog</a></blockquote>

There are a million and one ways to do (micro-)services, each with a million and one pitfalls.  At Curalate, we've been on a long journey of splitting out our monolith into composable and simple services.  It's never easy, as there are a lot of advantages to having a monolith. Things like refactoring, code-reuse, deployment, versioning, rollbacks, are all atomic in a monolith.  But there are a lot of disadvantages as well.  Monoliths encourage poor factoring, bugs in one part of the codebase force rollbacks/changes of the entire application, reasoning about the application in general becomes difficult, build times are slow, transient build errors increase, etc.

To that end our first foray into services was built on top of Twitter <a href="https://twitter.github.io/finagle/">Finagle stack</a>.  If you go to the page and can't figure out what <em>exactly</em> finagle does, I don't blame you. The documentation is lackluster and in and of itself is quite low-level. Finagle defines a service as a function that transforms a request into a response, and composes services with filters that manipulate requests/responses themselves.  It's a clean abstraction, given that this is basically what all web service frameworks do.

<h1>Thrift</h1>

Finagle by itself isn't super opinionated. It gives you building blocks to build services (service discovery, circuit breaking, monitoring/metrics, varying protocols, etc) but doesn't give you much else.  Our first set of services built on finagle used Thrift over HTTP. <a href="https://Thrift.apache.org/">Thrift</a>, similiar to protobuf, is an intermediate declarative language that creates RPC style services. For example:

[code lang=text]
namespace java tutorial
namespace py tutorial

typedef i32 int // We can use typedef to get pretty names for the types we are using
service MultiplicationService
{
        int multiply(1:int n1, 2:int n2),
}
[/code]

Will create an RPC service called <code>MultiplicationService</code> that takes 2 parameters.  Our implementation at Curalate hosted Thrift over HTTP (serializing Thrift as JSON) since all our services are web based behind ELB's in AWS.

We have a lot of services at Curalate that use Thrift, but we've found a few shortcomings:

<h2>Model Reuse</h2>

Thrift forces you to use primitives when defining service contracts, which makes it difficult to share lightweight models (with potentially useful utilities) to consumers.  We've ended up doing a lot of mapping between generated Thrift types and shared model types.  Curalate's backend services are all written in Scala, so we don't have the same issues that a company like Facebook (who invented Thrift) may have with varying languages needing easy access to RPC.

<h2>Requiring a client</h2>

Many times you want to be able to interact with a service without needing access to a client.  Needing a client has made developers to get used to cloning service repositories, building the entire service, then entering a Scala REPL in order to interact with a service.  As our service surface area expands, it's not always feasible to expect one developer to build another developers service (conflicting java versions, missing SBT/Maven dependencies or settings, etc).  The client requirement has led to services taking heavyweight dependencies on other services and leaking dependencies.  While Thrift doesn't force you to do this, this has been a side effect of it taking extra love and care to generate a Thrift client properly, either by distributing Thrift files in a jar or otherwise.

<h2>Over the wire inspection</h2>

With Thrift-over-HTTP, inspecting requests is difficult. This is due to the fact that these services use Thrift serialization, which unlike JSON, isn't human-readable.

Because Thrift over HTTP is all POSTs to <code>/</code>, tracing access and investigating ELB logs becomes a jumbled mess of trying to correlate times and IP's to other parts of our logging infrastructure.  The POST issue is frustrating, because it's impossible for us to do any semantic smart caching, such as being able to insert caches at the serving layer for retrieval calls. In a pure HTTP world, we could insert a cache for heavily used GETs given a GET is idempotent.

<h2>RPC API design</h2>

Regardless of Thrift, RPC encourages poorly unified API's with lots of specific endpoints that don't always jive. We have many services that have method topologies that are poorly composable.  A well designed API, and cluster of API's, should gently guide you to getting the data you need.  In an ideal world if you get an ID in a payload response for a data object, there should be an endpoint to get more information about that ID.  However, in the RPC world we end up with a batch call here, a specific RPC call there, sometimes requiring stitching several calls to get data that should have been a simple domain level call.

<h2>Internal vs External service writing</h2>

We have lot of public REST API's and they are written using the Lift framework (some of our oldest code).  Developers moving from internal to external services have to shift paradigms and move from writing REST with JSON to RPC with Thrift.

Overall Thrift is a great piece of technology, but after using it for a year we found that it's not necessarily for us. All of these things have prompted a shift to writing REST style services.

<h1>Finatra</h1>

<a href="https://twitter.github.io/Finatra/">Finatra</a> is an HTTP API framework built on top of Finagle.  Because it's still Finagle, we haven't lost any of our operational knowledge of the underlying framework, but instead we can now write lightweight HTTP API's with JSON.

With Finatra, all our new services have <a href="http://swagger.io/">Swagger</a> automatically enabled so API exploration is simple. And since it's just plain JSON using <a href="https://www.getpostman.com/">Postman</a> is now possible to debug and inspect APIs (as well as viewing requests in <a href="https://www.charlesproxy.com/">Charles</a> or other proxies).

With REST we can still distribute lightweight clients, or more importantly, if there are dependency conflicts a service consumer can very quickly roll an HTTP client to a service.  Our ELB logs now make sense and our new API's are unified in their verbiage (GET vs POST vs PUT vs DELETE) and if we <em>want</em> to write RPC for a particular service we still <em>can</em>.

There are a few other things we like about Finatra. For those developers coming from a background of writing HTTP services, Finatra feels familiar with the concept of controllers, filters, unified test-bed for spinning up build verification tests (local in memory servers), dependency injection (via <a href="https://github.com/google/guice">Guice</a>) baked in, sane serialization using Jackson, etc.  It's hard to do the wrong thing given that it builds strong production level opinions onto Finagle. And thankfully those opinions are ones we share at Curalate!

We're not in bad company -- Twitter, <a href="http://making.duolingo.com/rewriting-duolingos-engine-in-scala">Duolingo</a>, and others are using Finatra in production.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4939</wp:post_id>
		<wp:post_date><![CDATA[2017-07-24 23:36:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-07-24 23:36:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[thrift-finatra]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="cross-post"><![CDATA[Cross Post]]></category>
		<category domain="post_tag" nicename="finatra"><![CDATA[finatra]]></category>
		<category domain="post_tag" nicename="http"><![CDATA[http]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="post_tag" nicename="services"><![CDATA[services]]></category>
		<category domain="post_tag" nicename="thrift"><![CDATA[thrift]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1554367279;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4919;}i:1;a:1:{s:2:"id";i:4945;}i:2;a:1:{s:2:"id";i:4991;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Design patterns</title>
		<link>https://onoffswitch.net/2017/08/14/design-patterns/</link>
		<pubDate>Mon, 14 Aug 2017 01:41:22 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4942</guid>
		<description></description>
		<content:encoded><![CDATA[I was asked by a coworker to help write up some simple examples for junior engineers explaining some of the gang of four design patterns in a simpler more digestable format.  I took a stab at this this weekend and figured I'd share it to anyone who stumbles here. It's hosted on github pages and available via github:

<a href="https://devshorts.github.io/design-patterns/">https://devshorts.github.io/design-patterns/</a>]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4942</wp:post_id>
		<wp:post_date><![CDATA[2017-08-14 01:41:22]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-08-14 01:41:22]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[design-patterns]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1558700931;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4394;}i:1;a:1:{s:2:"id";i:532;}i:2;a:1:{s:2:"id";i:4575;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tracing High Volume Services</title>
		<link>https://onoffswitch.net/2017/10/05/tracing-high-volume-services/</link>
		<pubDate>Thu, 05 Oct 2017 20:02:57 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4945</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>This post was originally posted at <a href="http://engineering.curalate.com/2017/09/26/tracing-services.html">engineering.curalate.com</a></blockquote>

We like to think that building a service ecosystem is like stacking building blocks.  You start with a function in your code. That function is hosted in a class. That class in a service. That service is hosted in a cluster. That cluster in a region. That region in a data center, etc. At each level there's a myriad of challenges.

From the start, developers tend to use things like logging and metrics to debug their systems, but a certain class of problems crops up when you need to debug <em>across</em> services. From a debugging perspective, you'd like to have a higher projection of the view of the system: a linearized view of what requests are doing. I.e. You want to be able to see that <code>service A</code> called <code>service B</code> and <code>service C</code> called <code>service D</code> at the granularity of single requests.

<h1>Cross Service Logging</h1>

The simplest solution to this is to require that every call from service to service comes with some sort of trace identifier. Incoming requests into the system, either from public API's or client side requests, or even from async daemon invoked timers/schedules/etc generates a trace. This trace then gets propagated through the entire system. If you use this trace in all your log statements you can now correlate <em>cross service</em> calls.

How is this accomplished at Curalate? For the most part we use Finagle based services and the Twitter ecosystem has done a good job of providing the concept of a <a href="https://docs.oracle.com/javase/7/docs/api/java/lang/ThreadLocal.html">thread local</a> <a href="https://twitter.github.io/finagle/docs/com/twitter/finagle/tracing/Trace$.html">TraceId</a> and automatically propagating it to all other twitter-* components (yet another reason we like <a href="/2017/07/05/from-thrift-to-finatra.html">Finatra</a>!).

All of our service clients automatically pull this thread local trace id out and populate a known HTTP header field that services then pick up and re-assume.  For Finagle based clients this is auto-magick'd for you. For other clients that we use, like <a href="http://square.github.io/okhttp/">OkHttp</a>, we had to add custom interceptors that pulled the trace from the thread local and set it on the request.

Here is an example of the header being sent automatically as part of Zipkin based headers (which we re-use as our internal trace identifiers):

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/finagle_trace_id.png" alt="finagle_trace_id" width="580" height="262" class="aligncenter size-full wp-image-4949" />

Notice the <code>X-B3-TraceId</code> header. When a service receives this request it'll re-assume the trace id and set its SLF4j <a href="https://logback.qos.ch/manual/mdc.html">MDC</a> field of <code>traceId</code> to be that value. We can now include in our logback.xml configuration to include the trace id like in our STDOUT log configuration below:

[code]
&lt;appender name=&quot;STDOUT-COLOR&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt;
    &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt;
        &lt;level&gt;TRACE&lt;/level&gt;
    &lt;/filter&gt;
    &lt;encoder&gt;
        &lt;pattern&gt;%yellow(%d) [%magenta(%X{traceId})] [%thread] %highlight(%-5level) %cyan(%logger{36}) %marker - %msg%n&lt;/pattern&gt;
    &lt;/encoder&gt;
&lt;/appender&gt;
[/code]

And we can also send the trace id as a structured JSON field to Loggly.

Let's look at an example from our own logs:

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/tid_example.png" alt="tid_example" width="2492" height="200" class="aligncenter size-full wp-image-4951" />

What we're seeing here is a system called <code>media-api</code> made a query to a system called <code>networkinformationsvc</code>. The underlying request carried a correlating trace id across the service boundaries and both systems logged to Loggly with the <code>json.tid</code> (transaction id) field populated. Now we can query our logs and get a linear time based view of what's happening.

<h1>Thread local tracing</h1>

The trick here is to make sure that this implicit trace id that is pinned to the thread local of the initiating request properly <em>moves</em> from thread to thread as you make async calls.  We don't want anyone to have to ever <em>remember</em> to set the trace. It should just gracefully flow from thread to thread <em>implicity</em>.

To make sure that traces hop properly between systems we had to make sure to enforce that everybody uses an <code>ExecutionContext</code> that safely captures the callers thread local's before executing. This is critical, otherwise you can make an async call and the trace id gets dropped. In that case, bye bye go the logs!  It's hyper important to always <em>take an execution context</em> and to never <em>pin an execution context</em> when it comes to async scala code. Thankfully, we can make any execution context <em>safe</em> by wrapping it up in a delegate:

[code lang=scala]
/**
 * Wrapper around an existing ExecutionContext that makes it propagate MDC information.
 */
class PropagatingExecutionContextWrapper(wrapped: ExecutionContext)
  extends ExecutionContext { self =&gt;

   override def prepare(): ExecutionContext = new ExecutionContext {
     // Save the call-site state
     private val context = Local.save()

     def execute(r: Runnable): Unit = self.execute(new Runnable {
       def run(): Unit = {
         // re-assume the captured call site thread locals
         Local.let(context) {
           r.run()
         }
       }
     })

     def reportFailure(t: Throwable): Unit = self.reportFailure(t)
   }

  override def execute(r: Runnable): Unit = wrapped.execute(r)

  override def reportFailure(t: Throwable): Unit = wrapped.reportFailure(t)
}

class TwitterExecutionContextProvider extends ExecutionContextProvider {
  /**
   * Safely wrap any execution context into one that properly passes context
   *
   * @param executionContext
   * @return
   */
  override def of(executionContext: ExecutionContext) = new PropagatingExecutionContextWrapper(executionContext)
}
[/code]

We've taken this trace wrapping concept and applied to all kinds of executors like <code>ExecutorService</code>, and <code>ScheduledExecutorService</code>. Technically we don't really want to expose the internals of how we wrap traces, so we load an <code>ExecutionContextProvider</code> via a java <a href="https://docs.oracle.com/javase/7/docs/api/java/util/ServiceLoader.html">service loading</a> mechanism and provide an API contract so that people can wrap executors without caring how they are wrapped:

[code lang=scala]
/**
 * A provider that loads from the java service mechanism
 */
object ExecutionContextProvider {
  lazy val provider: ExecutionContextProvider = {
    Option(ServiceLoader.load(classOf[ExecutionContextProvider])).
      map(_.asScala).
      getOrElse(Nil).
      headOption.
      getOrElse(throw new MissingExecutionContextException)
  }
}

/**
 * Marker interfaces to provide contexts with custom logic. This
 * forces users to make sure to use the execution context providers that support request tracing
 * and maybe other tooling
 */
trait ProvidedExecutionContext extends ExecutionContext

/**
 * A context provider contract
 */
trait ExecutionContextProvider {
  def of(context: ExecutionContext): ProvidedExecutionContext

  ...
}
[/code]

From a callers perspective they now do:

[code lang=text]
implicit val execContext = ExecutionContextProvider.provider.of(scala.concurrent.ExecutionContext.Implicits.global)
[/code]

Which would wrap, in this example, the default scala context.

<h1>Service to Service dependency and performance tracing</h1>

Well that's great! We have a way to safely and easily pass trace id's, and we've tooled through our clients to all pass this trace id automatically, but this only gives us <em>logging</em> information.  We'd really like to be able to leverage the trace information to get more interesting statistics such as service to service dependencies, performance across service hops, etc.  Correlated logs is just the beginning of what we can do.

Zipkin is an open source tool that we've discussed here <a href="http://engineering.curalate.com/2016/09/12/zipkin-at-curalate.html">before</a> so we won't go too much into it, but needless to say that Zipkin hinges on us having proper trace identifiers.  It samples incoming requests to determine IF things should be traced or not (i.e. sent to Zipkin). By default, we have all our services send 0.1% of their requests to Zipkin to minimize impact on the service.

Let's look at an example:

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/zipkin.png" alt="zipkin" width="2004" height="480" class="aligncenter size-full wp-image-4952" />

In this Zipkin trace we can see that this batch call made a call to Dynamo. The whole call took 6 milliseconds and 4 of those milliseconds were spent calling Dynamo.  We've tooled through all our external client dependencies with Zipkin trace information automatically using java dynamic proxies so that as we upgrade our external dep's we get tracing on new functions as well.

If we dig further into the trace:

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/zipkin_w_trace.png" alt="zipkin_w_trace" width="1696" height="1182" class="aligncenter size-full wp-image-4953" />

We can now see (highlighted) the trace ID and search in our logs for logs related to this trace

<h1>Finding needles in the haystack</h1>

We have a way to correlate logs, and get sampled performance and dependency information between services via Zipkin. What we still can't do yet is trace an individual piece of data flowing through high volume queues and streams.

Some of our services at Curalate process 5 to 10 thousand items a second.  It's just not fiscally prudent to log all that information to Loggly or emit unique metrics to our metrics system (DataDog). Still, we want to know at the event level where things are in the system, where they passed through, where they got dropped etc. We want to answer the question of

<blockquote>
  Where is identifier XYZ.123 in the system and where did it go and come from?
</blockquote>

This is difficult to answer with the current tools we've discussed.

<img src="https://media.giphy.com/media/3o7aTskHEUdgCQAXde/giphy.gif" alt="" />

To solve this problem we have one more system in play.  This is our high volume auditing system that lets us write and filter audit events at a large scale (100k req/s+).  The basic architecture here is we have services write audit events via an Audit API which are funneled to Kinesis Firehose. The firehose stream buffers data for either 5 minutes or 128 MB (whichever comes first).  When the buffer limit is reached, firehose dumps newline separated JSON in a flat fi`le into S3.  We have a lambda function that waits for S3 create events on the bucket, reads the JSON, then transforms the JSON events into <a href="https://parquet.apache.org/">Parquet</a> which is an efficient columnar storage format.  The Parquet file is written back into S3 into a new folder with the naming scheme of

[code lang=text]
year=YYYY/month=MM/day=DD/hour=HH/minute=mm/&lt;uuid&gt;.parquet
[/code]

Where the minutes are grouped in 5 minute intervals.  This partition is then added to Athena, which is a managed map-reduce around PrestoDB, that lets you query large datasets in S3.

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/auditing_arch.png" alt="auditing_arch" width="1262" height="1206" class="aligncenter size-full wp-image-4947" />

What does this have to do with trace id's?  Each event emitted comes with a trace id that we can use to query back to logs or Zipkin or other correlating identifiers.  This means that even if services aren't logging to Loggly due to volume restrictions, we can still see how events trace through the system. Let's look at an example where we find a specific network identifier from Instagram and see when it was data mined and when we added semantic image tags to it (via our vision APIs):

[sql]
SELECT minute, app, message, timestamp, context
FROM curalateauditevents.&quot;audit_events&quot;
WHERE context['network_id'] = '1584258444344170009_249075471' and context['network']='instagram'
and day=18 and hour=22
order by timestamp desc
limit 100
[/sql]

This is the Athena query.  We've included the specific network ID and network we are looking for, as well as a limited partition scope.

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/athena_query.png" alt="athena_query" width="1312" height="949" class="aligncenter size-full wp-image-4946" />

Notice the two highlights.

Starting at the second highlight there is a message that we augmented the piece of data. In our particular pipe we only augment data under specific circumstances (not every image is analyzed) and so it was important to see that some images were dropped and this one was augmented. Now we can definitely say "yes, item ABC was augmented but item DEF was not and here is why".  Awesome.

Moving upwards, the first highlight is how much data was scanned.  This particular partition we looked through has 100MB of data, but we only searched through 2MB to find what we wanted (this is due to the optimization of Parquet). Athena is priced by how much data you scan at a cost of $5 per terabyte. So this query was pretty much free at a cost of $0.000004. The total set of files across all the partitions for the past week is roughly 21GB spanning about 3.5B records.  So even if we queried <em>all</em> the data, we'd only pay $.04.  In fact, the biggest cost here isn't in storage or query or lambda, it's in firehose! Firehose charges you $0.029 per GB transferred.  At this rate we pay 60 cents a week. The boss is going to be ok with that.

However, there are <em>still</em> some issues here. Remember the target scale is upwards of 100k req/s.  At that scale we're dealing with a LOT of data through Kinesis Firehose.  That's a lot of data into S3, a lot of IO reads to transform to Parquet, and a lot of opportunities to accidentally scan through tons of data in our athena partitions with poorly written queries that loop over repeated data (even though we limit partitions to a 2 week TTL).  We also now have issues of rate limiting with Kinesis Firehose.

On top of that, some services just pump so much repeated data that its not worth seeing it all the time.  To that end we need some sort of way to do live filters on the streams.  What we've done to solve this problem is leverage dynamically invoked <a href="https://www.javaworld.com/article/2144908/scripting-jvm-languages/nashorn--javascript-made-great-in-java-8.html">Nashorn javascript</a> filters.  We load up filters from a known remote location at an interval of 30 seconds, and if a service is marked for filtering (i.e. it has a really high load and needs to be filtered) then it'll run all of its audit events through the filter <em>before</em> it actually gets sent to the downstream firehose.  If an event fails the filter it's discarded. If it passes, the event is annotated with which filter name it passed through and sent through the stream.

Filters are just YML files for us:

[code]
name: &quot;Filter name&quot;
expiration: &lt;Optional DateTime. Epoch or string datetime of ISO formats parseable by JODA&gt;
js: |
    function filter(event) {
        // javascript that returns a boolean
    }
[/code]

And an example filter may look like

[code]
name: &quot;anton_client_filter&quot;
js: |
    function filter(event) {
      var client = event.context.get(&quot;client_id&quot;)

      return client != null &amp;&amp; client == &quot;3136&quot;
    }
[/code]

In this filter only events that are marked with the client id of my client will pass through. Some systems don't need to be filtered so all their events pass through anyway.

Now we can write queries like

[sql]
SELECT minute, app, message, timestamp, context
FROM curalateauditevents.&quot;audit_events&quot;
WHERE contains(trace_names, 'anton_client_filter')
and day=18 and hour=22
limit 100
[/sql]

To get events that were tagged with my filter in the current partition. From there, we now can do other exploratory queries to find related data (either by trace id or by other identifiers related to the data we care about).

Let's look at some graphs that show how dramatic this filtering can be

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/filtering.png" alt="filtering" width="1220" height="422" class="aligncenter size-full wp-image-4948" />

Here the purple line is one of our data mining ingestion endpoints. It's pumping a lot of data to firehose, most of which is repeated over time and so isn't super useful to get all the input from. The moment the graph drops is when the yml file was uploaded with a filter to add filtering to the service. The blue line is a downstream service that gets data after debouncing and other processing. Given its load is a lot less we don't care so much that it is sending all its data downstream. You can see the purple line slow to a trickle later on when the filter kicks in and data starts matching it.

<h2>Caveats with Nashorn</h2>

Building the system out there were a few interesting caveats when using Nashorn in a high volume pipeline like this.

The first was that subtle differences in javascript can have <em>massive</em> performance impacts.  Let's look at some examples and benchmark them.

[code]
function filter(event) {
  var anton = {
    &quot;136742&quot;: true,
    &quot;153353&quot;: true
  }

  var mineable = event.context.get(&quot;mineable_id&quot;)

  return mineable != null &amp;&amp; anton[mineable]
}
[/code]

The <a href="http://openjdk.java.net/projects/code-tools/jmh/">JMH</a> benchmarks of running this code is

[code]
[info] FiltersBenchmark.testInvoke  thrpt   20     1027.409 ±      29.922  ops/s
[info] FiltersBenchmark.testInvoke   avgt   20  1484234.075 ± 1783689.007  ns/op
[/code]

What?? 29 ops/second

<img src="https://media.giphy.com/media/3ohhwH6yMO7ED5xc7S/giphy.gif" alt="" />

Let's make some adjustments to the filter, given our internal system loads the javascript into an isolated scope per filter and then re-invokes just the function <code>filter</code> each time (letting us safely create global objects and pay heavy prices for things once):

[code]
var anton = {
  &quot;136742&quot;: true,
  &quot;153353&quot;: true
}

function filter(event) {
  var mineable = event.context.get(&quot;mineable_id&quot;)

  return mineable != null &amp;&amp; anton[mineable]
}
[/code]

[code]
[info] FiltersBenchmark.testInvoke  thrpt   20  7391161.402 ± 206020.703  ops/s
[info] FiltersBenchmark.testInvoke   avgt   20    14879.890 ±   8087.179  ns/op
[/code]

Ah, much better! 206k ops/sec.

<img src="https://media.giphy.com/media/Tud8FymnIZtW8/giphy.gif" alt="" />

If we use java constructs:

[code]
function filter(event) {
  var anton = new java.util.HashSet();
  anton.add(&quot;136742&quot;)
  anton.add(&quot;153353&quot;)

  var mineable = event.context.get(&quot;mineable_id&quot;)

  return mineable != null &amp;&amp; anton.contains(mineable)
}
[/code]

[code]
[info] FiltersBenchmark.testInvoke  thrpt   20  5662799.317 ± 301113.837  ops/s
[info] FiltersBenchmark.testInvoke   avgt   20    41963.710 ±  11349.277  ns/op
[/code]

Even better! 301k ops/sec

Something is clearly up with the anonymous object creation in Nashorn.  Needless to say, benchmarking is important, especially when these filters are going to be dynamically injected into every single service we have.  We need them to be performant, sandboxed, and safe to fail.

For that we make sure everything runs its own engine scope in a separate execution context isolated from main running code and is fired off asynchronously to not block the main calling thread.  This is also where we have monitoring and alerting on when someone uploads a non-performant filter so we can investigate and mitigate quickly.

For example, the discovery of the poorly performing json object came from this alert:

<img src="http://onoffswitch.net/wp-content/uploads/2017/10/high_cpu.png" alt="high_cpu" width="1244" height="426" class="aligncenter size-full wp-image-4950" />

<h1>Conclusion</h1>

Tracing is hard and it's incredibly difficult to tool through after the fact if you start to build service architectures without this in mind from the get go.  Tooling trace identifiers through the system from the beginning sets you up for success in building more interesting debugging infrastructure that isn't always possible without that.  When building larger service ecosystems it's important to keep in mind how to inspect things at varying granularity levels.  Sometimes building custom tools to help inspect the systems is worth the effort, especially if they help debug complicated escalations or data inconsistencies.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4945</wp:post_id>
		<wp:post_date><![CDATA[2017-10-05 20:02:57]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-10-05 20:02:57]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tracing-high-volume-services]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="athena"><![CDATA[athena]]></category>
		<category domain="post_tag" nicename="aws"><![CDATA[aws]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="cross-post"><![CDATA[Cross Post]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="post_tag" nicename="services"><![CDATA[services]]></category>
		<category domain="post_tag" nicename="tracing"><![CDATA[tracing]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"253e337fd64d0ba1a8ad1c43a153ed8a";a:2:{s:7:"expires";i:1559320643;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4939;}i:1;a:1:{s:2:"id";i:4905;}i:2;a:1:{s:2:"id";i:4919;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Functors in scala</title>
		<link>https://onoffswitch.net/2017/11/08/functors-scala/</link>
		<pubDate>Wed, 08 Nov 2017 01:59:09 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4961</guid>
		<description></description>
		<content:encoded><![CDATA[A coworker of mine and I frequently talk about higher kinded types, category theory, and lament about the lack of unified types in scala: namely functors. A functor is a fancy name for a thing that can be mapped on.  Wanting to abstract over something that is mappable comes up more often than you think.  I don't necessarily care that its an Option, or a List, or a whatever.  I just care that it has a map.

We're not the only ones who want this. Cats, Shapeless, Scalaz, all have implementations of functor.  The downside there is that usually these definitions tend to leak throughout your ecosystem. I've written before about ecosystem and library management, and it's an important thing to think about when working at a company of 50+ people.  You need to think long and hard about putting dependencies on things. Sometimes you can, if those libraries have good versioning or back-compat stories, or if they expose lightweight API's with heavyweight bindings that you can separate out.

Often times these libraries aren't really well suited for large scale use and so you're forced to either replicate, denormalize, or otherwise hide away how those things come into play.

In either case, this post isn't about that. I just wanted to know how the hell those libraries did the magic.

Let me lay out the final product first and we'll break it down:

[scala]
trait Functor[F[_]] {
  def map[A, B](f: F[A])(m: A =&gt; B): F[B]
}

object Functor {
  implicit class FunctorOps[F[_], A](f: F[A])(implicit functor: Functor[F]) {
    def map[B](m: A =&gt; B): F[B] = {
      functor.map(f)(m)
    }
  }

  implicit def iterableFunctor[T[X] &lt;: Traversable[X]] = new Functor[T] {
    override def map[A, B](f: T[A])(m: A =&gt; B) = {
      f.map(m).asInstanceOf[T[B]]
    }
  }

  implicit def optionFunctor = new Functor[Option] {
    override def map[A, B](f: Option[A])(m: A =&gt; B) = {
      f.map(m)
    }
  }

  implicit def futureFunctor(implicit executionContext: ExecutionContext) = new Functor[Future] {
    override def map[A, B](f: Future[A])(m: A =&gt; B) = {
      f.map(m)
    }
  }
}
[/scala]

And no code is complete without a test...

[scala]
class Tests extends FlatSpec with Matchers {

  import com.curalate.typelevel.Functor
  import com.curalate.typelevel.Functor._

  private def testMaps[T[_] : Functor](functor: T[Int]): T[Int] = {
    functor.map(x =&gt; x + 1)
  }

  &quot;A test&quot; should &quot;run&quot; in {
    testMaps(List(1)) shouldEqual List(2)

    testMaps(Some(1): Option[Int]) shouldEqual Some(2)

    testMaps(None: Option[Int]) shouldEqual None

    testMaps(Set(1)) shouldEqual Set(2)

    Await.result(testMaps(Future.successful(1)), Duration.Inf) shouldEqual 2
  }
}
[/scala]

How did we get here? First if you look at the definition of functor again

[scala]
trait Functor[F[_]] {
  def map[A, B](f: F[A])(m: A =&gt; B): F[B]
}
[/scala]

We're saying that

<ol>
<li>Given a type F that contains some other unknown type (i.e. F is a box, like List, or Set)</li>
<li>Define a map function from A to B and give me back a type of F of B</li>
</ol>

The nuanced part here is that the map takes an instance of <code>F[A]</code>.  We need this to get all the types to be happy, since we have to specify somewhere that <code>F[A]</code> and <code>A =&gt; B</code> are paired together.

Lets make a functor for list, since that one is pretty easy:

[scala]
object Functor {
  implicit lazy val listFunctor = new Functor[List] {
    override def map[A, B](f: List[A])(m: A =&gt; B) = {
      f.map(m)
    }
  }
}
[/scala]

Now we can get an instance of functor from a <code>List[T]</code>

We could use it like this now:

[scala]
def listMapper(f: Functor[List[Int]])(l: List[Int])  = {
  f.map(l)(_ + 1)
}
[/scala]

But that sort of sucks. I don't want to know I have a list, that defeats the purpose of a functor!

What if we do

[scala]
def intMapper[T[_]](f: Functor[T[Int]])(l: T[Int])  = {
  f.map(l)(_ + 1)
}
[/scala]

Kind of better. Now I have a higher kinded type that doesn't care about what the box is.  But I still need to somehow <em>get</em> an instance of a functor to do my mapping.

This is where the <code>ops</code> class come in:

[scala]
implicit class FunctorOps[F[_], A](f: F[A])(implicit functor: Functor[F]) {
  def map[B](m: A =&gt; B): F[B] = {
    functor.map(f)(m)
  }
}
[/scala]

This guy says <em>given a container, and a functor for that container, here is a helpful map function</em>.  It's giving us an extension method on <code>F[A]</code> that adds <code>map</code>. You may wonder, well dont' all things we're mapping on already have a map function? And the answer is yes, but the compiler doesn't <em>know</em> that since we're dealing with only generics here!

Now, we can import our functor ops class and finally get that last bit to work:

[scala]
class Tests extends FlatSpec with Matchers {

  import com.curalate.typelevel.Functor
  import com.curalate.typelevel.Functor._

  private def testMaps[T[_] : Functor](functor: T[Int]): T[Int] = {
    functor.map(x =&gt; x + 1)
  }

  &quot;A test&quot; should &quot;run&quot; in {
    testMaps(List(1)) shouldEqual List(2)

    testMaps(Some(1): Option[Int]) shouldEqual Some(2)

    testMaps(None: Option[Int]) shouldEqual None

    testMaps(Set(1)) shouldEqual Set(2)

    Await.result(testMaps(Future.successful(1)), Duration.Inf) shouldEqual 2
  }
}
[/scala]

Pulling it all together, we're asking for a type of <code>T</code> that is a box of anything that has an implicit <code>Functor[T]</code> typeclass. We want to use the <code>map</code> method on the functor of <code>T</code> and that map method comes because we leverage the implicit <code>FunctionOps</code>.

It helps to think of <code>functor</code> not as an interface that a thing implements, but as a typeclass/extension of a thing.  I.e. in order to get a map, you have to wrap something.

Anyways, big thanks to Christian for helping me out.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4961</wp:post_id>
		<wp:post_date><![CDATA[2017-11-08 01:59:09]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-08 01:59:09]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[functors-scala]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="functional"><![CDATA[functional]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560455198;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:7777;}i:1;a:1:{s:2:"id";i:4919;}i:2;a:1:{s:2:"id";i:3565;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Sbt sonatypeRelease on Travis</title>
		<link>https://onoffswitch.net/2017/11/16/sbt-sonatyperelease-travis/</link>
		<pubDate>Thu, 16 Nov 2017 18:52:01 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4970</guid>
		<description></description>
		<content:encoded><![CDATA[I figured I'd drop a quick note here for anyone else running into an issue. If you are trying to do a sonatypeRelease via sbt 1.0.3 on travis and getting a

[code lang=text]
Credentials file /home/travis/.sbt/credentials does not exist
[/code]

Even though you are supplying your own inline creds, just drop in a fake creds file into that location:

[code lang=text]
realm=Sonatype Nexus Repository Manager
host=x.y.z
user=none
password=none
[/code]

Via

[code lang=text]
cp scripts/creds.fake /home/travis/.sbt/credentials
[/code]

And save yourself the 2 days of headache I have had :p]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4970</wp:post_id>
		<wp:post_date><![CDATA[2017-11-16 18:52:01]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2017-11-16 18:52:01]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[sbt-sonatyperelease-travis]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="sbt"><![CDATA[sbt]]></category>
		<category domain="category" nicename="snippets"><![CDATA[Snippets]]></category>
		<category domain="post_tag" nicename="travis"><![CDATA[travis]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"37550b67d263a3ce789993dc25046c5f";a:2:{s:7:"expires";i:1557076392;s:7:"payload";a:6:{i:0;a:1:{s:2:"id";i:4892;}i:1;a:1:{s:2:"id";i:4939;}i:2;a:1:{s:2:"id";i:4991;}i:3;a:1:{s:2:"id";i:4737;}i:4;a:1:{s:2:"id";i:4327;}i:5;a:1:{s:2:"id";i:4191;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tracking batch queue fanouts</title>
		<link>https://onoffswitch.net/2018/02/15/tracking-batch-queue-fanouts/</link>
		<pubDate>Thu, 15 Feb 2018 00:17:40 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4973</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  Edit: This code now exists at <a href="https://github.com/paradoxical-io/carlyle">https://github.com/paradoxical-io/carlyle</a>
</blockquote>

When working in any resilient distributed system invariably queues come into play. You fire events to be handled into a queue, and you can horizontally scale workers out to churn through events.

One thing though that is difficult to do is to answer a question of <em>when is a batch of events finished?</em> This is a common scenario when you have a single event causing a fan-out of other events.  Imagine you have an event called <code>ProcessCatalog</code> and a catalog may have N catalog items.  The first handler for <code>ProcessCatalog</code> may pull some data and fire N events for catalog items.  You may want to know when all catalog items are processed by downstream listeners for the particular event.

It may seem easy though right? Just wait for the last item to come through. Ah, but distributed queues are loosely ordered. If an event fails and retries what used to be the last event is no longer the last.

What about having a counter somewhere? Well the same issue arises on event failure. If an event decrements the counter <em>twice</em> (because it was retried) now the counter is no longer deterministic.

To do this thoroughly you need to track every individual item and ack it as its processed. That would work but it could be really costly. Imagine tracking 1B items some data store <em>for every batch</em>!

Lets pretend we go this route though, what else do we need to do? We need the concept of a batch, and items to track in the batch.  If we have a workflow of opening a batch (creating a new one), adding items to it, then sealing the batch (no more items can be added to it) we can safely determine if the batch is empty. Without the concept of a <code>close/seal</code> you can have situations where you open a batch, fire off N events, N events are consumed ack'd by downstream systems, then you close the batch.  When all N are ack'd by downstream systems they can't <em>know</em> that there are no more incoming items since the batch is still open. Only when the batch is closed can you tell if the batch has been fully consumed.  To that note, both the producer AND consumer need to check if the batch is closed. In the previous example, if all N items are ack'd before the batch is closed, when you go to close the batch it needs to return back that the batch is empty! In the other situation if the batch is closed, then all N items are ack'd the last item to be ack'd needs to return that the batch is empty.

Back to the problem of storing and transferring N batch item ID's though. What if instead of storing each item you leveraged a <a href="http://onoffswitch.net/bit-packing-pacman">bitfield</a> representing a set of items? Now instead of N items you only need N bits to logically track every item.  But you also may now need to send N logical ID's back to the client.  We can also get around that by knowing that anytime you add a batch of items to a batch, for  example, adding 1000 items to batch id 1, that this sub group batch can be inserted as a unique hash corresponding to a bitfield set to all 1's with 1000 bits (any extra bits set to 0 and ignored).

Returning to the client all you  need to send back is the <em>hash</em> and how many items are related to that hash.  Now determistic id's can be created <em>client side</em> that are of the form <code>batchId:hash:index</code>.  When a client goes back to ack a message (or batch of messages) the ID contains enough information to

<ol>
<li>Locate all other hashes related to the batch</li>
<li>Get the bitfield for the ash</li>
<li>Flip the appropriate bit represented by the index</li>
</ol>

Something maybe like this

[scala]
case class BatchItemGroupId(value: UUID) extends UuidValue

object BatchItemId {
  def apply(batchId: BatchId, batchItemGroupId: BatchItemGroupId, index: Long): BatchItemId = BatchItemId(s&quot;${batchId.value}:$batchItemGroupId:$index&quot;)

  def generate(batchId: BatchId, batchItemGroupId: BatchItemGroupId, numberOfItems: Int): Iterable[BatchItemId] = {
    Stream.from(0, step = 1).take(numberOfItems).map(idx =&gt; apply(batchId, batchItemGroupId, idx))
  }

  def generate(batchId: BatchId, batchItemGroupId: List[BatchItemGroupInsert]): Iterable[BatchItemId] = {
    batchItemGroupId.toStream.flatMap(group =&gt; generate(batchId, group.id, group.upto))
  }
}

case class BatchItemId(value: String) extends StringValue {
  val (batchId, batchItemGroupId, index) = value.split(':').toList match {
    case batch::hash::index::_ =&gt; (BatchId(batch.toLong), BatchItemGroupId(UUID.fromString(hash)), index.toLong)
    case _ =&gt; throw new RuntimeException(s&quot;Invalid batch item id format $value&quot;)
  }
}

case class BatchId(value: Long) extends LongValue
[/scala]

We also need an abstraction on top of a byte array that lets us toggle bits in it. It also lets you count how many bits are set. We'll need to know that so we can answer the question of "is this subgroup hash empty".  I've shown bitmasking but I can show it again <a href="https://gist.github.com/devshorts/df36b7f042d8f64df382efb1a43c898a">at this gist</a>.

Now that we have that, all our sql needs to do is given a subgroup batch, pull the bitfield, put the bitfield into a <code>BitGroup</code>, flip the appropriate bits, write the bitfield back. Then it needs to query all batch groups for a batch, pull their blobs, and count their bits to determine if the batch is pending or complete.

If we're smart about it and limit the bitfield in MySQL to be large enough to get compression, but small enough to minimize over the wire overhead... say 1000 bytes =~ 10k (which works out to be about 8000 bits) we can store quit a bit with quite a little!

Unfortunately MySQL doesn't support bitwise operations on blobs (not till MySQL 8 apparently) so you need to pull the blob, manipulate it, then write it back. But you can safely do this in a transaction if you select the row using <code>FOR UPDATE</code> which provides row level read locking.

One last thing to think about before we call it a day though.  There are all sorts of situations where batches can be opened but never closed. Imagine a client opens a batch, adds items to it, then dies.  It may retry and create a new batch and add new items, but there is effectively an orphaned batch. To make our queue batcher work long term we need some asynchronous cleanup. You arbitrarily decide that any non-closed batches with no activity for N days get automatically deleted. Same with any closed-batches with no activity (maybe at a different interval). This lets the system deal with batches/events that are never going to complete.

Package this all up into a service with an API and blamo! Efficient queue batch tracking!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4973</wp:post_id>
		<wp:post_date><![CDATA[2018-02-15 00:17:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-15 00:17:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tracking-batch-queue-fanouts]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="architecture"><![CDATA[architecture]]></category>
		<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"37550b67d263a3ce789993dc25046c5f";a:2:{s:7:"expires";i:1557076392;s:7:"payload";a:6:{i:0;a:1:{s:2:"id";i:4783;}i:1;a:1:{s:2:"id";i:4750;}i:2;a:1:{s:2:"id";i:4945;}i:3;a:1:{s:2:"id";i:1587;}i:4;a:1:{s:2:"id";i:532;}i:5;a:1:{s:2:"id";i:390;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Chaos monkey for docker</title>
		<link>https://onoffswitch.net/2018/02/16/chaos-monkey-docker/</link>
		<pubDate>Fri, 16 Feb 2018 23:47:25 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4978</guid>
		<description></description>
		<content:encoded><![CDATA[I work at a mostly AWS shop, and while we still have services on raw EC2, nearly all of our new development is on Amazon ECS in docker.  I like docker because it provides a unified unit of operation (a container) that makes it easy to build shared tooling regardless of language/application.  It also lets you reproduce your applications local in the same environment they run remote, as well as starting fast and deploying fast.

However, many services run on a shared ECS node in a cluster, and so while things like Chaos Monkey may run around turning nodes off it'd be nice to have a little less of an impact during working hours while still being able to stress recovery and our alerting.

This is actually pretty easy though with a little docker container we call <code>The Beast</code>.  All the beast does is run on a ECS Scheduled event every 15-30 minutes from 10am - 3pm PST (we have teams east and west coasts) and the beast kills a random container from whatever cluster node its on.  It doesn't do a lot of damage, but it does test your fault tolerance.

Here's The Beast:

[ruby]
#!/usr/bin/env ruby

require 'json'
require 'pp'

class Hash
  def extract_subhash(*extract)
    h2 = self.select{|key, value| extract.include?(key) }
    self.delete_if {|key, value| extract.include?(key) }
    h2
  end
end

puts &quot;UNLEASH THE BEAST!&quot;

ignore_image_regex = ENV[&quot;IGNORED_REGEX&quot;]

raw = &quot;[#{`docker ps --format '{{json .}}'`.lines.join(',')}]&quot;

running_services = JSON.parse(raw).map { |val| val.extract_subhash(&quot;ID&quot;, &quot;Image&quot;)}

puts running_services

puts &quot;Ignoring regex #{ignore_image_regex}&quot;

if ignore_image_regex &amp;&amp; ignore_image_regex.length &gt; 0
  running_services.delete_if {|value|
    /#{ignore_image_regex}/ === value[&quot;Image&quot;]
  }
end

if !running_services || running_services.length == 0
  puts &quot;No services to kill&quot;

  Process.exit(0)
end

puts &quot;Bag of services to kill: &quot;

to_kill = running_services.sample

puts &quot;Killing #{pp to_kill}&quot;

`docker kill #{to_kill[&quot;ID&quot;]}`

prng = Random.new

quips = [
    &quot;Dont fear the reaper&quot;,
    &quot;BEAST MODE&quot;,
    &quot;You been rubby'd&quot;,
    &quot;Pager doody&quot;
]

puts &quot;#{quips[prng.rand(0..quips.length-1)]}&quot;
[/ruby]

Beast supports a regex of ignored images (so critical images like the ecs_agent and itself) can be marked as ignore.  This can also be used to update the beast to allow it to ignore services temporarily/etc.

We deploy The Beast with terraform, the general task definition looks like:

[code]
[
  {
    &quot;name&quot;: &quot;the-beast&quot;,
    &quot;image&quot;: &quot;${image}:${version}&quot;,
    &quot;cpu&quot;: 10,
    &quot;memory&quot;: 50,
    &quot;essential&quot;: true,
    &quot;logConfiguration&quot;: {
        &quot;logDriver&quot;: &quot;awslogs&quot;,
        &quot;options&quot;: {
          &quot;awslogs-group&quot;: &quot;${log_group}&quot;,
          &quot;awslogs-region&quot;: &quot;${region}&quot;,
          &quot;awslogs-stream-prefix&quot;: &quot;the-beast&quot;
        }
    },
    &quot;environment&quot;: [
        {
          &quot;name&quot;: &quot;IGNORED_REGEX&quot;, &quot;value&quot;: &quot;.*ecs_agent.*|.*the-beast.*&quot;
        }
    ],
    &quot;mountPoints&quot;: [
        { &quot;sourceVolume&quot;: &quot;docker-socket&quot;, &quot;containerPath&quot;: &quot;/var/run/docker.sock&quot;, &quot;readOnly&quot;: true }
    ]
  }
]
[/code]

And the terraform:

[code]
resource &quot;aws_ecs_task_definition&quot; &quot;beast_rule&quot; {
  family = &quot;beast-service&quot;
  container_definitions = &quot;${data.template_file.task_definition.rendered}&quot;

  volume {
    name = &quot;docker-socket&quot;
    host_path = &quot;/var/run/docker.sock&quot;
  }
}

data &quot;template_file&quot; &quot;task_definition&quot; {
  template = &quot;${file(&quot;${path.module}/files/task-definition.tpl&quot;)}&quot;

  vars {
    version = &quot;${var.beast-service[&quot;version&quot;]}&quot;
    region = &quot;${var.region}&quot;
    image = &quot;${data.terraform_remote_state.remote_env_state.docker_namespace}/the-beast&quot;
    log_group = &quot;${var.log-group}&quot;
  }
}

resource &quot;aws_cloudwatch_event_target&quot; &quot;beast_scheduled_job_target&quot; {
  target_id = &quot;${aws_ecs_task_definition.beast_rule.family}&quot;
  rule = &quot;${aws_cloudwatch_event_rule.beast_scheduled_job.name}&quot;
  arn = &quot;${data.aws_ecs_cluster.default_cluster.id}&quot;
  role_arn = &quot;${data.aws_iam_role.ecs_service_role.arn}&quot;
  ecs_target {
    task_count = 1
    task_definition_arn = &quot;${aws_ecs_task_definition.beast_rule.arn}&quot;
  }
}

resource &quot;aws_cloudwatch_event_rule&quot; &quot;beast_scheduled_job&quot; {
  name = &quot;${aws_ecs_task_definition.beast_rule.family}&quot;
  description = &quot;Beast kills a container every 30 minutes from 10AM to 3PM PST Mon-Thu&quot;
  schedule_expression = &quot;cron(0/30 18-23 ? * MON-THU *)&quot;
  is_enabled = false
}

resource &quot;aws_cloudwatch_log_group&quot; &quot;beast_log_group&quot; {
  name = &quot;${var.log-group}&quot;
}
[/code]

We can log to cloudwatch and correlate back information if a service was killed by the best as well.  It's important to note that you need to mount the docker socket for beast to work, since it needs docker to run.  A sample dockerfile looks like:

[code]
FROM ubuntu:xenial

RUN apt-get update &amp;&amp; apt-get install -y ruby-full docker.io build-essential

RUN gem install json

ADD beast.rb /app/beast.rb

RUN chmod +x /app/beast.rb

ENTRYPOINT &quot;/app/beast.rb&quot;
[/code]

It's bare bones, but it works, and the stupid quips at the end always make me chuckle.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4978</wp:post_id>
		<wp:post_date><![CDATA[2018-02-16 23:47:25]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-02-16 23:47:25]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[chaos-monkey-docker]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="fault-tolerance"><![CDATA[fault tolerance]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="post_tag" nicename="service-oriented-architecture"><![CDATA[service oriented architecture]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561472557;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4673;}i:2;a:1:{s:2:"id";i:5000;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>AETR an open source workflow engine</title>
		<link>https://onoffswitch.net/2018/05/02/aetr-open-source-workflow-engine/</link>
		<pubDate>Wed, 02 May 2018 17:37:43 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4984</guid>
		<description></description>
		<content:encoded><![CDATA[For the past several years I've been thinking about the idea of an open source workflow execution engine.  Something like AWS workflow but simpler.  No need to upload python, or javascript, or whatever. Just call an API with a callback url, and when the API completes its step, callback to the coordinator with a payload. Have the coordinator then send that payload to the next step in the workflow, etc.

This kind of simplified workflow process is really common and I keep running into it at different places that I work at.  For example, my company ingests client catalogs to augment imagery with their SKU numbers and other metadata. However that ingestion process is really fragmented and asynchronous.  There's an ingestion step, following that there is a normalization step, then a processing step, then an indexing step, etc.  In the happy case everyone is hooked together with a queue pipeline where once ingestion is done it publishes a message to the normalizer, etc.   But what happens when you want to take the principal components of this pipeline and create an adhoc pipeline?  We don't necessarily want the ingestor to always write to the normalizer. It'd be great to be able to compose these steps into different step trees that can own their own flow.

This is what <a href="https://github.com/paradoxical-io/aetr">AETR</a> lets you do.

<h2>How it works</h2>

The primary building blocks in AETR are

<ol>
<li>A step tree</li>
<li>A run tree</li>
</ol>

A step tree is literally a tree structure that represents what a sequence of steps is.  Leaf nodes in the tree are all API actions, and parent nodes in the tree are either a sequential or a parallel parent.  What this means is you can have trees like this:

[code lang=text]
Sequential
 |- Sequential
    |- API1
    |- API2
 |- Parallel
    |- API3
    |- API4
[/code]

In this tree the root is sequential, which means its child nodes must run... sequentially.  The first child is also a sequential parent, so the ordering of this node is the execution of <code>API1</code> followed by <code>API2</code> when <code>API1</code> completes.  When that branch completes, the next branch can execute. That branch is parallel, so both <code>API3</code> and <code>API4</code> execute concurrently. When both complete, the final root node is complete!

Int the nomenclature of AETR when you go to run a step tree, it becomes a run tree. A run tree is the same tree as a step tree but includes information such as state, timing, inputs/outputs, etc. For example:

[code lang=scala]
case class Run(
  id: RunInstanceId,
  var children: Seq[Run],
  rootId: RootId,
  repr: StepTree,
  executedAt: Option[Instant] = None,
  completedAt: Option[Instant] = None,
  version: Version = Version(1),
  createdAt: Instant = Instant.now(),
  updatedAt: Instant = Instant.now(),
  var parent: Option[Run] = None,
  var state: RunState = RunState.Pending,
  var input: Option[ResultData] = None,
  var output: Option[ResultData] = None
)
[/code]

<h2>DB layer</h2>

Run trees are stored in a postgres DB and are easy to reconstitute from a storage layer. Since every row contains the root, we can in one DB call get all the nodes for a run tree and then rebuild the graph in memory based on parent/child links.

Step trees related to run trees are a bit more complicated to rebuild since step trees can point to other step trees.  To rebuild a step tree there's a step tree table which contains each individual step node as a row in the db. And there is also a table called <code>step_children</code> which relates a parent to its ordered set of children.  We need a children link instead of a parent link for the reason described above. Step trees can be modified to link to other trees, and they can be re-used in many composable steps. This means that there's no clear parent of a tree, since the action of <code>API1</code> can be re-used in many different trees.

Here's an example of rebuilding a step tree:

[code lang=scala]
def getStep(stepTreeId: StepTreeId): Future[StepTree] = {
    val idQuery = sql&quot;&quot;&quot;
                     WITH RECURSIVE getChild(kids) AS (
                       SELECT ${stepTreeId}
                       UNION ALL
                       SELECT child_id FROM step_children
                       JOIN getChild ON kids = step_children.id
                     )
                     SELECT * FROM getChild&quot;&quot;&quot;.as[StepTreeId]

    val nodesQuery = for {
      ids &lt;- idQuery
      treeNodes &lt;- steps.query.filter(_.id inSet ids).result
      treeChildren &lt;- children.query.filter(_.id inSet ids).result
    } yield {
      (treeChildren, treeNodes)
    }

    provider.withDB(nodesQuery.withPinnedSession).map {
      case (children, nodes) =&gt;
        val allSteps = composer.reconstitute(nodes, children)

        allSteps.find(_.id == stepTreeId).get
    }
  }
[/code]

We can use a recursive CTE in postgres to find all the children starting at a given tree id, then we can slurp those childrens identities and rebuild the graph in memory.

Storing the children in a separate table also has an advantage that parent are child aware. Why does this matter? Well AETR wouldn't be as useful as it is if all it did was strictly call API's. We need a way to <em>transform</em> payloads between calls and we need a way to <em>reduce</em> parallel calls into a singular output, so that nodes can be composed. This matters because assume that <code>API1</code> returns some json shape, and <code>API2</code> requires a different json shape as its input. If we hooked <code>API`` -&gt;</code>API2<code>directly it'd never work. There needs to be a _mapper_.  But mapping functions are only related to their relative placement in the graph. If we rehook</code>API1<code>-&gt;</code>API3` now it may need a different mapping function.  To that end you can't store mappers directly on step nodes themselves, it has to be on the <em>child</em> relationship.

On top of that we have the concept of reducers in AETR.  Parallel parents can take the result set of all their children and reduce the payloads into one result.

Lets look at a concrete example:

<img src="https://github.com/paradoxical-io/aetr/raw/master/wiki:img/root_tree_1.png" alt="null" />

Here's a root tree that does things in parallel and has some sequential sub nodes.

If we look at one of the parallel parents we can see how to reduce the data:

<img src="https://github.com/paradoxical-io/aetr/raw/master/wiki:img/parallel_parent.png" alt="" />

Much the same way we can see how to map data between nodes for sequential parents

<img src="https://github.com/paradoxical-io/aetr/raw/master/wiki:img/sequential_parent.png" alt="" />

Mappers and reducers are executed in a sandboxed nashorn engine.

<h2>Concurrency</h2>

It's important in AETR to make sure that as we work on these trees that concurrent access doesn't introduce race conditions. AETR internally supports some optimistic locking on the trees as well as atomic version updates to prevent any concurrency issues.

<h2>Example</h2>

Lets take a look at a full flow!

<img src="https://github.com/paradoxical-io/aetr/raw/master/wiki:img/demo.gif" alt="" />

In this example we run the full tree and we can see the inputs and outputs of the data as they are mapped, and finally reduced. When the entire tree is complete the root node of the tree contains the <em>final</em> data. In this way the root is the final result and can be used to programmatically poll the AETR api.

Give AETR a shot and please feel free to leave feedback here or in the github issues!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4984</wp:post_id>
		<wp:post_date><![CDATA[2018-05-02 17:37:43]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-02 17:37:43]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[aetr-open-source-workflow-engine]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="aetr"><![CDATA[aetr]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="postgres"><![CDATA[postgres]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560876621;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4699;}i:1;a:1:{s:2:"id";i:4737;}i:2;a:1:{s:2:"id";i:4394;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Debugging &quot;Maximum String literal length exceeded&quot; with scala</title>
		<link>https://onoffswitch.net/2018/05/11/debugging-maximum-string-literal-length-exceeded-scala/</link>
		<pubDate>Fri, 11 May 2018 01:02:19 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=4991</guid>
		<description></description>
		<content:encoded><![CDATA[Today I ran into a fascinating bug.  We use <a href="https://github.com/iheartradio/ficus">ficus</a> as a HOCON auto parser for scala. It works great, because parsing configurations into strongly typed case classes is annoying. Ficus works by using a macro to invoke implicitly in scope <code>Reader[T]</code> classes for data types and recursively builds the nested parser.

I went to create a test for a new custom field I added to our config:

[scala]
class ProductConfigTests extends FlatSpec {
  &quot;Configs&quot; should &quot;be valid in QA&quot; in {
    assert(ConfigLoader.verify(ProductsConfig, Environment.QA).isSuccess)
  }
}
[/scala]

Our config verifier just invokes the hocon parser and makes sure it doesn't throw an error. <code>ProductsConfig</code> has a lot of fields to it, and I recently added a new one.  Suddenly the test broke with the following error:

[code]
error] Error while emitting com/services/products/service/tests/ConfigTests
[error] Maximum String literal length exceeded
[error] one error found
[error] (server/test:compileIncremental) Compilation failed
[error] Total time: 359 s, completed May 10, 2018 4:56:02 PM
> test:compile
[info] Compiling 36 Scala sources to /Users/antonkropp/src/products/server/target/scala-2.12/test-classes...
java.lang.IllegalArgumentException: Maximum String literal length exceeded
	at scala.tools.asm.ByteVector.putUTF8(ByteVector.java:213)
	at scala.tools.asm.ClassWriter.newUTF8(ClassWriter.java:1114)
	at scala.tools.asm.ClassWriter.newString(ClassWriter.java:1582)
	at scala.tools.asm.ClassWriter.newConstItem(ClassWriter.java:1064)
	at scala.tools.asm.MethodWriter.visitLdcInsn(MethodWriter.java:1187)
	at scala.tools.asm.tree.LdcInsnNode.accept(LdcInsnNode.java:71)
	at scala.tools.asm.tree.InsnList.accept(InsnList.java:162)
	at scala.tools.asm.tree.MethodNode.accept(MethodNode.java:820)
	at scala.tools.asm.tree.MethodNode.accept(MethodNode.java:730)
[/code]

Wat?

I fired up <code>sbt -jvm-debug 5005</code> and attached to the compiler.

<img src="http://onoffswitch.net/wp-content/uploads/2018/05/maxstring1.png" alt="" width="1705" height="660" class="aligncenter size-full wp-image-4992" />

I can def see that there is some sort of class being written with a large const string. But why?  I'd never seen this before.

I went to another service that has a test exactly like this for its config and used <a href="http://www.benf.org/other/cfr/">cfr</a> to decompile the generated scala files:

[code]
antonkropp at combaticus in ~/src/curalate/queue-batcher/server/target/scala-2.12/test-classes/com/curalate/services/queuebatcher/service/tests (devx/minimize-io-calls)
$ ls
total 1152
-rw-r--r--  1 antonkropp  staff   47987 May 10 11:20 BatchTrackerTests.class
-rw-r--r--  1 antonkropp  staff  145293 May 10 13:34 BitGroupTests.class
-rw-r--r--  1 antonkropp  staff    3112 May  9 13:23 ConfigTests$$anon$1$$anon$2$$anon$3.class
-rw-r--r--  1 antonkropp  staff    3553 May  9 13:23 ConfigTests$$anon$1$$anon$2$$anon$4$$anon$5.class
-rw-r--r--  1 antonkropp  staff    5190 May  9 13:23 ConfigTests$$anon$1$$anon$2$$anon$4.class
-rw-r--r--  1 antonkropp  staff    3627 May  9 13:23 ConfigTests$$anon$1$$anon$2$$anon$6.class
-rw-r--r--  1 antonkropp  staff    3906 May  9 13:23 ConfigTests$$anon$1$$anon$2.class
-rw-r--r--  1 antonkropp  staff    4904 May  9 13:23 ConfigTests$$anon$1$$anon$7.class
-rw-r--r--  1 antonkropp  staff    4598 May  9 13:23 ConfigTests$$anon$1$$anon$8.class
-rw-r--r--  1 antonkropp  staff    5063 May  9 13:23 ConfigTests$$anon$1.class
-rw-r--r--  1 antonkropp  staff    3125 May  9 13:23 ConfigTests$$anon$17$$anon$18$$anon$19.class
-rw-r--r--  1 antonkropp  staff    3573 May  9 13:23 ConfigTests$$anon$17$$anon$18$$anon$20$$anon$21.class
-rw-r--r--  1 antonkropp  staff    5213 May  9 13:23 ConfigTests$$anon$17$$anon$18$$anon$20.class
-rw-r--r--  1 antonkropp  staff    3640 May  9 13:23 ConfigTests$$anon$17$$anon$18$$anon$22.class
-rw-r--r--  1 antonkropp  staff    3924 May  9 13:23 ConfigTests$$anon$17$$anon$18.class
-rw-r--r--  1 antonkropp  staff    4914 May  9 13:23 ConfigTests$$anon$17$$anon$23.class
-rw-r--r--  1 antonkropp  staff    4606 May  9 13:23 ConfigTests$$anon$17$$anon$24.class
-rw-r--r--  1 antonkropp  staff    5073 May  9 13:23 ConfigTests$$anon$17.class
-rw-r--r--  1 antonkropp  staff    3119 May  9 13:23 ConfigTests$$anon$9$$anon$10$$anon$11.class
-rw-r--r--  1 antonkropp  staff    3566 May  9 13:23 ConfigTests$$anon$9$$anon$10$$anon$12$$anon$13.class
-rw-r--r--  1 antonkropp  staff    5205 May  9 13:23 ConfigTests$$anon$9$$anon$10$$anon$12.class
-rw-r--r--  1 antonkropp  staff    3634 May  9 13:23 ConfigTests$$anon$9$$anon$10$$anon$14.class
-rw-r--r--  1 antonkropp  staff    3915 May  9 13:23 ConfigTests$$anon$9$$anon$10.class
-rw-r--r--  1 antonkropp  staff    4909 May  9 13:23 ConfigTests$$anon$9$$anon$15.class
-rw-r--r--  1 antonkropp  staff    4601 May  9 13:23 ConfigTests$$anon$9$$anon$16.class
-rw-r--r--  1 antonkropp  staff    5066 May  9 13:23 ConfigTests$$anon$9.class
-rw-r--r--  1 antonkropp  staff   87180 May  9 13:23 ConfigTests.class
-rw-r--r--  1 antonkropp  staff   69451 May  9 13:23 DbTests.class
-rw-r--r--  1 antonkropp  staff   12985 May  9 13:23 MysqlTests.class
-rw-r--r--  1 antonkropp  staff   68418 May 10 12:40 Tests.class
drwxr-xr-x  4 antonkropp  staff     128 May  9 13:23 db
drwxr-xr-x  9 antonkropp  staff     288 May  9 13:23 modules

$ java -jar ~/tools/cfr.jar ConfigTests.class
[/code]

1000 lines later, I can see

<img src="http://onoffswitch.net/wp-content/uploads/2018/05/maxstring2.png" alt="" width="1203" height="556" class="aligncenter size-full wp-image-4996" />

So something is putting in a large string of the configuration parser compiled into the class file.

I checked the ficus source code and its not it, so it must be something with the test.

Turns out <code>assert</code> is a macro from scalatest:

[scala]
def assert(condition: Boolean)(implicit prettifier: Prettifier, pos: source.Position): Assertion = macro AssertionsMacro.assert
[/scala]

Where the macro
[scala]
def assert(context: Context)(condition: context.Expr[Boolean])(prettifier: context.Expr[Prettifier], pos: context.Expr[source.Position]): context.Expr[Assertion] =
    new BooleanMacro[context.type](context, &quot;assertionsHelper&quot;).genMacro[Assertion](condition, &quot;macroAssert&quot;, context.literal(&quot;&quot;), prettifier, pos)

[/scala]
Is looking for an implicit position.

Position is from scalactic which comes with scalatest

[scala]
case class Position(fileName: String, filePathname: String, lineNumber: Int)

/**
 * Companion object for &lt;code&gt;Position&lt;/code&gt; that defines an implicit
 * method that uses a macro to grab the enclosing position.
 */
object Position {

  import scala.language.experimental.macros

  /**
   * Implicit method, implemented with a macro, that returns the enclosing
   * source position where it is invoked.
   *
   * @return the enclosing source position
   */
  implicit def here: Position = macro PositionMacro.genPosition
}
[/scala]

And here we can ascertain that the macro expansion of the ficus config parser is being captured by the position file macro and auto compiled into the <em>assert</em> statement!

Changing the test to be

[scala]
class ProductConfigTests extends FlatSpec {
  &quot;Configs&quot; should &quot;be valid in QA&quot; in {
    validate(ConfigLoader.verify(ProductsConfig, Environment.QA).isSuccess)
  }

  /**
   * This validate function needs to exist because this bug is amazing.
   *
   * `assert` is a macro from scalatest that automatically compiles the contextual source tree
   * into the assert, so that you can get line number and metadata context if the line fails.
   *
   * The ficus macro expander for ProductConfig is larger than 65k characters, which is normally fine
   * for code, however since scalatest tries to compile this &gt; 65k anonymous class tree as a _string_
   * it breaks the java compiler!
   *
   * By breaking the function scope and having the macro create a closure around the _validate_ block
   * it no longer violates the 65k static string constraint
   */
  private def validate(block: =&gt; Boolean): Unit = {
    assert(block)
  }
}
[/scala]

Now makes the test pass.  What a day.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>4991</wp:post_id>
		<wp:post_date><![CDATA[2018-05-11 01:02:19]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-11 01:02:19]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[debugging-maximum-string-literal-length-exceeded-scala]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="bug"><![CDATA[bug]]></category>
		<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560002911;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:2735;}i:1;a:1:{s:2:"id";i:4862;}i:2;a:1:{s:2:"id";i:4068;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>3606</wp:comment_id>
			<wp:comment_author><![CDATA[dearbaba]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[dearbaba@foxmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[101.89.64.240]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-10-12 06:54:01]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-10-12 06:54:01]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[see https://stackoom.com]]></wp:comment_content>
			<wp:comment_approved><![CDATA[0]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[a:2:{s:4:"time";d:1570863241.6718719005584716796875;s:5:"event";s:9:"check-ham";}]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Productionalizing ECS</title>
		<link>https://onoffswitch.net/2018/05/16/5000-2/</link>
		<pubDate>Wed, 16 May 2018 18:05:02 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=5000</guid>
		<description></description>
		<content:encoded><![CDATA[<blockquote>
  This post was originally posted on my company's engineering blog here: <a href="http://engineering.curalate.com/2018/05/16/productionalizing-ecs.html">http://engineering.curalate.com/2018/05/16/productionalizing-ecs.html</a>
</blockquote>

In January of last year we decided as a company to move towards containerization and began a migration to move onto <a href="https://aws.amazon.com/ecs/">AWS ECS</a>.  We pushed to move to containers, and off of AMI based VM deployments, in order to speed up our deployments, simplify our build tooling (since it only has to work on containers), get the benefits of being able to run our production code in a sandbox even locally on our dev machines (something you can't really do easily with AMI's), and lower our costs by getting more out of the resources we're already paying for.

However, making ECS production ready was actually quite the challenge. In this post I'll discuss:

<ul>
<li>Scaling the underlying ECS cluster </li>
<li>Upgrading the backing cluster images</li>
<li>Monitoring our containers</li>
<li>Cleanup of images, container artifacts</li>
<li>Remote debugging of our JVM processes</li>
</ul>

Which is a short summary of the things we encountered and our solutions, finally making ECS a set it and forget it system.

<h2>Scaling the cluster</h2>

The first thing we struggled with was how to scale our cluster.  ECS is a container orchestrator, analogous to Kubernetes or Rancher, but you still need to have a set of EC2 machines to run as a cluster. The machines all need to have the ECS Docker agent installed on it and ECS doesn't provide a way to automatically scale and manage your cluster for you.  While this has changed recently with the announcement of Fargate, Fargate's pricing makes it cost prohibitive for organizations with a lot of containers.

The general recommendation that AWS gave with ECS was to scale based on CPU reservation limit OR memory limit. There's no clear way to scale with a combination of the two, since auto scaling rules need to apply to a single CloudWatch metric or you face potential thrashing.

Our first attempt on scaling was to try and scale on container placement failures.  ECS logs a message when containers are unable to be placed due to constraints (not enough memory on the cluster, or not enough CPU reservation left), but there is no way to actually capture that event programmatically (see <a href="https://github.com/aws/amazon-ecs-agent/issues/1221">this github issue</a>).  The goal here was to not preemptively scale, but instead scale on actual pressure. This way we wouldn't be overpaying for machines in the cluster that aren't heavily used. However we had to discard this idea since it wasn't possible due to API limitations.

Our second attempt, and one that we have been using now in production, is to use an AWS Lambda function to monitor the memory and CPU reservation of the cluster and emit a compound metric to CloudWatch that we can scale on.  We set a compound threshold with the logic of:

<ol>
<li>If either memory or CPU is above the max threshold, scale up</li>
<li>Else if both memory and CPU are below the min, scale down. </li>
<li>Else stay the same</li>
</ol>

We represent a scale up event with a CloudWatch value of <code>2</code>, a scale down as value <code>0</code> and otherwise the nominal state as value <code>1</code>.

The code for that is shown below:

[scala]
package com.curalate.lambdas.ecs_scaling

import com.amazonaws.services.CloudWatch.AmazonCloudWatch
import com.amazonaws.services.CloudWatch.model._
import com.curalate.lambdas.ecs_scaling.ScaleMetric.ScaleMetric
import com.curalate.lambdas.ecs_scaling.config.ClusterScaling
import org.joda.time.DateTime
import scala.collection.JavaConverters._

object ScaleMetric extends Enumeration {
  type ScaleMetric = Value

  val ScaleDown = Value(0)
  val StaySame = Value(1)
  val ScaleUp = Value(2)
}

case class ClusterMetric(
  clusterName: String,
  scaleMetric: ScaleMetric,
  periodSeconds: Int
)

class MetricResolver(clusterName: String, cloudWatch: AmazonCloudWatch) {
  private lazy val now = DateTime.now()
  private lazy val start = now.minusMinutes(3)

  private val dimension = new Dimension().withName(&quot;ClusterName&quot;).withValue(clusterName)

  val periodSeconds = 60

  protected def getMetric(name: String): Double = {
    val baseRequest = new GetMetricStatisticsRequest().withDimensions(dimension)

    cloudWatch.getMetricStatistics(baseRequest.
      withMetricName(name).
      withNamespace(&quot;AWS/ECS&quot;).
      withStartTime(start.toDate).
      withEndTime(now.toDate).
      withPeriod(periodSeconds).
      withStatistics(Statistic.Maximum)
    ).getDatapoints.asScala.head.getMaximum
  }

  lazy val currentCpuReservation: Double = getMetric(&quot;CPUReservation&quot;)

  lazy val currentMemoryReservation: Double = getMetric(&quot;MemoryReservation&quot;)
}

class ClusterStatus(
  scaling: ClusterScaling,
  metricResolver: MetricResolver
) {

  protected val logger = org.slf4j.LoggerFactory.getLogger(getClass)

  def getCompositeMetric(): ClusterMetric = {
    logger.info(s&quot;CPU: ${metricResolver.currentCpuReservation}, memory: ${metricResolver.currentMemoryReservation}&quot;)

    val state =
      if (metricResolver.currentCpuReservation &gt;= scaling.CPU.max || metricResolver.currentMemoryReservation &gt;= scaling.memory.max) {
        ScaleMetric.ScaleUp
      }
      else if (metricResolver.currentCpuReservation &lt;= scaling.CPU.min &amp;&amp; metricResolver.currentMemoryReservation &lt;= scaling.memory.min) {
        ScaleMetric.ScaleDown
      } else {
        ScaleMetric.StaySame
      }

    ClusterMetric(scaling.name, state, metricResolver.periodSeconds)
  }
}

class CloudwatchEmitter(cloudWatch: AmazonCloudWatch) {
  def writeMetric(metric: ClusterMetric): Unit = {
    val cluster = new Dimension().withName(&quot;ClusterName&quot;).withValue(metric.clusterName)

    cloudWatch.putMetricData(new PutMetricDataRequest().
      withMetricData(new MetricDatum().
        withMetricName(&quot;ScaleStatus&quot;).
        withDimensions(cluster).
        withTimestamp(DateTime.now().toDate).
        withStorageResolution(metric.periodSeconds).
        withValue(metric.scaleMetric.id.toDouble)
      ).withNamespace(&quot;Curalate/AutoScaling&quot;))
  }
}
[/scala]

Wiring in our ECS cluster to autoscale on this metric value in our Terraform configuration looks like
[code]
resource &quot;aws_cloudwatch_metric_alarm&quot; &quot;cluster_scale_status_high_blue&quot; {
  count               = &quot;${var.autoscale_enabled}&quot;
  alarm_name          = &quot;${var.cluster_name}_ScaleStatus_high_blue&quot;
  comparison_operator = &quot;${lookup(var.alarm_high, &quot;comparison_operator&quot;)}&quot;
  evaluation_periods  = &quot;${lookup(var.alarm_high, &quot;evaluation_periods&quot;)}&quot;
  period              = &quot;${lookup(var.alarm_high, &quot;period&quot;)}&quot;
  statistic           = &quot;${lookup(var.alarm_high, &quot;statistic&quot;)}&quot;
  threshold           = &quot;${lookup(var.alarm_high, &quot;threshold&quot;)}&quot;
  metric_name         = &quot;ScaleStatus&quot;
  namespace           = &quot;Curalate/AutoScaling&quot;

  dimensions {
    ClusterName = &quot;${var.cluster_name}&quot;
  }

  alarm_description = &quot;High cluster resource usage&quot;
  alarm_actions     = [&quot;${aws_autoscaling_policy.scale_up_blue.arn}&quot;]
}

resource &quot;aws_cloudwatch_metric_alarm&quot; &quot;cluster_scale_status_low_blue&quot; {
  count               = &quot;${var.autoscale_enabled}&quot;
  alarm_name          = &quot;${var.cluster_name}_ScaleStatus_low_blue&quot;
  comparison_operator = &quot;${lookup(var.alarm_low, &quot;comparison_operator&quot;)}&quot;
  evaluation_periods  = &quot;${lookup(var.alarm_low, &quot;evaluation_periods&quot;)}&quot;
  period              = &quot;${lookup(var.alarm_low, &quot;period&quot;)}&quot;
  statistic           = &quot;${lookup(var.alarm_low, &quot;statistic&quot;)}&quot;
  threshold           = &quot;${lookup(var.alarm_low, &quot;threshold&quot;)}&quot;
  metric_name         = &quot;ScaleStatus&quot;
  namespace           = &quot;Curalate/AutoScaling&quot;

  dimensions {
    ClusterName = &quot;${var.cluster_name}&quot;
  }

  alarm_description = &quot;Low cluster resource usage&quot;
  alarm_actions     = [&quot;${aws_autoscaling_policy.scale_down_blue.arn}&quot;]
}

variable &quot;alarm_high&quot; {
  type = &quot;map&quot;

  default = {
    comparison_operator = &quot;GreaterThanThreshold&quot;
    evaluation_periods  = 4
    period              = 60
    statistic           = &quot;Maximum&quot;
    threshold           = 1
  }
}

variable &quot;alarm_low&quot; {
  type = &quot;map&quot;

  default = {
    comparison_operator = &quot;LessThanThreshold&quot;
    evaluation_periods  = 10
    period              = 60
    statistic           = &quot;Maximum&quot;
    threshold           = 1
  }
}
[/code]

We made our Lambda dynamically configurable by loading data from our configuration system and allowing us to onboard new clusters to monitor, and to dynamically tune the values of the thresholds.

You can see this in effect here:

<img src="http://onoffswitch.net/wp-content/uploads/2018/05/ecs_cloudwatch.png" alt="" width="711" height="224" class="aligncenter size-full wp-image-5001" />

<h2>Host draining and ECS rescheduling</h2>

This leads us to another problem. When the ASG goes to down-scale from a CloudWatch event, it puts the boxes into DRAINING. However, draining doesn't necessarily mean that existing services have been re-scheduled on other boxes!  It just means that connections are drained from the existing hosts, and that the ECS scheduler now needs to move the containers elsewhere. This can be problematic in that if you are down-scaling 2 hosts that are serving both of your HA containers, then you can now have a situation where your service is at 0 instances! To solve this, we wired up a custom ASG lifecycle hook that polls the draining machines and makes sure that the containers are fully stopped, and that the active cluster contains at  least the min running instances of each service (where a service defines its minimum acceptable threshold and its min allowed running instances). For example if a service can run at 50% capacity and its min is set to 20, then we need to verify that there are at <em>least</em> 10 active before we fully allow the box to be removed, giving the ECS scheduler time to move things around.

<h2>Cluster upgrades</h2>

Solving cluster scaling and draining just introduced the next question: how do we do zero downtime cluster upgrades?  Because we now have many services running on the cluster, the blast radius for failure is much higher. If we fail a cluster upgrade we could take many of the services at Curalate down with us.

Our solution, while not fully automated, is beautiful in its simplicity.  Leveraging the draining Lambda, we keep all our clusters grouped into ASGs labeled <code>blue</code> and <code>green</code>. To upgrade, we spin up the unused cluster with new backing AMI's and wait for it to be steady state. Then we tear down the old cluster and rely on the draining Lambda to prevent any race issues with the ECS scheduler.

Each time we need to do a cluster upgrade, either for security updates, base AMI changes, or other infrastructure wide sweeping changes, we do a manual toggle using Terraform to drive the base changes.

For example, our Terraform ECS cluster module in QA looks like this

[code]
module &quot;ecs_cluster_default_bg&quot; {
  source = &quot;github.com/curalate/infra-modules.git//aws-ecs-cluster-blue-green?ref=2018.03.07.20.09.12&quot;

  cluster_name                       = &quot;${aws_ecs_cluster.default.name}&quot;
  availability_zones                 = &quot;${data.terraform_remote_state.remote_env_state.availability_zones}&quot;
  environment                        = &quot;${var.environment}&quot;
  region                             = &quot;${var.region}&quot;
  iam_instance_profile               = &quot;${data.terraform_remote_state.remote_env_state.iam_instance_profile}&quot;
  key_name                           = &quot;${data.terraform_remote_state.remote_env_state.key_name}&quot;
  security_group_ids                 = &quot;${data.terraform_remote_state.remote_env_state.ecs_security_groups}&quot;
  subnet_ids                         = &quot;${data.terraform_remote_state.remote_env_state.public_subnet_ids}&quot;
  drain_hook_notification_target_arn = &quot;${module.drain_hook.notification_target_arn}&quot;
  drain_hook_role_arn                = &quot;${module.drain_hook.role_arn}&quot;
  autoscale_enabled                  = true

  root_volume_size = 50
  team             = &quot;devops&quot;

  blue = {
    image_id      = &quot;ami-5ac76b27&quot;
    instance_type = &quot;c4.2xlarge&quot;

    min_size         = 2
    max_size         = 15
    desired_capacity = 5
  }

  green = {
    image_id      = &quot;ami-c868b6b5&quot;
    instance_type = &quot;c3.2xlarge&quot;

    min_size         = 0
    max_size         = 0
    desired_capacity = 0
  }
}
[/code]

<h2>Monitoring with statsd</h2>

Curalate uses Datadog as our graph visualization tool and we send metrics to datadog via the dogstatsd agent that is installed on our boxes.  Applications emit UDP events to the dogstatsd agent which then aggregates and sends messages to datadog over TCP.

In the containerized world we had 3 options for sending metrics

<ol>
<li>Send directly from the app</li>
<li>Deploy all containers with a sidecar of statsd </li>
<li>Proxy messages to the host box and leave dogstatsd on the host</li>
</ol>

We elected for option 3 since option 1 makes it difficult to do sweeping upgrades and option 2 uses extra resources on ECS we didn't want.

However we needed a way to determistically write messages from a Docker container to its host.  To do this we leveraged the docker0 bridge network

[code]
# returns x.x.x.1 base ip of the docker0 bridge IP
get_data_dog_host(){
    # extracts the ip address from eth0 of 1.2.3.4 and splits off the last octet (returning 1.2.3)
    BASE_IP=`ifconfig | grep eth0 -A 1 | grep inet | awk '{print $2}' | sed &quot;s/addr://&quot; | cut -d. -f1-3`

    echo &quot;${BASE_IP}.1&quot;
}
[/code]

And we configure our apps to use this IP to send messages to.

<h2>Cleanup</h2>

One thing we chose to do was to volume mount our log folders to the host system for semi-archival reasons.  By mounting our application logs to the host, if the container crashed or was removed from Docker, we'd still have a record of what happened.

That said, containers are transient; they come and go all the time. The first question we had was "where do logs go?". What folder do we mount them to? For us, we chose to mount all container logs in the following schema:

[code]
/var/log/curalate/&lt;service-name&gt;/containers/&lt;constainer-sha&gt;
[/code]

This way we can back correlate the logs for a particular container in a particular folder.  If we have multiple instances of the same image running a host the logs don't stomp on each other.

We normally have a log rotator on our AMI boxes that handles long running log files, however in our AMI based deployments machines and clusters are immutable and singular. This means that as we do new deploys the old logs are removed with the box and only one service is allowed to sit on one EC2 host.

In the new world the infrastructure is immutable at the container level, not the VM level. So in this sense, the base VM also has a log rotator to rotate all the container logs, but we didn't account for the fact that services will start and stop and deploy hundreds of times daily leaving hundreds of rotated log files in stale folders.

After the first disk alert though we added the following cron script:

[code]
buntu@ip-172-17-50-242:~$ crontab -l
# Chef Name: container-log-prune
*/10 * * * * /opt/curalate/docker/prune.rb
# Chef Name: volume-and-image-prune
0 0 * * * docker images -q | xargs docker rmi &amp;&amp; docker system prune -f
[/code]

We have 2 things happening here, the first is a Ruby script that checks for running containers and then deletes all container IDs in the recursive log glob that aren't active anymore.  We run this once an hour.

[ruby]
#!/usr/bin/env ruby

require 'set'
require 'fileutils'
require 'English'

containers = `docker ps --format '{{.ID}}'`.split(&quot;\n&quot;).to_set

unless $CHILD_STATUS.success?
  puts 'Failed to query docker'
  exit 1
end

dirs = Dir.glob('/var/log/**/containers/*')

to_delete = dirs.reject do |d|
  (containers.include? File.basename(d))
end

to_delete.each do |d|
  puts &quot;Deleting #{d}&quot;

  FileUtils.rm_rf d
end
[/ruby]

The second script is pretty straightforward and we leverage the Docker system prune command to remove old volume overlay data, images that are unused, and any other system cleanup stuff.  We run this daily because we want to leverage the existing images that are already downloaded on a box to speed up autoscaling.  We're ok with taking a once daily hit to download the image base layers at midnight if necessary during a scaling event.

<h2>JMXMP</h2>

JMX is a critical tool in our toolbox here at Curalate as nearly all of our services and applications are written using Scala on the JVM.  Normally in our AMI deployments we can singularly control the ports that are open and they are determistic. If we open port 5555 it's always open on that box.  However when we start to have many services run on the same host, we need to leverage dockers dynamic port routing which makes knowing <em>which</em> port maps to what more difficult.

Normally this isn't really an issue, as services that do need to expose ports to either each other or the public are routed through an ALB that manages that for us. But JMX is a different beast.  JMX, in its ultimate wisdom, requires a 2 port handshake in order to connect. What this means is that the port you connect to on JMX is not the ultimate port you <em>communicate</em> over in JMX. When you make a JMX connection to the connection port it replies back with the communication port and you then communicate on that.

But in the world of dynamic port mappings, we can find the first port from the dynamic mapping, but there is no way for us to know the <em>second</em> port. This is because the container itself has no information about what its port mapping is, for all it knows its port is what it says it is!

Thankfully there is a solution using an extension to JMX called JMXMP.  With some research from <a href="https://github.com/frankgrimes97/jmxmp-java-agent">this blog post</a> we rolled a jmxmp java agent:

[java]
package com.curalate.agents.jmxmp;

import javax.management.MBeanServer;
import javax.management.remote.JMXConnectorServer;
import javax.management.remote.JMXConnectorServerFactory;
import javax.management.remote.JMXServiceURL;
import java.lang.instrument.Instrumentation;
import java.lang.management.ManagementFactory;
import java.net.Inet4Address;
import java.net.InetAddress;
import java.net.NetworkInterface;
import java.net.UnknownHostException;
import java.util.Collections;
import java.util.HashMap;
import java.util.Map;

public class Agent {
    public static void premain(String agentArgs, Instrumentation inst) {
        Boolean enableLogging = Boolean.valueOf(System.getProperty(&quot;javax.management.remote.jmx.enable_logging&quot;, &quot;false&quot;));

        Boolean bindEth0 = Boolean.valueOf(System.getProperty(&quot;javax.management.remote.jmx.bind_eth0&quot;, &quot;true&quot;));

        try {
            Map&lt;String, String&gt; jmxEnvironment = new HashMap&lt;String, String&gt;();

            jmxEnvironment.put(&quot;jmx.remote.server.address.wildcard&quot;, &quot;false&quot;);

            final String defaultHostAddress = (bindEth0 ? getEth0() : getLocalHost()).replace(&quot;/&quot;,&quot;&quot;);

            JMXServiceURL jmxUrl = new JMXServiceURL(System.getProperty(&quot;javax.management.remote.jmxmp.url&quot;, &quot;service:jmx:jmxmp://&quot; + defaultHostAddress + &quot;:5555&quot;));

            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();

            JMXConnectorServer jmxRemoteServer = JMXConnectorServerFactory.newJMXConnectorServer(jmxUrl, jmxEnvironment, mbs);

            if (enableLogging) {
                System.out.println(&quot;Starting jmxmp agent on '&quot; + jmxUrl + &quot;'&quot;);
            }

            jmxRemoteServer.start();
        }
        catch (Throwable e) {
            if (enableLogging) {
                e.printStackTrace();
            }
        }
    }

    private static String getEth0() {
        try {
            return Collections.list(NetworkInterface.getByName(&quot;eth0&quot;).getInetAddresses())
                              .stream()
                              .filter(x -&gt; !x.isLoopbackAddress() &amp;&amp; x instanceof Inet4Address)
                              .findFirst()
                              .map(Object::toString)
                              .orElse(&quot;127.0.0.1&quot;);
        }
        catch (Exception e) {
            return &quot;127.0.0.1&quot;;
        }
    }

    private static String getLocalHost() {
        try {
            return InetAddress.getLocalHost().getHostName();
        }
        catch (UnknownHostException e) {
            return &quot;127.0.0.1&quot;;
        }
    }
}
[/java]

That we bundle in all our service startups:
[code]
exec java -agentpath:/usr/local/lib/libheapster.so -javaagent:agents/jmxmp-agent.jar $JVM_OPTS $JVM_ARGS -jar
[/code]

JMXMP does basically the same thing as JMX, except it only requires <em>one</em> port to be open.  By standardizing our ports on port 5555 we can look up the 5555 port mapping in ECS and connect to it via JMXMP and get all our "favorite" JMX goodies (if you're doing JMX you're already in a bad place).

For full reference all our dockerized java apps have a main entrypoint that Docker executes which is shown below. This allows us some sane default JVM settings but also exposes a way for us to manually override any of the settings via the <code>JVM_ARGS</code> env var (which we can set during our Terraform deployments)

[code]
#!/usr/bin/env bash

HOST_IP=&quot;localhost&quot;

# Entrypoint for service start
main() {
    set_host_ip

    DATADOG_HOST=`get_data_dog_host`

    # location the fat jar
    BIN_JAR=`ls /app/bin/*.jar | head`

    LOG_PATH=&quot;/var/log/${HOSTNAME}&quot;

    mkdir -p ${LOG_PATH}
    mkdir -p /heap_dumps

    JVM_OPTS=&quot;&quot;&quot;
        -server \
        -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/heap_dumps \
        -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled \
        -Xmx512m -Xms512m \
        -XX:+ScavengeBeforeFullGC -XX:+CMSScavengeBeforeRemark \
        -Dsun.net.inetaddr.ttl=5 \
        -Dcom.sun.management.jmxremote.port=1045 \
        -Dcom.sun.management.jmxremote.authenticate=false \
        -Dcom.sun.management.jmxremote.ssl=false \
        -Dcontainer.id=${HOSTNAME} \
        -Dhostname=${HOST_IP} \
        -Dlog.service.output=${LOG_PATH}/service.log \
        -Dlog.access.output=${LOG_PATH}/access.log \
        -Dloggly.enabled=${LOGGLY_ENABLED} \
        -Ddatadog.host=${DATADOG_HOST} \
        -Ddatadog.defaultTags=application:${CLOUD_APP}
    &quot;&quot;&quot;

    exec java -agentpath:/usr/local/lib/libheapster.so -javaagent:agents/jmxmp-agent.jar $JVM_OPTS $JVM_ARGS -jar ${BIN_JAR} $@
}

# Set the host IP variable of the EC2 host instance
# by querying the EC2 metadata api
# if there is no response after the timeout we'll default to localhost
set_host_ip () {
    if [ &quot;200&quot; == &quot;$(/usr/bin/curl --connect-timeout 2 --max-time 2 -s -o /dev/null -w &quot;%{http_code}&quot; 169.254.169.254/latest/meta-data/local-ipv4)&quot; ]
    then
        HOST_IP=$(curl 169.254.169.254/latest/meta-data/local-ipv4)
    else
        HOST_IP=&quot;$(hostname)&quot;

        if [ &quot;${HOST_IP}&quot; = &quot;${HOSTNAME}&quot; ]
        then
            HOST_IP=&quot;localhost&quot;
        fi
    fi
}

# returns x.x.x.1 base ip of the docker0 bridge IP
get_data_dog_host(){
    # extracts the ip address from eth0 of 1.2.3.4 and splits off the last octet (returning 1.2.3)
    BASE_IP=`ifconfig | grep eth0 -A 1 | grep inet | awk '{print $2}' | sed &quot;s/addr://&quot; | cut -d. -f1-3`

    echo &quot;${BASE_IP}.1&quot;
}

# execute main
main $@
[/code]

<h2>Choosing how to group a cluster</h2>

One thing we wrestled with is how to choose <em>where</em> a service will go.  For the most part we have a <code>default</code> cluster comprised of <code>c4.2xl</code>'s that everyone is allowed to deploy to.

I wanted to call out that choosing what service goes on what cluster and what machine types comprise a cluster can be tricky. For our GPU based services, the choice is obvious in that they go onto a cluster that has GPU acceleration.  For other clusters we tried smaller machines with fewer containers, and we tried larger machines with more containers.  We found that we preferred fewer larger machines since most of our services are not running at full capacity, so they get the benefit of extra IO and memory without overloading the host system. With smaller boxes we had less headroom and it was more difficult to pack services with varying degrees of memory/CPU reservation necessities.

On that note however, we've also chosen to segment some high priority applications onto their own clusters. These are services that under no circumstances can fail, or require more than average resources (whether IO or otherwise), or are particularly unstable.  While we don't get the cost savings by binpacking services onto that cluster, we still get the fast deploy/rollback/scaling properties with containers so we still consider it a net win.

<h2>Conclusion</h2>

ECS was really easy to get started on, but as with any production system there are always gotcha's.  Overall we're really pleased with the experience, even though it wasn't pain free.  In the end, we can now deploy in seconds, rollback in seconds, and still enjoy a pseudo immutable infrastructure that is simple to reason about as well as locally reproducible!]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5000</wp:post_id>
		<wp:post_date><![CDATA[2018-05-16 18:05:02]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-05-16 18:05:02]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[5000-2]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="category" nicename="cross-post"><![CDATA[Cross Post]]></category>
		<category domain="post_tag" nicename="ecs"><![CDATA[ecs]]></category>
		<category domain="post_tag" nicename="ruby"><![CDATA[ruby]]></category>
		<category domain="post_tag" nicename="scala"><![CDATA[scala]]></category>
		<category domain="post_tag" nicename="terraform"><![CDATA[terraform]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1560048767;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4851;}i:1;a:1:{s:2:"id";i:4978;}i:2;a:1:{s:2:"id";i:4699;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_syntaxhighlighter_encoded]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_dont_email_post_to_subs]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>1</title>
		<link>https://onoffswitch.net/2018/09/17/__trashed/</link>
		<pubDate>Mon, 17 Sep 2018 13:43:04 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=5176</guid>
		<description></description>
		<content:encoded><![CDATA[1]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>5176</wp:post_id>
		<wp:post_date><![CDATA[2018-09-17 13:43:04]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2018-09-17 13:43:04]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[__trashed]]></wp:post_name>
		<wp:status><![CDATA[trash]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wp_desired_post_slug]]></wp:meta_key>
		<wp:meta_value><![CDATA[]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>8 months of go</title>
		<link>https://onoffswitch.net/2019/04/01/8-months/</link>
		<pubDate>Mon, 01 Apr 2019 19:34:55 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=7777</guid>
		<description></description>
		<content:encoded><![CDATA[For the past 8 months I've primarily been writing in go.  I was hesitant to take a job that used go as its primary language for a lot of reasons, but I decided to give it a try because a lot of companies these days are using it, and it doesn't hurt to broaden my skillset.  In this post I'll describe the pros and cons of using go from my own experience.

First, I came at go with a skeptical and kind of disdainful eye which is probably not a good way to start.  But having just spent the last two and a half years writing scala the thought of leaving behind <code>[T]</code> (and even more so <code>T[Y]</code>) hurt.  That said, the experience after eight months has been relatively positive.   In this post I outline some of the things I like and don't like about go.

<h2>Let's start with the good things.</h2>

<b>Single binary</b>

Distributing a single binary in the JVM world is a surprisingly difficult task.  You can do fat jars, but you still need to shade things, rewrite embedded manifest files, etc. It's totally non trivial. On top of that, the fat jar isn't executable. To create an actual executable you need to bundle more manifest stuff, and then do even more magic to make it runnable.

In contrast, go builds are native binaries that are statically linked and are easy to distribute. Cross building is as easy as providing the target arch as a flag to the compiler with <code>GOARCH=&lt;foo&gt;</code> and you're done.  I really like this because it makes distributing these binaries super easy. You can load them into containers, or just drop them onto remote boxes.  They're also quite small, most of my applications are 10MB or less. Contrast with a lot of my JVM apps which fully shaded and packaged up were hundreds of MB in size.

<b>Built in testing</b>

The built in testing support for go is pretty good. It's not the <em>best</em> but it's more than enough to satisfy just about all your needs.  It's nice that there is just "one way" to do it, and all the tooling supports this way. There are better assertion libraries (like testify) but for the most part it's batteries included.  Extra bonus points for having property checking built into the main library, though I prefer another library (gopter) but the fact that it's even in the standard lib means that it has been given thought and value.

<b>Fast and low resource usage</b>

Go is just fast.  Because it compiles to native there isn't any runtime startup cost, it's garbage collected, and in general uses very low resources.  We recently did some comparisons of a simple CRUD app that read from mongo at my work of a version written in ruby (yes we know it will already be slow) compared to go, and the numbers are just hilarious. A single go app supported 100x the throughput at 1/10 the resources.

This can be a really big deal (low heap, low cpu, etc) when you are moving to serverless or containerized systems. A big issue we had at my last place with ECS was that each container needed at least a GB of RAM to satisfy the JVM heap requirements. This meant that even though many services used very low cpu (they didn't do a whole lot) we couldn't pack that many onto a single box. Had we written them in go however, we could have probably hosted our entire QA environment on 3 <code>c4.large</code>'s.  That actually boils down to cost savings for a company.

<b>Good IDE integration</b>

I'm a huge fan of tooling. I am way too lazy and way too stupid to muck with VIM or EMACS or any other pseudo-IDE in order to get autocomplete, semantic refactoring/nav, debugging, etc.  The go ecosystem comes with language servers, formatting tools, lots of built in stuff that nearly all IDE's can easily integrate with to get a really good developer experience.

While I don't like console based "IDE"s, many of my coworkers do, so it's nice to have the flexibility for everyone on a team to write in what they want to write in, but still get all the same consistent baseline tooling. This means everything looks and feels and writes the same with minimal fuss.

<b>Easy concurrency</b>

I like concurrency a lot, and I have no problems with JVM/CLR style concurrency models. I don't find threads daunting like many do. However, having built in concurrency via channels is pretty nice.  It reminds me a lot of f#'s mailbox processor model (aka a super simple akka).  Channels can be buffered, unbuffered, multi consume, multi publish, and it's all threadsafe.

There are still a ton of weird gotcha's with channels, but overall it does make it really easy to fire things off into an async goroutine.

On top of that, the concurrent primitives such as semaphores, mutexes, etc, are all really use to consume.  It's simple to create a critical section and then release it at the end of a method (with a defer) or to create a counting waitgroup to synchronize a known set of items.  Overall this is really an improvement over some other languages lock support.

<b>NewType support</b>

Finally, something for the functional person.  Scala supports type aliases but it's not compile time checked.  To do newtype's in scala you had to jump through a lot of hoops wrapping case classes for primitive values, and even then it didn't really work for complex types. On top of that serialization became a real hassle in scala when using newtypes (or as I call it in previous blog posts "tiny types").

Go supports compile time checked newtype support out of the box. It's fantastic.

[code lang=text]
type Name string

func UsesName(n Name) {}

func PassesName(raw string) {
   UsesName(Name(raw))
}

func UnboxesName(n Name) string {
   return string(n)
}
[/code]

Pretty nice! You can use it to alias even complex types to create simpler type signatures. I like it a lot for aliasing functional signatures like

[code lang=text]
type X struct{}
type Configuration func(X)
[/code]

<b>Explicit error contracts</b>

Go takes a lot of flack for how it handles errors

[code lang=text]
func ReturnsError() error {}

func ConsumesError() {
   err := ReturnsError()
   if err != nil {
     // now what
   }
}
[/code]

On the one hand, your code is absolutely <em>littered</em> with <code>if err != nil</code> checks, and that affects readability.  However, I really do like being able to check for errors explicitly (or ignoring them!) and being forced to propagate them back to the root caller.  The error contract is explicit without being obnoxious like in Java (I hated checked exceptions).

<b>First class functions</b>

Being able to do first class functions is super nice.  Not much to say here given that nearly every language supports this, but it's good to have.

<h2>And now the bad things</h2>

<b>Statics and package visibility</b>

In go the package visibility is defined by the capitalization of a method/variable. For example <code>Foo</code> is public but <code>foo</code> is private.  But, all values within a package (i.e any folder) are visible to each other. Which means that you can have different structs, interfaces, methods, whatever and they can reach into every private method or value anywhere within a package.

This leads to a really weak level of isolation and encourages less discipled developers to reach into privates willy nilly.

On top of that, because go encourages package level compartmentalization it's deemed "OK" to expose many static methods on a package. However, this often times encourages people to use global mutable variables as state and closes off extension and testability.  Statics, in my mind, are only acceptable if they are purely immutable.

<b>Sometimes inheritance can be good</b>

"Composition over inheritance" is something the engineering community has been beating down for years and go finally just made it the default. While in general I totally agree that composition is better than inheritance, sometimes inheritance <em>can</em> be incredibly useful.  It's not always possible to create the abstractions you want strictly with composition.  Because of this you can see that people circumvent this limitation by either creating mutable function variables on classes (so they can "override" methods) or they copy and paste entire swaths of code with needless duplication because they can't find a way to abstract the commonalities.

I understand the reasoning for the hard line of no inheritance, especially when it comes to variance, diamond problems, deep inheritance hierarchy smells, etc, but it's frustrating to lose this extremely powerful and valid tool in the toolbox.

<b>No generics</b>

No surprises here, no generics == sadness.  Without the capacity for true generics you can't create typechecked container classes. You can't do <code>Set[T]</code> you can't do <code>Stack[T]</code> you can't do <code>Publisher[T]</code>.  So many real world scenarios require generics it's a little unfathomable that they were omitted.

The common workaround is to just do untyped <code>interface{}</code> (aka <code>Object</code> in the JVM) and then do runtime casting.  While that does work, it's incredibly hard to reason about, and introduces weird edge errors at runtime which is exactly the reason you use a statically typed language to avoid!

The most infuriating thing about this is that generics <em>do</em> exist, but only in some standard library special methods like <code>append</code> and <code>map[T]Y</code>. Clearly the authors know that generics are useful, but chose not to expose that to the user.  I might have been less offended with generics if the data structure support was more robust.  For example, I want things like sets, stacks, trees, circular buffers, etc.  Having to write those yourself using untyped generics is a nightmare.

One of the workarounds is to use auto generated code to created typed versions of these generic collections and while it does work but the whole idea of that is just insanely jankey.

<b>Poor data transformation support</b>

If you want to get carpal tunnel, use go and try to map one data structure to another.  In scala you can do

[code lang=text]
val x = List(1, 2, 3) // List[Int]
val strings = x.map(_.toString) // List[String]
[/code]

Great, I've mapped a list of integers to a list of strings.

In go I can do this:

[code lang=text]
x := []int { 1, 2, 3 }
var strings []string
for _, i := range x {
   strings = append(string, strconv.Itoa(i))
}
[/code]

To me that's way less readable, full of noise, is mutable, uses magic extra functions and is 4x the length.  This is just a trivial example at that. Imagine you have a list of structs with fields within them that need extraction.

Any kind of data transformation in go is done with a for loop.  It's certainly a hammer and it does work. But it's inelegant, verbose, and frankly hard on the hands. As an engineer I type for a living and I'm well aware that sometimes verbosity is better, but in this scenario it adds nothing and frankly just pisses me off.

If we had generics we could write map functions outside of the stdlib but we can't, so we're forced to write things the go way.

<b>Godoc by convention</b>

I kind of hate the godoc convention of <code>MethodName &lt;doc&gt;</code>, for example

[code lang=text]
// Foo is a bar
func Foo
[/code]

In theory it's elegant and simple. But in practice it lacks support for properly documenting arguments, and not all items easily fit into that kind of grammatical verbiage. On top of that relying on the magic string instead without any extra structured annotations or formatting wreaks havoc on IDE refactoring support.

It's a minor gripe, but it grates on me.

<b>Duck typing</b>

Duck typing could have been in the good things, but I put it in the bad things if only because it causes refactoring and analysis to be painful.

What exactly is the duck typing in go? Well you can never actually declare that a struct adheres to an interface contract. If a struct has the same signatures of an interface it implicitly adheres to the contract. For example:

[code lang=text]
type Person interface {
   Name() string
}

type Dog struct {}
func (Dog) Name string {
   return &quot;fido&quot;
}

type Human struct{}
func Human Name string {
   return &quot;Bob&quot;
}
[/code]

Both dog and human adhere to the <code>Person</code> interface because they both satisfy the interfaces method signatures: <code>Name() string</code>.

Ok, that can be cool. That means that I can have types decoupled from the contracts in different packages.

But, in practice I hardly ever do this.  I more often do explicitly want to say that a Human is a Person and that when I rename <code>Name</code> on the <code>Person</code> interface I want <code>Name</code> renamed on the <code>Human</code> struct. Seems easy right? But semantically most tooling <em>cannot do this for you</em> because it can't <em>know</em> that the intention is to maintain the <code>Human : Person</code> relationship.

Often times I end up writing things like

[code lang=text]
var _ Human = Person {}
[/code]

Which forces the compiler to do a type check that a person always satisfies the human relationship. This way if I make any mistakes (especially when writing a library that may not have direct usages of every struct!) that my objects are still constrained by the contracts I've defined.

<b>No anonymous interfaces</b>

Not being able to create anonymous interfaces makes testing hard.  In the JVM you can do things like

[code lang=text]
interface Foo {
   Name() String
}

class TestFoo {
   @Test
   def MocksFoo() = {
      val fakeFoo = new Foo {
         override Name string {
              &quot;yay tests!&quot;
         }
      }
   }
}
[/code]

So you can create instances that close over test data, capture data, etc, and pass it downstream. It's a poor mans mock.

Which is super useful especially when in go there isn't a good mocking story.

So the question is how do you mock things in go? The answer common is to create test objects that take mock data in their constructors, etc. So you end up writing even <em>more</em> verbose noisy garbage just to test things.  I really dislike this as I don't want to spend a majority of my time maintaining test objects.

<b>Explicit error contracts</b>

While this is in the good section there are also bad things about it.  There are two things I particularly dislike about the explicit error handling in go.

First is that the error handling is repetition and verbose.  Like I mentioned above your code is littered with <code>if err != nil</code> everywhere.  This is a common source of discussion for go2 proposals and I'm curious to see where it goes.

The second is that its impossible to discriminate against different errors. If I have a database that fails to connect I may want to know if it failed because the connection couldn't have been established OR if the credentials are bad. In one scenario I want to retry, and in the other I don't. In go, when you are just given a single <code>error</code> object you can either type cast it (a gross pattern used in some libraries) or check the error text (an even grosser pattern used in some places).

I'd like for there to be a way to distinguish varying error cases or to just say "well there was an error :shrug:" and handle the generic case.

<b>Mutability everywhere</b>

Go prefers mutability.  It is encouraged.  And while this makes sense in a performance aspect, it's unfortunate that there isn't any way to <em>not</em> be immutable. For example, in scala most things are immutable, but you can opt into mutability if you want. I feel like go should have had something similiar, where things are mutable by default but you can opt into immutable.

Things get extra spicy when passing pointers around (which is also encouraged).  While the claim is that concurrency is threadsafe, passing shared data through concurrent channels is <em>not</em> threadsafe.

<b>No covariance</b>

Similar to generics, it's impossible to do covariance with go.  Imagine this scenario:

[code lang=text]
type Item interface{
  //...
}
type ItemA struct { // implements Item }

func TakesArrayOfItem(items []Item)
[/code]

You can't pass an array of ItemA to the method or an array of ItemB to the method. Instead you have to loop through the source array, cast it to the interface, THEN pass it. For example, you can't do this

[code lang=text]
itemA := []ItemA { ... }

TakesArrayOfItem(itemA) // fail

var items []Item 
for _, item := itemA {
   items = append(items, item.(Item)) // cast each item to its interface
}

TakesArrayOfItem(items) // works
[/code]

That's stupid and requires extra processing for no gain.  I understand the reason they don't support this kind of variance at a language design level, but it's still frustrating to have to do these kinds of silly data mutations to work around it.  Maybe if mapping on data wasn't such a pain I wouldn't mind it so much, but having to do the same 4 lines of boilerplate each time I want to do this is a nightmare.

<b>Dependency management</b>

In general the libraries are pretty good and well fleshed out in go.  However, the primary way to use libraries is to get them from github.  Their source is pulled into either your <code>$GOPATH</code> or you vendor all your sources using an external tool called <code>dep</code>. I like vendoring, however, because there's no concept of versioning a library (you just get the HEAD of master at any period of time) it makes it really hard for library writers to make breaking changes and to pivot.  There are tools to create github proxies such that library writers can make major version changes in a branch (which is great) but the fact that the go language maintainers didn't really have a good story to this shows. There is now multiple ways to manage dependencies and none of them that ideal.

With vendoring and dep, there is also an unspoken rule that libraries should not vendor their code.  If you do, things get all sorts of crazy. Trying to vendor a library that has stuff vendored breaks the world.  Given how opinionated go is, it's surprising they let people do the wrong thing here.

I really miss a unified dependency system like maven central honestly.

<h2>Conclusion</h2>

Overall I like go. I don't hate it as much as I thought I would and I'd certainly use it for some projects (lambda's, cli tooling, simple web services).  I like a boring language frankly. And I really like that there is "just one way" to do stuff.  If generics, data transformation, and basic immutability were solved, I'd probably use go for everything.

But I don't think that the language is set up well to scale for larger projects. I find it clunky and overall limiting such that I am often times finding ways to work around the pain points.  Nothing is perfect though, and I'm still writing in go, so I guess thats <code>go 1, anton 0</code>.]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>7777</wp:post_id>
		<wp:post_date><![CDATA[2019-04-01 19:34:55]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-04-01 19:34:55]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[8-months]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="go"><![CDATA[go]]></category>
		<category domain="post_tag" nicename="golang"><![CDATA[golang]]></category>
		<category domain="post_tag" nicename="review"><![CDATA[review]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561943019;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:3615;}i:1;a:1:{s:2:"id";i:2020;}i:2;a:1:{s:2:"id";i:3295;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:comment>
			<wp:comment_id>189</wp:comment_id>
			<wp:comment_author><![CDATA[8 months of go &#8211; Golang News]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>http://golangnews.org/2019/04/8-months-of-go/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[148.72.2.50]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-04-02 01:38:49]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-04-02 01:38:49]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] by  /u/monumentshorts  [link] [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>190</wp:comment_id>
			<wp:comment_author><![CDATA[Variance in Java | Hey Android - Android World]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[]]></wp:comment_author_email>
			<wp:comment_author_url>https://www.hello-android.com/2019/04/13/variance-in-java-hey-android/</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[23.239.65.122]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-04-13 13:54:12]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-04-13 13:54:12]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[[&#8230;] opposite day I got here throughout this put up describing what the creator sees as execs and cons of Go after Eight months of expertise. I [&#8230;]]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[pingback]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>191</wp:comment_id>
			<wp:comment_author><![CDATA[em vee]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[mvswdev@comcast.net]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[24.130.127.130]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-04-16 17:37:17]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-04-16 17:37:17]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[actually, vendoring is absolutely the best and only sane way to manage dependencies, an area which almost everybody gets wrong almost all the time

maybe if you're doing some kind of silly 'social' website where nothing of consequence is involved, you can go on merrily breaking things

but for nontrivial applications, enterprise and mission critical applications, where reliability and stability matter, the loopy treadmill of package managers and cascading third party dependencies is an ongoing trainwreck]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>192</wp:comment_id>
			<wp:comment_author><![CDATA[Mirko Friedenhagen]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[mfriedenhagen@gmx.de]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[89.245.244.170]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-04-17 19:46:21]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-04-17 19:46:21]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Thanks for the article. It was as if you had read my mind. All good and bad points I encountered during the last year were mentioned (well, I just did small stuff so all-in-all 1 month of fiddling to create some CLI tools).

I too used an injectible HttpGetter Interface to mock remote calls during testing and having no generics made me almost crazy ☺️]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>193</wp:comment_id>
			<wp:comment_author><![CDATA[Lou]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[lou@curalate.com]]></wp:comment_author_email>
			<wp:comment_author_url>http://lou.dev</wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[38.122.23.194]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-04-24 13:34:33]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-04-24 13:34:33]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Was thinking a lot of the same things.

Miss you.]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>0</wp:comment_parent>
			<wp:comment_user_id>0</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					<wp:comment>
			<wp:comment_id>194</wp:comment_id>
			<wp:comment_author><![CDATA[Anton Kropp]]></wp:comment_author>
			<wp:comment_author_email><![CDATA[akropp@gmail.com]]></wp:comment_author_email>
			<wp:comment_author_url></wp:comment_author_url>
			<wp:comment_author_IP><![CDATA[139.45.161.8]]></wp:comment_author_IP>
			<wp:comment_date><![CDATA[2019-04-26 21:39:22]]></wp:comment_date>
			<wp:comment_date_gmt><![CDATA[2019-04-26 21:39:22]]></wp:comment_date_gmt>
			<wp:comment_content><![CDATA[Miss you too Lou!  Hope all is well]]></wp:comment_content>
			<wp:comment_approved><![CDATA[1]]></wp:comment_approved>
			<wp:comment_type><![CDATA[]]></wp:comment_type>
			<wp:comment_parent>193</wp:comment_parent>
			<wp:comment_user_id>1</wp:comment_user_id>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_result]]></wp:meta_key>
			<wp:meta_value><![CDATA[false]]></wp:meta_value>
			</wp:commentmeta>
						<wp:commentmeta>
	<wp:meta_key><![CDATA[akismet_history]]></wp:meta_key>
			<wp:meta_value><![CDATA[]]></wp:meta_value>
			</wp:commentmeta>
							</wp:comment>
					</item>
					<item>
		<title>Infra graphs with neo4j</title>
		<link>https://onoffswitch.net/2019/04/26/infra-graphs-neo4j/</link>
		<pubDate>Fri, 26 Apr 2019 21:51:12 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=8009</guid>
		<description></description>
		<content:encoded><![CDATA[I spent some time recently mucking around with neo4j attempting to model infrastructure, incidents, teams, users, etc. Basically what does it take to answer questions about organizations.

Getting neo4j set up with go was non trivial and thankfully someone had documented how to do it already (instructions in the readme: <a href="https://github.com/devshorts/graphql">https://github.com/devshorts/graphql</a>).  In the sample API I exposed we can

<ul>
<li>Find related incidents.  The pathway here is incidentA is failing because of infraA.  incidentB is failing because of infraB. InfraB depends on some pathway that ends up infraA. This means that from InfraB -> InfraA there is a relationship, and so that implies that IncidentA and IncidentB are related. </li>
<li>Find betweeness of the graph. This shows graph nodes that have heavy flow (high connections) and can be potential hot spots</li>
<li>Find communities in the graph. This shows clusterability of infrastructure/teams/etc.</li>
</ul>

The API exposed in the github is meant to model dynamically creating incidents and adding semantic links. So for example, you can post an incident to the <code>/incidents</code> api and then add links to the incident (users/failing infra/etc) via the <code>/links</code> api. As you add links you can query for related incidents and then find pathways from your incident to another.

Pretty neat!

Included in the project is a way to build a sample graph:

<img src="http://onoffswitch.net/wp-content/uploads/2019/04/graph-6.png" alt="" width="1001" height="725" class="aligncenter size-full wp-image-8012" />]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8009</wp:post_id>
		<wp:post_date><![CDATA[2019-04-26 21:51:12]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-04-26 21:51:12]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[open]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[infra-graphs-neo4j]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_wpcom_is_markdown]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_jetpack_related_posts_cache]]></wp:meta_key>
		<wp:meta_value><![CDATA[a:1:{s:32:"8f6677c9d6b0f903e98ad32ec61f8deb";a:2:{s:7:"expires";i:1561804019;s:7:"payload";a:3:{i:0;a:1:{s:2:"id";i:4899;}i:1;a:1:{s:2:"id";i:4673;}i:2;a:1:{s:2:"id";i:4167;}}}}]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_wpas_done_all]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Thoughts on monorepos</title>
		<link>https://onoffswitch.net/2019/07/29/thoughts-on-monorepos/</link>
		<pubDate>Mon, 29 Jul 2019 18:05:54 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=8016</guid>
		<description></description>
		<content:encoded><![CDATA[<!-- wp:paragraph -->
<p>For the past year I've been working at an organization that structures all their code in monorepos.  A monorepo is basically a single version controlled repository that contains all of the organizations sourcecode.  This is in contrast to many smaller repositories that contain either one (or a handful) of services/libraries etc.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>In nearly every place I've worked at monorepos are the defacto beginning of an organization.  Companies start as single repo's and while a single repo doesn't mean a single service, it means that there is only one (usually) git repo to manage the entire companies engineering property.  Each place I've been at this monorepo has grown organically, often times becoming a tangled mess of dependencies due to poor patterns and lack of contracts and encapsulation. After all, it takes significant effort to organize and structure a large codebase. It's almost an art. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>This tangled mess tends to lower productivity, lower morale, impede innovation and growth, and in general hamper scaling.  It's been a popular choice to pull a Chernobyl sarcophagus move on the original repo. Expose endpoints with services and move to a micro-repo pattern where teams can own their own repo's independently and communicate with primary services via exposed APIs.  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>On the one hand this has a lot of advantages.  By ditching the legacy cruft of the old monorepo teams can pivot with new technologies, patterns, deployment mechanisms, etc. Each team becomes a startup in and of itself.  Oftentimes there's a team or set of teams that manage shared libraries and infrastructure to empower this, so it's not like each time needs to rebuild the world from scratch (unless they want to).  In a world of containerization, deployments can still be uniform. Especially when using orchestrators like ECS or K8.  Even delegating builds into containers via Jenkins can allow infrastructure teams to abstract the runtime platform of any different repo.   Teams can deploy at independent cadences because their repositories are independent. No other team would accidentally deploy their code.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>But there are issues. Upgrading libraries across N repos is complicated, and you often times get into interdependency conflicts depending on who is consuming what library at which version.  The company ecosystem can be fragmented with different languages and different patterns at varying degrees of sophistication.  Did every team remember to append trace logging to each request? Are all services using an up to date serialization library (remember equifax)?  Moving between teams becomes harder as each team may use a different set of languages and tools.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Given these problems a lot of companies have chosen to stick with the monorepo.  Big famous companies use the monorepo and claim to have a lot of success with it. Google, Facebook, etc.  The motivations are that when all the code is co-located it makes navigating dependencies and call graphs easier. You can do large migrations and theoretically migrate everyone to new patterns.  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>But again, nothing is perfect.  You still often times have the ball of spaghetti that companies started with. Except now there's no way to draw a line in the sand to either fix or move portions out. Many developers working in the same codebase all the time means you need to solve problems like constant conflicts, deployment ordering, accidental changes unrelated to tea commits, version control limitations, dealing with multiple languages, and having to handle different parts of the codebase using different dependencies.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Some tools exist to help the monorepo. Bazel is a good example, developed by Google.  Its hyper-explicit declarations allow you to manage cross language builds and inter dependencies.  But it has an enormous learning curve, and most people find it bulky to use.  The big companies tend to have teams dedicated to managing the monorepo: solving what happens when you reach git's limitations (which happened to Google), or how to speed up builds (Facebook doing distributed builds).  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Even with tooling provided by the big players it's incredibly challenging to know what changed in a monorepo at any given point. What artifacts need to be packaged up? Do you build everything and run all the tests for all code? That might be incredibly costly for what could very well be a simple change in one area.  What is the cost in developer time in navigating the repo? Building it local? Is that even possible?  Does a repo of that size work with standard IDE's?  Is it realistic to claim you can do repo wide refactoring when hundreds of developers are working in it at the same time? </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>There are a lot of workflow questions that need to be solved. </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Personally I shy away from monorepos. To me the complexity is just too great to gracefully scale with an organization. I think a lot of the benefits of monorepos can be simulated with things like git submodules (where logical groupings are independently versioned, but you can visualize and edit parts of the org together).  Building a company with the ideal of individual repos means that people can experiment and pivot. Monorepos tend to encourage repeating the same patterns, <em>whether those patterns are good or not</em>.  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>As companies scale you need to have teams dedicated to supporting other teams. I would rather teams specialize by language (and a company have a list of supported languages it allows) to build critical libraries and workflows.  Enforcing upgrades of dependencies can be done by plugging into build tools and systems that contain minimum allowances of versions.  Just as an example, <a href="https://github.com/Verizon/sbt-blockade">SBT can do</a> exactly this.  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>There's no magic bullet, in either model companies need to be prepared to invest a lot of time and energy in maintaining and growing their systems. But starting with a logical monorepo (and making sure to be very strict about abstractions and cross cutting contracts!) is a good starting place.   Making sure infrastructure and tooling works for non-monorepos is critical for empowering growth as well though.  Never make assumptions that the one repo will be the only repo in the future, because even in monorepo companies there are always exceptions the rule.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p></p>
<!-- /wp:paragraph -->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8016</wp:post_id>
		<wp:post_date><![CDATA[2019-07-29 18:05:54]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-07-29 18:05:54]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[thoughts-on-monorepos]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="post_tag" nicename="architecture"><![CDATA[architecture]]></category>
		<category domain="category" nicename="discussion"><![CDATA[Discussion]]></category>
		<category domain="post_tag" nicename="monorepo"><![CDATA[monorepo]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Tools Matter</title>
		<link>https://onoffswitch.net/2019/07/29/tools-matter/</link>
		<pubDate>Mon, 29 Jul 2019 22:11:40 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=8018</guid>
		<description></description>
		<content:encoded><![CDATA[<!-- wp:paragraph -->
<p>I feel like I've written about this before, but it continues to come up in my career.  Tools matter.  Without them you can't be an effective engineer.  When I bring this up sometimes some intrepid person will give me the "<em>it's a poor craftsman</em>" schpiel, but have you ever met a craftsperson who doesn't care deeply about their tools? Ask any woodworker and they will tell you the countless hours fine tuning, honing, sharpening, and improving their tools. Without them, the job will take longer, be done poorer, and cost more.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>In my mind the biggest most important tool of all for an engineer is their IDE.  Lots of people have opinions on what flavor editor/IDE they want but given it's nearly 2020, I think there are a few critical tooling features that <strong>must work</strong> otherwise they are a non starter.</p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->
<ol><li><strong>Go to definition</strong>. It seems crazy to me that I have to spell this out, but code is data.  Any editor should be able to <em>semantically</em> tell you the source of a method/class/function/variable.  Grep is not that.  Languages are context sensitive, and thats why you need a context sensitive system to help navigate it<br></li><li><strong>Find references</strong>.  Same as above, just like you can find the definition, you need to find who is using it.  Again its context sensitive, I don't want to someone to convince me that vim ctags or grep are good enough. They. Are. Not.<br></li><li><strong>Semantic Autocomplete</strong>.  CTAGS are not semantic autocomplete. CTAGS are the type of completion where an editor indexes the current file(s) and autocompletes based on what's in the file.  I see many developers using this and it pains me to no end.  As an engineer my time isn't worth reading scaladoc or ruby docs to find what arguments are to <code>map</code>. I need to hit dot and have the IDE tell me. And if I'm wrong, the IDE should squiggle it and tell me I'm wrong so I can fix it fast and move on.  I'm not getting paid to be a compiler. That's a compilers job.<br></li><li><strong>Semantic Rename and Refactor</strong>. Again, semantics are key. Sed is not semantic.  You need to be able to do repo wide renames, refactors, moves/etc, with confidence that the AST is still properly intact and semantically rigorous.  <br></li><li><strong>Integrated debugging and test running</strong>.  Context switching to the CLI to run tests, and in particular a single test, is insane.  While many would argue that test runners and debuggers can be done out of band, and they're not <em>wrong</em>, but they are missing the fact that if you are doing this task hundreds of times a day (easily) then each moment spent typing or context switching adds up.  In an incident any of those things that slow you down matter and have a monetary impact.</li></ol>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>What do I mean by "semantic"?  Using common CLI tools like sed, grep, is it possible to disambiguate a search for "Foo" in the context of A vs the context of B?:</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code -->
<pre class="wp-block-syntaxhighlighter-code">class A {
   def Foo() String
}

class B {
   def Foo() String
}</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>Could you refactor this:</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code -->
<pre class="wp-block-syntaxhighlighter-code">class C(a A, b B){
   def Foo() String {
         a.Foo()
         b.Foo()
   }
}</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>To be</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code -->
<pre class="wp-block-syntaxhighlighter-code">class C(a A, b B){
   def Foo() String {
         a.Bar()
         b.Foo()
   }
}</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>You can't.  And yet these situations arise constantly in day to day engineering.  Good code is refactored code, and codebases that don't get constantly improved upon rot and fall apart.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>I feel like these requirements are so basic and primitive that I'm constantly shocked when I encounter organizations that run without them! </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>My thoughts are that anytime systems are being built we need to make sure that the tools continue to work with them. Tooling is important not because I'm too stupid to be a l337 hacker on the CLI, but because I'm smart enough to know that I don't need to waste my time on that.</p>
<!-- /wp:paragraph -->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8018</wp:post_id>
		<wp:post_date><![CDATA[2019-07-29 22:11:40]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-07-29 22:11:40]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[tools-matter]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="uncategorized"><![CDATA[Uncategorized]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>Typed react native router</title>
		<link>https://onoffswitch.net/2019/08/05/typed-react-native-router/</link>
		<pubDate>Mon, 05 Aug 2019 21:20:36 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=8020</guid>
		<description></description>
		<content:encoded><![CDATA[<!-- wp:quote -->
<blockquote class="wp-block-quote"><p>Note, a seasoned UI friend of mine told me this is a terrible way to do things and to instead use hooks instead</p></blockquote>
<!-- /wp:quote -->

<!-- wp:paragraph -->
<p>I decided to give react native a go the last few days, just to try something different. For what it's worth I haven't done any real front-end work in quite a bit. The last stuff I did was angular 2/(8/a million?) and it was internal dev tooling stuff that was kind of hacky.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Seems like the javascript world has come quite a distance since back when typescript was still in beta!  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>That said, I dove into a sample react native app that I scaffolded out using <a href="https://expo.io/">expo</a>.  Then I popped openn IntelliJ and started to play with react-native-paper and react-navigation.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>The first thing I wanted to see how to do properly was to do DI, since in my mind without clearly having DI you can't build large scale applications. There's a lot of ways to do DI but you need to make sure to not couple your code to its dependencies.  Seems like React's answer to that is props, contexts, and redux. I haven't looked much at redux yet, but my initial reaction to props and context was lackluster.  Props smells like a giant property god object to pass around and that never ends well.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>However, typescript does make this simpler and I do kind of like the higher order container stuff.  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>After making a little hello world app and playing with the react router, I wanted to see if we could type the router.  For example, in all the examples I can see you use</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code {"language":"jscript"} -->
<pre class="wp-block-syntaxhighlighter-code"> this.props.navigation.navigate("Home")</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>To navigate around. This screams bad design to me because I can tell already people will start sprinkling in static strings everywhere, running into typos and other hard to track down problems. What I would rather have is</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code {"language":"jscript"} -->
<pre class="wp-block-syntaxhighlighter-code"> this.navigation.home()</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>Which is typed, discoverable, and easy to re-use.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>However, it took me quite a bit to figure out how to inject </p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->
<ol><li>Capture an instance of props</li><li>Wrap the navigation object into an object we can type</li><li>Pass that object to a component</li></ol>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>Turns out higher order containers were needed and the magic sauce is here. First I can make my routable class that given an instance of the screen router can do the things it wants to do. </p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code {"language":"jscript"} -->
<pre class="wp-block-syntaxhighlighter-code">type RoutableProps = {
    navigation: NavigationScreenProp&lt;any, any>
}

class Routable {
    private navigation: NavigationScreenProp&lt;any, any>;

    constructor(navigation: NavigationScreenProp&lt;any, any>) {
        this.navigation = navigation;
    }

    home() {
        console.log("home")
        this.navigation.navigate("Home")
    }

    back() {
        console.log("back")
        this.navigation.goBack()
    }

    profile() {
        console.log("profile")
        this.navigation.navigate("Profile")
    }
}</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>Assuming we have this (somehow) we can use it in our nav bar</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code {"language":"jscript"} -->
<pre class="wp-block-syntaxhighlighter-code">interface NavProps {
    router: Routable
}

export class NavBar extends React.Component&lt;NavProps> {
    render() {
        const navigate = this.props.router;

        return (
            &lt;Appbar>
                &lt;Appbar.BackAction onPress={() => navigate.back()}/>
                &lt;Appbar.Action icon="archive" onPress={() => navigate.home()}/>
                &lt;Appbar.Action icon="mail" onPress={() => navigate.profile()}/>
            &lt;/Appbar>
        );
    }
}</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>Now here we can wrap a higher order container to capture the props, inject the setting we want, and return a new decorated component</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code {"language":"jscript"} -->
<pre class="wp-block-syntaxhighlighter-code">function withRouter(Component) {
    // inject the navigation route to the props of the component
    return withNavigation(
        class extends React.Component&lt;RoutableProps> {
            render() {
                const nav = this.props.navigation;
                // give the component a router
                return (
                    &lt;Component router={new Routable(nav)}/>
                );
            }

        }
    );
}

export const Nav = withRouter(NavBar);</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>Now we can safely use our typed navigation component </p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code {"language":"jscript"} -->
<pre class="wp-block-syntaxhighlighter-code">export default function Home() {
    return (
        &lt;PaperProvider theme={theme}>
            &lt;SafeAreaView style={styles.bottom}>
                &lt;Nav/>
                &lt;Title>Hello&lt;/Title>
            &lt;/SafeAreaView>
        &lt;/PaperProvider>
    );
}</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>And whamo! Typed navigation.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Full source available at my github <a href="https://github.com/devshorts/react-native-samples">https://github.com/devshorts/react-native-samples</a></p>
<!-- /wp:paragraph -->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8020</wp:post_id>
		<wp:post_date><![CDATA[2019-08-05 21:20:36]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-08-05 21:20:36]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[typed-react-native-router]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="dependency-injection"><![CDATA[dependency injection]]></category>
		<category domain="post_tag" nicename="frontend"><![CDATA[frontend]]></category>
		<category domain="post_tag" nicename="react"><![CDATA[react]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							</item>
					<item>
		<title>JIRA CLI Tooling</title>
		<link>https://onoffswitch.net/2019/08/12/jira-cli-tooling/</link>
		<pubDate>Mon, 12 Aug 2019 19:33:10 +0000</pubDate>
		<dc:creator><![CDATA[akropp]]></dc:creator>
		<guid isPermaLink="false">http://onoffswitch.net/?p=8027</guid>
		<description></description>
		<content:encoded><![CDATA[<!-- wp:paragraph -->
<p>I love tooling.  I am too lazy to do things manually and whenever a process has impedance I have two choices, either don't do it, or make it better.  I usually try and choose the latter.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>JIRA is the beast everyone loves to hate. Rightfully so frankly, it's slow, it's tedious, it's repetitive.  But issue and project trackers like JIRA have <em>value</em>.  As you become a more seasoned professional you realize that being able to see what you are working, what <em>others</em> are working on, and the state of the team in general is huge.  It means you can plan projects in advance, share status with stakeholders, allow people to see what you're doing and keep you from forgetting the things that are in play.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>While other trackers exist (fogbugz, trello, etc) JIRA is by far the most widely used.  For the longest time I hated using JIRA, because it didn't map to my developer workflow.  But lately I've been <a href="https://github.com/devshorts/jira-cli-tooling">using a collection of ruby scripts that wrap the go-jira CLI </a> that makes my life a whole lot easier.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>For example, making a new ticket to track a random idea should be easy:</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":8028,"width":608,"height":386} -->
<figure class="wp-block-image is-resized"><img src="https://onoffswitch.net/wp-content/uploads/2019/08/jira_new.gif" alt="" class="wp-image-8028" width="608" height="386"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>This fires off a ticket into my teams backlog and gives me a link. Now whenever I think to myself "it'd be great to do xyz" I can just pop open the shell and fire that idea off.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>But we can take this so much further!  I really like tying my commits to JIRA so I can see in the JIRA opened PR's, merged PR's, commits etc.  To do that JIRA needs to integrate with github and know about your commits via (usually) a regex that looks for the jira ticket number in branch, commit, or PR titles.  </p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>Since I'm too lazy to ever remember doing that, I wanted to have my branches auto named with the current JIRA I'm on. If we have the ticket in the branch name, we can use git commit hooks to auto format commit messages that are prefixed with the JIRA. </p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code -->
<pre class="wp-block-syntaxhighlighter-code"># file: .git/hooks/prepare-commit-message-std

#!/bin/sh

COMMIT_MSG_FILE=$1
COMMIT_SOURCE=$2
SHA1=$3

function detect_branch_name() {
  # of the format username-JIRA-3586/description
  # get the branch, remove the username, 
  #    split by / and take first segment, 
  #    split by - and take second 2 segments
  git branch | grep \* | cut -d ' ' -f2 | sed 's/$USER-//' | cut -d '/' -f1 | cut -d '-' -f2,3
}

branch_name=`detect_branch_name`

if [[ "$branch_name" =~ "TRAFFIC-" ]]; then
  data=`cat $COMMIT_MSG_FILE`

  echo "$branch_name - $data" > $COMMIT_MSG_FILE
fi</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>Now if I make a commit like:</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code -->
<pre class="wp-block-syntaxhighlighter-code">~/src (akropp-JIRA-123/title)
$ git commit -am "foo"</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>My git log would look something like</p>
<!-- /wp:paragraph -->

<!-- wp:syntaxhighlighter/code -->
<pre class="wp-block-syntaxhighlighter-code">JIRA-123 - foo</pre>
<!-- /wp:syntaxhighlighter/code -->

<!-- wp:paragraph -->
<p>What if we could automatically create a JIRA and set it to in progress and add it to my sprint when I make a branch? </p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":8029} -->
<figure class="wp-block-image"><img src="https://onoffswitch.net/wp-content/uploads/2019/08/new_branch.gif" alt="" class="wp-image-8029"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>Well we can! For example, we can make a new branch that </p>
<!-- /wp:paragraph -->

<!-- wp:list {"ordered":true} -->
<ol><li>Creates a JIRA </li><li>Adds it to my sprint</li><li>Marks it in progress</li><li>Git checkouts with a standardized branch name scheme that works with my commit hooks</li></ol>
<!-- /wp:list -->

<!-- wp:paragraph -->
<p>All in a single one liner!  On top of that making a new branch can pull down the open JIRA's I have and prompt me to start working on it if some exist!  This way anytime I start a branch it's automatically tracked. All the random one-offs I do at work now get visibility and I can see at the end of the sprint how much work <em>I really did</em> vs how much work we planned.</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>On top of that,  imagine I have open JIRAs and my branch names  have the JIRA name in them. I can then switch between working tickets easily by resuming work on a ticket</p>
<!-- /wp:paragraph -->

<!-- wp:image {"id":8030} -->
<figure class="wp-block-image"><img src="https://onoffswitch.net/wp-content/uploads/2019/08/resume.gif" alt="" class="wp-image-8030"/></figure>
<!-- /wp:image -->

<!-- wp:paragraph -->
<p>My code even works when I have multiple branches for the same JIRA, so if I have multiple logical flows for the same JIRA I can resume different subsections easily (great for large migrations!)</p>
<!-- /wp:paragraph -->

<!-- wp:paragraph -->
<p>As always source code on my <a href="https://github.com/devshorts/jira-cli-tooling">github</a></p>
<!-- /wp:paragraph -->]]></content:encoded>
		<excerpt:encoded><![CDATA[]]></excerpt:encoded>
		<wp:post_id>8027</wp:post_id>
		<wp:post_date><![CDATA[2019-08-12 19:33:10]]></wp:post_date>
		<wp:post_date_gmt><![CDATA[2019-08-12 19:33:10]]></wp:post_date_gmt>
		<wp:comment_status><![CDATA[closed]]></wp:comment_status>
		<wp:ping_status><![CDATA[open]]></wp:ping_status>
		<wp:post_name><![CDATA[jira-cli-tooling]]></wp:post_name>
		<wp:status><![CDATA[publish]]></wp:status>
		<wp:post_parent>0</wp:post_parent>
		<wp:menu_order>0</wp:menu_order>
		<wp:post_type><![CDATA[post]]></wp:post_type>
		<wp:post_password><![CDATA[]]></wp:post_password>
		<wp:is_sticky>0</wp:is_sticky>
										<category domain="category" nicename="code"><![CDATA[Code]]></category>
		<category domain="post_tag" nicename="jira"><![CDATA[jira]]></category>
		<category domain="post_tag" nicename="productivity"><![CDATA[productivity]]></category>
		<category domain="post_tag" nicename="tooling"><![CDATA[tooling]]></category>
						<wp:postmeta>
		<wp:meta_key><![CDATA[_edit_last]]></wp:meta_key>
		<wp:meta_value><![CDATA[1]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_title]]></wp:meta_key>
		<wp:meta_value><![CDATA[jira productivity cli]]></wp:meta_value>
		</wp:postmeta>
							<wp:postmeta>
		<wp:meta_key><![CDATA[_su_rich_snippet_type]]></wp:meta_key>
		<wp:meta_value><![CDATA[none]]></wp:meta_value>
		</wp:postmeta>
							</item>
				</channel>
</rss>
	